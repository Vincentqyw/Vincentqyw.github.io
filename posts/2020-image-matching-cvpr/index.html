<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/realcat-apple-touch-icon.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/realcat-32x32.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/realcat-32x32.png?v=7.4.0">
  <link rel="mask-icon" href="/images/realcat-safari-pinned-tab.svg?v=7.4.0" color="#222">
  <link rel="alternate" href="/atom.xml" title="RealCat" type="application/atom+xml">
  <link rel="alternate" href="https://vincentqin.gitee.io/" title="RealCat" type="application/atom+xml">
  <meta name="google-site-verification" content="u46QTaG_Dv3OZLpOBKYtqyuiNtIdnhSG5ASKoNvGBCM">
  <meta name="baidu-site-verification" content="MtcbwE45ft">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"hide","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":false,"style":"flat"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: true,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":10,"unescape":true,"preload":true},
    path: 'search.xml',
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="从一系列的图像中恢复物体的3D结构是计算机视觉研究中一个热门课题，这使得我们可以相隔万里从google map中看到复活节岛的风景。这得益于图像来自于可控的条件，使得最终的重建效果一致性且质量都很高，但是这却限制了采集设备以及视角。畅想一下，假如我们不使用专业设备，而是利用sfm技术根据互联网上大量的图片重建出这个复杂世界。    为了加快这个领域的研究，更好地利用图像数据有效信息，谷歌联合 UV">
<meta name="keywords" content="SLAM,ORB,特征匹配,SuperPoint,SIFT">
<meta property="og:type" content="article">
<meta property="og:title" content="CVPR2020图像匹配挑战赛，新数据集+新评测方法，SOTA正瑟瑟发抖！">
<meta property="og:url" content="https://www.vincentqin.tech/posts/2020-image-matching-cvpr/index.html">
<meta property="og:site_name" content="RealCat">
<meta property="og:description" content="从一系列的图像中恢复物体的3D结构是计算机视觉研究中一个热门课题，这使得我们可以相隔万里从google map中看到复活节岛的风景。这得益于图像来自于可控的条件，使得最终的重建效果一致性且质量都很高，但是这却限制了采集设备以及视角。畅想一下，假如我们不使用专业设备，而是利用sfm技术根据互联网上大量的图片重建出这个复杂世界。    为了加快这个领域的研究，更好地利用图像数据有效信息，谷歌联合 UV">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2020-05-24T15:00:05.764Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CVPR2020图像匹配挑战赛，新数据集+新评测方法，SOTA正瑟瑟发抖！">
<meta name="twitter:description" content="从一系列的图像中恢复物体的3D结构是计算机视觉研究中一个热门课题，这使得我们可以相隔万里从google map中看到复活节岛的风景。这得益于图像来自于可控的条件，使得最终的重建效果一致性且质量都很高，但是这却限制了采集设备以及视角。畅想一下，假如我们不使用专业设备，而是利用sfm技术根据互联网上大量的图片重建出这个复杂世界。    为了加快这个领域的研究，更好地利用图像数据有效信息，谷歌联合 UV">
  <link rel="canonical" href="https://www.vincentqin.tech/posts/2020-image-matching-cvpr/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>CVPR2020图像匹配挑战赛，新数据集+新评测方法，SOTA正瑟瑟发抖！ | RealCat</title>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-97856334-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-97856334-1');
    }
  </script>








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">RealCat</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Turn on, Tune in, Drop out</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-collections">
      
    

    <a href="/collections" rel="section"><i class="menu-item-icon fa fa-fw fa-diamond"></i> <br>Collections</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-guest_comments">
      
    

    <a href="/guestbook" rel="section"><i class="menu-item-icon fa fa-fw fa-send"></i> <br>Messager</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-schedule">
      
    

    <a href="/schedule/" rel="section"><i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>Schedule</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-top">
      
    

    <a href="/top/" rel="section"><i class="menu-item-icon fa fa-fw fa-signal"></i> <br>Top</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="menu-item-icon fa fa-search fa-fw"></i> <br>Search</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://www.vincentqin.tech/posts/2020-image-matching-cvpr/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Vincent Qin">
      <meta itemprop="description" content="Keep Your Curiosity">
      <meta itemprop="image" content="https://vincentqin.gitee.io/images/qin_small.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RealCat">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">CVPR2020图像匹配挑战赛，新数据集+新评测方法，SOTA正瑟瑟发抖！

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-05-17 12:23:09" itemprop="dateCreated datePublished" datetime="2020-05-17T12:23:09+08:00">2020-05-17</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-24 23:00:05" itemprop="dateModified" datetime="2020-05-24T23:00:05+08:00">2020-05-24</time>
              </span>
            
          

          
            <span id="/posts/2020-image-matching-cvpr/" class="post-meta-item leancloud_visitors" data-flag-title="CVPR2020图像匹配挑战赛，新数据集+新评测方法，SOTA正瑟瑟发抖！" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Comments: </span>
    
    <a title="valine" href="/posts/2020-image-matching-cvpr/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/2020-image-matching-cvpr/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>从一系列的图像中恢复物体的3D结构是计算机视觉研究中一个热门课题，这使得我们可以相隔万里从google map中看到复活节岛的风景。这得益于图像来自于可控的条件，使得最终的重建效果一致性且质量都很高，但是这却限制了采集设备以及视角。畅想一下，假如我们不使用专业设备，而是利用sfm技术根据互联网上大量的图片重建出这个复杂世界。</p>
<!-- ![A 3D reconstruction generated from over 3000 images, including those from the previous figure](https://1.bp.blogspot.com/-loSqCB3NnM0/XoTiOGP9SYI/AAAAAAAAFlE/rs8iCTq63FYapA7HbljF8iWa7fyHvh3UgCLcBGAsYHQ/s400/image3.gif) -->
<!-- ![](https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/image_sfm.gif) -->
<p><img alt data-src="https://vincentqin.gitee.io/posts/2020-image-matching-cvpr/image_sfm.gif"></p>
<p>为了加快这个领域的研究，更好地利用图像数据有效信息，谷歌联合 <a href="https://www.uvic.ca/" target="_blank" rel="noopener">UVIC</a>, <a href="https://www.cvut.cz/en" target="_blank" rel="noopener">CTU</a>以及EPFL发表了这篇文章 “<a href="https://arxiv.org/abs/2003.01587" target="_blank" rel="noopener">Image Matching across Wide Baselines: From Paper to Practice</a>”，[<strong><a href="http://xxx.itp.ac.cn/pdf/2003.01587v2" target="_blank" rel="noopener">PDF</a></strong>]，旨在公布一种新的衡量用于3D重建方法的标准模块+数据集，这里主要是指2D图像间的匹配。这个评价模块可以很方便地集成并评估现有流行的特征匹配算法，包括传统方法或者基于机器学习的方法。</p>
<p>谷歌公布2020图像匹配挑战的数据集：<a href="https://image-matching-workshop.github.io/" target="_blank" rel="noopener">官网</a>，<a href="http://ai.googleblog.com/2020/04/announcing-2020-image-matching.html" target="_blank" rel="noopener">博客</a>，文末有排行榜。</p>
<a id="more"></a>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>图像特征匹配是计算机视觉的基础+核心问题之一，包括image retrieval <sup><a href="#fn_48" id="reffn_48">48</a></sup> <sup><a href="#fn_7" id="reffn_7">7</a></sup> <sup><a href="#fn_69" id="reffn_69">69</a></sup> <sup><a href="#fn_91" id="reffn_91">91</a></sup> <sup><a href="#fn_63" id="reffn_63">63</a></sup>, 3D reconstruction<sup><a href="#fn_3" id="reffn_3">3</a></sup> <sup><a href="#fn_43" id="reffn_43">43</a></sup> <sup><a href="#fn_79" id="reffn_79">79</a></sup> <sup><a href="#fn_106" id="reffn_106">106</a></sup>，re-localization <sup><a href="#fn_74" id="reffn_74">74</a></sup> <sup><a href="#fn_75" id="reffn_75">75</a></sup> <sup><a href="#fn_51" id="reffn_51">51</a></sup>以及 SLAM <sup><a href="#fn_61" id="reffn_61">61</a></sup> <sup><a href="#fn_30" id="reffn_30">30</a></sup> <sup><a href="#fn_31" id="reffn_31">31</a></sup>等在内的诸多研究领域都会用到特征匹配。这个问题已经研究了几十年，但仍未被很好地解决。特征匹配面临的问题很多，主要包括以下挑战：视角，尺度，旋转，光照，遮挡以及相机渲染等。</p>
<p>近些年来，研究者开始将视线转移到端到端的学习方法（图像-&gt;位姿），但是这些方法甚至没有达到传统的方法（图像-&gt;匹配-&gt;BA优化）的性能。我们可以看到，传统的方法将3D重建问题拆分成为2个子问题：特征匹配与位姿解算。解决每个子问题的新方法，诸如特征匹配/位姿解算，都使用了“临时指标”，但是单独地评价单个子问题的性能不足以说明整体性能。例如，一些研究仅在某个数据集上展现了相较于手工特征SIFT的优势，但是这些算法是否能够在真实应用中仍然展现出优势呢？我们通过后续实验说明传统算法经过调整之后也可匹敌现有的标称“sota”的算法（着实打脸）。</p>
<p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig1.png"></p>
<p>是时候换一种方式进行评价了，本文不去过多关注在临时指标上的表现，而关注在下游任务上的表现。本文贡献：</p>
<ol>
<li>30k图像+深度图+真实位姿（posed image）</li>
<li>模块化流水线处理流程，结合了数十种经典的和最新的特征提取和匹配以及姿态估计方法，以及多种启发式方法，可以分别交换和调整</li>
<li>两个下游任务，双目/多视角重建</li>
<li>全面研究了手工特征以及学习特征数十种方法和技术，以及它们的结合以及超参数选择的过程</li>
</ol>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="局部特征"><a href="#局部特征" class="headerlink" title="局部特征"></a>局部特征</h3><p>在引入SIFT特征之后，局部特征变成了主流。它的处理流程主要分为几个步骤：特征提取，旋转估计，描述子提取。除了SIFT，手工特征还有SURF <sup><a href="#fn_15" id="reffn_15">15</a></sup>, ORB <sup><a href="#fn_73" id="reffn_73">73</a></sup>, 以及 AKAZE <sup><a href="#fn_4" id="reffn_4">4</a></sup>等。</p>
<p>现代描述子通常在SIFT关键点（即DoG）的预裁剪图像块上训练深度网络，其中包括：Deepdesc <sup><a href="#fn_82" id="reffn_82">82</a></sup>, TFeat <sup><a href="#fn_11" id="reffn_11">11</a></sup>, L2-Net <sup><a href="#fn_89" id="reffn_89">89</a></sup>, Hardnet <sup><a href="#fn_57" id="reffn_57">57</a></sup>, SOSNet [90]以及 LogPolarDesc <sup><a href="#fn_34" id="reffn_34">34</a></sup>（它们中绝大多数都是在同一个数据集上进行的训练）。</p>
<p>最近有一些工作利用了其它线索，诸如几何或全局上下文信息进行训练，其中包括GeoDesc [50] and ContextDesc <sup><a href="#fn_49" id="reffn_49">49</a></sup>。</p>
<p>另外还有一些方法将特征点以及描述子进行单独训练，例如TILDE <sup><a href="#fn_95" id="reffn_95">95</a></sup>, TCDet <sup><a href="#fn_103" id="reffn_103">103</a></sup>, QuadNet <sup><a href="#fn_78" id="reffn_78">78</a></sup>, and Key.Net <sup><a href="#fn_13" id="reffn_13">13</a></sup>。当前还有一些算法将二者联合起来训练，例如LIFT <sup><a href="#fn_99" id="reffn_99">99</a></sup>,DELF <sup><a href="#fn_63" id="reffn_63">63</a></sup>, SuperPoint <sup><a href="#fn_31" id="reffn_31">31</a></sup>, LF-Net <sup><a href="#fn_64" id="reffn_64">64</a></sup>, D2-Net <sup><a href="#fn_33" id="reffn_33">33</a></sup>,R2D2 <sup><a href="#fn_72" id="reffn_72">72</a></sup>。</p>
<h3 id="鲁棒匹配"><a href="#鲁棒匹配" class="headerlink" title="鲁棒匹配"></a>鲁棒匹配</h3><p>大基线的双目匹配的外点内点率可低至10%，甚至更低。要做匹配的话需要从中选择出能够解算出位姿的算法。常用的方式包括基于随机一致采样RANSAC的5-<sup><a href="#fn_62" id="reffn_62">62</a></sup>，7-<sup><a href="#fn_41" id="reffn_41">41</a></sup>，8-point<sup><a href="#fn_39" id="reffn_39">39</a></sup>算法。它的改进算法包括local optimization <sup><a href="#fn_24" id="reffn_24">24</a></sup>, MLESAC <sup><a href="#fn_92" id="reffn_92">92</a></sup>, PROSAC <sup><a href="#fn_23" id="reffn_23">23</a></sup>, DEGENSAC <sup><a href="#fn_26" id="reffn_26">26</a></sup>, GC-RANSAC <sup><a href="#fn_12" id="reffn_12">12</a></sup>,  MAGSAC <sup><a href="#fn_29" id="reffn_29">29</a></sup>，CNe (Context Networks) <sup><a href="#fn_100" id="reffn_100">100</a></sup>+RANSAC，同样还有<sup><a href="#fn_70" id="reffn_70">70</a></sup> <sup><a href="#fn_104" id="reffn_104">104</a></sup> <sup><a href="#fn_85" id="reffn_85">85</a></sup> <sup><a href="#fn_102" id="reffn_102">102</a></sup>。作者最后加了一句“Despite their promise, it remains unclear how well they perform in real settings”（质疑中，哈哈）。</p>
<h3 id="运动恢复结构（SfM）"><a href="#运动恢复结构（SfM）" class="headerlink" title="运动恢复结构（SfM）"></a>运动恢复结构（SfM）</h3><p>方法 <sup><a href="#fn_3" id="reffn_3">3</a></sup> <sup><a href="#fn_43" id="reffn_43">43</a></sup> <sup><a href="#fn_27" id="reffn_27">27</a></sup> <sup><a href="#fn_37" id="reffn_37">37</a></sup> <sup><a href="#fn_106" id="reffn_106">106</a></sup>，最流行的包括VisualSFM <sup><a href="#fn_98" id="reffn_98">98</a></sup>以及COLMAP <sup><a href="#fn_79" id="reffn_79">79</a></sup>（作为真值）。</p>
<h3 id="数据集和标准"><a href="#数据集和标准" class="headerlink" title="数据集和标准"></a>数据集和标准</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig2.jpg"></p>
<p>以前的特征匹配数据集如下：</p>
<ul>
<li>Oxford dataset <sup><a href="#fn_54" id="reffn_54">54</a></sup>, 48张图像+真值单应矩阵</li>
<li>HPatches <sup><a href="#fn_9" id="reffn_9">9</a></sup>, 696张光照以及视角变化，无遮挡平面图像</li>
<li>DTU <sup><a href="#fn_1" id="reffn_1">1</a></sup>, Edge Foci <sup><a href="#fn_107" id="reffn_107">107</a></sup>, Webcam <sup><a href="#fn_95" id="reffn_95">95</a></sup>, AMOS <sup><a href="#fn_67" id="reffn_67">67</a></sup>, 以及 Strecha’s <sup><a href="#fn_83" id="reffn_83">83</a></sup></li>
</ul>
<p>上述数据集都有其限制：窄基线，真值噪声大，图像数量少。基于学习的描述子通常在<sup><a href="#fn_21" id="reffn_21">21</a></sup>上进行训练，它们之所以比SIFT好的原因可能在于过拟合了（作者看到会不会脸红）。<br>另外，用于导航/重定位以及slam的数据集包括Kitti <sup><a href="#fn_38" id="reffn_38">38</a></sup>, Aachen <sup><a href="#fn_76" id="reffn_76">76</a></sup>, Robotcar <sup><a href="#fn_52" id="reffn_52">52</a></sup>以及CMU seasons <sup><a href="#fn_75" id="reffn_75">75</a></sup> <sup><a href="#fn_8" id="reffn_8">8</a></sup>，但并不包含Phototourism数据中的多种变换。</p>
<h2 id="Phototourism-数据集"><a href="#Phototourism-数据集" class="headerlink" title="Phototourism 数据集"></a>Phototourism 数据集</h2><p>上述数据集这么“烂”，于是作者搞出了他们心目中最好的公开数据集——Phototourism 数据集。作者从<sup><a href="#fn_43" id="reffn_43">43</a></sup> <sup><a href="#fn_88" id="reffn_88">88</a></sup>中选择的25个受欢迎的地标集合（共30k）为基础，每个地标都有成百上千的图像。论文中，作者从中选择出11个场景，其中9个测试集和2个验证集做实验。将它们缩减为最大尺寸为1024像素，并使用COLMAP <sup><a href="#fn_79" id="reffn_79">79</a></sup>对其进行求解位姿以及点云和深度，通过建立好的模型去除遮挡物。</p>
<p>具体地，如下2个表格所示：</p>
<p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab1.png"></p>
<p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab2.png"></p>
<h2 id="处理流程图Pipeline"><a href="#处理流程图Pipeline" class="headerlink" title="处理流程图Pipeline"></a>处理流程图Pipeline</h2><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig7.png"></p>
<p>流程如上图，蓝色框就是要进行的几个处理，分别介绍一下。</p>
<h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>作者选择了3大类特征：</p>
<ol>
<li>完全手工特征:<br>SIFT <sup><a href="#fn_48" id="reffn_48">48</a></sup> (以及RootSIFT <sup><a href="#fn_6" id="reffn_6">6</a></sup>), SURF <sup><a href="#fn_15" id="reffn_15">15</a></sup>, ORB <sup><a href="#fn_73" id="reffn_73">73</a></sup>, AKAZE <sup><a href="#fn_4" id="reffn_4">4</a></sup>，FREAK <sup><a href="#fn_107" id="reffn_107">107</a></sup>描述子+BRISK <sup><a href="#fn_108" id="reffn_108">108</a></sup>特征点，使用OpenCV的实现，除了ORB特征，降低特征提取阈值以多提取一些特征；<br>除此之外，也考虑VLFeat<sup><a href="#fn_94" id="reffn_94">94</a></sup>中DoG的一些变种：(VL-)DoG, Hessian <sup><a href="#fn_16" id="reffn_16">16</a></sup>, Hessian-Laplace <sup><a href="#fn_55" id="reffn_55">55</a></sup>, Harris-Laplace <sup><a href="#fn_55" id="reffn_55">55</a></sup>, MSER <sup><a href="#fn_53" id="reffn_53">53</a></sup>; 以及它们的仿射变种: DoG-Affine, Hessian-Affine <sup><a href="#fn_55" id="reffn_55">55</a></sup> <sup><a href="#fn_14" id="reffn_14">14</a></sup>, DoG-AffNet <sup><a href="#fn_59" id="reffn_59">59</a></sup>, Hessian-AffNet <sup><a href="#fn_59" id="reffn_59">59</a></sup></li>
<li>描述子从DoG特征学习得到的特征：<br>L2-Net <sup><a href="#fn_89" id="reffn_89">89</a></sup>, Hardnet <sup><a href="#fn_57" id="reffn_57">57</a></sup>,Geodesc <sup><a href="#fn_50" id="reffn_50">50</a></sup>, SOSNet <sup><a href="#fn_90" id="reffn_90">90</a></sup>, ContextDesc <sup><a href="#fn_49" id="reffn_49">49</a></sup>, LogPolarDesc <sup><a href="#fn_34" id="reffn_34">34</a></sup></li>
<li>端到端学习来的特征：<br>Superpoint <sup><a href="#fn_31" id="reffn_31">31</a></sup>, LF-Net <sup><a href="#fn_64" id="reffn_64">64</a></sup>, and D2-Net <sup><a href="#fn_33" id="reffn_33">33</a></sup>以及它们的多尺度变种：single- (SS) 以及 multi-scale (MS)</li>
</ol>
<h3 id="特征匹配"><a href="#特征匹配" class="headerlink" title="特征匹配"></a>特征匹配</h3><p>此处用的是最近邻。</p>
<h3 id="外点滤除"><a href="#外点滤除" class="headerlink" title="外点滤除"></a>外点滤除</h3><p>Context Networks <sup><a href="#fn_100" id="reffn_100">100</a></sup>+RANSAC<sup><a href="#fn_100" id="reffn_100">100</a></sup> <sup><a href="#fn_85" id="reffn_85">85</a></sup>，简称CNe，效果如下：</p>
<p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig3.png"></p>
<h3 id="Stereo-task"><a href="#Stereo-task" class="headerlink" title="Stereo task"></a>Stereo task</h3><p>给定图像$\mathbf{I}_i$以及$\mathbf{I}_j$，解算基础矩阵 $\mathbf{F}_{i,j}$，除了现有的OpenCV<sup><a href="#fn_19" id="reffn_19">19</a></sup>以及sklearn<sup><a href="#fn_65" id="reffn_65">65</a></sup>中实现的RANSAC <sup><a href="#fn_36" id="reffn_36">36</a></sup> <sup><a href="#fn_25" id="reffn_25">25</a></sup>，作者也用到了DEGENSAC <sup><a href="#fn_26" id="reffn_26">26</a></sup>, GC-RANSAC <sup><a href="#fn_12" id="reffn_12">12</a></sup> and MAGSAC <sup><a href="#fn_29" id="reffn_29">29</a></sup>。最后通过OpenCV的<code>recoverPose</code>函数解算位姿。</p>
<h3 id="Multi-view-task"><a href="#Multi-view-task" class="headerlink" title="Multi-view task"></a>Multi-view task</h3><p>由于是评价<strong>特征的好坏</strong>而不是SfM算法，作者从几个大场景中<strong>随机选择</strong>出图片构成几个小的数据集，称为”bags”。其中包含3/5张图像的各有100bags，10张图像的各有50bags，25张图像的各有25bags，总共275个bags。将外点滤除后的结果送入COLMAP <sup><a href="#fn_79" id="reffn_79">79</a></sup>作为输入进行SfM重建。</p>
<h3 id="误差指标"><a href="#误差指标" class="headerlink" title="误差指标"></a>误差指标</h3><ol>
<li>mAA(mean Average Accuracy): Stereo task/Multi-view task</li>
<li>ATE(Absolute Trajectory Error): Multi-view task</li>
</ol>
<h2 id="实验开始——配置细节很重要"><a href="#实验开始——配置细节很重要" class="headerlink" title="实验开始——配置细节很重要"></a>实验开始——配置细节很重要</h2><p>首先比较了RANSAC在不同参数配置（置信度，极线对齐误差阈值以及最大迭代次数）下的表现：<br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig8.png"><br>总体来说，MAGSAC表现最好，DEGENSAC表现次之。另外，作者提到“default settings can be woefully inadequate. For example, OpenCV sets τ = 0.99 and η = 3 pixels, which results in a mAP at 10o of 0.5292 on the validation set – a performance drop of 23.9% relative.” 所以在日常使用OpenCV的RANSAC函数时需要自己调整下参数。</p>
<p>作者认为RANSAC的内点阈值对于每种局部特征也是不同的，作者做了如下实验。<br><img alt="Figure 5. RANSAC – Inlier threshold $\eta$" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig9.png"><br>上图可以直观看到从DOG学习的特征都聚集在了一起，其它特征比较分散，这也是太难选择了，于是作者使用了其他论文作者推荐的配置参数或者一些合理的参数作为内点阈值。</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>作者列出了很多结果以及结论，我们仅去关注几个感兴趣的。</p>
<h3 id="8k特征"><a href="#8k特征" class="headerlink" title="8k特征"></a>8k特征</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab5.png"></p>
<p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab6.png"></p>
<p>大家期待已久的真的sota到底是谁呢？作者在以上特征的超参调整到最优后进行了实验，测试结果如下：</p>
<ol>
<li>mAA指标上DoG特征点占据了Top的位置，其中SOSNet排名#1，紧随其后的是HardNet。</li>
<li>‘HardNetAmos+’ <sup><a href="#fn_56" id="reffn_56">56</a></sup>,它在更多的数据(Brown <sup><a href="#fn_20" id="reffn_20">20</a></sup>, HPatches <sup><a href="#fn_9" id="reffn_9">9</a></sup>, AMOS <sup><a href="#fn_67" id="reffn_67">67</a></sup>)上进行了训练，但是效果却比不上在Brown的‘Liberty’上训练模型的效果。</li>
<li>multi-view任务中，DoG+HardNet表现属于top水平，略优于ContextDesc, SOSNet，LogpolarDesc；</li>
<li>R2D2是表现最好的端到端方法，同样在multi-view任务中表现较好（#6），但是在stereo任务中不如SIFT；</li>
<li>D2-net表现并不太好，可能由于图像下采样造成了较差的定位误差；</li>
<li>适当调整参数后的SIFT尤其是RootSIFT能够在stereo任务中排名#9，multi-view任务中排名#9，与所谓sota相差13.1%以及4.9%.（真为咱传统特征争气！）</li>
</ol>
<h3 id="2k特征"><a href="#2k特征" class="headerlink" title="2k特征"></a>2k特征</h3><p>这样做的理由是能够与LF-Net与Superpoint进行比较，结果如下图：</p>
<p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab7.png"></p>
<p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab8.png"></p>
<p>结论：</p>
<ol>
<li>Key.Net+HardNet获得最好的表现，第二名是LogPolarDesc；</li>
<li>R2D2在stereo任务中排名#2，multi-view任务中排名#7</li>
</ol>
<h3 id="8k-vs-2k"><a href="#8k-vs-2k" class="headerlink" title="8k vs. 2k"></a>8k vs. 2k</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig16.png"></p>
<p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig17.png"></p>
<p>结论：</p>
<ol>
<li><strong>基于DoG的方法容易受益于多个特征，而学习的方法收益于重新训练</strong>（该结论来自于Key.Net+Hardnet的组合，作者进行了重新训练，表现优异）</li>
<li>整体来说基于学习的特征KeyNet, SuperPoint, R2D2, LF-Net在multi-view任务配置下比stereo任务配置下表现更好；(作者的假设是它们的鲁棒性好，但定位精度低)</li>
</ol>
<h3 id="光照变化"><a href="#光照变化" class="headerlink" title="光照变化"></a>光照变化</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig26.png"></p>
<p>作者用了直方图均衡化（CLAHE<sup><a href="#fn_66" id="reffn_66">66</a></sup>）去调整图像光度，结果如上图，可以看到几乎所有的基于学习的方法的测试效果都下降了，这可能由于没有专门地在这种场景中进行训练。而SIFT也没有得到明显提升，可能在于SIFT描述子是在某些假设条件下最佳表现。</p>
<h3 id="新指标-vs-传统指标"><a href="#新指标-vs-传统指标" class="headerlink" title="新指标 vs. 传统指标"></a>新指标 vs. 传统指标</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig18.png"></p>
<p>这里要说明的是传统的评价方式与本文提出方式的关系。</p>
<ol>
<li>matching score的选择还是比较明智的，它似乎与mAA相关，但也很难保证高的匹配得分就一定有助于提升mAA，例如RootSIFT vs ContextDesc；</li>
<li>repeatability则比较难去诠释它对最后位姿解算的效果。AKAZE的repeatability最好但是matching score和pose mAA都非常差，作者的原话(arxiv版本1)就是<strong>descriptor may hurt its performance</strong>。</li>
<li>Key.Net获得最好的repeatability，但是在mAA指标上弱于DoG的方法，即使使用了相同的描述子HardNet;</li>
</ol>
<p><strong>注意</strong>，以上结果都是论文发布在arxiv平台时给出的结果，最新结果参考这个官网<a href="https://vision.uvic.ca/image-matching-challenge/leaderboard/" target="_blank" rel="noopener">排行榜</a>。</p>
<p>由于目前正在使用superpoint特征（SuperPoint (2k features, NMS=4), DEGENSAC），所以比较关注它的表现。感觉在2k特征阵营，它的表现并不好（屈居#35,目前共52个算法），然而SuperPoint + SuperGlue + DEGENSAC以及SuperPoint+GIFT+Graph Motion Coherence Network+DEGENSAC分别位列#1以及#2，这也是结果很让人欣慰！</p>
<p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/leadboard_superglue.png"></p>
<p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/leadboard_superpoint.png"></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><blockquote id="fn_1">
<sup>1</sup>. H. Aanaes, A. L. Dahl, and K. Steenstrup Pedersen. Interesting Interest Points. IJCV, 97:18–35, 2012. 2<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. H. Aanaes and F. Kahl. Estimation of Deformable Structure and Motion. In Vision and Modelling of Dynamic Scenes Workshop, 2002. 6<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_3">
<sup>3</sup>. S. Agarwal, N. Snavely, I. Simon, S.M. Seitz, and R. Szeliski. Building Rome in One Day. In ICCV, 2009. 1, 2<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_4">
<sup>4</sup>. P. F. Alcantarilla, J. Nuevo, and A. Bartoli. Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces. In BMVC, 2013. 2, 3<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_5">
<sup>5</sup>. Anonymous. DeepSFM: Structure From Motion Via Deep Bundle Adjustment. In Submission to ICLR, 2020. 2<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_6">
<sup>6</sup>. Relja Arandjelovic. Three things everyone should know to improve object retrieval. In CVPR, 2012. 3<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_7">
<sup>7</sup>. Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. NetVLAD: CNN Architecture for Weakly Supervised Place Recognition. In CVPR, 2016. 1<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_8">
<sup>8</sup>. Hernan Badino, Daniel Huber, and Takeo Kanade. The CMU Visual Localization Data Set. <a href="http://3dvis" target="_blank" rel="noopener">http://3dvis</a>. ri.cmu.edu/data-sets/localization, 2011. 2<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_9">
<sup>9</sup>. V. Balntas, K. Lenc, A. Vedaldi, and K. Mikolajczyk. HPatches: A Benchmark and Evaluation of Handcrafted and Learned Local Descriptors. In CVPR, 2017. 2, 7<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_10">
<sup>10</sup>. Vassileios Balntas, Shuda Li, and Victor Prisacariu. RelocNet: Continuous Metric Learning Relocalisation using Neural Nets. In The European Conference on Computer Vision (ECCV), September 2018. 1<a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_11">
<sup>11</sup>. V. Balntas, E. Riba, D. Ponsa, and K. Mikolajczyk. Learning Local Feature Descriptors with Triplets and Shallow Convolutional Neural Networks. In BMVC, 2016. 2<a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_12">
<sup>12</sup>. Daniel Barath and Ji Matas. Graph-cut ransac. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 2, 4<a href="#reffn_12" title="Jump back to footnote [12] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_13">
<sup>13</sup>. Axel Barroso-Laguna, Edgar Riba, Daniel Ponsa, and Krystian Mikolajczyk. Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters. In Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision, 2019. 2, 3<a href="#reffn_13" title="Jump back to footnote [13] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_14">
<sup>14</sup>. A. Baumberg. Reliable Feature Matching Across Widely Separated Views. In CVPR, pages 774–781, 2000. 3, 6<a href="#reffn_14" title="Jump back to footnote [14] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_15">
<sup>15</sup>. H. Bay, T. Tuytelaars, and L. Van Gool. SURF: Speeded Up Robust Features. In ECCV, 2006. 2, 3<a href="#reffn_15" title="Jump back to footnote [15] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_16">
<sup>16</sup>. P. R. Beaudet. Rotationally invariant image operators. In Proceedings of the 4th International Joint Conference on Pattern Recognition, pages 579–583, Kyoto, Japan, Nov. 1978. 3, 6<a href="#reffn_16" title="Jump back to footnote [16] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_17">
<sup>17</sup>. Jia-Wang Bian, Yu-Huan Wu, Ji Zhao, Yun Liu, Le Zhang, Ming-Ming Cheng, and Ian Reid. An Evaluation of Feature Matchers for Fundamental Matrix Estimation. In BMVC, 2019. 2<a href="#reffn_17" title="Jump back to footnote [17] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_18">
<sup>18</sup>. Eric Brachmann and Carsten Rother. Neural- Guided RANSAC: Learning Where to Sample Model Hypotheses. In ICCV, 2019. 2<a href="#reffn_18" title="Jump back to footnote [18] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_19">
<sup>19</sup>. G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000. 4<a href="#reffn_19" title="Jump back to footnote [19] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_20">
<sup>20</sup>. M. Brown, G. Hua, and S. Winder. Discriminative Learning of Local Image Descriptors. PAMI, 2011. 1, 2, 7<a href="#reffn_20" title="Jump back to footnote [20] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_21">
<sup>21</sup>. M. Brown and D. Lowe. Automatic Panoramic Image Stitching Using Invariant Features. IJCV, 74:59–73, 2007. 2<a href="#reffn_21" title="Jump back to footnote [21] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_22">
<sup>22</sup>. Mai Bui, Christoph Baur, Nassir Navab, Slobodan Ilic, and Shadi Albarqouni. Adversarial Networks for Camera Pose Regression and Reﬁnement. In The IEEE International Conference on Computer Vision (ICCV) Workshops, Oct 2019. 1<a href="#reffn_22" title="Jump back to footnote [22] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_23">
<sup>23</sup>. Ondˇrej Chum and Jiˇr´ı Matas. Matching with PROSAC Progressive Sample Consensus. In CVPR, pages 220–226, June 2005. 2<a href="#reffn_23" title="Jump back to footnote [23] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_24">
<sup>24</sup>. Ondˇrej Chum, Jiˇr´ı Matas, and Josef Kittler. Locally Optimized RANSAC. In PR, 2003. 2<a href="#reffn_24" title="Jump back to footnote [24] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_25">
<sup>25</sup>. Ondˇrej Chum, Jiˇr´ı Matas, and Josef Kittler. Locally optimized ransac. In Pattern Recognition, 2003. 4<a href="#reffn_25" title="Jump back to footnote [25] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_26">
<sup>26</sup>. Ondrej Chum, Tomas Werner, and Jiri Matas. Two-View Geometry Estimation Unaffected by a Dominant Plane. In CVPR, 2005. 2, 4<a href="#reffn_26" title="Jump back to footnote [26] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_27">
<sup>27</sup>. Hainan Cui, Xiang Gao, Shuhan Shen, and Zhanyi Hu. Hsfm: Hybrid structure-from-motion. In CVPR, July 2017. 2<a href="#reffn_27" title="Jump back to footnote [27] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_28">
<sup>28</sup>. Zheng Dang, Kwang Moo Yi, Yinlin Hu, Fei Wang, Pascal Fua, and Mathieu Salzmann. Eigendecomposition-Free Training of Deep Networks with Zero Eigenvalue-Based Losses. In ECCV, 2018. 4<a href="#reffn_28" title="Jump back to footnote [28] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_29">
<sup>29</sup>. Jana Noskova Daniel Barath, Jiri Matas. MAGSAC: marginalizing sample consensus. In CVPR, 2019. 1, 2, 4<a href="#reffn_29" title="Jump back to footnote [29] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_30">
<sup>30</sup>. D. Detone, T. Malisiewicz, and A. Rabinovich. Toward Geometric Deep SLAM. arXiv preprint arXiv:1707.07410, 2017. 1<a href="#reffn_30" title="Jump back to footnote [30] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_31">
<sup>31</sup>. D. Detone, T. Malisiewicz, and A. Rabinovich. Superpoint: Self-Supervised Interest Point Detection and Description. CVPR Workshop on Deep Learning for Visual SLAM, 2018. 1, 2, 3, 8<a href="#reffn_31" title="Jump back to footnote [31] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_32">
<sup>32</sup>. J. Dong and S. Soatto. Domain-Size Pooling in Local Descriptors: DSP-SIFT. In CVPR, 2015. 6<a href="#reffn_32" title="Jump back to footnote [32] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_33">
<sup>33</sup>. M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, and T. Sattler. D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. In CVPR, 2019. 1, 2, 3, 8<a href="#reffn_33" title="Jump back to footnote [33] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_34">
<sup>34</sup>. Patrick Ebel, Anastasiia Mishchuk, Kwang Moo Yi, Pascal Fua, and Eduard Trulls. Beyond Cartesian Representations for Local Descriptors. In ICCV, 2019. 2, 3, 6<a href="#reffn_34" title="Jump back to footnote [34] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_35">
<sup>35</sup>. Vassileios Balntas et.al. SILDa: A Multi-Task Dataset for Evaluating Visual Localization. <a href="https://github" target="_blank" rel="noopener">https://github</a>. com/scape-research/silda, 2018. 2<a href="#reffn_35" title="Jump back to footnote [35] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_36">
<sup>36</sup>. M.A Fischler and R.C. Bolles. Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography. Communications ACM, 24(6):381–395, 1981. 1, 2, 4<a href="#reffn_36" title="Jump back to footnote [36] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_37">
<sup>37</sup>. P. Gay, V. Bansal, C. Rubino, and A. D. Bue. Probabilistic Structure from Motion with Objects (PSfMO). In ICCV, 2017. 2<a href="#reffn_37" title="Jump back to footnote [37] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_38">
<sup>38</sup>. Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite. In CVPR, 2012. 2<a href="#reffn_38" title="Jump back to footnote [38] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_39">
<sup>39</sup>. R.I. Hartley. In Defense of the Eight-Point Algorithm. PAMI, 19(6):580–593, June 1997. 2<a href="#reffn_39" title="Jump back to footnote [39] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_40">
<sup>40</sup>. R. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, 2000. 1<a href="#reffn_40" title="Jump back to footnote [40] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_41">
<sup>41</sup>. R. I. Hartley. Projective reconstruction and invariants from multiple images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(10):1036–1041, Oct 1994. 1, 2<a href="#reffn_41" title="Jump back to footnote [41] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_42">
<sup>42</sup>. K. He, Y. Lu, and S. Sclaroff. Local Descriptors Optimized for Average Precision. In CVPR, 2018. 1<a href="#reffn_42" title="Jump back to footnote [42] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_43">
<sup>43</sup>. J. Heinly, J.L. Schoenberger, E. Dunn, and J-M. Frahm. Reconstructing the World in Six Days. In CVPR, 2015. 1, 2, 3<a href="#reffn_43" title="Jump back to footnote [43] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_44">
<sup>44</sup>. Karel Lenc and Varun Gulshan and Andrea Vedaldi. VLBenchmarks. <a href="http://www.vlfeat.org/" target="_blank" rel="noopener">http://www.vlfeat.org/</a> benchmarks/, 2011. 2<a href="#reffn_44" title="Jump back to footnote [44] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_45">
<sup>45</sup>. A. Kendall, M. Grimes, and R. Cipolla. Posenet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization. In ICCV, pages 2938–2946, 2015. 1<a href="#reffn_45" title="Jump back to footnote [45] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_46">
<sup>46</sup>. J. Krishna Murthy, Ganesh Iyer, and Liam Paull. gradSLAM: Dense SLAM meets Automatic Differentiation. arXiv, 2019. 2<a href="#reffn_46" title="Jump back to footnote [46] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_47">
<sup>47</sup>. Zhengqi Li and Noah Snavely. MegaDepth: Learning Single-View Depth Prediction from Internet Photos. In CVPR, 2018. 2<a href="#reffn_47" title="Jump back to footnote [47] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_48">
<sup>48</sup>. David G. Lowe. Distinctive Image Features from ScaleInvariant Keypoints. IJCV, 20(2):91–110, November 2004. 1, 2, 3, 4, 6, 8, 15<a href="#reffn_48" title="Jump back to footnote [48] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_49">
<sup>49</sup>. Zixin Luo, Tianwei Shen, Lei Zhou, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan. ContextDesc: Local Descriptor Augmentation with Cross-Modality Context. In CVPR, 2019. 2, 3<a href="#reffn_49" title="Jump back to footnote [49] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_50">
<sup>50</sup>. Z. Luo, T. Shen, L. Zhou, S. Zhu, R. Zhang, Y. Yao, T. Fang, and L. Quan. Geodesc: Learning Local Descriptors by Integrating Geometry Constraints. In ECCV, 2018. 2, 3<a href="#reffn_50" title="Jump back to footnote [50] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_51">
<sup>51</sup>. Simon Lynen, Bernhard Zeisl, Dror Aiger, Michael Bosse, Joel Hesch, Marc Pollefeys, Roland Siegwart, and Torsten Sattler. Large-scale, real-time visual-inertial localization revisited. arXiv Preprint, 2019. 1<a href="#reffn_51" title="Jump back to footnote [51] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_52">
<sup>52</sup>. Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year, 1000 km: The Oxford RobotCar dataset. IJRR, 36(1):3–15, 2017. 2<a href="#reffn_52" title="Jump back to footnote [52] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_53">
<sup>53</sup>. J. Matas, O. Chum, M. Urban, and T. Pajdla. Robust WideBaseline Stereo from Maximally Stable Extremal Regions. IVC, 22(10):761–767, 2004. 3, 6<a href="#reffn_53" title="Jump back to footnote [53] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_54">
<sup>54</sup>. K. Mikolajczyk and C. Schmid. A Performance Evaluation of Local Descriptors. PAMI, 27(10):1615–1630, 2004. 2<a href="#reffn_54" title="Jump back to footnote [54] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_55">
<sup>55</sup>. K. Mikolajczyk, C. Schmid, and A. Zisserman. Human Detection Based on a Probabilistic Assembly of Robust Part Detectors. In ECCV, pages 69–82, 2004. 3, 6<a href="#reffn_55" title="Jump back to footnote [55] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_56">
<sup>56</sup>. Jiri Matas Milan Pultar, Dmytro Mishkin. Leveraging Outdoor Webcams for Local Descriptor Learning. In Proceedings of CVWW 2019, 2019. 7<a href="#reffn_56" title="Jump back to footnote [56] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_57">
<sup>57</sup>. A. Mishchuk, D. Mishkin, F. Radenovic, and J. Matas. Working Hard to Know Your Neighbor’s Margins: Local Descriptor Learning Loss. In NeurIPS, 2017. 2, 3, 6<a href="#reffn_57" title="Jump back to footnote [57] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_58">
<sup>58</sup>. Dmytro Mishkin, Jiri Matas, and Michal Perdoch. MODS: PAMI, 19(6):580–593, June 1997. 2 Fast and robust method for two-view matching. CVIU, 2015. 6, 15<a href="#reffn_58" title="Jump back to footnote [58] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_59">
<sup>59</sup>. D. Mishkin, F. Radenovic, and J. Matas. Repeatability is Not Enough: Learning Affine Regions via Discriminability. In ECCV, 2018. 3, 6<a href="#reffn_59" title="Jump back to footnote [59] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_60">
<sup>60</sup>. Arun Mukundan, Giorgos Tolias, and Ondrej Chum. Explicit Spatial Encoding for Deep Local Descriptors. In CVPR, 2019. 1<a href="#reffn_60" title="Jump back to footnote [60] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_61">
<sup>61</sup>. R. Mur-Artal, J. Montiel, and J. Tardos. Orb-Slam: A Versatile and Accurate Monocular Slam System. IEEE Transactions on Robotics, 31(5):1147–1163, 2015. 1<a href="#reffn_61" title="Jump back to footnote [61] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_62">
<sup>62</sup>. D. Nister. An Efficient Solution to the Five-Point Relative Pose Problem. In CVPR, June 2003. 2<a href="#reffn_62" title="Jump back to footnote [62] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_63">
<sup>63</sup>. Hyeonwoo Noh, Andre Araujo, Jack Sim, and Tobias Weyanda nd Bohyung Han. Large-Scale Image Retrieval with Attentive Deep Local Features. In ICCV, 2017. 1, 2<a href="#reffn_63" title="Jump back to footnote [63] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_64">
<sup>64</sup>. Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi. LF-Net: Learning Local Features from Images. In NeurIPS, 2018. 2, 3<a href="#reffn_64" title="Jump back to footnote [64] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_65">
<sup>65</sup>. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. 4<a href="#reffn_65" title="Jump back to footnote [65] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_66">
<sup>66</sup>. Stephen M. Pizer, E. Philip Amburn, John D. Austin, Robert Cromartie, Ari Geselowitz, Trey Greer, Bart ter Haar Romeny, John B. Zimmerman, and Karel Zuiderveld. Adaptive histogram equalization and its variations. Computer vision, graphics, and image processing, 1987. 15<a href="#reffn_66" title="Jump back to footnote [66] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_67">
<sup>67</sup>. M. Pultar, D. Mishkin, and J. Matas. Leveraging Outdoor Webcams for Local Descriptor Learning. In Computer Vision Winter Workshop, 2019. 2, 7<a href="#reffn_67" title="Jump back to footnote [67] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_68">
<sup>68</sup>. C.R. Qi, H. Su, K. Mo, and L.J. Guibas. Pointnet: Deep Learning on Point Sets for 3D Classiﬁcation and Segmentation. In CVPR, 2017. 4<a href="#reffn_68" title="Jump back to footnote [68] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_69">
<sup>69</sup>. Filip Radenovic, Georgios Tolias, and Ondra Chum. CNN image retrieval learns from BoW: Unsupervised ﬁne-tuning with hard examples. In ECCV, 2016. 1<a href="#reffn_69" title="Jump back to footnote [69] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_70">
<sup>70</sup>. R. Ranftl and V. Koltun. Deep Fundamental Matrix Estimation. In ECCV, 2018. 2, 4<a href="#reffn_70" title="Jump back to footnote [70] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_71">
<sup>71</sup>. J. Revaud, P. Weinzaepfel, C. De Souza, N. Pion, G. Csurka, Y. Cabon, and M. Humenberger. R2D2: Repeatable and Reliable Detector and Descriptor. In arXiv Preprint, 2019. 8<a href="#reffn_71" title="Jump back to footnote [71] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_72">
<sup>72</sup>. J´erˆome Revaud, Philippe Weinzaepfel, C´esar Roberto de Souza, Noe Pion, Gabriela Csurka, Yohann Cabon, and Martin Humenberger. R2D2: Repeatable and Reliable Detector and Descriptor. In NeurIPS, 2019. 2<a href="#reffn_72" title="Jump back to footnote [72] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_73">
<sup>73</sup>. E. Rublee, V. Rabaud, K. Konolidge, and G. Bradski. ORB: An Efﬁcient Alternative to SIFT or SURF. In ICCV, 2011. 2, 3, 6<a href="#reffn_73" title="Jump back to footnote [73] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_74">
<sup>74</sup>. Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Improving Image-Based Localization by Active Correspondence Search. In ECCV, 2012. 1<a href="#reffn_74" title="Jump back to footnote [74] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_75">
<sup>75</sup>. T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg, D. Safari, M. Okutomi, M. Pollefeys, J. Sivic, F. Kahl, and T. Pajdla. Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions. In CVPR, 2018. 1, 2<a href="#reffn_75" title="Jump back to footnote [75] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_76">
<sup>76</sup>. Torsten Sattler, Tobias Weyand, Bastian Leibe, and Leif Kobbelt. Image Retrieval for Image-Based Localization Revisited. In BMVC, 2012. 2<a href="#reffn_76" title="Jump back to footnote [76] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_77">
<sup>77</sup>. Torsten Sattler, Qunjie Zhou, Marc Pollefeys, and Laura Leal-Taixe. Understanding the Limitations of CNN-based Absolute Camera Pose Regression. In CVPR, 2019. 1<a href="#reffn_77" title="Jump back to footnote [77] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_78">
<sup>78</sup>. N. Savinov, A. Seki, L. Ladicky, T. Sattler, and M. Pollefeys. Quad-Networks: Unsupervised Learning to Rank for Interest Point Detection. CVPR, 2017. 2<a href="#reffn_78" title="Jump back to footnote [78] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_79">
<sup>79</sup>. J.L. Sch¨onberger and J.M. Frahm. Structure-From-Motion Revisited. In CVPR, 2016. 1, 2, 3, 4, 6<a href="#reffn_79" title="Jump back to footnote [79] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_80">
<sup>80</sup>. J.L. Sch¨onberger, H. Hardmeier, T. Sattler, and M. Pollefeys. Comparative Evaluation of Hand-Crafted and Learned Local Features. In CVPR, 2017. 2<a href="#reffn_80" title="Jump back to footnote [80] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_81">
<sup>81</sup>. Yunxiao Shi, Jing Zhu, Yi Fang, Kuochin Lien, and Junli Gu. Self-Supervised Learning of Depth and Ego-motion with Differentiable Bundle Adjustment. arXiv Preprint, 2019. 2<a href="#reffn_81" title="Jump back to footnote [81] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_82">
<sup>82</sup>. E. Simo-serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. Moreno-Noguer. Discriminative Learning of Deep Convolutional Feature Point Descriptors. In ICCV, 2015. 2<a href="#reffn_82" title="Jump back to footnote [82] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_83">
<sup>83</sup>. C. Strecha, W.V. Hansen, L. Van Gool, P. Fua, and U. Thoennessen. On Benchmarking Camera Calibration and Multi-View Stereo for High Resolution Imagery. In CVPR, 2008. 2<a href="#reffn_83" title="Jump back to footnote [83] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_84">
<sup>84</sup>. J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. A Benchmark for the Evaluation of RGB-D SLAM Systems. In IROS, 2012. 4<a href="#reffn_84" title="Jump back to footnote [84] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_85">
<sup>85</sup>. Weiwei Sun, Wei Jiang, Eduard Trulls, Andrea Tagliasacchi, and Kwang Moo Yi. Attentive Context Normalization for Robust Permutation-Equivariant Learning. In arXiv Preprint, 2019. 2, 4, 8<a href="#reffn_85" title="Jump back to footnote [85] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_86">
<sup>86</sup>. Chengzhou Tang and Ping Tan. Ba-Net: Dense Bundle Adjustment Network. In ICLR, 2019. 2<a href="#reffn_86" title="Jump back to footnote [86] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_87">
<sup>87</sup>. Keisuke Tateno, Federico Tombari, Iro Laina, and Nassir Navab. Cnn-slam: Real-time dense monocular slam with learned depth prediction. In CVPR, July 2017. 2<a href="#reffn_87" title="Jump back to footnote [87] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_88">
<sup>88</sup>. B. Thomee, D.A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L. Li. YFCC100M: the New Data in Multimedia Research. In CACM, 2016. 3<a href="#reffn_88" title="Jump back to footnote [88] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_89">
<sup>89</sup>. Y. Tian, B. Fan, and F. Wu. L2-Net: Deep Learning of Discriminative Patch Descriptor in Euclidean Space. In CVPR, 2017. 2, 3<a href="#reffn_89" title="Jump back to footnote [89] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_90">
<sup>90</sup>. Yurun Tian, Xin Yu, Bin Fan, Fuchao Wu, Huub Heijnen, and Vassileios Balntas. SOSNet: Second Order Similarity Regularization for Local Descriptor Learning. In CVPR, 2019. 1, 2, 3<a href="#reffn_90" title="Jump back to footnote [90] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_91">
<sup>91</sup>. Giorgos Tolias, Yannis Avrithis, and Herv´e J´egou. Image Search with Selective Match Kernels: Aggregation Across Single and Multiple Images. IJCV, 116(3):247–261, Feb 2016. 1<a href="#reffn_91" title="Jump back to footnote [91] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_92">
<sup>92</sup>. P.H.S. Torr and A. Zisserman. MLESAC: A New Robust Estimator with Application to Estimating Image Geometry. CVIU, 78:138–156, 2000. 2<a href="#reffn_92" title="Jump back to footnote [92] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_93">
<sup>93</sup>. B. Triggs, P. Mclauchlan, R. Hartley, and A. Fitzgibbon. Bundle Adjustment – A Modern Synthesis. In Vision Algorithms: Theory and Practice, pages 298–372, 2000. 1<a href="#reffn_93" title="Jump back to footnote [93] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_94">
<sup>94</sup>. Andrea Vedaldi and Brian Fulkerson. Vlfeat: An open and portable library of computer vision algorithms. In Proceedings of the 18th ACM International Conference on Multimedia, MM ’10, pages 1469–1472, 2010. 3<a href="#reffn_94" title="Jump back to footnote [94] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_95">
<sup>95</sup>. Y. Verdie, K. M. Yi, P. Fua, and V. Lepetit. TILDE: A Temporally Invariant Learned DEtector. In CVPR, 2015. 2<a href="#reffn_95" title="Jump back to footnote [95] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_96">
<sup>96</sup>. S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, and K. Fragkiadaki. Sfm-Net: Learning of Structure and Motion from Video. arXiv Preprint, 2017. 2<a href="#reffn_96" title="Jump back to footnote [96] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_97">
<sup>97</sup>. X. Wei, Y. Zhang, Y. Gong, and N. Zheng. Kernelized Subspace Pooling for Deep Local Descriptors. In CVPR, 2018. 1<a href="#reffn_97" title="Jump back to footnote [97] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_98">
<sup>98</sup>. Changchang Wu. Towards Linear-Time Incremental Structure from Motion. In 3DV, 2013. 2, 6<a href="#reffn_98" title="Jump back to footnote [98] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_99">
<sup>99</sup>. Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal Fua. LIFT: Learned Invariant Feature Transform. In ECCV, 2016. 2<a href="#reffn_99" title="Jump back to footnote [99] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_100">
<sup>100</sup>. K. M. Yi, E. Trulls, Y. Ono, V. Lepetit, M. Salzmann, and P. Fua. Learning to Find Good Correspondences. In CVPR, 2018. 2, 3, 4, 7, 13, 17<a href="#reffn_100" title="Jump back to footnote [100] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_101">
<sup>101</sup>. S. Zagoruyko and N. Komodakis. Learning to Compare Image Patches via Convolutional Neural Networks. In CVPR, 2015. 6<a href="#reffn_101" title="Jump back to footnote [101] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_102">
<sup>102</sup>. Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou, Tianwei Shen, Yurong Chen, Long Quan, and Hongen Liao. Learning Two-View Correspondences and Geometry Using Order-Aware Network. ICCV, 2019. 2, 3, 4<a href="#reffn_102" title="Jump back to footnote [102] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_103">
<sup>103</sup>. Xu Zhang, Felix X. Yu, Svebor Karaman, and Shih-Fu Chang. Learning Discriminative and Transformation Covariant Local Feature Detectors. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. 2<a href="#reffn_103" title="Jump back to footnote [103] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_104">
<sup>104</sup>. Chen Zhao, Zhiguo Cao, Chi Li, Xin Li, and Jiaqi Yang. NM-Net: Mining Reliable Neighbors for Robust Feature Correspondences. In CVPR, 2019. 2, 4<a href="#reffn_104" title="Jump back to footnote [104] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_105">
<sup>105</sup>. Qunjie Zhou, Torsten Sattler, Marc Pollefeys, and Laura Leal-Taixe. To learn or not to learn: Visual localization from essential matrices. arXiv Preprint, 2019. 1<a href="#reffn_105" title="Jump back to footnote [105] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_106">
<sup>106</sup>. Siyu Zhu, Runze Zhang, Lei Zhou, Tianwei Shen, Tian Fang, Ping Tan, and Long Quan. Very Large-Scale Global SfM by Distributed Motion Averaging. In CVPR, June 2018. 1, 2<a href="#reffn_106" title="Jump back to footnote [106] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_107">
<sup>107</sup>. C.L. Zitnick and K. Ramnath. Edge Foci Interest Points. In ICCV, 2011. 2<a href="#reffn_107" title="Jump back to footnote [107] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_108">
<sup>108</sup>. A. Alahi, R. Ortiz, and P. Vandergheynst. FREAK: Fast Retina Keypoint. In CVPR, 2012. 7, 11<a href="#reffn_108" title="Jump back to footnote [108] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_109">
<sup>109</sup>. S. Leutenegger, M. Chli, and R. Y. Siegwart. Brisk: Binary robust invariant scalable keypoints. In ICCV, pages 2548–2555, 2011.7<a href="#reffn_109" title="Jump back to footnote [109] in the text."> &#8617;</a>
</blockquote>

    </div>

    
    
    
        
      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Vincent Qin</li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://www.vincentqin.tech/posts/2020-image-matching-cvpr/" title="CVPR2020图像匹配挑战赛，新数据集+新评测方法，SOTA正瑟瑟发抖！">https://www.vincentqin.tech/posts/2020-image-matching-cvpr/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.</li>
</ul>
</div>

      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/SLAM/" rel="tag"># SLAM</a>
            
              <a href="/tags/ORB/" rel="tag"># ORB</a>
            
              <a href="/tags/特征匹配/" rel="tag"># 特征匹配</a>
            
              <a href="/tags/SuperPoint/" rel="tag"># SuperPoint</a>
            
              <a href="/tags/SIFT/" rel="tag"># SIFT</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/posts/superglue/" rel="next" title="SuperGlue:Learning Feature Matching with Graph Neural Networks">
                  <i class="fa fa-chevron-left"></i> SuperGlue:Learning Feature Matching with Graph Neural Networks
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/posts/redirect-arxiv/" rel="prev" title="国内加速访问arxiv">
                  国内加速访问arxiv <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="comments"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#相关工作"><span class="nav-number">2.</span> <span class="nav-text">相关工作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#局部特征"><span class="nav-number">2.1.</span> <span class="nav-text">局部特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#鲁棒匹配"><span class="nav-number">2.2.</span> <span class="nav-text">鲁棒匹配</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#运动恢复结构（SfM）"><span class="nav-number">2.3.</span> <span class="nav-text">运动恢复结构（SfM）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集和标准"><span class="nav-number">2.4.</span> <span class="nav-text">数据集和标准</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Phototourism-数据集"><span class="nav-number">3.</span> <span class="nav-text">Phototourism 数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#处理流程图Pipeline"><span class="nav-number">4.</span> <span class="nav-text">处理流程图Pipeline</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#特征提取"><span class="nav-number">4.1.</span> <span class="nav-text">特征提取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征匹配"><span class="nav-number">4.2.</span> <span class="nav-text">特征匹配</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#外点滤除"><span class="nav-number">4.3.</span> <span class="nav-text">外点滤除</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stereo-task"><span class="nav-number">4.4.</span> <span class="nav-text">Stereo task</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-view-task"><span class="nav-number">4.5.</span> <span class="nav-text">Multi-view task</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#误差指标"><span class="nav-number">4.6.</span> <span class="nav-text">误差指标</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实验开始——配置细节很重要"><span class="nav-number">5.</span> <span class="nav-text">实验开始——配置细节很重要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#结果"><span class="nav-number">6.</span> <span class="nav-text">结果</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8k特征"><span class="nav-number">6.1.</span> <span class="nav-text">8k特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2k特征"><span class="nav-number">6.2.</span> <span class="nav-text">2k特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8k-vs-2k"><span class="nav-number">6.3.</span> <span class="nav-text">8k vs. 2k</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#光照变化"><span class="nav-number">6.4.</span> <span class="nav-text">光照变化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#新指标-vs-传统指标"><span class="nav-number">6.5.</span> <span class="nav-text">新指标 vs. 传统指标</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">7.</span> <span class="nav-text">References</span></a></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="https://vincentqin.gitee.io/images/qin_small.png"
      alt="Vincent Qin">
  <p class="site-author-name" itemprop="name">Vincent Qin</p>
  <div class="site-description" itemprop="description">Keep Your Curiosity</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">51</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">99</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/Vincentqyw" title="GitHub &rarr; https://github.com/Vincentqyw" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:realcat@126.com" title="Email &rarr; mailto:realcat@126.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>Email</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://vincentqin.gitee.io/images/qrcode_realcat.jpg" title="Wechat &rarr; https://vincentqin.gitee.io/images/qrcode_realcat.jpg" rel="noopener" target="_blank"><i class="fa fa-fw fa-weixin"></i>Wechat</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.zhihu.com/people/i_vincent/activities" title="Zhihu &rarr; https://www.zhihu.com/people/i_vincent/activities" rel="noopener" target="_blank"><i class="fa fa-fw fa-quora"></i>Zhihu</a>
      </span>
    
  </div>



  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-dashboard"></i>
      Scholar
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://xxx.itp.ac.cn" title="http://xxx.itp.ac.cn" rel="noopener" target="_blank">arxiv</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://arxiv-sanity.com/" title="http://arxiv-sanity.com/" rel="noopener" target="_blank">arxiv-sanity</a>
        </li>
      
    </ul>
  </div>



  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-battery-three-quarters"></i>
      Friends Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.wangpengan.com/" title="http://www.wangpengan.com/" rel="noopener" target="_blank">Tensorboy</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://simtalk.cn/" title="http://simtalk.cn/" rel="noopener" target="_blank">Simshang</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://sttomato.github.io" title="https://sttomato.github.io" rel="noopener" target="_blank">Tomato</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://newdee.cf/" title="https://newdee.cf/" rel="noopener" target="_blank">Newdee</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://cs-people.bu.edu/yfhu/" title="http://cs-people.bu.edu/yfhu/" rel="noopener" target="_blank">WhoIf</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://yulunzhang.com/" title="http://yulunzhang.com/" rel="noopener" target="_blank">Yulun</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://sanglongbest.github.io/" title="https://sanglongbest.github.io/" rel="noopener" target="_blank">YangLiu</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://www.fi-ads.com/" title="https://www.fi-ads.com/" rel="noopener" target="_blank">Future iDeal</a>
        </li>
      
    </ul>
  </div>





  <div class="feed-link motion-element">
    <a href="https://vincentqin.gitee.io/" rel="alternate">
       <i class="fa fa-home"></i>Homepage Backup
    </a>
  </div>
      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2016 – <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vincent Qin</span>
</div>

  <script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
  <script>
    function timer() {
      var ages = moment.preciseDiff(moment(),moment(20160701,"YYYYMMDD"));
      ages = ages.replace(/years?/, "年");
      ages = ages.replace(/months?/, "月");
      ages = ages.replace(/days?/, "天");
      ages = ages.replace(/hours?/, "小时");
      ages = ages.replace(/minutes?/, "分");
      ages = ages.replace(/seconds?/, "秒");
      ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
      div.innerHTML = `已运行 ${ages}`;
    }
    var div = document.createElement("div");
    //插入到copyright之后
    var copyright = document.querySelector(".copyright");
    document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
    timer();
    setInterval("timer()",1000)
  </script>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="Total Visitors">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="Total Views">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>


  <div style="display: none;">
    <script src="//s95.cnzz.com/z_stat.php?id=1273219530&web_id=1273219530"></script>
  </div>






  <script>
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=65489609";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
<script src="/js/utils.js?v=7.4.0"></script>
<script src="/js/schemes/pisces.js?v=7.4.0"></script>

<script src="/js/next-boot.js?v=7.4.0"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>








  <script src="/js/local-search.js?v=7.4.0"></script>








<script>
if (document.querySelectorAll('div.pdf').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/pdfobject@2/pdfobject.min.js', () => {
    document.querySelectorAll('div.pdf').forEach(element => {
      PDFObject.embed(element.getAttribute('target'), element, {
        pdfOpenParams: {
          navpanes: 0,
          toolbar: 0,
          statusbar: 0,
          pagemode: 'thumbs',
          view: 'FitH'
        },
        PDFJS_URL: '/lib/pdf/web/viewer.html',
        height: element.getAttribute('height') || '500px'
      });
    });
  }, window.PDFObject);
}
</script>






  

  
    
      
<script type="text/x-mathjax-config">
    MathJax.Ajax.config.path['mhchem'] = '//cdn.jsdelivr.net/npm/mathjax-mhchem@3';

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        extensions: ['[mhchem]/mhchem.js'],
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: true,
    notify: false,
    appId: 'E2yzANt8H4UiIuw4c95dTaXH-MdYXbMMI',
    appKey: 'NF1yxeki6kw4KM5glHkwjvKc',
    placeholder: 'Just go go',
    avatar: 'wavatar',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn',
    path: location.pathname
  });
}, window.Valine);
</script>


  





  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>


</body>
</html>
