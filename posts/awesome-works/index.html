<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/realcat-apple-touch-icon.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/realcat-32x32.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/realcat-32x32.png?v=7.4.0">
  <link rel="mask-icon" href="/images/realcat-safari-pinned-tab.svg?v=7.4.0" color="#222">
  <link rel="alternate" href="/atom.xml" title="RealCat" type="application/atom+xml">
  <link rel="alternate" href="https://realcat.vercel.app/" title="RealCat" type="application/atom+xml">
  <meta name="google-site-verification" content="u46QTaG_Dv3OZLpOBKYtqyuiNtIdnhSG5ASKoNvGBCM">
  <meta name="baidu-site-verification" content="MtcbwE45ft">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"hide","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":false,"style":"flat"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: true,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":10,"unescape":true,"preload":true},
    path: 'search.xml',
    motion: {"enable":false,"async":true,"transition":{"post_block":"fadeIn","post_header":"fadeIn","post_body":"fadeIn","coll_header":"fadeIn","sidebar":"fadeIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="The post contains papers-with-code about SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc.">
<meta name="keywords" content="SLAM,disparity,pose-tracking,object-tracking,depth-estimation,flow-estimation,3D-graphics">
<meta property="og:type" content="article">
<meta property="og:title" content="ğŸ”¥Awesome CV Works">
<meta property="og:url" content="https://www.vincentqin.tech/posts/awesome-works/index.html">
<meta property="og:site_name" content="RealCat">
<meta property="og:description" content="The post contains papers-with-code about SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc.">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2020-08-30T13:59:21.496Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ğŸ”¥Awesome CV Works">
<meta name="twitter:description" content="The post contains papers-with-code about SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc.">
  <link rel="canonical" href="https://www.vincentqin.tech/posts/awesome-works/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>ğŸ”¥Awesome CV Works | RealCat</title>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-97856334-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-97856334-1');
    }
  </script>








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">RealCat</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Turn on, Tune in, Drop out</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives<span class="badge">56</span></a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags<span class="badge">106</span></a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories<span class="badge">13</span></a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-collections">
      
    
      
    

    <a href="/collections" rel="section"><i class="menu-item-icon fa fa-fw fa-diamond"></i> <br>Collections</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-top">
      
    
      
    

    <a href="/top/" rel="section"><i class="menu-item-icon fa fa-fw fa-signal"></i> <br>Top</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-guest_comments">
      
    
      
    

    <a href="/guestbook" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://www.vincentqin.tech/posts/awesome-works/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Vincent Qin">
      <meta itemprop="description" content="Keep Your Curiosity">
      <meta itemprop="image" content="https://vincentqin.gitee.io/images/qin_small.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RealCat">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">ğŸ”¥Awesome CV Works

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-03-31 20:15:41" itemprop="dateCreated datePublished" datetime="2019-03-31T20:15:41+08:00">2019-03-31</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-30 21:59:21" itemprop="dateModified" datetime="2020-08-30T21:59:21+08:00">2020-08-30</time>
              </span>
            
          

          
            <span id="/posts/awesome-works/" class="post-meta-item leancloud_visitors" data-flag-title="ğŸ”¥Awesome CV Works" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Comments: </span>
    
    <a title="valine" href="/posts/awesome-works/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/awesome-works/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>The post contains papers-with-code about SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc. </p>
<p><a href="https://github.com/Vincentqyw/Recent-Stars-2020" target="_blank" rel="noopener"><img alt="ReadMe Card" data-src="https://github-readme-stats.vercel.app/api/pin/?username=Vincentqyw&amp;repo=Recent-Stars-2020&amp;show_owner=false&amp;theme=default"></a> </p>
<!-- [![GitHub stars](https://img.shields.io/github/stars/Vincentqyw/Recent-Stars-2019.svg?logo=github&label=Stars)](https://github.com/Vincentqyw/Recent-Stars-2019) -->
<a id="more"></a>
<!--# Recent Stars 2020-->
<!-- <p align="center">
 <img width="100px" src="https://cdn.jsdelivr.net/gh/Vincentqyw/blog-resources/awesome-works/github-star.png" align="center" alt="" />
</p> -->
<!-- <p align="center">
  <a href="https://github.com/Vincentqyw/Recent-Stars-2020">
    <img alt="Awesome" src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" />
  </a>
  <a href="http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019">
    <img alt="HitCount" src="http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019.svg" />
  </a>
  <a href="https://vincentqin.tech">
    <img alt="LICENSE" src="https://img.shields.io/badge/license-Anti%20996-blue.svg?style=flat-square" />
  </a>
</p> -->
<!--
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/Vincentqyw/Recent-Stars-2020)
[![HitCount](http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019.svg)](http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019)
[![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg?style=flat-square)](https://github.com/Vincentqyw/Recent-Stars-2020)
âœ” This repo collects some links with papers which I recently starred related on SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, etc.
-->
<h2 id="SLAM-related"><a href="#SLAM-related" class="headerlink" title="SLAM related"></a>SLAM related</h2><ul>
<li>[<strong>SLAM</strong>]<a href="https://github.com/UZ-SLAMLab/ORB_SLAM3" target="_blank" rel="noopener">ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM</a>, <strong>[<a href="https://arxiv.org/abs/2007.11898" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>SLAM</strong>]<a href="https://github.com/TixiaoShan/LIO-SAM" target="_blank" rel="noopener">LIO-SAM</a>, æ¿€å…‰é›·è¾¾IMUç´§è€¦åˆSLAM</li>
<li>[<strong>Tool</strong>]<a href="https://github.com/petercorke/robotics-toolbox-python" target="_blank" rel="noopener">Robotics Toolbox for Python</a>,  a Python implementation of the <a href="https://github.com/petercorke/robotics-toolbox-matlab" target="_blank" rel="noopener">Robotics Toolbox for MATLABÂ®</a></li>
<li>[<strong>Matching</strong>]<a href="https://github.com/rpautrat/LISRD" target="_blank" rel="noopener">LISRD</a>,ECCV 2020, <strong>[<a href="https://arxiv.org/abs/2007.08988" target="_blank" rel="noopener">PDF</a>]</strong>ï¼Œåœ¨çº¿å±€éƒ¨ä¸å˜ç‰¹å¾åŒ¹é…ï¼é‡è¦ï¼</li>
<li>[<strong>Matching</strong>]<a href="https://github.com/cavalli1234/AdaLAM" target="_blank" rel="noopener">AdaLAM</a>,ç‰¹å¾åŒ¹é…å¿«é€Ÿæ»¤é™¤å¤–ç‚¹</li>
<li>[<strong>Calib</strong>]<a href="https://github.com/3DCVer/fisheye_pinhole_calib_demo" target="_blank" rel="noopener">fisheye_pinhole_calib_demo</a>, åŒ…æ‹¬é±¼çœ¼æ¨¡å‹ã€é’ˆå­”æ¨¡å‹çš„ç›¸æœºæ ‡å®šï¼Œå°è£…äº†è‡ªåŠ¨ç¼–è¯‘ã€åº“çš„æ‰“åŒ…ä»¥åŠå¤–éƒ¨åº“çš„è°ƒç”¨æµ‹è¯•</li>
<li>[<strong>Calib</strong>]<a href="https://github.com/FENGChenxi0823/SensorCalibration" target="_blank" rel="noopener">SensorCalibration</a>, IMUé›·è¾¾æ ‡å®š</li>
<li>[<strong>VO</strong>]<a href="https://github.com/PyojinKim/LPVO" target="_blank" rel="noopener">Low-Drift Visual Odometry in Structured Environments by Decoupling Rotational and Translational Motion</a>,ICRA 2018, <strong>[<a href="http://pyojinkim.com/download/papers/2018_ICRA.pdf" target="_blank" rel="noopener">PDF</a>]</strong>, ç»“æ„åŒ–ç¯å¢ƒä¸­å°†æ—‹è½¬é‡ä¸å¹³ç§»é‡è¿›è¡Œåˆ†ç¦»ä¼˜åŒ–</li>
<li>[<strong>VIO</strong>]<a href="https://github.com/iamwangyabin/VIO-SLAM" target="_blank" rel="noopener">VIO-SLAM</a>, ä»é›¶å¼€å§‹æ‰‹å†™VIOè¯¾åä½œä¸š</li>
<li>[<strong>Matching</strong>]<a href="https://github.com/lzx551402/tfmatch" target="_blank" rel="noopener">TFMatch: Learning-based image matching in TensorFlow</a>,TensorFlow å®ç°çš„ GeoDesc,ASLFeatä»¥åŠContextDesc</li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/yanyan-li/SLAM-BOOK" target="_blank" rel="noopener">SLAM-BOOK</a>, ä¸€æœ¬å…³äºSLAMçš„ä¹¦ç¨¿ï¼Œæ¸…æ¥šçš„ä»‹ç»SLAMç³»ç»Ÿä¸­çš„ä½¿ç”¨çš„å‡ ä½•æ–¹æ³•å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ŒæŒç»­æ›´æ–°ä¸­</li>
<li>[<strong>Loop Closing</strong>]<a href="https://github.com/PRBonn/OverlapNet" target="_blank" rel="noopener">OverlapNet - Loop Closing for 3D LiDAR-based SLAM</a>, RSS 2020, <strong>[<a href="https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/chen2020rss.pdf" target="_blank" rel="noopener">PDF</a>]</strong>, 3Dæ¿€å…‰é›·è¾¾SLAMé—­ç¯</li>
<li>[<strong>SLAM</strong>]<a href="https://github.com/halajun/VDO_SLAM" target="_blank" rel="noopener">VDO_SLAM</a>, RGB-Dç›¸æœºæ•°æ®ä½œä¸ºè¾“å…¥ï¼Œå®ç°è¿½è¸ªåŠ¨æ€ç‰©ä½“SLAMçš„åŠŸèƒ½, <strong>[<a href="https://arxiv.org/abs/2005.11052" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>SLAM</strong>]<a href="https://github.com/TUMFTM/orbslam-map-saving-extension" target="_blank" rel="noopener">orbslam-map-saving-extension</a>ï¼Œåœ¨ORB-SLAMçš„åŸºç¡€ä¸Šå¢åŠ ä¿å­˜+åŠ è½½åœ°å›¾åŠŸèƒ½</li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/NxRLab/ModernRobotics" target="_blank" rel="noopener">Modern Robotics: Mechanics, Planning, and Control Code Library</a>, ç°ä»£æœºå™¨äººå­¦, <strong>[<a href="http://hades.mech.northwestern.edu/index.php/Modern_Robotics" target="_blank" rel="noopener">Homepage</a>]</strong></li>
<li>[<strong>Matching</strong>]<a href="https://github.com/vcg-uvic/image-matching-benchmark-baselines" target="_blank" rel="noopener">image-matching-benchmark-baselines</a>, å›¾åƒç‰¹å¾åŒ¹é…æŒ‘æˆ˜èµ›ä¸»é¡µ</li>
<li>[<strong>Matching</strong>]<a href="https://github.com/mameng1/GraphLineMatching" target="_blank" rel="noopener">GraphLineMatching</a></li>
<li>[<strong>Matching</strong>]<a href="https://github.com/jiayi-ma/LPM" target="_blank" rel="noopener">Locality Preserving Matching</a>, IJCAI 2017, <strong>[<a href="https://ai.tencent.com/ailab/media/publications/YuanGao_IJCAI2017_LocalityPreservingMatching.pdf" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>IMU</strong>]<a href="https://github.com/ydsf16/IMUOrientationEstimator" target="_blank" rel="noopener">IMUOrientationEstimator</a></li>
<li>[<strong>Feature</strong>]<a href="https://github.com/iago-suarez/BEBLID" target="_blank" rel="noopener">BEBLID: Boosted Efficient Binary Local Image Descriptor</a></li>
<li>[<strong>Relocalization</strong>]<a href="https://github.com/zlthinker/KFNet" target="_blank" rel="noopener">KFNet: Learning Temporal Camera Relocalization using Kalman Filtering</a>,CVPR 2020,<strong>[<a href="https://arxiv.org/abs/2003.10629" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>Matching</strong>]<a href="https://github.com/vcg-uvic/image-matching-benchmark" target="_blank" rel="noopener">image-matching-benchmark</a></li>
<li>[<strong>Matching</strong>]<a href="https://github.com/JiawangBian/GMS-Feature-Matcher" target="_blank" rel="noopener">GMS: Grid-based Motion Statistics for Fast, Ultra-robust Feature Correspondence</a>,CVPR 17 &amp; IJCV 19,<strong>[<a href="http://jwbian.net/Papers/GMS_CVPR17.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://jwbian.net/gms" target="_blank" rel="noopener">Project page</a>]</strong></li>
<li>[<strong>Reloc</strong>]<a href="https://github.com/Artisense-ai/GN-Net-Benchmark" target="_blank" rel="noopener">GN-Net-Benchmark</a>, CVPR 2020,GN-Net: The Gauss-Newton Loss for Multi-Weather Relocalization, <strong>[<a href="https://arxiv.org/abs/1904.11932" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://vision.in.tum.de/gn-net" target="_blank" rel="noopener">Project page</a>]</strong></li>
<li>[<strong>Matching</strong>]<a href="https://github.com/magicleap/SuperGluePretrainedNetwork" target="_blank" rel="noopener">SuperGluePretrainedNetwork</a>, CVPR 2020, <strong>[<a href="https://arxiv.org/abs/1911.11763" target="_blank" rel="noopener">PDF</a>]</strong>, åˆ’é‡ç‚¹ï¼2020å¹´sotaè¶…å¤§è§†è§’2Dç‰¹å¾åŒ¹é…ï¼Œ<a href="https://www.vincentqin.tech/posts/superglue/">Blog</a></li>
<li>[<strong>Feature</strong>]<a href="https://github.com/XuyangBai/D3Feat" target="_blank" rel="noopener">D3Feat</a>, CVPR 2020, <strong>[<a href="https://arxiv.org/abs/2003.03164" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>Feature</strong>]<a href="https://github.com/lzx551402/ASLFeat" target="_blank" rel="noopener">ASLFeat</a>, CVPR 2020, ASLFeat: Learning Local Features of Accurate Shape and Localization, <strong>[<a href="https://arxiv.org/abs/2003.10071" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>Feature</strong>]<a href="https://github.com/XuyangBai/D3Feat" target="_blank" rel="noopener">GMS-Feature-Matcher</a>, CVPR 2018, GMS: Grid-based Motion Statistics for Fast, Ultra-robust Feature Correspondence, <strong>[<a href="http://jwbian.net/Papers/GMS_CVPR17.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://jwbian.net/gms" target="_blank" rel="noopener">Project page</a>]</strong></li>
<li>[<strong>Feature</strong>]<a href="https://github.com/XuyangBai/D3Feat" target="_blank" rel="noopener">D3Feat</a>, CVPR 2020, <strong>[<a href="https://arxiv.org/abs/2003.03164" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>Feature</strong>]<a href="https://github.com/yewzijian/3DFeatNet" target="_blank" rel="noopener">3DFeatNet</a>, ECCV 2018, <strong>[<a href="https://arxiv.org/abs/1807.09413" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/microsoft/AutonomousDrivingCookbook" target="_blank" rel="noopener">AutonomousDrivingCookbook</a>ï¼ŒScenarios, tutorials and demos for Autonomous Driving</li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/PaoPaoRobot/SLAMPaperReading" target="_blank" rel="noopener">SLAMPaperReading</a>ï¼Œæ³¡æ³¡æœºå™¨äººåŒ—äº¬çº¿ä¸‹SLAMè®ºæ–‡åˆ†äº«èµ„æ–™</li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/lishuwei0424/VIO_Tutotial_Course" target="_blank" rel="noopener">VIO_Tutotial_Course</a></li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/MichaelBeechan/VO-SLAM-Review" target="_blank" rel="noopener">VO-SLAM-Review</a></li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/QingSimon/VINS-Mono-code-annotation" target="_blank" rel="noopener">VINS-Mono-code-annotation</a>,VINS-Monoä»£ç æ³¨é‡Šä»¥åŠå…¬å¼æ¨å¯¼</li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/ManiiXu/VINS-Mono-Learning" target="_blank" rel="noopener">VINS-Mono-Learning</a>,VINS-Monoä»£ç æ³¨é‡Š</li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/HeYijia/VINS-Course" target="_blank" rel="noopener">VINS-Course</a>,VINS-Mono code without Ceres or ROS</li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/StevenCui/VIO-Doc" target="_blank" rel="noopener">VIO-Doc</a>,ä¸»æµVIOè®ºæ–‡æ¨å¯¼åŠä»£ç è§£æ</li>
<li>[<strong>VO</strong>]<a href="https://github.com/muskie82/CNN-DSO" target="_blank" rel="noopener">CNN-DSO</a>, Direct Sparse Odometry with CNN Depth Prediction</li>
<li>[<strong>VO</strong>]<a href="https://github.com/lsyads/fisheye-ORB-SLAM" target="_blank" rel="noopener">fisheye-ORB-SLAM</a>, A real-time robust monocular visual SLAM system based on ORB-SLAM for fisheye cameras, without rectifying or cropping the input images</li>
<li>[<strong>VO</strong>]<a href="https://github.com/robotseu/ORB_Line_SLAM" target="_blank" rel="noopener">ORB_Line_SLAM</a>, Real-Time SLAM with BoPLW Pairs for Stereo Cameras, with Loop Detection and Relocalization Capabilities</li>
<li>[<strong>VO</strong>]<a href="https://github.com/ChiWeiHsiao/DeepVO-pytorch.git" target="_blank" rel="noopener">DeepVO-pytorch</a>, ICRA 2017 <a href="https://ieeexplore.ieee.org/document/7989236/" target="_blank" rel="noopener">DeepVO: Towards end-to-end visual odometry with deep Recurrent Convolutional Neural Networks</a></li>
<li>[<strong>Calib</strong>]<a href="https://github.com/MegviiRobot/CamOdomCalibraTool" target="_blank" rel="noopener">CamOdomCalibraTool</a>, The tool to calibrate extrinsic param between camera and wheel.</li>
<li>[<strong>Calib</strong>]<a href="https://github.com/heethesh/lidar_camera_calibration" target="_blank" rel="noopener">lidar_camera_calibration</a>,<a href="https://github.com/ankitdhall/lidar_camera_calibration" target="_blank" rel="noopener">another version</a></li>
<li>[<strong>Calib</strong>]<a href="https://github.com/MegviiRobot/OdomLaserCalibraTool.git" target="_blank" rel="noopener">OdomLaserCalibraTool</a>ï¼Œç›¸æœºä¸2Dé›·è¾¾æ ‡å®š</li>
<li>[<strong>Calib</strong>]<a href="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration" target="_blank" rel="noopener">extrinsic_lidar_camera_calibration</a>, LiDARTag: A Real-Time Fiducial Tag using Point Clouds, arXiv 2019, <strong>[<a href="https://arxiv.org/abs/1908.10349" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>Calib</strong>]<a href="https://github.com/beltransen/velo2cam_calibration" target="_blank" rel="noopener">velo2cam_calibration</a>, Automatic Calibration algorithm for Lidar-Stereo camera, <strong>[<a href="http://wiki.ros.org/velo2cam_calibration" target="_blank" rel="noopener">Project page</a>]</strong></li>
<li>[<strong>Dataset</strong>]<a href="https://github.com/HKBU-HPML/IRS.git" target="_blank" rel="noopener">IRS: A Large Synthetic Indoor Robotics Stereo Dataset for Disparity and Surface Normal Estimation</a></li>
<li>[<strong>Tools</strong>]<a href="https://github.com/christophhagen/averaging-quaternions" target="_blank" rel="noopener">averaging-quaternions</a>,å››å…ƒæ•°å¹³å‡</li>
</ul>
<hr>
<p>åˆ†å‰²çº¿ï¼Œä»¥ä¸‹æ˜¯2019å¹´çš„æ˜Ÿæ ‡é¡¹ç›®ï¼Œä¸Šé¢æ˜¯2020å¹´æ–°æ˜Ÿæ ‡çš„ã€‚</p>
<ul>
<li><a href="https://github.com/naver/r2d2" target="_blank" rel="noopener">R2D2: Reliable and Repeatable Detector and Descriptor</a>,NeurIPS 2019,<strong>[<a href="https://arxiv.org/abs/1906.06195" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://europe.naverlabs.com/research/publications/r2d2-reliable-and-repeatable-detectors-and-descriptors-for-joint-sparse-local-keypoint-detection-and-feature-extraction/" target="_blank" rel="noopener">Project page</a>]</strong>ï¼Œæ·±åº¦å­¦ä¹ ç‰¹å¾ç‚¹+æè¿°å­</li>
<li><a href="https://github.com/1989Ryan/Semantic_SLAM" target="_blank" rel="noopener">Semantic_SLAM</a>,è¯­ä¹‰SLAMï¼šROS + ORB SLAM + PSPNet101</li>
<li><a href="https://github.com/BAILOOL/PlaceRecognition-LoopDetection" target="_blank" rel="noopener">PlaceRecognition-LoopDetection</a>, Light-weight place recognition and loop detection using road markings</li>
<li><a href="https://github.com/MISTLab/DOOR-SLAM" target="_blank" rel="noopener">DOOR-SLAM: Distributed, online, and outlier resilient SLAM for robotic teams</a>,<strong>[<a href="https://arxiv.org/abs/1909.12198" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://mistlab.ca/DOOR-SLAM/" target="_blank" rel="noopener">Project page</a>]</strong>ï¼Œå¤šæœºå™¨äººåä½œSLAMï¼Œå¢å¼ºäº†åœºæ™¯çš„é€‚ç”¨æ€§</li>
<li><a href="https://github.com/shamangary/awesome-local-global-descriptor" target="_blank" rel="noopener">awesome-local-global-descriptor</a>, è¶…è¯¦ç»†æ·±åº¦å­¦ä¹ ç‰¹å¾ç‚¹æè¿°å­é›†åˆï¼Œéœ€è¦é‡ç‚¹å…³æ³¨ä¸€ä¸‹è¿™ä¸ªrepo</li>
<li><a href="https://github.com/zju3dv/GIFT" target="_blank" rel="noopener">GIFT: Learning Transformation-Invariant Dense Visual Descriptors via Group CNNs</a>, NeurIPS 2019ï¼Œ<strong>[<a href="https://arxiv.org/abs/1911.05932" target="_blank" rel="noopener">PDF</a>]</strong>, <strong>[<a href="https://zju3dv.github.io/GIFT/" target="_blank" rel="noopener">Project page</a>]</strong>ï¼Œæµ™å¤§CAD+å•†æ±¤è”åˆå®éªŒå®¤å‡ºå“ï¼Œåˆ©ç”¨Group CNNæ¥æ”¹è¿›superpointæè¿°å­ï¼ˆä»…æè¿°ï¼Œç‰¹å¾ç‚¹æå–å¯ä»»æ„é€‰æ‹©ï¼‰ï¼Œå¯ä»¥å¤§å¹…åº¦å¢å¼ºè§†è§’å˜åŒ–æ—¶çš„ç‰¹å¾ç‚¹å¤æ£€ç‡ä¸åŒ¹é…ç‚¹æ•°</li>
<li><a href="https://github.com/axelBarroso/Key.Net" target="_blank" rel="noopener">Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters</a>,ICCV 2019, <strong>[<a href="https://arxiv.org/abs/1904.00889" target="_blank" rel="noopener">PDF</a>]</strong>, æ·±åº¦å­¦ä¹ ç‰¹å¾ç‚¹</li>
<li><a href="https://github.com/TRI-ML/KP3D" target="_blank" rel="noopener">Self-Supervised 3D Keypoint Learning for Ego-motion Estimation</a>,<strong>[<a href="https://arxiv.org/abs/1912.03426" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://www.youtube.com/watch?v=4hFhSD8QUPM" target="_blank" rel="noopener">Youtube</a>]</strong>, æ·±åº¦å­¦ä¹ ç‰¹å¾ç‚¹</li>
<li><a href="https://github.com/Jichao-Peng/VINS-Mono-Optimization" target="_blank" rel="noopener">VINS-Mono-Optimization</a>, å®ç°ç‚¹çº¿ç´§è€¦åˆä¼˜åŒ–çš„VINS-Mono</li>
<li><a href="https://github.com/PetWorm/msckf_vio_zhushi" target="_blank" rel="noopener">msckf_vioæ³¨é‡Šç‰ˆæœ¬</a></li>
<li><a href="https://github.com/lyakaap/NetVLAD-pytorch" target="_blank" rel="noopener">NetVLAD-pytorch</a>, NetVLADåœºæ™¯è¯†åˆ«çš„pytorchå®ç°</li>
<li><a href="http://microgps.cs.princeton.edu/" target="_blank" rel="noopener">High-Precision Localization Using Ground Texture (Micro-GPS)</a>,ECCV 2018,<strong>[<a href="https://arxiv.org/abs/1710.10687" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://microgps.cs.princeton.edu/" target="_blank" rel="noopener">Project page</a>]</strong>,<strong>[<a href="http://microgps.cs.princeton.edu/data/micro-gps-cpp-master.zip" target="_blank" rel="noopener">code</a>]</strong>ï¼Œåœ°å‘ï¼ˆæ‘„åƒæœºæœå‘åœ°é¢ï¼‰SLAMï¼Œè·å¾—é«˜ç²¾åº¦é‡å®šä½æ•ˆæœã€‚</li>
<li><a href="https://github.com/LRMPUT/PlaneSLAM" target="_blank" rel="noopener">PlaneSLAM</a>, Paper: â€œOn the Representation of Planes for Efficient Graph-based SLAM with High-level Featuresâ€</li>
<li><a href="https://github.com/ucla-vision/xivo" target="_blank" rel="noopener">XIVO: X Inertial-aided Visual Odometry and Sparse Mapping</a>, an open-source repository for visual-inertial odometry/mapping. </li>
<li><a href="https://github.com/lmb-freiburg/deeptam" target="_blank" rel="noopener">DeepTAM</a>,ECCV 2018,<strong>[<a href="https://arxiv.org/pdf/1808.01900.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://lmb.informatik.uni-freiburg.de/people/zhouh/deeptam/" target="_blank" rel="noopener">Project page</a>]</strong>,a learnt system for keyframe-based dense camera tracking and mapping.</li>
<li><a href="https://github.com/ajparra/iRotAvg" target="_blank" rel="noopener">iRotAvg, Why bundle adjust?</a>,ICRA 2019,<strong>[<a href="https://cs.adelaide.edu.au/~aparra/publication/parra19_icra/" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li><a href="https://github.com/Kelym/FAST" target="_blank" rel="noopener">Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation</a>,CVPR 2019,<strong>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Ke_Tactical_Rewind_Self-Correction_via_Backtracking_in_Vision-And-Language_Navigation_CVPR_2019_paper.html" target="_blank" rel="noopener">PDF</a>]</strong>ï¼Œè§†è§‰+è¯­è¨€å¯¼èˆª</li>
<li><a href="https://github.com/MISTLab/DOOR-SLAM" target="_blank" rel="noopener">DOOR-SLAM</a></li>
<li><a href="https://github.com/JiawangBian/FM-Bench" target="_blank" rel="noopener">An Evaluation of Feature Matchers for Fundamental Matrix Estimation</a>,BMVC 2019,<strong>[<a href="https://jwbian.net/Papers/FM_BMVC19.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://jwbian.net/fm-bench" target="_blank" rel="noopener">Project Page</a>]</strong>ï¼Œç‰¹å¾åŒ¹é…</li>
<li><a href="https://github.com/hyye/lio-mapping" target="_blank" rel="noopener">A Tightly Coupled 3D Lidar and Inertial Odometry and Mapping Approach</a>,ICRA 2019,<strong>[<a href="https://arxiv.org/abs/1904.06993" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://sites.google.com/view/lio-mapping" target="_blank" rel="noopener">Project Page</a>]</strong>ï¼Œç´§è€¦åˆé›·è¾¾+IMU SLAM</li>
<li><a href="https://github.com/LRMPUT/PlaneSLAM" target="_blank" rel="noopener">On the Representation of Planes for Efficient Graph-based SLAM with High-level Features</a>,åˆ©ç”¨å¹³é¢ä¿¡æ¯çš„SLAM</li>
<li><a href="https://github.com/Huangying-Zhan/DF-VO" target="_blank" rel="noopener">Visual Odometry Revisited: What Should Be Learnt?</a>,arXiv 2019,<strong>[<a href="https://arxiv.org/abs/1909.09803" target="_blank" rel="noopener">PDF</a>]</strong>, æ·±åº¦å­¦ä¹ æ·±åº¦+å…‰æµè¿›è¡ŒVO</li>
<li><a href="https://github.com/Xylon-Sean/rfnet" target="_blank" rel="noopener">RF-Net: An End-to-End Image Matching Network based on Receptive Field</a>,CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1906.00604" target="_blank" rel="noopener">PDF</a>]</strong>, ç«¯åˆ°ç«¯å›¾åƒåŒ¹é…</li>
<li><a href="https://github.com/HKUST-Aerial-Robotics/Fast-Planner" target="_blank" rel="noopener">Fast-Planner</a>,IEEE Robotics and Automation Letters (RA-L), 2019,<strong>[<a href="https://ieeexplore.ieee.org/document/8758904" target="_blank" rel="noopener">PDF</a>]</strong>, æ— äººæœºè½¨è¿¹ç”Ÿæˆ</li>
<li><a href="https://github.com/dongjing3309/minisam" target="_blank" rel="noopener">A general and flexible factor graph non-linear least square optimization framework</a>,CoRR 2019,<strong>[<a href="http://arxiv.org/abs/1909.00903" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://minisam.readthedocs.io/" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/gao-ouyang/demo_for_kalmanFilter" target="_blank" rel="noopener">Demo for Kalman filter in ranging system</a>,å¡å°”æ›¼æ»¤æ³¢åŸç†æ¼”ç¤º</li>
<li><a href="https://github.com/Ahmedest61/CNN-Region-VLAD-VPR" target="_blank" rel="noopener">A Holistic Visual Place Recognition Approach using Lightweight CNNs for Severe ViewPoint and Appearance Changes</a>ï¼Œåœºæ™¯è¯†åˆ«ï¼ˆå¤–è§‚ä¸è§†è§’å˜åŒ–æ—¶ï¼‰,<a href="https://github.com/ethz-asl/hierarchical_loc" target="_blank" rel="noopener">è®­ç»ƒå’Œéƒ¨ç½²æºç </a></li>
<li><a href="https://github.com/uzh-rpg/sips2_open" target="_blank" rel="noopener">SIPs: Succinct Interest Points from Unsupervised Inlierness Probability Learning</a>,3D Vision (3DV) 2019,<strong>[<a href="https://arxiv.org/abs/1805.01358" target="_blank" rel="noopener">PDF</a>]</strong>ï¼ŒRPGå®éªŒå®¤å‡ºå“ï¼Œæ·±åº¦å­¦ä¹ ç‰¹å¾ç‚¹ï¼ˆæœ‰ç‰¹å¾æè¿°å­ï¼‰</li>
<li><a href="https://github.com/uzh-rpg/imips_open" target="_blank" rel="noopener">Matching Features Without Descriptors: Implicitly Matched Interest Points</a>,BMVC 2019,<strong>[<a href="http://rpg.ifi.uzh.ch/docs/BMVC19_Cieslewski.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,RPGå®éªŒå®¤å‡ºå“ï¼Œæ— éœ€ç‰¹å¾æè¿°å³å¯è¿›è¡Œç‰¹å¾åŒ¹é…</li>
<li><a href="https://github.com/cardwing/Codes-for-Lane-Detection" target="_blank" rel="noopener">Learning Lightweight Lane Detection CNNs by Self Attention Distillation (ICCV 2019)</a>,ICCV 2019,<strong>[<a href="https://arxiv.org/abs/1908.00821" target="_blank" rel="noopener">PDF</a>]</strong>ï¼Œæ·±åº¦å­¦ä¹ é“è·¯æ£€æµ‹</li>
<li><a href="https://github.com/youngguncho/awesome-slam-datasets" target="_blank" rel="noopener">Awesome SLAM Datasets</a>,å²ä¸Šæœ€å…¨SLAMæ•°æ®é›†ï¼Œ <strong><a href="https://mp.weixin.qq.com/s/BzcghUnXTR9RQqA3Pc9MhA" target="_blank" rel="noopener">å…¬ä¼—å·è¯´æ˜: æœ€å…¨ SLAM å¼€æºæ•°æ®é›†</a></strong></li>
<li><a href="https://github.com/Aceinna/gnss-ins-sim" target="_blank" rel="noopener">GNSS-INS-SIM</a>,æƒ¯å¯¼èåˆæ¨¡æ‹Ÿå™¨ï¼Œæ”¯æŒIMUæ•°æ®ï¼Œè½¨è¿¹ç”Ÿæˆç­‰</li>
<li><a href="https://github.com/2013fangwentao/Multi-Sensor-Combined-Navigation" target="_blank" rel="noopener">Multi-Sensor Combined Navigation Program(GNSS, IMU, Camera and so on) å¤šæºå¤šä¼ æ„Ÿå™¨èåˆå®šä½ GPS/INSç»„åˆå¯¼èˆª</a></li>
<li><a href="https://github.com/scape-research/SOSNet" target="_blank" rel="noopener">SOSNet: Second Order Similarity Regularization for Local Descriptor Learning</a>,CVPR 2019,<strong><a href="https://research.scape.io/sosnet/" target="_blank" rel="noopener">[Project page]</a></strong> <strong><a href="https://arxiv.org/abs/1904.05019" target="_blank" rel="noopener">[Paper]</a></strong> <strong><a href="imgs/sosnet-poster.pdf">[Poster]</a></strong> <strong><a href="imgs/sosnet-oral.pdf">[Slides]</a></strong>ï¼Œä¸€ç§æ·±åº¦å­¦ä¹ ç‰¹å¾æè¿°å­</li>
<li><a href="https://github.com/oravus/seq2single" target="_blank" rel="noopener">Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation</a>,ICRA 2019,<strong>[<a href="https://arxiv.org/abs/1902.07381" target="_blank" rel="noopener">PDF</a>]</strong>,åˆ©ç”¨æ·±åº¦å›¾åƒå®ç°äº†å¤§è§†è§’é•¿æ—¶é—´çš„åœºæ™¯è¯†åˆ«ï¼ˆæ ¹æ®æ·±åº¦å›¾ç­›é€‰å¾—åˆ°ä¸åŒæ·±åº¦å±‚æ¬¡çš„ç‰¹å¾ç‚¹ç„¶åä¸å½“å‰å¸§è¿›è¡ŒåŒ¹é…ï¼Œæé«˜äº†åœºæ™¯å¬å›ç‡ï¼‰</li>
<li><a href="https://github.com/rpng/calc2.0" target="_blank" rel="noopener">CALC2.0</a>,Convolutional Autoencoder for Loop Closure 2.0,ç”¨äºé—­ç¯æ£€æµ‹</li>
<li><a href="https://github.com/ethz-asl/segmap" target="_blank" rel="noopener">SegMap</a>,RSS 2018,<strong>[<a href="http://www.roboticsproceedings.org/rss14/p03.pdf" target="_blank" rel="noopener">PDF</a>]</strong>, ä¸€ç§åŸºäº3Dçº¿æ®µçš„åœ°å›¾è¡¨ç¤ºï¼Œå¯ç”¨äºåœºæ™¯è¯†åˆ«/æœºå™¨äººå®šä½/ç¯å¢ƒé‡å»ºç­‰</li>
<li><a href="https://github.com/cggos/msckf_vio_cg" target="_blank" rel="noopener">MSCKF_VIO</a>, a stereo version of MSCKFï¼ŒåŸºäºMSCKFçš„åŒç›®VIO</li>
<li><a href="https://github.com/Relja/netvlad" target="_blank" rel="noopener">NetVLAD: CNN architecture for weakly supervised place recognition</a>ï¼ŒCVPR 2016, CNNæ¡†æ¶å¼±ç›‘ç£å­¦ä¹ åœºæ™¯è¯†åˆ«,<strong>[<a href="https://www.di.ens.fr/willow/research/netvlad/" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/IFL-CAMP/easy_handeye" target="_blank" rel="noopener">easy_handeye</a>,Simple, straighforward ROS library for hand-eye calibration</li>
<li><a href="https://github.com/KinglittleQ/SuperPoint_SLAM" target="_blank" rel="noopener">SuperPoint-SLAM</a>,åˆ©ç”¨SuperPointæ›¿æ¢ORBç‰¹å¾ç‚¹</li>
<li><a href="https://github.com/facebookresearch/pyrobot" target="_blank" rel="noopener">PyRobot: An Open Source Robotics Research Platform</a></li>
<li><a href="https://github.com/ethz-asl/hfnet" target="_blank" rel="noopener">From Coarse to Fine: Robust Hierarchical Localization at Large Scale with HF-Net</a>,<strong>[<a href="https://arxiv.org/abs/1812.03506" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li><a href="https://github.com/mp3guy/ICPCUDA" target="_blank" rel="noopener">Super fast implementation of ICP in CUDA</a></li>
<li><a href="https://github.com/ethz-asl/volumetric_mapping" target="_blank" rel="noopener"> A generic interface for disparity map and pointcloud insertion</a></li>
<li><a href="https://github.com/tdsuper/SPHORB" target="_blank" rel="noopener">SPHORB: A Fast and Robust Binary Feature on the Sphere</a>,International Journal of Computer Vision 2015,<strong>[<a href="http://scs.tju.edu.cn/~lwan/paper/SPHORB/pdf/SPHORB-final-small.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://scs.tju.edu.cn/~lwan/paper/SPHORB/SPHORB.html" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/ETH3D/badslam" target="_blank" rel="noopener">BADSLAM: Bundle Adjusted Direct RGB-D SLAM</a>,CVPR 2019,<strong>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Schops_BAD_SLAM_Bundle_Adjusted_Direct_RGB-D_SLAM_CVPR_2019_paper.pdf" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li><a href="https://github.com/uzh-rpg/rpg_e2vid" target="_blank" rel="noopener">High Speed and High Dynamic Range Video with an Event Camera</a>,arXiv,<strong>[<a href="http://rpg.ifi.uzh.ch/docs/arXiv19_Rebecq.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://rpg.ifi.uzh.ch/E2VID.html" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/PaoPaoRobot/Awesome-VIO" target="_blank" rel="noopener">Awesome-VIO</a>,Discuss about VIO in PaoPaoRobot group</li>
<li><a href="https://github.com/XinLiGH/GyroAllan" target="_blank" rel="noopener">GyroAllan</a>,é™€èºä»ªéšæœºè¯¯å·®çš„ Allan æ–¹å·®åˆ†æ, <a href="https://github.com/rpng/kalibr_allan" target="_blank" rel="noopener">Another version</a></li>
<li><a href="https://github.com/fangchangma/self-supervised-depth-completion" target="_blank" rel="noopener">Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and Monocular Camera</a>,ICRA 2019,<strong>[<a href="https://arxiv.org/pdf/1807.00275.pdf" target="_blank" rel="noopener">PDF</a>]</strong>, ä¼˜åŒ–LiDARä»¥åŠå•ç›®å¾—åˆ°çš„æ·±åº¦å›¾</li>
<li><a href="https://github.com/NVlabs/planercnn" target="_blank" rel="noopener">PlaneRCNN: 3D Plane Detection and Reconstruction from a Single Image</a>,CVPR 2019,<strong>[<a href="https://arxiv.org/pdf/1812.04072.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://research.nvidia.com/publication/2019-06_PlaneRCNN" target="_blank" rel="noopener">Project Page</a>]</strong>,é€šè¿‡å•å¹…å›¾åƒè¿›è¡Œ3Då¹³é¢æ£€æµ‹ä»¥åŠé‡å»º</li>
<li><a href="https://github.com/kokerf/DBow3" target="_blank" rel="noopener">DBow3</a>,æ³¨é‡Šç‰ˆçš„DBow3ä»£ç </li>
<li><a href="https://github.com/VladyslavUsenko/basalt-mirror" target="_blank" rel="noopener">Visual-Inertial Mapping with Non-Linear Factor Recovery</a>,<strong>[<a href="https://arxiv.org/abs/1904.06504" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://vision.in.tum.de/research/vslam/basalt" target="_blank" rel="noopener">Project Page</a>]</strong>, æ—¶ç©ºè”åˆçš„VIOä¼˜åŒ–æ–¹æ¡ˆ</li>
<li><a href="https://github.com/PaoPaoRobot/ICRA2019-paper-list" target="_blank" rel="noopener">ICRA2019-paper-list</a>,ICRA 2019è®ºæ–‡åˆ—è¡¨ï¼ˆæ³¡æ³¡æœºå™¨äººå‡ºå“æš‚æ—¶æ— é“¾æ¥ï¼‰</li>
<li><a href="https://github.com/pedropro/CAPE" target="_blank" rel="noopener">Fast Cylinder and Plane Extraction from Depth Cameras for Visual Odometry</a>, IROS 2018,<strong>[<a href="https://arxiv.org/abs/1803.02380" target="_blank" rel="noopener">PDF</a>]</strong>,åˆ©ç”¨æ·±åº¦å›¾è¿›è¡Œåœ†æŸ±æ£€æµ‹ä»¥åŠå¹³é¢æ£€æµ‹è¿›è¡ŒVO</li>
<li><a href="https://github.com/kiran-mohan/SLAM-Algorithms-Octave" target="_blank" rel="noopener">Solutions to assignments of Robot Mapping Course WS 2013/14 by Dr. Cyrill Stachniss at University of Freiburg</a>,SLAMç®—æ³•å­¦ä¹ è¯¾åä½œä¸šç­”æ¡ˆ</li>
<li><a href="https://github.com/RonaldSun/VI-Stereo-DSO" target="_blank" rel="noopener">Direct sparse odometry combined with stereo cameras and IMU</a>,åŒç›®DSO+IMU</li>
<li><a href="https://github.com/HorizonAD/stereo_dso" target="_blank" rel="noopener">Direct Sparse Odometry with Stereo Cameras</a>,åŒç›®DSO</li>
<li><a href="https://github.com/uoip/g2opy" target="_blank" rel="noopener">Python binding of SLAM graph optimization framework g2o</a>,pythonç‰ˆæœ¬çš„g2oå®ç°</li>
<li><a href="https://github.com/rpautrat/SuperPoint" target="_blank" rel="noopener">SuperPoint: Self-Supervised Interest Point Detection and Description</a>, CVPR 2018, <strong>[<a href="https://arxiv.org/abs/1712.07629" target="_blank" rel="noopener">Paper</a>]</strong>, æ·±åº¦å­¦ä¹ æè¿°å­+æè¿°</li>
<li><a href="https://github.com/lzx551402/contextdesc" target="_blank" rel="noopener">ContextDesc: Local Descriptor Augmentation with Cross-Modality Context</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1904.04084" target="_blank" rel="noopener">Paper</a>]</strong>, æ·±åº¦å­¦ä¹ æè¿°å­</li>
<li><a href="https://github.com/mihaidusmanu/d2-net" target="_blank" rel="noopener">D2-Net: A Trainable CNN for Joint Description and Detection of Local Features</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1905.03561" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://dsmn.ml/publications/d2-net.html" target="_blank" rel="noopener">Project Page</a>]</strong>, æ·±åº¦å­¦ä¹ å…³é”®ç‚¹+æè¿°</li>
<li><a href="https://github.com/ethz-asl/orb_slam_2_ros" target="_blank" rel="noopener">ROS interface for ORBSLAM2</a>,ROSç‰ˆæœ¬çš„ORBSLAM2</li>
<li><a href="https://github.com/yan99033/CNN-SVO" target="_blank" rel="noopener">CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction</a>ï¼Œ <strong>[<a href="https://arxiv.org/pdf/1810.01011.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/ManiiXu/VINS-Mono-Learning" target="_blank" rel="noopener">VINS-Mono-Learning</a>ï¼Œä»£ç æ³¨é‡Šç‰ˆVINS-Monoï¼Œåˆå­¦è€…å­¦ä¹ </li>
<li><a href="https://github.com/xdspacelab/openvslam" target="_blank" rel="noopener">OpenVSLAM: Versatile Visual SLAM Framework</a>, <strong>[<a href="https://openvslam.readthedocs.io/" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/fabianschenk/RESLAM" target="_blank" rel="noopener">RESLAM: A real-time robust edge-based SLAM system</a>, ICRA 2019, <strong>[<a href="https://github.com/fabianschenk/fabianschenk.github.io/raw/master/files/schenk_icra_2019.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/rubengooj/pl-slam" target="_blank" rel="noopener">PL-SLAM: a Stereo SLAM System through the Combination of Points and Line Segments</a>, <strong>[<a href="https://arxiv.org/abs/1705.09479" target="_blank" rel="noopener">Paper</a>]</strong>ï¼Œçº¿ç‰¹å¾SLAM</li>
<li><a href="https://github.com/YipuZhao/GF_PL_SLAM" target="_blank" rel="noopener">Good Line Cutting: towards Accurate Pose Tracking of Line-assisted VO/VSLAM</a>, ECCV 2018, <strong>[<a href="https://sites.google.com/site/zhaoyipu/good-feature-visual-slam" target="_blank" rel="noopener">Project Page</a>]</strong>, æ”¹è¿›çš„PL-SLAM</li>
<li><a href="https://github.com/leoshine/Spherical_Regression" target="_blank" rel="noopener">Spherical Regression: Learning Viewpoints, Surface Normals and 3D Rotations on n-Spheres</a>, CVPR 2019, <strong>[<a href="http://arxiv.org/abs/1904.05404" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/icsl-Jeon/traj_gen_vis" target="_blank" rel="noopener">svo_edgelet</a>, åœ¨çº¿è½¨è¿¹ç”Ÿæˆ</li>
<li><a href="https://github.com/TimboKZ/caltech_samaritan" target="_blank" rel="noopener">Drone SLAM project for Caltechâ€™s ME 134 Autonomy class</a>, <strong>[<a href="https://github.com/TimboKZ/caltech_samaritan/blob/master/CS134_Final_Project_Report.pdf" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li><a href="https://github.com/icsl-Jeon/traj_gen_vis" target="_blank" rel="noopener">Online Trajectory Generation of a MAV for Chasing a Moving Target in 3D Dense Environments</a>, <strong>[<a href="https://arxiv.org/pdf/1904.03421.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/AtsushiSakai/PythonRobotics" target="_blank" rel="noopener">PythonRobotics</a>,<strong>[<a href="https://arxiv.org/abs/1808.10703" target="_blank" rel="noopener">Paper</a>]</strong>, <a href="https://github.com/onlytailei/CppRobotics" target="_blank" rel="noopener">CppRobotics</a></li>
<li><a href="https://github.com/izhengfan/ba_demo_ceres" target="_blank" rel="noopener">Bundle adjustment demo using Ceres Solver</a>,  <strong>[<a href="https://fzheng.me/2018/01/23/ba-demo-ceres/" target="_blank" rel="noopener">Blog</a>]</strong>, cereså®ç°BA</li>
<li><a href="https://github.com/shichaoy/cube_slam" target="_blank" rel="noopener">CubeSLAM: Monocular 3D Object Detection and SLAM</a>, <strong>[<a href="https://arxiv.org/abs/1806.00557" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/sshaoshuai/PointRCNN" target="_blank" rel="noopener">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.04244" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/nrupatunga/GIST-global-Image-Descripor" target="_blank" rel="noopener">GIST-Global Image Descriptor</a>, GISTæè¿°å­</li>
<li><a href="https://github.com/ethz-asl/mav_voxblox_planning" target="_blank" rel="noopener">mav voxblox planning</a>, MAV planning tools using voxblox as the map representation.</li>
<li><a href="https://github.com/zziz/kalman-filter" target="_blank" rel="noopener">Python Kalman Filter</a>, 30è¡Œå®ç°å¡å°”æ›¼æ»¤æ³¢</li>
<li><a href="https://github.com/arpg/vicalib" target="_blank" rel="noopener">vicalib</a>, è§†è§‰æƒ¯å¯¼ç³»ç»Ÿæ ‡å®šå·¥å…·</li>
<li><a href="https://github.com/simondlevy/BreezySLAM" target="_blank" rel="noopener">BreezySLAM</a>, åŸºäºé›·è¾¾çš„SLAMï¼Œæ”¯æŒPython(&amp;Matlab, C++, and Java)</li>
<li><a href="https://github.com/Yvon-Shong/Probabilistic-Robotics" target="_blank" rel="noopener">Probabilistic-Robotics</a>, ã€Šæ¦‚ç‡æœºå™¨äººã€‹ä¸­æ–‡ç‰ˆï¼Œä¹¦å’Œè¯¾åä¹ é¢˜</li>
<li><a href="https://github.com/emmjaykay/stanford_self_driving_car_code" target="_blank" rel="noopener">Stanford Self Driving Car Code</a>, <strong>[<a href="http://robots.stanford.edu/papers/junior08.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, æ–¯å¦ç¦è‡ªåŠ¨é©¾é©¶è½¦ä»£ç </li>
<li><a href="https://github.com/ndrplz/self-driving-car" target="_blank" rel="noopener">Udacity Self-Driving Car Engineer Nanodegree projects</a></li>
<li><a href="https://github.com/TUMFTM/Lecture_AI_in_Automotive_Technology" target="_blank" rel="noopener">Artificial Intelligence in Automotive Technology</a>, TUMè‡ªåŠ¨é©¾é©¶æŠ€æœ¯ä¸­çš„äººå·¥æ™ºèƒ½è¯¾ç¨‹</li>
<li><a href="https://github.com/hlzz/DeepMatchVO" target="_blank" rel="noopener">DeepMatchVO: Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation</a>,ICRA 2019, <strong>[<a href="https://arxiv.org/abs/1902.09103" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/zdzhaoyong/GSLAM" target="_blank" rel="noopener">GSLAM: A General SLAM Framework and Benchmark</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1902.07995" target="_blank" rel="noopener">Paper</a>]</strong>, é›†æˆäº†å„ç§ä¼ æ„Ÿå™¨è¾“å…¥çš„SLAMç»Ÿä¸€æ¡†æ¶</li>
<li><a href="https://github.com/izhengfan/se2lam" target="_blank" rel="noopener">Visual-Odometric Localization and Mapping for Ground Vehicles Using SE(2)-XYZ Constraints</a>ï¼ŒICRA 2019,åŸºäºSE(2)-XYZçº¦æŸçš„VOç³»ç»Ÿ</li>
<li><a href="https://github.com/nicolov/simple_slam_loop_closure" target="_blank" rel="noopener">Simple bag-of-words loop closure for visual SLAM</a>, <strong>[<a href="https://nicolovaligi.com/bag-of-words-loop-closure-visual-slam.html" target="_blank" rel="noopener">Blog</a>]</strong>, å›ç¯</li>
<li><a href="https://github.com/rmsalinas/fbow" target="_blank" rel="noopener">FBOW (Fast Bag of Words), an extremmely optimized version of the DBow2/DBow3 libraries</a>,ä¼˜åŒ–ç‰ˆæœ¬çš„DBow2/DBow3</li>
<li><a href="https://github.com/tomas789/tonav" target="_blank" rel="noopener">Multi-State Constraint Kalman Filter (MSCKF) for Vision-aided Inertial Navigation(masterâ€™s thesis)</a></li>
<li><a href="https://github.com/yuzhou42/MSCKF" target="_blank" rel="noopener">MSCKF</a>, MSCKFä¸­æ–‡æ³¨é‡Šç‰ˆ</li>
<li><a href="https://github.com/hbtang/calibcamodo" target="_blank" rel="noopener">Calibration algorithm for a camera odometry system</a>, VOç³»ç»Ÿçš„æ ‡å®šç¨‹åº</li>
<li><a href="https://github.com/cggos/vins_mono_cg" target="_blank" rel="noopener">Modified version of VINS-Mono</a>, æ³¨é‡Šç‰ˆæœ¬VINS Mono</li>
<li><a href="https://github.com/zhenpeiyang/RelativePose" target="_blank" rel="noopener">Extreme Relative Pose Estimation for RGB-D Scans via Scene Completion</a>,<strong>[<a href="https://arxiv.org/abs/1901.00063" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/jessecw/EPnP_Eigen" target="_blank" rel="noopener">Implementation of EPnP algorithm with Eigen</a>,åˆ©ç”¨Eigenç¼–å†™çš„EPnP</li>
<li><a href="https://github.com/jiexiong2016/GCNv2_SLAM" target="_blank" rel="noopener">Real-time SLAM system with deep features</a>, æ·±åº¦å­¦ä¹ æè¿°å­(ORB vs. GCNv2)</li>
<li><a href="https://github.com/Huangying-Zhan/Depth-VO-Feat" target="_blank" rel="noopener">Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction</a>, CVPR 2018, æ— ç›‘ç£å•ç›®æ·±åº¦æ¢å¤ä»¥åŠVO</li>
<li><a href="https://github.com/Phylliida/orbslam-windows" target="_blank" rel="noopener">ORB-SLAM-windows</a>, Windowsç‰ˆæœ¬çš„ORB-SLAM</li>
<li><a href="https://github.com/danping/structvio" target="_blank" rel="noopener">StructVIO : Visual-inertial Odometry with Structural Regularity of Man-made Environments</a>,<strong>[<a href="http://drone.sjtu.edu.cn/dpzou/project/structvio.html" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/irvingzhang/KalmanFiltering" target="_blank" rel="noopener">KalmanFiltering</a>, å„ç§å¡å°”æ›¼æ»¤æ³¢å™¨çš„demo</li>
<li><a href="https://github.com/ZhenghaoFei/visual_odom" target="_blank" rel="noopener">Stereo Odometry based on careful Feature selection and Tracking</a>, <strong>[<a href="https://lamor.fer.hr/images/50020776/Cvisic2017.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, C++ OpenCVå®ç°SOFT</li>
<li><a href="https://github.com/dzunigan/zSLAM" target="_blank" rel="noopener">Visual SLAM with RGB-D Cameras based on Pose Graph Optimization</a></li>
<li><a href="https://github.com/drsrinathsridhar/GRANSAC" target="_blank" rel="noopener">Multi-threaded generic RANSAC implemetation</a>, å¤šçº¿ç¨‹RANSAC</li>
<li><a href="https://github.com/PyojinKim/OPVO" target="_blank" rel="noopener">Visual Odometry with Drift-Free Rotation Estimation Using Indoor Scene Regularities</a>, BMVC 2017, <strong>[<a href="http://pyojinkim.me/pub/Visual-Odometry-with-Drift-Free-Rotation-Estimation-Using-Indoor-Scene-Regularities/" target="_blank" rel="noopener">Project Page</a>]</strong>ï¼Œåˆ©ç”¨å¹³é¢æ­£äº¤ä¿¡æ¯è¿›è¡ŒVO</li>
<li><a href="https://github.com/baidu/ICE-BA" target="_blank" rel="noopener">ICE-BA</a>, CVPR 2018, <strong>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_ICE-BA_Incremental_Consistent_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/AIBluefisher/GraphSfM" target="_blank" rel="noopener">GraphSfM: Robust and Efficient Graph-based Structure from Motion</a>, <strong>[<a href="https://aibluefisher.github.io/GraphSfM/" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/cuitaixiang/LOAM_NOTED" target="_blank" rel="noopener">LOAM_NOTED</a>, loamä¸­æ–‡æ³¨è§£ç‰ˆ</li>
<li><a href="https://github.com/Ethan-Zhou/MWO" target="_blank" rel="noopener">Divide and Conquer: Effcient Density-Based Tracking of 3D Sensors in Manhattan Worlds</a>,ACCV 2016,<strong>[<a href="http://users.cecs.anu.edu.au/~u5535909/" target="_blank" rel="noopener">Project Page</a>]</strong>,æ›¼å“ˆé¡¿ä¸–ç•Œåˆ©ç”¨æ·±åº¦ä¼ æ„Ÿå™¨è¿›è¡Œæ—‹è½¬é‡å¹³ç§»é‡åˆ†ç¦»ä¼˜åŒ–</li>
<li><a href="https://github.com/jstraub/rtmf" target="_blank" rel="noopener">Real-time Manhattan World Rotation Estimation in 3D</a>,IROS 2015,å®æ—¶æ›¼å“ˆé¡¿ä¸–ç•Œæ—‹è½¬ä¼°è®¡</li>
<li><a href="https://github.com/uzh-rpg/event-based_vision_resources" target="_blank" rel="noopener">Event-based Vision Resources</a>ï¼Œå…³äºäº‹ä»¶ç›¸æœºçš„èµ„æº</li>
<li><a href="https://github.com/DeepTecher/AutonomousVehiclePaper" target="_blank" rel="noopener">AutonomousVehiclePaper</a>ï¼Œæ— äººé©¾é©¶ç›¸å…³è®ºæ–‡é€Ÿé€’</li>
<li><a href="https://github.com/wutianyiRosun/Segmentation.X" target="_blank" rel="noopener">Segmentation.X</a>, Segmentationç›¸å…³è®ºæ–‡&amp;ä»£ç </li>
<li><a href="https://github.com/amusi/CVPR2019-Code" target="_blank" rel="noopener">CVPR-2019</a>, CVPR 2019 è®ºæ–‡å¼€æºé¡¹ç›®åˆé›†</li>
<li><a href="https://github.com/kanster/awesome-slam" target="_blank" rel="noopener">awesome-slam</a>, SLAMåˆé›†</li>
<li><a href="https://github.com/tzutalin/awesome-visual-slam" target="_blank" rel="noopener">awesome-visual-slam</a>, è§†è§‰SLAMåˆé›†</li>
<li><a href="https://github.com/zziz/pwc" target="_blank" rel="noopener">Papers with code</a>, å‘¨æ›´è®ºæ–‡withä»£ç </li>
<li><a href="https://github.com/cbsudux/awesome-human-pose-estimation" target="_blank" rel="noopener">Awesome Human Pose Estimation</a>,<a href="https://github.com/nkalavak/awesome-object-pose" target="_blank" rel="noopener">awesome-object-pose</a>, ä½å§¿ä¼°è®¡åˆé›†</li>
<li><a href="https://github.com/Ewenwan/MVision" target="_blank" rel="noopener">MVision</a>, å¤§ç¤¼åŒ…ï¼šæœºå™¨äººè§†è§‰ ç§»åŠ¨æœºå™¨äºº VS-SLAM ORB-SLAM2 æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹ yolov3 è¡Œä¸ºæ£€æµ‹ opencv PCL æœºå™¨å­¦ä¹  æ— äººé©¾é©¶</li>
</ul>
<h2 id="Pose-Object-tracking"><a href="#Pose-Object-tracking" class="headerlink" title="Pose/Object tracking"></a>Pose/Object tracking</h2><ul>
<li><a href="https://github.com/KovenYu/MAR" target="_blank" rel="noopener">Unsupervised person re-identification by soft multilabel learning</a>,CVPR 2019,  <strong>[<a href="https://kovenyu.com/papers/2019_CVPR_MAR.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/tianzhi0549/FCOS" target="_blank" rel="noopener">FCOS: Fully Convolutional One-Stage Object Detection</a>,ICCV 2019,  <strong>[<a href="https://arxiv.org/abs/1904.01355" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/yangli18/hand_detection" target="_blank" rel="noopener">Hand Detection and Orientation Estimation</a></li>
<li><a href="https://github.com/Wanggcong/Spatial-Temporal-Re-identification" target="_blank" rel="noopener">Spatial-Temporal Person Re-identification</a>,AAAI 2019,<strong>[<a href="https://arxiv.org/abs/1812.03282" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/layumi/Person_reID_baseline_pytorch" target="_blank" rel="noopener">A tiny, friendly, strong pytorch implement of person re-identification baseline. <strong>Tutorial</strong></a>,CVPR 2019,  <strong>[<a href="https://arxiv.org/abs/1904.07223" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/tengteng95/Pose-Transfer" target="_blank" rel="noopener">Progressive Pose Attention for Person Image Generation</a>,CVPR 2019,<strong>[<a href="http://arxiv.org/abs/1904.03349" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/shamangary/FSA-Net" target="_blank" rel="noopener">FSA-Net: Learning Fine-Grained Structure Aggregation for Head Pose Estimation from a Single Image</a>, CVPR 2019,<strong>[<a href="https://github.com/shamangary/FSA-Net/blob/master/0191.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/yuanyuanli85/Fast_Human_Pose_Estimation_Pytorch" target="_blank" rel="noopener">An unoffical implemention for paper â€œFast Human Pose Estimationâ€</a>, CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1811.05419" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/edvardHua/PoseEstimationForMobile" target="_blank" rel="noopener">Real-time single person pose estimation for Android and iOS</a>,æ‰‹æœºç«¯å®ç°äººä½“ä½å§¿ä¼°è®¡</li>
<li><a href="https://github.com/cbsudux/Human-Pose-Estimation-101" target="_blank" rel="noopener">Basics of 2D and 3D Human Pose Estimation</a>,äººä½“å§¿æ€ä¼°è®¡å…¥é—¨</li>
<li><a href="https://github.com/OceanPang/Libra_R-CNN" target="_blank" rel="noopener">Libra R-CNN: Towards Balanced Learning for Object Detection</a></li>
<li><a href="https://github.com/HRNet/HRNet-Object-Detection" target="_blank" rel="noopener">High-resolution networks (HRNets) for object detection</a>, <strong>[<a href="https://arxiv.org/pdf/1904.04514.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/xiaolonw/TimeCycle" target="_blank" rel="noopener">Learning Correspondence from the Cycle-Consistency of Time</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1903.07593" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/zju3dv/pvnet" target="_blank" rel="noopener">PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.11788" target="_blank" rel="noopener">Paper</a>], [<a href="https://zju3dv.github.io/pvnet" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/mkocabas/EpipolarPose" target="_blank" rel="noopener">Self-Supervised Learning of 3D Human Pose using Multi-view Geometry</a>, CVPR 2018, <strong>[<a href="https://arxiv.org/abs/1903.02330" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/vita-epfl/openpifpaf" target="_blank" rel="noopener">PifPaf: Composite Fields for Human Pose Estimation</a>, <strong>[<a href="https://arxiv.org/abs/1903.06593" target="_blank" rel="noopener">Paper</a>]</strong> </li>
<li><a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" target="_blank" rel="noopener">Deep High-Resolution Representation Learning for Human Pose Estimation</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09212.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://jingdongwang2017.github.io/Projects/HRNet/PoseEstimation.html" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/YuliangXiu/PoseFlow" target="_blank" rel="noopener">PoseFlow: Efficient Online Pose Tracking)</a>, BMVC 2018, <strong>[<a href="https://arxiv.org/abs/1802.00977" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/vana77/Bottom-up-Clustering-Person-Re-identification" target="_blank" rel="noopener">A Bottom-Up Clustering Approach to Unsupervised Person Re-identification</a>ï¼ŒAAAI 2019, é‡å®šä½</li>
<li><a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener">Fast Online Object Tracking and Segmentation: A Unifying Approach</a>,CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1812.05050" target="_blank" rel="noopener">Paper</a>] [<a href="https://youtu.be/I_iOVrcpEBw" target="_blank" rel="noopener">Video</a>] [<a href="http://www.robots.ox.ac.uk/~qwang/SiamMask" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/TuSimple/simpledet" target="_blank" rel="noopener">SimpleDet - A Simple and Versatile Framework for Object Detection and Instance Recognition</a>,<strong>[<a href="https://arxiv.org/abs/1903.05831" target="_blank" rel="noopener">Paper</a>]</strong> </li>
</ul>
<h2 id="Depth-Disparity-amp-Flow-estimation"><a href="#Depth-Disparity-amp-Flow-estimation" class="headerlink" title="Depth/Disparity &amp; Flow estimation"></a>Depth/Disparity &amp; Flow estimation</h2><ul>
<li>[<strong>Depth</strong>]<a href="https://github.com/ethan-li-coding/SemiGlobalMatching" target="_blank" rel="noopener">SemiGlobalMatching</a>, SGMåŒç›®ç«‹ä½“åŒ¹é…ç®—æ³•å®Œæ•´å®ç°ï¼Œä»£ç è§„èŒƒï¼Œæ³¨é‡Šä¸°å¯Œä¸”æ¸…æ™°ï¼ŒCSDNåŒæ­¥æ•™å­¦</li>
<li><a href="https://github.com/callmeray/PointMVSNet" target="_blank" rel="noopener">PointMVSNet: Point-based Multi-view Stereo Network</a>,ICCV 2019,<strong>[<a href="https://arxiv.org/abs/1908.04422" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/JiaxiongQ/DeepLiDAR" target="_blank" rel="noopener">DeepLiDAR</a>,CVPR 2019, <strong>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qiu_DeepLiDAR_Deep_Surface_Normal_Guided_Depth_Prediction_for_Outdoor_Scene_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, å•å¼ RGBå›¾åƒ+ç¨€ç–é›·è¾¾æ•°æ®è¿›è¡Œå®¤å¤–åœºæ™¯æ·±åº¦ä¼°è®¡</li>
<li><a href="https://github.com/atapour/monocularDepth-Inference" target="_blank" rel="noopener">Real-Time Monocular Depth Estimation using Synthetic Data with Domain Adaptation via Image Style Transfer</a>,CVPR 2018, <strong>[<a href="http://breckon.eu/toby/publications/papers/abarghouei18monocular.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/princeton-vl/YouTube3D" target="_blank" rel="noopener">Learning Single-Image Depth from Videos using Quality Assessment Networks</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1806.09573" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="http://www-personal.umich.edu/~wfchen/youtube3d/" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/WERush/SCDA" target="_blank" rel="noopener">SCDA: Adapting Object Detectors via Selective Cross-Domain Alignment</a>,CVPR 2019, <strong>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Adapting_Object_Detectors_via_Selective_Cross-Domain_Alignment_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="http://zhuxinge.me/aboutme.html" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/fabiotosi92/monoResMatch-Tensorflow" target="_blank" rel="noopener">Learning monocular depth estimation infusing traditional stereo knowledge</a>,CVPR 2019,<strong>[<a href="https://vision.disi.unibo.it/~ftosi/papers/monoResMatch.pdf" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li><a href="https://github.com/laoreja/HPLFlowNet" target="_blank" rel="noopener">HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on Large-scale Point Clouds</a>,CVPR 2019,<strong>[<a href="hhttps://web.cs.ucdavis.edu/~yjlee/projects/cvpr2019-HPLFlowNet.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/feihuzhang/GANet" target="_blank" rel="noopener">GA-Net: Guided Aggregation Net for End-to-end Stereo Matching</a>,CVPR 2019,<strong>[<a href="https://arxiv.org/pdf/1904.06587.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/sunghoonim/DPSNet" target="_blank" rel="noopener">DPSNet: End-to-end Deep Plane Sweep Stereo</a>,ICLR 2019,<strong>[<a href="https://openreview.net/pdf?id=ryeYHi0ctQ" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/muskie82/AR-Depth-cpp" target="_blank" rel="noopener">Fast Depth Densification for Occlusion-aware Augmented Reality</a>, SIGGRAPH-Asia 2018, <strong>[<a href="https://homes.cs.washington.edu/~holynski/publications/occlusion/index.html" target="_blank" rel="noopener">Project Page</a>]</strong>,<a href="https://github.com/facebookresearch/AR-Depth" target="_blank" rel="noopener">another version</a></li>
<li><a href="https://github.com/CVLAB-Unibo/Learning2AdaptForStereo" target="_blank" rel="noopener">Learning To Adapt For Stereo</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1904.02957" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/JiaRenChang/PSMNet" target="_blank" rel="noopener">Pyramid Stereo Matching Network</a>,<strong>[<a href="https://arxiv.org/abs/1803.08669" target="_blank" rel="noopener">Paper</a>]</strong> </li>
<li><a href="https://github.com/lelimite4444/BridgeDepthFlow" target="_blank" rel="noopener">Bridging Stereo Matching and Optical Flow via Spatiotemporal Correspondence</a>, <strong>[<a href="https://arxiv.org/abs/1905.09265" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/wvangansbeke/Sparse-Depth-Completion" target="_blank" rel="noopener">Sparse Depth Completion</a>, <strong>[<a href="https://arxiv.org/pdf/1902.05356.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, RGBå›¾åƒè¾…åŠ©é›·è¾¾æ·±åº¦ä¼°è®¡</li>
<li><a href="https://github.com/sshan-zhao/GASDA" target="_blank" rel="noopener">GASDA</a>, CVPR 2019, <strong>[<a href="https://sshan-zhao.github.io/papers/gasda.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/xy-guo/MVSNet_pytorch" target="_blank" rel="noopener">MVSNet: Depth Inference for Unstructured Multi-view Stereo</a>, <strong>[<a href="https://arxiv.org/abs/1804.02505" target="_blank" rel="noopener">Paper</a>]</strong>, éå®˜æ–¹å®ç°ç‰ˆæœ¬çš„MVSNet</li>
<li><a href="https://github.com/HKUST-Aerial-Robotics/Stereo-RCNN" target="_blank" rel="noopener">Stereo R-CNN based 3D Object Detection for Autonomous Driving</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09738.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo" target="_blank" rel="noopener">Real-time self-adaptive deep stereo</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1810.05424" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/ialhashim/DenseDepth" target="_blank" rel="noopener">High Quality Monocular Depth Estimation via Transfer Learning</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.11941" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://ialhashim.github.io/publications/index.html" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/xy-guo/GwcNet" target="_blank" rel="noopener">Group-wise Correlation Stereo Network</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1903.04025" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/phuang17/DeepMVS" target="_blank" rel="noopener">DeepMVS: Learning Multi-View Stereopsis</a>, CVPR 2018,<strong>[<a href="https://phuang17.github.io/DeepMVS/index.html" target="_blank" rel="noopener">Project Page</a>]</strong>,å¤šç›®æ·±åº¦ä¼°è®¡</li>
<li><a href="https://github.com/sampepose/flownet2-tf" target="_blank" rel="noopener">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</a>, CVPR 2017, æ·±åº¦å­¦ä¹ å…‰æµæ¢å¤</li>
<li><a href="https://github.com/DLuensch/StereoVision-ADCensus" target="_blank" rel="noopener">StereoVision-ADCensus</a>,æ·±åº¦æ¢å¤ä»£ç é›†åˆ(<strong>ADCensus, SGBM, BM</strong>)</li>
<li><a href="https://github.com/yangguorun/SegStereo" target="_blank" rel="noopener">SegStereo: Exploiting Semantic Information for Disparity Estimation</a>, æ¢ç©¶è¯­ä¹‰ä¿¡æ¯åœ¨æ·±åº¦ä¼°è®¡ä¸­çš„ä½œç”¨</li>
<li><a href="https://github.com/kuantingchen04/Light-Field-Depth-Estimation" target="_blank" rel="noopener">Light Filed Depth Estimation using GAN</a>ï¼Œåˆ©ç”¨GANè¿›è¡Œå…‰åœºæ·±åº¦æ¢å¤</li>
<li><a href="https://github.com/daniilidis-group/EV-FlowNet" target="_blank" rel="noopener">EV-FlowNet: Self-Supervised Optical Flow for Event-based Cameras</a>,Proceedings of Robotics 2018,<strong>[<a href="https://arxiv.org/abs/1802.06898" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/vt-vl-lab/DF-Net" target="_blank" rel="noopener">DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency</a>, ECCV 2018, <strong>[<a href="https://arxiv.org/abs/1809.01649" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/yzcjtr/GeoNet" target="_blank" rel="noopener">GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose</a>, CVPR 2018, <strong>[<a href="https://arxiv.org/abs/1803.02276" target="_blank" rel="noopener">Paper</a>]</strong></li>
</ul>
<h2 id="3D-amp-Graphic"><a href="#3D-amp-Graphic" class="headerlink" title="3D &amp; Graphic"></a>3D &amp; Graphic</h2><ul>
<li><a href="https://github.com/WangYueFt/prnet" target="_blank" rel="noopener">PRNet: Self-Supervised Learning for Partial-to-Partial Registration</a>,NeurIPS 2019</li>
<li><a href="https://github.com/nkolot/SPIN" target="_blank" rel="noopener">Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop</a>,ICCV 2019, <strong>[<a href="https://arxiv.org/pdf/1909.12828.pdf" target="_blank" rel="noopener">Paper</a>]</strong> , <strong>[<a href="https://www.seas.upenn.edu/~nkolot/projects/spin/" target="_blank" rel="noopener">Project Page</a>]</strong> </li>
<li><a href="https://github.com/microsoft/multiview-human-pose-estimation-pytorch" target="_blank" rel="noopener">Cross View Fusion for 3D Human Pose Estimation</a>,ICCV 2019, <strong>[<a href="https://arxiv.org/abs/1909.01203" target="_blank" rel="noopener">Paper</a>]</strong> ,è·¨è§†è§’3Dä½å§¿ä¼°è®¡</li>
<li><a href="https://github.com/Fanziapril/mvfnet" target="_blank" rel="noopener">MVF-Net: Multi-View 3D Face Morphable Model Regression</a>,å¤šè§†è§’3Däººè„¸é‡å»º, <strong>[<a href="https://arxiv.org/abs/1904.04473" target="_blank" rel="noopener">Paper</a>]</strong> </li>
<li><a href="https://github.com/saurabheights/KillingFusion" target="_blank" rel="noopener">KillingFusion</a></li>
<li><a href="https://github.com/PRBonn/refusion" target="_blank" rel="noopener">ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals</a>, <strong>[<a href="https://arxiv.org/pdf/1905.02082.pdf" target="_blank" rel="noopener">Paper</a>]</strong> </li>
<li><a href="https://github.com/Lotayou/densebody_pytorch" target="_blank" rel="noopener">densebody_pytorch</a>, <strong>[<a href="https://arxiv.org/abs/1903.10153v3" target="_blank" rel="noopener">Paper</a>]</strong> </li>
<li><a href="https://github.com/svip-lab/PlanarReconstruction" target="_blank" rel="noopener">Single-Image Piece-wise Planar 3D Reconstruction via Associative Embedding</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09777.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, å•ç›®3Dé‡å»º</li>
<li><a href="https://github.com/sunset1995/HorizonNet" target="_blank" rel="noopener">HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch Data Augmentation</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1901.03861" target="_blank" rel="noopener">Paper</a>]</strong>, æ·±åº¦å­¦ä¹ å…¨æ™¯è½¬3D</li>
<li><a href="https://github.com/Microsoft/O-CNN" target="_blank" rel="noopener">Adaptive O-CNN: A Patch-based Deep Representation of 3D Shapes</a>,SIGGRAPH Asia 2018, <strong>[<a href="https://wang-ps.github.io/AO-CNN.html" target="_blank" rel="noopener">Project Page</a>]</strong></li>
</ul>
<h2 id="Other-Collections"><a href="#Other-Collections" class="headerlink" title="Other Collections"></a>Other Collections</h2><ul>
<li><a href="https://github.com/timqian/chinese-independent-blogs" target="_blank" rel="noopener">chinese-independent-blogs</a>, ä¸­æ–‡ç‹¬ç«‹åšå®¢é›†é”¦</li>
<li><a href="https://github.com/RenYurui/StructureFlow" target="_blank" rel="noopener">StructureFlow: Image Inpainting via Structure-aware Appearance Flow</a>,å›¾åƒinpainting</li>
<li><a href="https://github.com/ruanyf/free-books" target="_blank" rel="noopener">free-books</a>,äº’è”ç½‘ä¸Šçš„å…è´¹ä¹¦ç±</li>
<li><a href="https://github.com/academicpages/academicpages.github.io" target="_blank" rel="noopener">AcademicPages</a>,é€šç”¨çš„å­¦æœ¯ä¸»é¡µæ¨¡ç‰ˆ</li>
<li><a href="https://github.com/microsoft/MMdnn" target="_blank" rel="noopener">MMdnn</a>,å®ç°æ·±åº¦å­¦ä¹ æ¨¡å‹ä¹‹é—´çš„ç›¸äº’è½¬æ¢</li>
<li><a href="https://github.com/abner2015/tensorflow2caffemodel" target="_blank" rel="noopener">tensorflow2caffemodel</a>,tensorflowæ¨¡å‹è½¬caffemodel</li>
<li><a href="https://github.com/fengdu78/lihang-code" target="_blank" rel="noopener">lihang-code</a>,ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹çš„ä»£ç å®ç°</li>
<li><a href="https://github.com/DLTcollab/sse2neon" target="_blank" rel="noopener">sse2neon</a>,<a href="https://github.com/jratcliff63367/sse2neon" target="_blank" rel="noopener">sse2neon</a>,SSEè½¬neonï¼ŒåµŒå…¥å¼ç§»æ¤æ—¶å¯èƒ½ä¼šç”¨åˆ°;</li>
<li><a href="https://github.com/alirezadir/Production-Level-Deep-Learning" target="_blank" rel="noopener">Production-Level-Deep-Learning</a>,æ·±åº¦å­¦ä¹ æ¨¡å‹éƒ¨ç½²æµç¨‹</li>
<li><a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch" target="_blank" rel="noopener">åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ Dive-into-DL-PyTorch</a></li>
<li><a href="https://github.com/deeplearning-ai/machine-learning-yearning-cn" target="_blank" rel="noopener">machine-learning-yearning-cn</a>ï¼ŒMachine Learning Yearning ä¸­æ–‡ç‰ˆ - ã€Šæœºå™¨å­¦ä¹ è®­ç»ƒç§˜ç±ã€‹ - Andrew Ng è‘—</li>
<li><a href="https://github.com/academicpages/academicpages.github.io" target="_blank" rel="noopener">academicpages.github.io</a>ï¼Œå­¦æœ¯ä¸»é¡µæ¨¡æ¿</li>
<li><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">Coursera-ML-AndrewNg-Notes</a>,å´æ©è¾¾è€å¸ˆçš„æœºå™¨å­¦ä¹ è¯¾ç¨‹ä¸ªäººç¬”è®°</li>
<li><a href="https://github.com/roboticcam/machine-learning-notes" target="_blank" rel="noopener">machine-learning-notes</a>,æœºå™¨å­¦ä¹ ï¼Œæ¦‚ç‡æ¨¡å‹å’Œæ·±åº¦å­¦ä¹ çš„è®²ä¹‰(1500+é¡µ)å’Œè§†é¢‘é“¾æ¥</li>
<li><a href="https://github.com/scutan90/CNN-Visualization" target="_blank" rel="noopener">CNN-Visualization</a>,CNNå¯è§†åŒ–ã€ç†è§£CNN</li>
<li><a href="https://github.com/mrgloom/awesome-semantic-segmentation" target="_blank" rel="noopener">Awesome Semantic Segmentation</a>, è¯­ä¹‰åˆ†å‰²é›†åˆ</li>
<li><a href="https://github.com/mengyuest/iros2018-slam-papers" target="_blank" rel="noopener">IROS2018 SLAM Collections</a>, IROS 2018é›†åˆ</li>
<li><a href="https://github.com/TerenceCYJ/VP-SLAM-SC-papers" target="_blank" rel="noopener">VP-SLAM-SC-papers</a>,Visual Positioning &amp; SLAM &amp; Spatial Cognition è®ºæ–‡ç»Ÿè®¡ä¸åˆ†æ</li>
<li><a href="https://github.com/HuaizhengZhang/Awesome-System-for-Machine-Learning" target="_blank" rel="noopener">Awesome System for Machine Learning</a></li>
<li><a href="https://github.com/Thinkgamer/Machine-Learning-With-Python" target="_blank" rel="noopener">Machine-Learning-With-Python</a>, ã€Šæœºå™¨å­¦ä¹ å®æˆ˜ã€‹pythonä»£ç å®ç°</li>
<li><a href="https://github.com/qqfly/how-to-learn-robotics" target="_blank" rel="noopener">How to learn robotics</a>, å¼€æºæœºå™¨äººå­¦å­¦ä¹ æŒ‡å—</li>
<li><a href="https://github.com/kjw0612/awesome-deep-vision" target="_blank" rel="noopener">Awesome Deep Vision</a>,DLåœ¨CVé¢†åŸŸçš„åº”ç”¨</li>
<li><a href="https://github.com/YapengTian/Single-Image-Super-Resolution" target="_blank" rel="noopener">Single-Image-Super-Resolution</a>, ä¸€ä¸ªæœ‰å…³<strong>å›¾åƒè¶…åˆ†è¾¨</strong>çš„åˆé›†</li>
<li><a href="https://github.com/wifity/ai-report" target="_blank" rel="noopener">ai report</a>, AIç›¸å…³çš„ç ”ç©¶æŠ¥å‘Š</li>
<li><a href="https://paperswithcode.com/sota" target="_blank" rel="noopener">State-of-the-art papers and code</a>,æœé›†äº†ç›®å‰sotaçš„è®ºæ–‡ä»¥åŠä»£ç </li>
<li><a href="https://github.com/extreme-assistant/cvpr2019" target="_blank" rel="noopener">CVPR 2019 (Papers/Codes/Project/Paper reading)</a></li>
<li><a href="https://github.com/openMVG/awesome_3DReconstruction_list" target="_blank" rel="noopener">A curated list of papers &amp; resources linked to 3D reconstruction from images</a>,æœ‰å…³ä¸‰ç»´é‡å»ºçš„è®ºæ–‡æ±‡æ€»</li>
<li><a href="https://github.com/nebula-beta/SLAM-Jobs" target="_blank" rel="noopener">SLAM-Jobs</a>, SLAM/SFMæ±‚èŒæŒ‡å—</li>
<li><a href="https://github.com/stevewongv/SPANet" target="_blank" rel="noopener">Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset</a>,CVPR 2019,å»é›¨</li>
<li><a href="https://github.com/hezhangsprinter/DCPDN" target="_blank" rel="noopener">Densely Connected Pyramid Dehazing Network</a>,CVPR 2018,å»é›¾</li>
<li><a href="https://github.com/open-mmlab/mmsr" target="_blank" rel="noopener">MMSR</a>ï¼ŒMMLABæ¨å‡ºçš„è¶…åˆ†è¾¨å·¥å…·ç®±</li>
<li><a href="https://github.com/Bartzi/stn-ocr" target="_blank" rel="noopener">æ·±åº¦å­¦ä¹ OCR</a></li>
<li><a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">è¥¿ç“œä¹¦ğŸ‰å­¦ä¹ ç¬”è®°</a></li>
<li><a href="https://github.com/wwxFromTju/awesome-reinforcement-learning-zh" target="_blank" rel="noopener">awesome-reinforcement-learning-zh</a>,å¼ºåŒ–å­¦ä¹ ä»å…¥é—¨åˆ°æ”¾å¼ƒçš„èµ„æ–™</li>
<li><a href="https://github.com/cszn/DPSR" target="_blank" rel="noopener">Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels</a>,CVPR 2019,è¶…åˆ†è¾¨</li>
<li><a href="https://github.com/lzhbrian/Cool-Fashion-Papers" target="_blank" rel="noopener">Cool Fashion Papers</a>, Cool resources about Fashion + AI.</li>
<li><a href="https://github.com/nbei/Deep-Flow-Guided-Video-Inpainting" target="_blank" rel="noopener">Deep Flow-Guided Video Inpainting</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1806.10447.pdf" target="_blank" rel="noopener">Paper</a>]</strong> ,å›¾åƒä¿®å¤</li>
<li><a href="https://github.com/dbolya/yolact" target="_blank" rel="noopener">YOLACT: Real-time Instance Segmentation</a></li>
<li><a href="https://github.com/lyl8213/Plate_Recognition-LPRnet" target="_blank" rel="noopener">LPRNet: License Plate Recognition via Deep Neural Networks</a>, <strong>[<a href="https://arxiv.org/pdf/1806.10447.pdf" target="_blank" rel="noopener">Paper</a>]</strong> </li>
<li><a href="https://github.com/xiaofengShi/CHINESE-OCR" target="_blank" rel="noopener">CHINESE-OCR</a>, è¿ç”¨tfå®ç°è‡ªç„¶åœºæ™¯æ–‡å­—æ£€æµ‹</li>
<li><a href="https://github.com/PerpetualSmile/BeautyCamera" target="_blank" rel="noopener">BeautyCamera</a>, ç¾é¢œç›¸æœºï¼Œå…·æœ‰äººè„¸æ£€æµ‹ã€ç£¨çš®ç¾ç™½äººè„¸ã€æ»¤é•œã€è°ƒèŠ‚å›¾ç‰‡ã€æ‘„åƒåŠŸèƒ½</li>
<li><a href="https://github.com/zhengzhugithub/CV-arXiv-Daily" target="_blank" rel="noopener">CV-arXiv-Daily</a>, åˆ†äº«è®¡ç®—æœºè§†è§‰æ¯å¤©çš„arXivæ–‡ç« </li>
<li><a href="https://github.com/lyndonzheng/Pluralistic-Inpainting" target="_blank" rel="noopener">Pluralistic-Inpainting</a>, <a href="https://arxiv.org/abs/1903.04227" target="_blank" rel="noopener">ArXiv</a> | <a href="http://www.chuanxiaz.com/publication/pluralistic/" target="_blank" rel="noopener">Project Page</a> | <a href="http://www.chuanxiaz.com/project/pluralistic/" target="_blank" rel="noopener">Online Demo</a> | <a href="https://www.youtube.com/watch?v=9V7rNoLVmSs" target="_blank" rel="noopener">Video(demo)</a></li>
<li><a href="https://github.com/Jezzamonn/fourier" target="_blank" rel="noopener">An Interactive Introduction to Fourier Transforms</a>, è¶…æ£’çš„å‚…é‡Œå¶å˜æ¢å›¾å½¢åŒ–è§£é‡Š</li>
<li><a href="https://github.com/datawhalechina/pumpkin-book" target="_blank" rel="noopener">pumpkin-book</a>, ã€Šæœºå™¨å­¦ä¹ ã€‹ï¼ˆè¥¿ç“œä¹¦ï¼‰å…¬å¼æ¨å¯¼è§£æ</li>
<li><a href="https://github.com/JuliaLang/julia" target="_blank" rel="noopener">Julia</a></li>
<li><a href="https://github.com/alan-turing-institute/MLJ.jl" target="_blank" rel="noopener">A Julia machine learning framework</a>ï¼Œä¸€ç§åŸºäºJuliaçš„æœºå™¨å­¦ä¹ æ¡†æ¶</li>
<li><a href="https://github.com/ZhaoJ9014/face.evoLVe.PyTorch" target="_blank" rel="noopener">High-Performance Face Recognition Library on PyTorch</a>ï¼Œäººè„¸è¯†åˆ«åº“</li>
<li><a href="https://github.com/enggen/Deep-Learning-Coursera" target="_blank" rel="noopener">Deep-Learning-Coursera</a>ï¼Œæ·±åº¦å­¦ä¹ æ•™ç¨‹ï¼ˆdeeplearning.aiï¼‰</li>
<li><a href="https://github.com/RemoteML/bestofml" target="_blank" rel="noopener">The best resources around Machine Learning</a></li>
<li><a href="https://github.com/cydonia999/VGGFace2-pytorch" target="_blank" rel="noopener">VGGFace2: A dataset for recognising faces across pose and age</a></li>
<li><a href="https://github.com/SmirkCao/Lihang" target="_blank" rel="noopener">Statistical learning methods</a>ï¼Œç»Ÿè®¡å­¦ä¹ æ–¹æ³•</li>
<li><a href="https://live.bilibili.com/7332534?visit_id=9ytrx9lpsy80" target="_blank" rel="noopener">End-to-end Adversarial Learning for Generative Conversational Agents</a>ï¼Œ2017ï¼Œä»‹ç»äº†ä¸€ç§ç«¯åˆ°ç«¯çš„åŸºäºGANçš„èŠå¤©æœºå™¨äºº</li>
<li><a href="https://github.com/yulunzhang/RNAN" target="_blank" rel="noopener">Residual Non-local Attention Networks for Image Restoration</a>,ICLR 2019.</li>
<li><a href="https://github.com/HelenMao/MSGAN" target="_blank" rel="noopener">MSGAN: Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis</a>, CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1903.05628" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/NVlabs/SPADE" target="_blank" rel="noopener">SPADE: Semantic Image Synthesis with Spatially-Adaptive Normalization</a>,CVPR 2019, <strong>[<a href="https://nvlabs.github.io/SPADE/" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/Oldpan/Faceswap-Deepfake-Pytorch" target="_blank" rel="noopener">Faceswap with Pytorch or DeepFake with Pytorch</a>, æ¢è„¸</li>
<li><a href="https://github.com/iperov/DeepFaceLab" target="_blank" rel="noopener">DeepFaceLab</a>, æ¢è„¸</li>
</ul>
<!-- ![](https://cdn.jsdelivr.net/gh/Vincentqyw/blog-resources/westlake/westlake-1.jpg) -->
    </div>

    
    
    
        
      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Vincent Qin</li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://www.vincentqin.tech/posts/awesome-works/" title="ğŸ”¥Awesome CV Works">https://www.vincentqin.tech/posts/awesome-works/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.</li>
</ul>
</div>

      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/SLAM/" rel="tag"># SLAM</a>
            
              <a href="/tags/disparity/" rel="tag"># disparity</a>
            
              <a href="/tags/pose-tracking/" rel="tag"># pose-tracking</a>
            
              <a href="/tags/object-tracking/" rel="tag"># object-tracking</a>
            
              <a href="/tags/depth-estimation/" rel="tag"># depth-estimation</a>
            
              <a href="/tags/flow-estimation/" rel="tag"># flow-estimation</a>
            
              <a href="/tags/3D-graphics/" rel="tag"># 3D-graphics</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/posts/build-ssr-server/" rel="next" title="å¼€å¯SSRæ¨¡å¼">
                  <i class="fa fa-chevron-left"></i> å¼€å¯SSRæ¨¡å¼
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/posts/first-black-hole/" rel="prev" title="Black Hole">
                  Black Hole <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="comments"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#SLAM-related"><span class="nav-number">1.</span> <span class="nav-text">SLAM related</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pose-Object-tracking"><span class="nav-number">2.</span> <span class="nav-text">Pose/Object tracking</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Depth-Disparity-amp-Flow-estimation"><span class="nav-number">3.</span> <span class="nav-text">Depth/Disparity &amp; Flow estimation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3D-amp-Graphic"><span class="nav-number">4.</span> <span class="nav-text">3D &amp; Graphic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Other-Collections"><span class="nav-number">5.</span> <span class="nav-text">Other Collections</span></a></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="https://vincentqin.gitee.io/images/qin_small.png"
      alt="Vincent Qin">
  <p class="site-author-name" itemprop="name">Vincent Qin</p>
  <div class="site-description" itemprop="description">Keep Your Curiosity</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">56</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">106</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/Vincentqyw" title="GitHub &rarr; https://github.com/Vincentqyw" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:realcat@126.com" title="Email &rarr; mailto:realcat@126.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>Email</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://vincentqin.gitee.io/images/qrcode_realcat.jpg" title="Wechat &rarr; https://vincentqin.gitee.io/images/qrcode_realcat.jpg" rel="noopener" target="_blank"><i class="fa fa-fw fa-weixin"></i>Wechat</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.zhihu.com/people/i_vincent/activities" title="Zhihu &rarr; https://www.zhihu.com/people/i_vincent/activities" rel="noopener" target="_blank"><i class="fa fa-fw fa-quora"></i>Zhihu</a>
      </span>
    
  </div>



  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-dashboard"></i>
      Scholar
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://xxx.itp.ac.cn" title="http://xxx.itp.ac.cn" rel="noopener" target="_blank">arxiv</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://arxiv-sanity.com/" title="http://arxiv-sanity.com/" rel="noopener" target="_blank">arxiv-sanity</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://openaccess.thecvf.com/menu.py" title="http://openaccess.thecvf.com/menu.py" rel="noopener" target="_blank">CVF</a>
        </li>
      
    </ul>
  </div>



  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-battery-three-quarters"></i>
      Friends Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.wangpengan.com/" title="http://www.wangpengan.com/" rel="noopener" target="_blank">Tensorboy</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://simtalk.cn/" title="http://simtalk.cn/" rel="noopener" target="_blank">Simshang</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://sttomato.github.io" title="https://sttomato.github.io" rel="noopener" target="_blank">Tomato</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://newdee.cf/" title="https://newdee.cf/" rel="noopener" target="_blank">Newdee</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://cs-people.bu.edu/yfhu/" title="http://cs-people.bu.edu/yfhu/" rel="noopener" target="_blank">WhoIf</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://yulunzhang.com/" title="http://yulunzhang.com/" rel="noopener" target="_blank">Yulun</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://sanglongbest.github.io/" title="https://sanglongbest.github.io/" rel="noopener" target="_blank">YangLiu</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://erenship.com/" title="https://erenship.com/" rel="noopener" target="_blank">Eren</a>
        </li>
      
    </ul>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-briefcase"></i>
      å¸¸ç”¨é“¾æ¥
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://realcat.avosapps.us" title="https://realcat.avosapps.us" rel="noopener" target="_blank">è¯„è®ºç®¡ç†</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://gitee.com/vincentqin/vincentqin" title="https://gitee.com/vincentqin/vincentqin" rel="noopener" target="_blank">ç½‘ç«™æºç </a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://xyz.realcat.icu:65432" title="http://xyz.realcat.icu:65432" rel="noopener" target="_blank">ä¸å¯æè¿°</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://www.notion.so/realcat" title="https://www.notion.so/realcat" rel="noopener" target="_blank">Notion</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://www.matrixcalculus.org/" title="http://www.matrixcalculus.org/" rel="noopener" target="_blank">çŸ©é˜µæ±‚å¯¼</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://emojipedia.org/" title="https://emojipedia.org/" rel="noopener" target="_blank">Emoji</a>
        </li>
      
    </ul>
  </div>


  <div class="feed-link motion-element">
    <a href="https://realcat.vercel.app/" rel="alternate">
       <i class="fa fa-home"></i>Homepage Backup
    </a>
  </div>


      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2016 â€“ <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vincent Qin</span>
</div>

  <script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
  <script>
    function timer() {
      var ages = moment.preciseDiff(moment(),moment(20160701,"YYYYMMDD"));
      ages = ages.replace(/years?/, "å¹´");
      ages = ages.replace(/months?/, "æœˆ");
      ages = ages.replace(/days?/, "å¤©");
      ages = ages.replace(/hours?/, "å°æ—¶");
      ages = ages.replace(/minutes?/, "åˆ†");
      ages = ages.replace(/seconds?/, "ç§’");
      ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
      div.innerHTML = `å·²è¿è¡Œ ${ages}`;
    }
    var div = document.createElement("div");
    //æ’å…¥åˆ°copyrightä¹‹å
    var copyright = document.querySelector(".copyright");
    document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
    timer();
    setInterval("timer()",1000)
  </script>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="Total Visitors">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="Total Views">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>


  <div style="display: none;">
    <script src="//s95.cnzz.com/z_stat.php?id=1273219530&web_id=1273219530"></script>
  </div>






  <script>
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=65489609";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
<script src="/js/utils.js?v=7.4.0"></script>
<script src="/js/schemes/pisces.js?v=7.4.0"></script>

<script src="/js/next-boot.js?v=7.4.0"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>















<script>
if (document.querySelectorAll('div.pdf').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/pdfobject@2/pdfobject.min.js', () => {
    document.querySelectorAll('div.pdf').forEach(element => {
      PDFObject.embed(element.getAttribute('target'), element, {
        pdfOpenParams: {
          navpanes: 0,
          toolbar: 0,
          statusbar: 0,
          pagemode: 'thumbs',
          view: 'FitH'
        },
        PDFJS_URL: '/lib/pdf/web/viewer.html',
        height: element.getAttribute('height') || '500px'
      });
    });
  }, window.PDFObject);
}
</script>


<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme: 'forest',
      logLevel: 3,
      flowchart: { curve: 'linear' },
      gantt: { axisFormat: '%m/%d/%Y' },
      sequence: { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>




  

  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: true,
    notify: false,
    appId: 'E2yzANt8H4UiIuw4c95dTaXH-MdYXbMMI',
    appKey: 'NF1yxeki6kw4KM5glHkwjvKc',
    placeholder: 'Just go go',
    avatar: 'wavatar',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn',
    path: location.pathname
  });
}, window.Valine);
</script>


  





  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>



</body>
</html>
