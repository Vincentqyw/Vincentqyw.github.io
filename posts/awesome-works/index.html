<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/realcat-apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/realcat-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/realcat-32x32.png">
  <link rel="mask-icon" href="/images/realcat-safari-pinned-tab.svg" color="#222">
  <meta name="google-site-verification" content="u46QTaG_Dv3OZLpOBKYtqyuiNtIdnhSG5ASKoNvGBCM">
  <meta name="baidu-site-verification" content="MtcbwE45ft">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.vincentqin.tech","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.13.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"show_result":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":"waline","storage":true,"lazyload":true,"nav":null,"activeClass":"waline"},"stickytabs":true,"motion":{"enable":false,"async":true,"transition":{"post_block":"fadeIn","post_header":"fadeIn","post_body":"fadeIn","coll_header":"fadeIn","sidebar":"fadeIn"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="The post contains papers-with-code about SLAM, Pose&#x2F;Object tracking, Depth&#x2F;Disparity&#x2F;Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc.">
<meta property="og:type" content="article">
<meta property="og:title" content="ğŸ”¥Awesome CV Works">
<meta property="og:url" content="https://www.vincentqin.tech/posts/awesome-works/index.html">
<meta property="og:site_name" content="RealCat">
<meta property="og:description" content="The post contains papers-with-code about SLAM, Pose&#x2F;Object tracking, Depth&#x2F;Disparity&#x2F;Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://vincentqin.tech/blog-resources/awesome-works/github-star.png">
<meta property="og:image" content="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg">
<meta property="og:image" content="http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019.svg">
<meta property="og:image" content="https://img.shields.io/badge/license-Anti%20996-blue.svg?style=flat-square">
<meta property="article:published_time" content="2019-03-31T12:15:41.000Z">
<meta property="article:modified_time" content="2022-09-04T18:19:11.061Z">
<meta property="article:author" content="Vincent Qin">
<meta property="article:tag" content="SLAM">
<meta property="article:tag" content="pose-tracking">
<meta property="article:tag" content="object-tracking">
<meta property="article:tag" content="disparity">
<meta property="article:tag" content="depth-estimation">
<meta property="article:tag" content="flow-estimation">
<meta property="article:tag" content="3D-graphics">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://vincentqin.tech/blog-resources/awesome-works/github-star.png">


<link rel="canonical" href="https://www.vincentqin.tech/posts/awesome-works/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://www.vincentqin.tech/posts/awesome-works/","path":"posts/awesome-works/","title":"ğŸ”¥Awesome CV Works"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>ğŸ”¥Awesome CV Works | RealCat</title>
  
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-97856334-1","only_pageview":true}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>





<link rel="dns-prefetch" href="https://comments.vincentqin.tech">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">RealCat</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Turn on, Tune in, Drop out</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">77</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">14</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">111</span></a></li><li class="menu-item menu-item-collections"><a href="/collections" rel="section"><i class="fa fa-diamond fa-fw"></i>Collections</a></li><li class="menu-item menu-item-guest_comments"><a href="/guestbook" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#slam-related"><span class="nav-number">1.</span> <span class="nav-text">SLAM related</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#poseobject-tracking"><span class="nav-number">2.</span> <span class="nav-text">Pose&#x2F;Object tracking</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#depthdisparity-flow-estimation"><span class="nav-number">3.</span> <span class="nav-text">Depth&#x2F;Disparity &amp; Flow
estimation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#d-graphic"><span class="nav-number">4.</span> <span class="nav-text">3D &amp; Graphic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#other-collections"><span class="nav-number">5.</span> <span class="nav-text">Other Collections</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Vincent Qin"
      src="https://vincentqin.gitee.io/images/qin_small.png">
  <p class="site-author-name" itemprop="name">Vincent Qin</p>
  <div class="site-description" itemprop="description">Keep Your Curiosity</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">77</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">111</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Vincentqyw" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;Vincentqyw" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:realcat@126.com" title="Email â†’ mailto:realcat@126.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>Email</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://vincentqin.gitee.io/images/qrcode_realcat.jpg" title="Wechat â†’ https:&#x2F;&#x2F;vincentqin.gitee.io&#x2F;images&#x2F;qrcode_realcat.jpg" rel="noopener" target="_blank"><i class="fab fa-weixin fa-fw"></i>Wechat</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/i_vincent/activities" title="Zhihu â†’ https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;i_vincent&#x2F;activities" rel="noopener" target="_blank"><i class="fab fa-quora fa-fw"></i>Zhihu</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/AlphaRealcat" title="Twitter â†’ https:&#x2F;&#x2F;twitter.com&#x2F;AlphaRealcat" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/18136563" title="Bilibili â†’ https:&#x2F;&#x2F;space.bilibili.com&#x2F;18136563" rel="noopener" target="_blank"><i class="fa fa-video-camera fa-fw"></i>Bilibili</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://bafybeic2jt62kpyh6cz2g4ngxs4kazojfw3dhx53mco3wc6f56dejty4xm.ipfs.infura-ipfs.io/" title="Web3.0 â†’ https:&#x2F;&#x2F;bafybeic2jt62kpyh6cz2g4ngxs4kazojfw3dhx53mco3wc6f56dejty4xm.ipfs.infura-ipfs.io" rel="noopener" target="_blank"><i class="link fa-fw"></i>Web3.0</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-dashboard"></i>
      Scholar
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://xxx.itp.ac.cn/" title="http:&#x2F;&#x2F;xxx.itp.ac.cn" rel="noopener" target="_blank">Arxiv-Mirror</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://arxiv-sanity.com/" title="http:&#x2F;&#x2F;arxiv-sanity.com&#x2F;" rel="noopener" target="_blank">Arxiv-sanity</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://openaccess.thecvf.com/menu.py" title="http:&#x2F;&#x2F;openaccess.thecvf.com&#x2F;menu.py" rel="noopener" target="_blank">CVF</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://paperswithcode.com/sota" title="https:&#x2F;&#x2F;paperswithcode.com&#x2F;sota" rel="noopener" target="_blank">Paper&Code</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://scihub.wikicn.top/" title="https:&#x2F;&#x2F;scihub.wikicn.top&#x2F;" rel="noopener" target="_blank">Scihub</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://ras.papercept.net/conferences/scripts/start.pl" title="http:&#x2F;&#x2F;ras.papercept.net&#x2F;conferences&#x2F;scripts&#x2F;start.pl" rel="noopener" target="_blank">RAS</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://openreview.net/" title="https:&#x2F;&#x2F;openreview.net&#x2F;" rel="noopener" target="_blank">OpenReview</a>
        </li>
    </ul>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-battery-three-quarters fa-fw"></i>
      Friends Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.wangpengan.com/" title="http:&#x2F;&#x2F;www.wangpengan.com&#x2F;" rel="noopener" target="_blank">Tensorboy</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://simtalk.cn/" title="http:&#x2F;&#x2F;simtalk.cn&#x2F;" rel="noopener" target="_blank">Simshang</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sttomato.github.io/" title="https:&#x2F;&#x2F;sttomato.github.io" rel="noopener" target="_blank">Tomato</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://dfine.tech/" title="http:&#x2F;&#x2F;dfine.tech&#x2F;" rel="noopener" target="_blank">Newdee</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://cs-people.bu.edu/yfhu/" title="http:&#x2F;&#x2F;cs-people.bu.edu&#x2F;yfhu&#x2F;" rel="noopener" target="_blank">WhoIf</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://yulunzhang.com/" title="http:&#x2F;&#x2F;yulunzhang.com&#x2F;" rel="noopener" target="_blank">Yulun</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sanglongbest.github.io/" title="https:&#x2F;&#x2F;sanglongbest.github.io&#x2F;" rel="noopener" target="_blank">YangLiu</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.erenship.com/" title="https:&#x2F;&#x2F;www.erenship.com&#x2F;" rel="noopener" target="_blank">Eren</a>
        </li>
    </ul>
  </div>

  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-briefcase"></i>
      Common Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://comments.vincentqin.tech/ui" title="https:&#x2F;&#x2F;comments.vincentqin.tech&#x2F;ui" rel="noopener" target="_blank">Comments</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://gitee.com/vincentqin/vincentqin" title="https:&#x2F;&#x2F;gitee.com&#x2F;vincentqin&#x2F;vincentqin" rel="noopener" target="_blank">Source</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.notion.so/realcat" title="https:&#x2F;&#x2F;www.notion.so&#x2F;realcat" rel="noopener" target="_blank">Notion</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.matrixcalculus.org/" title="http:&#x2F;&#x2F;www.matrixcalculus.org&#x2F;" rel="noopener" target="_blank">Calculus</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://emojipedia.org/" title="https:&#x2F;&#x2F;emojipedia.org&#x2F;" rel="noopener" target="_blank">Emoji</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://unstoppabledomains.com/" title="https:&#x2F;&#x2F;unstoppabledomains.com&#x2F;" rel="noopener" target="_blank">UD</a>
        </li>
    </ul>
  </div>




        </div>

      <div class="wechat_QR_code">
      <!-- äºŒç»´ç  -->
      <img src ="https://vincentqin.tech/blog-resources/qrcode_realcat.jpg">
      <span>Follow Me on Wechat</span>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://www.vincentqin.tech/posts/awesome-works/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://vincentqin.gitee.io/images/qin_small.png">
      <meta itemprop="name" content="Vincent Qin">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RealCat">
      <meta itemprop="description" content="Keep Your Curiosity">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="ğŸ”¥Awesome CV Works | RealCat">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ğŸ”¥Awesome CV Works
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-03-31 20:15:41" itemprop="dateCreated datePublished" datetime="2019-03-31T20:15:41+08:00">2019-03-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-09-05 02:19:11" itemprop="dateModified" datetime="2022-09-05T02:19:11+08:00">2022-09-05</time>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline: </span>
  
    <a title="waline" href="/posts/awesome-works/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/posts/awesome-works/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-item" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="waline-pageview-count" data-path="/posts/awesome-works/"></span>
    </span>
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <script src="/assets/js/DPlayer.min.js"> </script><div class="note success"><p>The post contains papers-with-code about SLAM, Pose/Object tracking,
Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep
Learning etc.</p>
</div>
<!-- [![GitHub stars](https://img.shields.io/github/stars/Vincentqyw/Recent-Stars-2019.svg?logo=github&label=Stars)](https://github.com/Vincentqyw/Recent-Stars-2019) -->
<span id="more"></span>
<p>Here is the <a
target="_blank" rel="noopener" href="https://github.com/Vincentqyw/Recent-Stars-2022">repo</a>: <a
target="_blank" rel="noopener" href="https://github.com/Vincentqyw/Recent-Stars-2022"><img data-src="https://github-readme-stats.vercel.app/api/pin/?username=Vincentqyw&amp;repo=Recent-Stars-2022&amp;show_owner=false&amp;theme=default"
alt="ReadMe Card" /></a></p>
<p>I posted the content of the repo as follows.
<!--# Recent Stars 2020--></p>
<!-- <p align="center">
 <img width="100px" data-src="https://vincentqin.tech/blog-resources/awesome-works/github-star.png" align="center" alt="" />
</p> -->
<!-- <p align="center">
  <a target="_blank" rel="noopener" href="https://github.com/Vincentqyw/Recent-Stars-2020">
    <img alt="Awesome" data-src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" />
  </a>
  <a target="_blank" rel="noopener" href="http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019">
    <img alt="HitCount" data-src="http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019.svg" />
  </a>
  <a target="_blank" rel="noopener" href="https://vincentqin.tech">
    <img alt="LICENSE" data-src="https://img.shields.io/badge/license-Anti%20996-blue.svg?style=flat-square" />
  </a>
</p> -->
<!--
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/Vincentqyw/Recent-Stars-2020)
[![HitCount](http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019.svg)](http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019)
[![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg?style=flat-square)](https://github.com/Vincentqyw/Recent-Stars-2020)
âœ” This repo collects some links with papers which I recently starred related on SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, etc.
-->
<h2 id="slam-related">SLAM related</h2>
<ul>
<li>[<strong>SLAM</strong>][ORB-SLAM3: An Accurate Open-Source Library
for Visual, Visual-Inertial and Multi-Map
SLAM](https://github.com/UZ-SLAMLab/ORB_SLAM3), <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.11898">PDF</a>]</strong></li>
<li>[<strong>SLAM</strong>][LIO-SAM](https://github.com/TixiaoShan/LIO-SAM),
æ¿€å…‰é›·è¾¾IMUç´§è€¦åˆSLAM</li>
<li>[<strong>Tool</strong>][Robotics Toolbox for
Python](https://github.com/petercorke/robotics-toolbox-python), a Python
implementation of the <a
target="_blank" rel="noopener" href="https://github.com/petercorke/robotics-toolbox-matlab">Robotics
Toolbox for MATLABÂ®</a></li>
<li>[<strong>Matching</strong>][LISRD](https://github.com/rpautrat/LISRD),ECCV
2020, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.08988">PDF</a>]</strong>ï¼Œåœ¨çº¿å±€éƒ¨ä¸å˜ç‰¹å¾åŒ¹é…ï¼é‡è¦ï¼</li>
<li>[<strong>Matching</strong>][AdaLAM](https://github.com/cavalli1234/AdaLAM),ç‰¹å¾åŒ¹é…å¿«é€Ÿæ»¤é™¤å¤–ç‚¹</li>
<li>[<strong>Calib</strong>][fisheye_pinhole_calib_demo](https://github.com/3DCVer/fisheye_pinhole_calib_demo),
åŒ…æ‹¬é±¼çœ¼æ¨¡å‹ã€é’ˆå­”æ¨¡å‹çš„ç›¸æœºæ ‡å®šï¼Œå°è£…äº†è‡ªåŠ¨ç¼–è¯‘ã€åº“çš„æ‰“åŒ…ä»¥åŠå¤–éƒ¨åº“çš„è°ƒç”¨æµ‹è¯•</li>
<li>[<strong>Calib</strong>][SensorCalibration](https://github.com/FENGChenxi0823/SensorCalibration),
IMUé›·è¾¾æ ‡å®š</li>
<li>[<strong>VO</strong>][Low-Drift Visual Odometry in Structured
Environments by Decoupling Rotational and Translational
Motion](https://github.com/PyojinKim/LPVO),ICRA 2018, <strong>[<a
target="_blank" rel="noopener" href="http://pyojinkim.com/download/papers/2018_ICRA.pdf">PDF</a>]</strong>,
ç»“æ„åŒ–ç¯å¢ƒä¸­å°†æ—‹è½¬é‡ä¸å¹³ç§»é‡è¿›è¡Œåˆ†ç¦»ä¼˜åŒ–</li>
<li>[<strong>VIO</strong>][VIO-SLAM](https://github.com/iamwangyabin/VIO-SLAM),
ä»é›¶å¼€å§‹æ‰‹å†™VIOè¯¾åä½œä¸š</li>
<li>[<strong>Matching</strong>][TFMatch: Learning-based image matching
in TensorFlow](https://github.com/lzx551402/tfmatch),TensorFlow å®ç°çš„
GeoDesc,ASLFeatä»¥åŠContextDesc</li>
<li>[<strong>Tutorial</strong>][SLAM-BOOK](https://github.com/yanyan-li/SLAM-BOOK),
ä¸€æœ¬å…³äºSLAMçš„ä¹¦ç¨¿ï¼Œæ¸…æ¥šçš„ä»‹ç»SLAMç³»ç»Ÿä¸­çš„ä½¿ç”¨çš„å‡ ä½•æ–¹æ³•å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ŒæŒç»­æ›´æ–°ä¸­</li>
<li>[<strong>Loop Closing</strong>][OverlapNet - Loop Closing for 3D
LiDAR-based SLAM](https://github.com/PRBonn/OverlapNet), RSS 2020,
<strong>[<a
target="_blank" rel="noopener" href="https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/chen2020rss.pdf">PDF</a>]</strong>,
3Dæ¿€å…‰é›·è¾¾SLAMé—­ç¯</li>
<li>[<strong>SLAM</strong>][VDO_SLAM](https://github.com/halajun/VDO_SLAM),
RGB-Dç›¸æœºæ•°æ®ä½œä¸ºè¾“å…¥ï¼Œå®ç°è¿½è¸ªåŠ¨æ€ç‰©ä½“SLAMçš„åŠŸèƒ½, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.11052">PDF</a>]</strong></li>
<li>[<strong>SLAM</strong>][orbslam-map-saving-extension](https://github.com/TUMFTM/orbslam-map-saving-extension)ï¼Œåœ¨ORB-SLAMçš„åŸºç¡€ä¸Šå¢åŠ ä¿å­˜+åŠ è½½åœ°å›¾åŠŸèƒ½</li>
<li>[<strong>Tutorial</strong>][Modern Robotics: Mechanics, Planning,
and Control Code Library](https://github.com/NxRLab/ModernRobotics),
ç°ä»£æœºå™¨äººå­¦, <strong>[<a
target="_blank" rel="noopener" href="http://hades.mech.northwestern.edu/index.php/Modern_Robotics">Homepage</a>]</strong></li>
<li>[<strong>Matching</strong>][image-matching-benchmark-baselines](https://github.com/vcg-uvic/image-matching-benchmark-baselines),
å›¾åƒç‰¹å¾åŒ¹é…æŒ‘æˆ˜èµ›ä¸»é¡µ</li>
<li>[<strong>Matching</strong>][GraphLineMatching](https://github.com/mameng1/GraphLineMatching)</li>
<li>[<strong>Matching</strong>][Locality Preserving
Matching](https://github.com/jiayi-ma/LPM), IJCAI 2017, <strong>[<a
target="_blank" rel="noopener" href="https://ai.tencent.com/ailab/media/publications/YuanGao_IJCAI2017_LocalityPreservingMatching.pdf">PDF</a>]</strong></li>
<li>[<strong>IMU</strong>][IMUOrientationEstimator](https://github.com/ydsf16/IMUOrientationEstimator)</li>
<li>[<strong>Feature</strong>][BEBLID: Boosted Efficient Binary Local
Image Descriptor](https://github.com/iago-suarez/BEBLID)</li>
<li>[<strong>Relocalization</strong>][KFNet: Learning Temporal Camera
Relocalization using Kalman
Filtering](https://github.com/zlthinker/KFNet),CVPR 2020,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.10629">PDF</a>]</strong></li>
<li>[<strong>Matching</strong>][image-matching-benchmark](https://github.com/vcg-uvic/image-matching-benchmark)</li>
<li>[<strong>Matching</strong>][GMS: Grid-based Motion Statistics for
Fast, Ultra-robust Feature
Correspondence](https://github.com/JiawangBian/GMS-Feature-Matcher),CVPR
17 &amp; IJCV 19,<strong>[<a
target="_blank" rel="noopener" href="http://jwbian.net/Papers/GMS_CVPR17.pdf">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://jwbian.net/gms">Project page</a>]</strong></li>
<li>[<strong>Reloc</strong>][GN-Net-Benchmark](https://github.com/Artisense-ai/GN-Net-Benchmark),
CVPR 2020,GN-Net: The Gauss-Newton Loss for Multi-Weather
Relocalization, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.11932">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://vision.in.tum.de/gn-net">Project page</a>]</strong></li>
<li>[<strong>Matching</strong>][SuperGluePretrainedNetwork](https://github.com/magicleap/SuperGluePretrainedNetwork),
CVPR 2020, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.11763">PDF</a>]</strong>,
åˆ’é‡ç‚¹ï¼2020å¹´sotaè¶…å¤§è§†è§’2Dç‰¹å¾åŒ¹é…ï¼Œ<a
href="https://www.vincentqin.tech/posts/superglue/">Blog</a></li>
<li>[<strong>Feature</strong>][D3Feat](https://github.com/XuyangBai/D3Feat),
CVPR 2020, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.03164">PDF</a>]</strong></li>
<li>[<strong>Feature</strong>][ASLFeat](https://github.com/lzx551402/ASLFeat),
CVPR 2020, ASLFeat: Learning Local Features of Accurate Shape and
Localization, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.10071">PDF</a>]</strong></li>
<li>[<strong>Feature</strong>][GMS-Feature-Matcher](https://github.com/XuyangBai/D3Feat),
CVPR 2018, GMS: Grid-based Motion Statistics for Fast, Ultra-robust
Feature Correspondence, <strong>[<a
target="_blank" rel="noopener" href="http://jwbian.net/Papers/GMS_CVPR17.pdf">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://jwbian.net/gms">Project page</a>]</strong></li>
<li>[<strong>Feature</strong>][D3Feat](https://github.com/XuyangBai/D3Feat),
CVPR 2020, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.03164">PDF</a>]</strong></li>
<li>[<strong>Feature</strong>][3DFeatNet](https://github.com/yewzijian/3DFeatNet),
ECCV 2018, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.09413">PDF</a>]</strong></li>
<li>[<strong>Tutorial</strong>][AutonomousDrivingCookbook](https://github.com/microsoft/AutonomousDrivingCookbook)ï¼ŒScenarios,
tutorials and demos for Autonomous Driving</li>
<li>[<strong>Tutorial</strong>][SLAMPaperReading](https://github.com/PaoPaoRobot/SLAMPaperReading)ï¼Œæ³¡æ³¡æœºå™¨äººåŒ—äº¬çº¿ä¸‹SLAMè®ºæ–‡åˆ†äº«èµ„æ–™</li>
<li>[<strong>Tutorial</strong>][VIO_Tutotial_Course](https://github.com/lishuwei0424/VIO_Tutotial_Course)</li>
<li>[<strong>Tutorial</strong>][VO-SLAM-Review](https://github.com/MichaelBeechan/VO-SLAM-Review)</li>
<li>[<strong>Tutorial</strong>][VINS-Mono-code-annotation](https://github.com/QingSimon/VINS-Mono-code-annotation),VINS-Monoä»£ç æ³¨é‡Šä»¥åŠå…¬å¼æ¨å¯¼</li>
<li>[<strong>Tutorial</strong>][VINS-Mono-Learning](https://github.com/ManiiXu/VINS-Mono-Learning),VINS-Monoä»£ç æ³¨é‡Š</li>
<li>[<strong>Tutorial</strong>][VINS-Course](https://github.com/HeYijia/VINS-Course),VINS-Mono
code without Ceres or ROS</li>
<li>[<strong>Tutorial</strong>][VIO-Doc](https://github.com/StevenCui/VIO-Doc),ä¸»æµVIOè®ºæ–‡æ¨å¯¼åŠä»£ç è§£æ</li>
<li>[<strong>VO</strong>][CNN-DSO](https://github.com/muskie82/CNN-DSO),
Direct Sparse Odometry with CNN Depth Prediction</li>
<li>[<strong>VO</strong>][fisheye-ORB-SLAM](https://github.com/lsyads/fisheye-ORB-SLAM),
A real-time robust monocular visual SLAM system based on ORB-SLAM for
fisheye cameras, without rectifying or cropping the input images</li>
<li>[<strong>VO</strong>][ORB_Line_SLAM](https://github.com/robotseu/ORB_Line_SLAM),
Real-Time SLAM with BoPLW Pairs for Stereo Cameras, with Loop Detection
and Relocalization Capabilities</li>
<li>[<strong>VO</strong>][DeepVO-pytorch](https://github.com/ChiWeiHsiao/DeepVO-pytorch.git),
ICRA 2017 <a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/7989236/">DeepVO: Towards
end-to-end visual odometry with deep Recurrent Convolutional Neural
Networks</a></li>
<li>[<strong>Calib</strong>][CamOdomCalibraTool](https://github.com/MegviiRobot/CamOdomCalibraTool),
The tool to calibrate extrinsic param between camera and wheel.</li>
<li>[<strong>Calib</strong>][lidar_camera_calibration](https://github.com/heethesh/lidar_camera_calibration),<a
target="_blank" rel="noopener" href="https://github.com/ankitdhall/lidar_camera_calibration">another
version</a></li>
<li>[<strong>Calib</strong>][OdomLaserCalibraTool](https://github.com/MegviiRobot/OdomLaserCalibraTool.git)ï¼Œç›¸æœºä¸2Dé›·è¾¾æ ‡å®š</li>
<li>[<strong>Calib</strong>][extrinsic_lidar_camera_calibration](https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration),
LiDARTag: A Real-Time Fiducial Tag using Point Clouds, arXiv 2019,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.10349">PDF</a>]</strong></li>
<li>[<strong>Calib</strong>][velo2cam_calibration](https://github.com/beltransen/velo2cam_calibration),
Automatic Calibration algorithm for Lidar-Stereo camera, <strong>[<a
target="_blank" rel="noopener" href="http://wiki.ros.org/velo2cam_calibration">Project
page</a>]</strong></li>
<li>[<strong>Dataset</strong>][IRS: A Large Synthetic Indoor Robotics
Stereo Dataset for Disparity and Surface Normal
Estimation](https://github.com/HKBU-HPML/IRS.git)</li>
<li>[<strong>Tools</strong>][averaging-quaternions](https://github.com/christophhagen/averaging-quaternions),å››å…ƒæ•°å¹³å‡</li>
</ul>
<hr />
<p>åˆ†å‰²çº¿ï¼Œä»¥ä¸‹æ˜¯2019å¹´çš„æ˜Ÿæ ‡é¡¹ç›®ï¼Œä¸Šé¢æ˜¯2020å¹´æ–°æ˜Ÿæ ‡çš„ã€‚</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/naver/r2d2">R2D2: Reliable and
Repeatable Detector and Descriptor</a>,NeurIPS 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.06195">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://europe.naverlabs.com/research/publications/r2d2-reliable-and-repeatable-detectors-and-descriptors-for-joint-sparse-local-keypoint-detection-and-feature-extraction/">Project
page</a>]</strong>ï¼Œæ·±åº¦å­¦ä¹ ç‰¹å¾ç‚¹+æè¿°å­</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/1989Ryan/Semantic_SLAM">Semantic_SLAM</a>,è¯­ä¹‰SLAMï¼šROS
+ ORB SLAM + PSPNet101</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/BAILOOL/PlaceRecognition-LoopDetection">PlaceRecognition-LoopDetection</a>,
Light-weight place recognition and loop detection using road
markings</li>
<li><a target="_blank" rel="noopener" href="https://github.com/MISTLab/DOOR-SLAM">DOOR-SLAM:
Distributed, online, and outlier resilient SLAM for robotic
teams</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.12198">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://mistlab.ca/DOOR-SLAM/">Project
page</a>]</strong>ï¼Œå¤šæœºå™¨äººåä½œSLAMï¼Œå¢å¼ºäº†åœºæ™¯çš„é€‚ç”¨æ€§</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/shamangary/awesome-local-global-descriptor">awesome-local-global-descriptor</a>,
è¶…è¯¦ç»†æ·±åº¦å­¦ä¹ ç‰¹å¾ç‚¹æè¿°å­é›†åˆï¼Œéœ€è¦é‡ç‚¹å…³æ³¨ä¸€ä¸‹è¿™ä¸ªrepo</li>
<li><a target="_blank" rel="noopener" href="https://github.com/zju3dv/GIFT">GIFT: Learning
Transformation-Invariant Dense Visual Descriptors via Group CNNs</a>,
NeurIPS 2019ï¼Œ<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.05932">PDF</a>]</strong>, <strong>[<a
target="_blank" rel="noopener" href="https://zju3dv.github.io/GIFT/">Project
page</a>]</strong>ï¼Œæµ™å¤§CAD+å•†æ±¤è”åˆå®éªŒå®¤å‡ºå“ï¼Œåˆ©ç”¨Group
CNNæ¥æ”¹è¿›superpointæè¿°å­ï¼ˆä»…æè¿°ï¼Œç‰¹å¾ç‚¹æå–å¯ä»»æ„é€‰æ‹©ï¼‰ï¼Œå¯ä»¥å¤§å¹…åº¦å¢å¼ºè§†è§’å˜åŒ–æ—¶çš„ç‰¹å¾ç‚¹å¤æ£€ç‡ä¸åŒ¹é…ç‚¹æ•°</li>
<li><a target="_blank" rel="noopener" href="https://github.com/axelBarroso/Key.Net">Key.Net: Keypoint
Detection by Handcrafted and Learned CNN Filters</a>,ICCV 2019,
<strong>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00889">PDF</a>]</strong>,
æ·±åº¦å­¦ä¹ ç‰¹å¾ç‚¹</li>
<li><a target="_blank" rel="noopener" href="https://github.com/TRI-ML/KP3D">Self-Supervised 3D Keypoint
Learning for Ego-motion Estimation</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.03426">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=4hFhSD8QUPM">Youtube</a>]</strong>,
æ·±åº¦å­¦ä¹ ç‰¹å¾ç‚¹</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Jichao-Peng/VINS-Mono-Optimization">VINS-Mono-Optimization</a>,
å®ç°ç‚¹çº¿ç´§è€¦åˆä¼˜åŒ–çš„VINS-Mono</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/PetWorm/msckf_vio_zhushi">msckf_vioæ³¨é‡Šç‰ˆæœ¬</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/lyakaap/NetVLAD-pytorch">NetVLAD-pytorch</a>,
NetVLADåœºæ™¯è¯†åˆ«çš„pytorchå®ç°</li>
<li><a target="_blank" rel="noopener" href="http://microgps.cs.princeton.edu/">High-Precision
Localization Using Ground Texture (Micro-GPS)</a>,ECCV 2018,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.10687">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://microgps.cs.princeton.edu/">Project
page</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://microgps.cs.princeton.edu/data/micro-gps-cpp-master.zip">code</a>]</strong>ï¼Œåœ°å‘ï¼ˆæ‘„åƒæœºæœå‘åœ°é¢ï¼‰SLAMï¼Œè·å¾—é«˜ç²¾åº¦é‡å®šä½æ•ˆæœã€‚</li>
<li><a target="_blank" rel="noopener" href="https://github.com/LRMPUT/PlaneSLAM">PlaneSLAM</a>, Paper:
â€œOn the Representation of Planes for Efficient Graph-based SLAM with
High-level Featuresâ€</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ucla-vision/xivo">XIVO: X Inertial-aided
Visual Odometry and Sparse Mapping</a>, an open-source repository for
visual-inertial odometry/mapping.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/lmb-freiburg/deeptam">DeepTAM</a>,ECCV
2018,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.01900.pdf">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://lmb.informatik.uni-freiburg.de/people/zhouh/deeptam/">Project
page</a>]</strong>,a learnt system for keyframe-based dense camera
tracking and mapping.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ajparra/iRotAvg">iRotAvg, Why bundle
adjust?</a>,ICRA 2019,<strong>[<a
target="_blank" rel="noopener" href="https://cs.adelaide.edu.au/~aparra/publication/parra19_icra/">PDF</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Kelym/FAST">Tactical Rewind:
Self-Correction via Backtracking in Vision-and-Language
Navigation</a>,CVPR 2019,<strong>[<a
target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Ke_Tactical_Rewind_Self-Correction_via_Backtracking_in_Vision-And-Language_Navigation_CVPR_2019_paper.html">PDF</a>]</strong>ï¼Œè§†è§‰+è¯­è¨€å¯¼èˆª</li>
<li><a target="_blank" rel="noopener" href="https://github.com/MISTLab/DOOR-SLAM">DOOR-SLAM</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/JiawangBian/FM-Bench">An Evaluation of
Feature Matchers for Fundamental Matrix Estimation</a>,BMVC
2019,<strong>[<a
target="_blank" rel="noopener" href="https://jwbian.net/Papers/FM_BMVC19.pdf">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://jwbian.net/fm-bench">Project
Page</a>]</strong>ï¼Œç‰¹å¾åŒ¹é…</li>
<li><a target="_blank" rel="noopener" href="https://github.com/hyye/lio-mapping">A Tightly Coupled 3D
Lidar and Inertial Odometry and Mapping Approach</a>,ICRA
2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.06993">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://sites.google.com/view/lio-mapping">Project
Page</a>]</strong>ï¼Œç´§è€¦åˆé›·è¾¾+IMU SLAM</li>
<li><a target="_blank" rel="noopener" href="https://github.com/LRMPUT/PlaneSLAM">On the Representation
of Planes for Efficient Graph-based SLAM with High-level
Features</a>,åˆ©ç”¨å¹³é¢ä¿¡æ¯çš„SLAM</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Huangying-Zhan/DF-VO">Visual Odometry
Revisited: What Should Be Learnt?</a>,arXiv 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.09803">PDF</a>]</strong>,
æ·±åº¦å­¦ä¹ æ·±åº¦+å…‰æµè¿›è¡ŒVO</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Xylon-Sean/rfnet">RF-Net: An End-to-End
Image Matching Network based on Receptive Field</a>,CVPR
2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.00604">PDF</a>]</strong>,
ç«¯åˆ°ç«¯å›¾åƒåŒ¹é…</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/HKUST-Aerial-Robotics/Fast-Planner">Fast-Planner</a>,IEEE
Robotics and Automation Letters (RA-L), 2019,<strong>[<a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8758904">PDF</a>]</strong>,
æ— äººæœºè½¨è¿¹ç”Ÿæˆ</li>
<li><a target="_blank" rel="noopener" href="https://github.com/dongjing3309/minisam">A general and
flexible factor graph non-linear least square optimization
framework</a>,CoRR 2019,<strong>[<a
target="_blank" rel="noopener" href="http://arxiv.org/abs/1909.00903">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://minisam.readthedocs.io/">Project Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/gao-ouyang/demo_for_kalmanFilter">Demo
for Kalman filter in ranging system</a>,å¡å°”æ›¼æ»¤æ³¢åŸç†æ¼”ç¤º</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Ahmedest61/CNN-Region-VLAD-VPR">A
Holistic Visual Place Recognition Approach using Lightweight CNNs for
Severe ViewPoint and Appearance
Changes</a>ï¼Œåœºæ™¯è¯†åˆ«ï¼ˆå¤–è§‚ä¸è§†è§’å˜åŒ–æ—¶ï¼‰,<a
target="_blank" rel="noopener" href="https://github.com/ethz-asl/hierarchical_loc">è®­ç»ƒå’Œéƒ¨ç½²æºç </a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/uzh-rpg/sips2_open">SIPs: Succinct
Interest Points from Unsupervised Inlierness Probability Learning</a>,3D
Vision (3DV) 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.01358">PDF</a>]</strong>ï¼ŒRPGå®éªŒå®¤å‡ºå“ï¼Œæ·±åº¦å­¦ä¹ ç‰¹å¾ç‚¹ï¼ˆæœ‰ç‰¹å¾æè¿°å­ï¼‰</li>
<li><a target="_blank" rel="noopener" href="https://github.com/uzh-rpg/imips_open">Matching Features
Without Descriptors: Implicitly Matched Interest Points</a>,BMVC
2019,<strong>[<a
target="_blank" rel="noopener" href="http://rpg.ifi.uzh.ch/docs/BMVC19_Cieslewski.pdf">PDF</a>]</strong>,RPGå®éªŒå®¤å‡ºå“ï¼Œæ— éœ€ç‰¹å¾æè¿°å³å¯è¿›è¡Œç‰¹å¾åŒ¹é…</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/cardwing/Codes-for-Lane-Detection">Learning
Lightweight Lane Detection CNNs by Self Attention Distillation (ICCV
2019)</a>,ICCV 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.00821">PDF</a>]</strong>ï¼Œæ·±åº¦å­¦ä¹ é“è·¯æ£€æµ‹</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/youngguncho/awesome-slam-datasets">Awesome SLAM
Datasets</a>,å²ä¸Šæœ€å…¨SLAMæ•°æ®é›†ï¼Œ <strong><a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/BzcghUnXTR9RQqA3Pc9MhA">å…¬ä¼—å·è¯´æ˜:
æœ€å…¨ SLAM å¼€æºæ•°æ®é›†</a></strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Aceinna/gnss-ins-sim">GNSS-INS-SIM</a>,æƒ¯å¯¼èåˆæ¨¡æ‹Ÿå™¨ï¼Œæ”¯æŒIMUæ•°æ®ï¼Œè½¨è¿¹ç”Ÿæˆç­‰</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/2013fangwentao/Multi-Sensor-Combined-Navigation">Multi-Sensor
Combined Navigation Program(GNSS, IMU, Camera and so on)
å¤šæºå¤šä¼ æ„Ÿå™¨èåˆå®šä½ GPS/INSç»„åˆå¯¼èˆª</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/scape-research/SOSNet">SOSNet: Second
Order Similarity Regularization for Local Descriptor Learning</a>,CVPR
2019,<strong><a target="_blank" rel="noopener" href="https://research.scape.io/sosnet/">[Project
page]</a></strong> <strong><a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.05019">[Paper]</a></strong> <strong><a
href="imgs/sosnet-poster.pdf">[Poster]</a></strong> <strong><a
href="imgs/sosnet-oral.pdf">[Slides]</a></strong>ï¼Œä¸€ç§æ·±åº¦å­¦ä¹ ç‰¹å¾æè¿°å­</li>
<li><a target="_blank" rel="noopener" href="https://github.com/oravus/seq2single">Look No Deeper:
Recognizing Places from Opposing Viewpoints under Varying Scene
Appearance using Single-View Depth Estimation</a>,ICRA 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.07381">PDF</a>]</strong>,åˆ©ç”¨æ·±åº¦å›¾åƒå®ç°äº†å¤§è§†è§’é•¿æ—¶é—´çš„åœºæ™¯è¯†åˆ«ï¼ˆæ ¹æ®æ·±åº¦å›¾ç­›é€‰å¾—åˆ°ä¸åŒæ·±åº¦å±‚æ¬¡çš„ç‰¹å¾ç‚¹ç„¶åä¸å½“å‰å¸§è¿›è¡ŒåŒ¹é…ï¼Œæé«˜äº†åœºæ™¯å¬å›ç‡ï¼‰</li>
<li><a target="_blank" rel="noopener" href="https://github.com/rpng/calc2.0">CALC2.0</a>,Convolutional
Autoencoder for Loop Closure 2.0,ç”¨äºé—­ç¯æ£€æµ‹</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ethz-asl/segmap">SegMap</a>,RSS
2018,<strong>[<a
target="_blank" rel="noopener" href="http://www.roboticsproceedings.org/rss14/p03.pdf">PDF</a>]</strong>,
ä¸€ç§åŸºäº3Dçº¿æ®µçš„åœ°å›¾è¡¨ç¤ºï¼Œå¯ç”¨äºåœºæ™¯è¯†åˆ«/æœºå™¨äººå®šä½/ç¯å¢ƒé‡å»ºç­‰</li>
<li><a target="_blank" rel="noopener" href="https://github.com/cggos/msckf_vio_cg">MSCKF_VIO</a>, a
stereo version of MSCKFï¼ŒåŸºäºMSCKFçš„åŒç›®VIO</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Relja/netvlad">NetVLAD: CNN architecture
for weakly supervised place recognition</a>ï¼ŒCVPR 2016,
CNNæ¡†æ¶å¼±ç›‘ç£å­¦ä¹ åœºæ™¯è¯†åˆ«,<strong>[<a
target="_blank" rel="noopener" href="https://www.di.ens.fr/willow/research/netvlad/">Project
Page</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/IFL-CAMP/easy_handeye">easy_handeye</a>,Simple,
straighforward ROS library for hand-eye calibration</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/KinglittleQ/SuperPoint_SLAM">SuperPoint-SLAM</a>,åˆ©ç”¨SuperPointæ›¿æ¢ORBç‰¹å¾ç‚¹</li>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/pyrobot">PyRobot: An
Open Source Robotics Research Platform</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ethz-asl/hfnet">From Coarse to Fine:
Robust Hierarchical Localization at Large Scale with
HF-Net</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.03506">PDF</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/mp3guy/ICPCUDA">Super fast
implementation of ICP in CUDA</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ethz-asl/volumetric_mapping">A generic
interface for disparity map and pointcloud insertion</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/tdsuper/SPHORB">SPHORB: A Fast and
Robust Binary Feature on the Sphere</a>,International Journal of
Computer Vision 2015,<strong>[<a
target="_blank" rel="noopener" href="http://scs.tju.edu.cn/~lwan/paper/SPHORB/pdf/SPHORB-final-small.pdf">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://scs.tju.edu.cn/~lwan/paper/SPHORB/SPHORB.html">Project
Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ETH3D/badslam">BADSLAM: Bundle Adjusted
Direct RGB-D SLAM</a>,CVPR 2019,<strong>[<a
target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Schops_BAD_SLAM_Bundle_Adjusted_Direct_RGB-D_SLAM_CVPR_2019_paper.pdf">PDF</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/uzh-rpg/rpg_e2vid">High Speed and High
Dynamic Range Video with an Event Camera</a>,arXiv,<strong>[<a
target="_blank" rel="noopener" href="http://rpg.ifi.uzh.ch/docs/arXiv19_Rebecq.pdf">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://rpg.ifi.uzh.ch/E2VID.html">Project Page</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/PaoPaoRobot/Awesome-VIO">Awesome-VIO</a>,Discuss
about VIO in PaoPaoRobot group</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/XinLiGH/GyroAllan">GyroAllan</a>,é™€èºä»ªéšæœºè¯¯å·®çš„
Allan æ–¹å·®åˆ†æ, <a target="_blank" rel="noopener" href="https://github.com/rpng/kalibr_allan">Another
version</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/fangchangma/self-supervised-depth-completion">Self-supervised
Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and
Monocular Camera</a>,ICRA 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1807.00275.pdf">PDF</a>]</strong>,
ä¼˜åŒ–LiDARä»¥åŠå•ç›®å¾—åˆ°çš„æ·±åº¦å›¾</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVlabs/planercnn">PlaneRCNN: 3D Plane
Detection and Reconstruction from a Single Image</a>,CVPR
2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.04072.pdf">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://research.nvidia.com/publication/2019-06_PlaneRCNN">Project
Page</a>]</strong>,é€šè¿‡å•å¹…å›¾åƒè¿›è¡Œ3Då¹³é¢æ£€æµ‹ä»¥åŠé‡å»º</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/kokerf/DBow3">DBow3</a>,æ³¨é‡Šç‰ˆçš„DBow3ä»£ç </li>
<li><a
target="_blank" rel="noopener" href="https://github.com/VladyslavUsenko/basalt-mirror">Visual-Inertial
Mapping with Non-Linear Factor Recovery</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.06504">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://vision.in.tum.de/research/vslam/basalt">Project
Page</a>]</strong>, æ—¶ç©ºè”åˆçš„VIOä¼˜åŒ–æ–¹æ¡ˆ</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/PaoPaoRobot/ICRA2019-paper-list">ICRA2019-paper-list</a>,ICRA
2019è®ºæ–‡åˆ—è¡¨ï¼ˆæ³¡æ³¡æœºå™¨äººå‡ºå“æš‚æ—¶æ— é“¾æ¥ï¼‰</li>
<li><a target="_blank" rel="noopener" href="https://github.com/pedropro/CAPE">Fast Cylinder and Plane
Extraction from Depth Cameras for Visual Odometry</a>, IROS
2018,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.02380">PDF</a>]</strong>,åˆ©ç”¨æ·±åº¦å›¾è¿›è¡Œåœ†æŸ±æ£€æµ‹ä»¥åŠå¹³é¢æ£€æµ‹è¿›è¡ŒVO</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/kiran-mohan/SLAM-Algorithms-Octave">Solutions
to assignments of Robot Mapping Course WS 2013/14 by Dr. Cyrill
Stachniss at University of Freiburg</a>,SLAMç®—æ³•å­¦ä¹ è¯¾åä½œä¸šç­”æ¡ˆ</li>
<li><a target="_blank" rel="noopener" href="https://github.com/RonaldSun/VI-Stereo-DSO">Direct sparse
odometry combined with stereo cameras and IMU</a>,åŒç›®DSO+IMU</li>
<li><a target="_blank" rel="noopener" href="https://github.com/HorizonAD/stereo_dso">Direct Sparse
Odometry with Stereo Cameras</a>,åŒç›®DSO</li>
<li><a target="_blank" rel="noopener" href="https://github.com/uoip/g2opy">Python binding of SLAM graph
optimization framework g2o</a>,pythonç‰ˆæœ¬çš„g2oå®ç°</li>
<li><a target="_blank" rel="noopener" href="https://github.com/rpautrat/SuperPoint">SuperPoint:
Self-Supervised Interest Point Detection and Description</a>, CVPR 2018,
<strong>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1712.07629">Paper</a>]</strong>,
æ·±åº¦å­¦ä¹ æè¿°å­+æè¿°</li>
<li><a target="_blank" rel="noopener" href="https://github.com/lzx551402/contextdesc">ContextDesc:
Local Descriptor Augmentation with Cross-Modality Context</a>, CVPR
2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.04084">Paper</a>]</strong>,
æ·±åº¦å­¦ä¹ æè¿°å­</li>
<li><a target="_blank" rel="noopener" href="https://github.com/mihaidusmanu/d2-net">D2-Net: A Trainable
CNN for Joint Description and Detection of Local Features</a>, CVPR
2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.03561">Paper</a>]</strong>, <strong>[<a
target="_blank" rel="noopener" href="https://dsmn.ml/publications/d2-net.html">Project
Page</a>]</strong>, æ·±åº¦å­¦ä¹ å…³é”®ç‚¹+æè¿°</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ethz-asl/orb_slam_2_ros">ROS interface
for ORBSLAM2</a>,ROSç‰ˆæœ¬çš„ORBSLAM2</li>
<li><a target="_blank" rel="noopener" href="https://github.com/yan99033/CNN-SVO">CNN-SVO: Improving the
Mapping in Semi-Direct Visual Odometry Using Single-Image Depth
Prediction</a>ï¼Œ <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.01011.pdf">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/ManiiXu/VINS-Mono-Learning">VINS-Mono-Learning</a>ï¼Œä»£ç æ³¨é‡Šç‰ˆVINS-Monoï¼Œåˆå­¦è€…å­¦ä¹ </li>
<li><a target="_blank" rel="noopener" href="https://github.com/xdspacelab/openvslam">OpenVSLAM:
Versatile Visual SLAM Framework</a>, <strong>[<a
target="_blank" rel="noopener" href="https://openvslam.readthedocs.io/">Project Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/fabianschenk/RESLAM">RESLAM: A real-time
robust edge-based SLAM system</a>, ICRA 2019, <strong>[<a
target="_blank" rel="noopener" href="https://github.com/fabianschenk/fabianschenk.github.io/raw/master/files/schenk_icra_2019.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/rubengooj/pl-slam">PL-SLAM: a Stereo
SLAM System through the Combination of Points and Line Segments</a>,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.09479">Paper</a>]</strong>ï¼Œçº¿ç‰¹å¾SLAM</li>
<li><a target="_blank" rel="noopener" href="https://github.com/YipuZhao/GF_PL_SLAM">Good Line Cutting:
towards Accurate Pose Tracking of Line-assisted VO/VSLAM</a>, ECCV 2018,
<strong>[<a
target="_blank" rel="noopener" href="https://sites.google.com/site/zhaoyipu/good-feature-visual-slam">Project
Page</a>]</strong>, æ”¹è¿›çš„PL-SLAM</li>
<li><a target="_blank" rel="noopener" href="https://github.com/leoshine/Spherical_Regression">Spherical
Regression: Learning Viewpoints, Surface Normals and 3D Rotations on
n-Spheres</a>, CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="http://arxiv.org/abs/1904.05404">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/icsl-Jeon/traj_gen_vis">svo_edgelet</a>,
åœ¨çº¿è½¨è¿¹ç”Ÿæˆ</li>
<li><a target="_blank" rel="noopener" href="https://github.com/TimboKZ/caltech_samaritan">Drone SLAM
project for Caltech's ME 134 Autonomy class</a>, <strong>[<a
target="_blank" rel="noopener" href="https://github.com/TimboKZ/caltech_samaritan/blob/master/CS134_Final_Project_Report.pdf">PDF</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/icsl-Jeon/traj_gen_vis">Online
Trajectory Generation of a MAV for Chasing a Moving Target in 3D Dense
Environments</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.03421.pdf">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/AtsushiSakai/PythonRobotics">PythonRobotics</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.10703">Paper</a>]</strong>, <a
target="_blank" rel="noopener" href="https://github.com/onlytailei/CppRobotics">CppRobotics</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/izhengfan/ba_demo_ceres">Bundle
adjustment demo using Ceres Solver</a>, <strong>[<a
target="_blank" rel="noopener" href="https://fzheng.me/2018/01/23/ba-demo-ceres/">Blog</a>]</strong>,
cereså®ç°BA</li>
<li><a target="_blank" rel="noopener" href="https://github.com/shichaoy/cube_slam">CubeSLAM: Monocular
3D Object Detection and SLAM</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.00557">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/sshaoshuai/PointRCNN">PointRCNN: 3D
Object Proposal Generation and Detection from Point Cloud</a>, CVPR
2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.04244">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/nrupatunga/GIST-global-Image-Descripor">GIST-Global
Image Descriptor</a>, GISTæè¿°å­</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ethz-asl/mav_voxblox_planning">mav
voxblox planning</a>, MAV planning tools using voxblox as the map
representation.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/zziz/kalman-filter">Python Kalman
Filter</a>, 30è¡Œå®ç°å¡å°”æ›¼æ»¤æ³¢</li>
<li><a target="_blank" rel="noopener" href="https://github.com/arpg/vicalib">vicalib</a>,
è§†è§‰æƒ¯å¯¼ç³»ç»Ÿæ ‡å®šå·¥å…·</li>
<li><a target="_blank" rel="noopener" href="https://github.com/simondlevy/BreezySLAM">BreezySLAM</a>,
åŸºäºé›·è¾¾çš„SLAMï¼Œæ”¯æŒPython(&amp;Matlab, C++, and Java)</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Yvon-Shong/Probabilistic-Robotics">Probabilistic-Robotics</a>,
ã€Šæ¦‚ç‡æœºå™¨äººã€‹ä¸­æ–‡ç‰ˆï¼Œä¹¦å’Œè¯¾åä¹ é¢˜</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/emmjaykay/stanford_self_driving_car_code">Stanford
Self Driving Car Code</a>, <strong>[<a
target="_blank" rel="noopener" href="http://robots.stanford.edu/papers/junior08.pdf">Paper</a>]</strong>,
æ–¯å¦ç¦è‡ªåŠ¨é©¾é©¶è½¦ä»£ç </li>
<li><a target="_blank" rel="noopener" href="https://github.com/ndrplz/self-driving-car">Udacity
Self-Driving Car Engineer Nanodegree projects</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/TUMFTM/Lecture_AI_in_Automotive_Technology">Artificial
Intelligence in Automotive Technology</a>,
TUMè‡ªåŠ¨é©¾é©¶æŠ€æœ¯ä¸­çš„äººå·¥æ™ºèƒ½è¯¾ç¨‹</li>
<li><a target="_blank" rel="noopener" href="https://github.com/hlzz/DeepMatchVO">DeepMatchVO: Beyond
Photometric Loss for Self-Supervised Ego-Motion Estimation</a>,ICRA
2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.09103">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/zdzhaoyong/GSLAM">GSLAM: A General SLAM
Framework and Benchmark</a>, CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.07995">Paper</a>]</strong>,
é›†æˆäº†å„ç§ä¼ æ„Ÿå™¨è¾“å…¥çš„SLAMç»Ÿä¸€æ¡†æ¶</li>
<li><a target="_blank" rel="noopener" href="https://github.com/izhengfan/se2lam">Visual-Odometric
Localization and Mapping for Ground Vehicles Using SE(2)-XYZ
Constraints</a>ï¼ŒICRA 2019,åŸºäºSE(2)-XYZçº¦æŸçš„VOç³»ç»Ÿ</li>
<li><a target="_blank" rel="noopener" href="https://github.com/nicolov/simple_slam_loop_closure">Simple
bag-of-words loop closure for visual SLAM</a>, <strong>[<a
target="_blank" rel="noopener" href="https://nicolovaligi.com/bag-of-words-loop-closure-visual-slam.html">Blog</a>]</strong>,
å›ç¯</li>
<li><a target="_blank" rel="noopener" href="https://github.com/rmsalinas/fbow">FBOW (Fast Bag of
Words), an extremmely optimized version of the DBow2/DBow3
libraries</a>,ä¼˜åŒ–ç‰ˆæœ¬çš„DBow2/DBow3</li>
<li><a target="_blank" rel="noopener" href="https://github.com/tomas789/tonav">Multi-State Constraint
Kalman Filter (MSCKF) for Vision-aided Inertial Navigation(master's
thesis)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/yuzhou42/MSCKF">MSCKF</a>,
MSCKFä¸­æ–‡æ³¨é‡Šç‰ˆ</li>
<li><a target="_blank" rel="noopener" href="https://github.com/hbtang/calibcamodo">Calibration
algorithm for a camera odometry system</a>, VOç³»ç»Ÿçš„æ ‡å®šç¨‹åº</li>
<li><a target="_blank" rel="noopener" href="https://github.com/cggos/vins_mono_cg">Modified version of
VINS-Mono</a>, æ³¨é‡Šç‰ˆæœ¬VINS Mono</li>
<li><a target="_blank" rel="noopener" href="https://github.com/zhenpeiyang/RelativePose">Extreme
Relative Pose Estimation for RGB-D Scans via Scene
Completion</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.00063">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/jessecw/EPnP_Eigen">Implementation of
EPnP algorithm with Eigen</a>,åˆ©ç”¨Eigenç¼–å†™çš„EPnP</li>
<li><a target="_blank" rel="noopener" href="https://github.com/jiexiong2016/GCNv2_SLAM">Real-time SLAM
system with deep features</a>, æ·±åº¦å­¦ä¹ æè¿°å­(ORB vs. GCNv2)</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Huangying-Zhan/Depth-VO-Feat">Unsupervised
Learning of Monocular Depth Estimation and Visual Odometry with Deep
Feature Reconstruction</a>, CVPR 2018, æ— ç›‘ç£å•ç›®æ·±åº¦æ¢å¤ä»¥åŠVO</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Phylliida/orbslam-windows">ORB-SLAM-windows</a>,
Windowsç‰ˆæœ¬çš„ORB-SLAM</li>
<li><a target="_blank" rel="noopener" href="https://github.com/danping/structvio">StructVIO :
Visual-inertial Odometry with Structural Regularity of Man-made
Environments</a>,<strong>[<a
target="_blank" rel="noopener" href="http://drone.sjtu.edu.cn/dpzou/project/structvio.html">Project
Page</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/irvingzhang/KalmanFiltering">KalmanFiltering</a>,
å„ç§å¡å°”æ›¼æ»¤æ³¢å™¨çš„demo</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ZhenghaoFei/visual_odom">Stereo Odometry
based on careful Feature selection and Tracking</a>, <strong>[<a
target="_blank" rel="noopener" href="https://lamor.fer.hr/images/50020776/Cvisic2017.pdf">Paper</a>]</strong>,
C++ OpenCVå®ç°SOFT</li>
<li><a target="_blank" rel="noopener" href="https://github.com/dzunigan/zSLAM">Visual SLAM with RGB-D
Cameras based on Pose Graph Optimization</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/drsrinathsridhar/GRANSAC">Multi-threaded
generic RANSAC implemetation</a>, å¤šçº¿ç¨‹RANSAC</li>
<li><a target="_blank" rel="noopener" href="https://github.com/PyojinKim/OPVO">Visual Odometry with
Drift-Free Rotation Estimation Using Indoor Scene Regularities</a>, BMVC
2017, <strong>[<a
target="_blank" rel="noopener" href="http://pyojinkim.me/pub/Visual-Odometry-with-Drift-Free-Rotation-Estimation-Using-Indoor-Scene-Regularities/">Project
Page</a>]</strong>ï¼Œåˆ©ç”¨å¹³é¢æ­£äº¤ä¿¡æ¯è¿›è¡ŒVO</li>
<li><a target="_blank" rel="noopener" href="https://github.com/baidu/ICE-BA">ICE-BA</a>, CVPR 2018,
<strong>[<a
target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_ICE-BA_Incremental_Consistent_CVPR_2018_paper.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/AIBluefisher/GraphSfM">GraphSfM: Robust
and Efficient Graph-based Structure from Motion</a>, <strong>[<a
target="_blank" rel="noopener" href="https://aibluefisher.github.io/GraphSfM/">Project
Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/cuitaixiang/LOAM_NOTED">LOAM_NOTED</a>,
loamä¸­æ–‡æ³¨è§£ç‰ˆ</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Ethan-Zhou/MWO">Divide and Conquer:
Effcient Density-Based Tracking of 3D Sensors in Manhattan
Worlds</a>,ACCV 2016,<strong>[<a
target="_blank" rel="noopener" href="http://users.cecs.anu.edu.au/~u5535909/">Project
Page</a>]</strong>,æ›¼å“ˆé¡¿ä¸–ç•Œåˆ©ç”¨æ·±åº¦ä¼ æ„Ÿå™¨è¿›è¡Œæ—‹è½¬é‡å¹³ç§»é‡åˆ†ç¦»ä¼˜åŒ–</li>
<li><a target="_blank" rel="noopener" href="https://github.com/jstraub/rtmf">Real-time Manhattan World
Rotation Estimation in 3D</a>,IROS 2015,å®æ—¶æ›¼å“ˆé¡¿ä¸–ç•Œæ—‹è½¬ä¼°è®¡</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/uzh-rpg/event-based_vision_resources">Event-based
Vision Resources</a>ï¼Œå…³äºäº‹ä»¶ç›¸æœºçš„èµ„æº</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/DeepTecher/AutonomousVehiclePaper">AutonomousVehiclePaper</a>ï¼Œæ— äººé©¾é©¶ç›¸å…³è®ºæ–‡é€Ÿé€’</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/wutianyiRosun/Segmentation.X">Segmentation.X</a>,
Segmentationç›¸å…³è®ºæ–‡&amp;ä»£ç </li>
<li><a target="_blank" rel="noopener" href="https://github.com/amusi/CVPR2019-Code">CVPR-2019</a>, CVPR
2019 è®ºæ–‡å¼€æºé¡¹ç›®åˆé›†</li>
<li><a target="_blank" rel="noopener" href="https://github.com/kanster/awesome-slam">awesome-slam</a>,
SLAMåˆé›†</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/tzutalin/awesome-visual-slam">awesome-visual-slam</a>,
è§†è§‰SLAMåˆé›†</li>
<li><a target="_blank" rel="noopener" href="https://github.com/zziz/pwc">Papers with code</a>,
å‘¨æ›´è®ºæ–‡withä»£ç </li>
<li><a
target="_blank" rel="noopener" href="https://github.com/cbsudux/awesome-human-pose-estimation">Awesome
Human Pose Estimation</a>,<a
target="_blank" rel="noopener" href="https://github.com/nkalavak/awesome-object-pose">awesome-object-pose</a>,
ä½å§¿ä¼°è®¡åˆé›†</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Ewenwan/MVision">MVision</a>,
å¤§ç¤¼åŒ…ï¼šæœºå™¨äººè§†è§‰ ç§»åŠ¨æœºå™¨äºº VS-SLAM ORB-SLAM2 æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹ yolov3
è¡Œä¸ºæ£€æµ‹ opencv PCL æœºå™¨å­¦ä¹  æ— äººé©¾é©¶</li>
</ul>
<h2 id="poseobject-tracking">Pose/Object tracking</h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/KovenYu/MAR">Unsupervised person
re-identification by soft multilabel learning</a>,CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://kovenyu.com/papers/2019_CVPR_MAR.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/tianzhi0549/FCOS">FCOS: Fully
Convolutional One-Stage Object Detection</a>,ICCV 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.01355">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/yangli18/hand_detection">Hand Detection
and Orientation Estimation</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Wanggcong/Spatial-Temporal-Re-identification">Spatial-Temporal
Person Re-identification</a>,AAAI 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.03282">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/layumi/Person_reID_baseline_pytorch">A
tiny, friendly, strong pytorch implement of person re-identification
baseline. <strong>Tutorial</strong></a>,CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.07223">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/tengteng95/Pose-Transfer">Progressive
Pose Attention for Person Image Generation</a>,CVPR 2019,<strong>[<a
target="_blank" rel="noopener" href="http://arxiv.org/abs/1904.03349">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/shamangary/FSA-Net">FSA-Net: Learning
Fine-Grained Structure Aggregation for Head Pose Estimation from a
Single Image</a>, CVPR 2019,<strong>[<a
target="_blank" rel="noopener" href="https://github.com/shamangary/FSA-Net/blob/master/0191.pdf">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/yuanyuanli85/Fast_Human_Pose_Estimation_Pytorch">An
unoffical implemention for paper "Fast Human Pose Estimation"</a>, CVPR
2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.05419">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/edvardHua/PoseEstimationForMobile">Real-time
single person pose estimation for Android and
iOS</a>,æ‰‹æœºç«¯å®ç°äººä½“ä½å§¿ä¼°è®¡</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/cbsudux/Human-Pose-Estimation-101">Basics of 2D
and 3D Human Pose Estimation</a>,äººä½“å§¿æ€ä¼°è®¡å…¥é—¨</li>
<li><a target="_blank" rel="noopener" href="https://github.com/OceanPang/Libra_R-CNN">Libra R-CNN:
Towards Balanced Learning for Object Detection</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/HRNet/HRNet-Object-Detection">High-resolution
networks (HRNets) for object detection</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.04514.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/xiaolonw/TimeCycle">Learning
Correspondence from the Cycle-Consistency of Time</a>, CVPR 2019,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.07593">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/zju3dv/pvnet">PVNet: Pixel-wise Voting
Network for 6DoF Pose Estimation</a>, CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.11788">Paper</a>], [<a
target="_blank" rel="noopener" href="https://zju3dv.github.io/pvnet">Project Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/mkocabas/EpipolarPose">Self-Supervised
Learning of 3D Human Pose using Multi-view Geometry</a>, CVPR 2018,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.02330">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/vita-epfl/openpifpaf">PifPaf: Composite
Fields for Human Pose Estimation</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.06593">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch">Deep
High-Resolution Representation Learning for Human Pose
Estimation</a>,CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.09212.pdf">Paper</a>]</strong>,
<strong>[<a
target="_blank" rel="noopener" href="https://jingdongwang2017.github.io/Projects/HRNet/PoseEstimation.html">Project
Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/YuliangXiu/PoseFlow">PoseFlow: Efficient
Online Pose Tracking)</a>, BMVC 2018, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.00977">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/vana77/Bottom-up-Clustering-Person-Re-identification">A
Bottom-Up Clustering Approach to Unsupervised Person
Re-identification</a>ï¼ŒAAAI 2019, é‡å®šä½</li>
<li><a target="_blank" rel="noopener" href="https://github.com/foolwood/SiamMask">Fast Online Object
Tracking and Segmentation: A Unifying Approach</a>,CVPR 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.05050">Paper</a>] [<a
target="_blank" rel="noopener" href="https://youtu.be/I_iOVrcpEBw">Video</a>] [<a
target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~qwang/SiamMask">Project
Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/TuSimple/simpledet">SimpleDet - A Simple
and Versatile Framework for Object Detection and Instance
Recognition</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.05831">Paper</a>]</strong></li>
</ul>
<h2 id="depthdisparity-flow-estimation">Depth/Disparity &amp; Flow
estimation</h2>
<ul>
<li>[<strong>Depth</strong>][SemiGlobalMatching](https://github.com/ethan-li-coding/SemiGlobalMatching),
SGMåŒç›®ç«‹ä½“åŒ¹é…ç®—æ³•å®Œæ•´å®ç°ï¼Œä»£ç è§„èŒƒï¼Œæ³¨é‡Šä¸°å¯Œä¸”æ¸…æ™°ï¼ŒCSDNåŒæ­¥æ•™å­¦</li>
<li><a target="_blank" rel="noopener" href="https://github.com/callmeray/PointMVSNet">PointMVSNet:
Point-based Multi-view Stereo Network</a>,ICCV 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.04422">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/JiaxiongQ/DeepLiDAR">DeepLiDAR</a>,CVPR
2019, <strong>[<a
target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qiu_DeepLiDAR_Deep_Surface_Normal_Guided_Depth_Prediction_for_Outdoor_Scene_CVPR_2019_paper.pdf">Paper</a>]</strong>,
å•å¼ RGBå›¾åƒ+ç¨€ç–é›·è¾¾æ•°æ®è¿›è¡Œå®¤å¤–åœºæ™¯æ·±åº¦ä¼°è®¡</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/atapour/monocularDepth-Inference">Real-Time
Monocular Depth Estimation using Synthetic Data with Domain Adaptation
via Image Style Transfer</a>,CVPR 2018, <strong>[<a
target="_blank" rel="noopener" href="http://breckon.eu/toby/publications/papers/abarghouei18monocular.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/princeton-vl/YouTube3D">Learning
Single-Image Depth from Videos using Quality Assessment
Networks</a>,CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.09573">Paper</a>]</strong>, <strong>[<a
target="_blank" rel="noopener" href="http://www-personal.umich.edu/~wfchen/youtube3d/">Project
Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/WERush/SCDA">SCDA: Adapting Object
Detectors via Selective Cross-Domain Alignment</a>,CVPR 2019,
<strong>[<a
target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Adapting_Object_Detectors_via_Selective_Cross-Domain_Alignment_CVPR_2019_paper.pdf">Paper</a>]</strong>,
<strong>[<a target="_blank" rel="noopener" href="http://zhuxinge.me/aboutme.html">Project
Page</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/fabiotosi92/monoResMatch-Tensorflow">Learning
monocular depth estimation infusing traditional stereo
knowledge</a>,CVPR 2019,<strong>[<a
target="_blank" rel="noopener" href="https://vision.disi.unibo.it/~ftosi/papers/monoResMatch.pdf">PDF</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/laoreja/HPLFlowNet">HPLFlowNet:
Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on
Large-scale Point Clouds</a>,CVPR 2019,<strong>[<a
href="hhttps://web.cs.ucdavis.edu/~yjlee/projects/cvpr2019-HPLFlowNet.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/feihuzhang/GANet">GA-Net: Guided
Aggregation Net for End-to-end Stereo Matching</a>,CVPR 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.06587.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/sunghoonim/DPSNet">DPSNet: End-to-end
Deep Plane Sweep Stereo</a>,ICLR 2019,<strong>[<a
target="_blank" rel="noopener" href="https://openreview.net/pdf?id=ryeYHi0ctQ">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/muskie82/AR-Depth-cpp">Fast Depth
Densification for Occlusion-aware Augmented Reality</a>, SIGGRAPH-Asia
2018, <strong>[<a
target="_blank" rel="noopener" href="https://homes.cs.washington.edu/~holynski/publications/occlusion/index.html">Project
Page</a>]</strong>,<a
target="_blank" rel="noopener" href="https://github.com/facebookresearch/AR-Depth">another
version</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/CVLAB-Unibo/Learning2AdaptForStereo">Learning
To Adapt For Stereo</a>, CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.02957">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/JiaRenChang/PSMNet">Pyramid Stereo
Matching Network</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.08669">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/lelimite4444/BridgeDepthFlow">Bridging
Stereo Matching and Optical Flow via Spatiotemporal Correspondence</a>,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.09265">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/wvangansbeke/Sparse-Depth-Completion">Sparse
Depth Completion</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.05356.pdf">Paper</a>]</strong>,
RGBå›¾åƒè¾…åŠ©é›·è¾¾æ·±åº¦ä¼°è®¡</li>
<li><a target="_blank" rel="noopener" href="https://github.com/sshan-zhao/GASDA">GASDA</a>, CVPR 2019,
<strong>[<a
target="_blank" rel="noopener" href="https://sshan-zhao.github.io/papers/gasda.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/xy-guo/MVSNet_pytorch">MVSNet: Depth
Inference for Unstructured Multi-view Stereo</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.02505">Paper</a>]</strong>,
éå®˜æ–¹å®ç°ç‰ˆæœ¬çš„MVSNet</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/HKUST-Aerial-Robotics/Stereo-RCNN">Stereo R-CNN
based 3D Object Detection for Autonomous Driving</a>, CVPR 2019,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.09738.pdf">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo">Real-time
self-adaptive deep stereo</a>, CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.05424">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ialhashim/DenseDepth">High Quality
Monocular Depth Estimation via Transfer Learning</a>,CVPR 2019,
<strong>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.11941">Paper</a>]</strong>,
<strong>[<a
target="_blank" rel="noopener" href="https://ialhashim.github.io/publications/index.html">Project
Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/xy-guo/GwcNet">Group-wise Correlation
Stereo Network</a>,CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.04025">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/phuang17/DeepMVS">DeepMVS: Learning
Multi-View Stereopsis</a>, CVPR 2018,<strong>[<a
target="_blank" rel="noopener" href="https://phuang17.github.io/DeepMVS/index.html">Project
Page</a>]</strong>,å¤šç›®æ·±åº¦ä¼°è®¡</li>
<li><a target="_blank" rel="noopener" href="https://github.com/sampepose/flownet2-tf">FlowNet 2.0:
Evolution of Optical Flow Estimation with Deep Networks</a>, CVPR 2017,
æ·±åº¦å­¦ä¹ å…‰æµæ¢å¤</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/DLuensch/StereoVision-ADCensus">StereoVision-ADCensus</a>,æ·±åº¦æ¢å¤ä»£ç é›†åˆ(<strong>ADCensus,
SGBM, BM</strong>)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/yangguorun/SegStereo">SegStereo:
Exploiting Semantic Information for Disparity Estimation</a>,
æ¢ç©¶è¯­ä¹‰ä¿¡æ¯åœ¨æ·±åº¦ä¼°è®¡ä¸­çš„ä½œç”¨</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/kuantingchen04/Light-Field-Depth-Estimation">Light
Filed Depth Estimation using GAN</a>ï¼Œåˆ©ç”¨GANè¿›è¡Œå…‰åœºæ·±åº¦æ¢å¤</li>
<li><a target="_blank" rel="noopener" href="https://github.com/daniilidis-group/EV-FlowNet">EV-FlowNet:
Self-Supervised Optical Flow for Event-based Cameras</a>,Proceedings of
Robotics 2018,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.06898">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/vt-vl-lab/DF-Net">DF-Net: Unsupervised
Joint Learning of Depth and Flow using Cross-Task Consistency</a>, ECCV
2018, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.01649">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/yzcjtr/GeoNet">GeoNet: Unsupervised
Learning of Dense Depth, Optical Flow and Camera Pose</a>, CVPR 2018,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.02276">Paper</a>]</strong></li>
</ul>
<h2 id="d-graphic">3D &amp; Graphic</h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/WangYueFt/prnet">PRNet: Self-Supervised
Learning for Partial-to-Partial Registration</a>,NeurIPS 2019</li>
<li><a target="_blank" rel="noopener" href="https://github.com/nkolot/SPIN">Learning to Reconstruct 3D
Human Pose and Shape via Model-fitting in the Loop</a>,ICCV 2019,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.12828.pdf">Paper</a>]</strong> ,
<strong>[<a
target="_blank" rel="noopener" href="https://www.seas.upenn.edu/~nkolot/projects/spin/">Project
Page</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/microsoft/multiview-human-pose-estimation-pytorch">Cross
View Fusion for 3D Human Pose Estimation</a>,ICCV 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.01203">Paper</a>]</strong>
,è·¨è§†è§’3Dä½å§¿ä¼°è®¡</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Fanziapril/mvfnet">MVF-Net: Multi-View
3D Face Morphable Model Regression</a>,å¤šè§†è§’3Däººè„¸é‡å»º, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.04473">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/saurabheights/KillingFusion">KillingFusion</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/PRBonn/refusion">ReFusion: 3D
Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting
Residuals</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.02082.pdf">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Lotayou/densebody_pytorch">densebody_pytorch</a>,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.10153v3">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/svip-lab/PlanarReconstruction">Single-Image
Piece-wise Planar 3D Reconstruction via Associative Embedding</a>,CVPR
2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.09777.pdf">Paper</a>]</strong>,
å•ç›®3Dé‡å»º</li>
<li><a target="_blank" rel="noopener" href="https://github.com/sunset1995/HorizonNet">HorizonNet:
Learning Room Layout with 1D Representation and Pano Stretch Data
Augmentation</a>,CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.03861">Paper</a>]</strong>,
æ·±åº¦å­¦ä¹ å…¨æ™¯è½¬3D</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Microsoft/O-CNN">Adaptive O-CNN: A
Patch-based Deep Representation of 3D Shapes</a>,SIGGRAPH Asia 2018,
<strong>[<a target="_blank" rel="noopener" href="https://wang-ps.github.io/AO-CNN.html">Project
Page</a>]</strong></li>
</ul>
<h2 id="other-collections">Other Collections</h2>
<ul>
<li><a
target="_blank" rel="noopener" href="https://github.com/timqian/chinese-independent-blogs">chinese-independent-blogs</a>,
ä¸­æ–‡ç‹¬ç«‹åšå®¢é›†é”¦</li>
<li><a target="_blank" rel="noopener" href="https://github.com/RenYurui/StructureFlow">StructureFlow:
Image Inpainting via Structure-aware Appearance
Flow</a>,å›¾åƒinpainting</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/ruanyf/free-books">free-books</a>,äº’è”ç½‘ä¸Šçš„å…è´¹ä¹¦ç±</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>,é€šç”¨çš„å­¦æœ¯ä¸»é¡µæ¨¡ç‰ˆ</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/microsoft/MMdnn">MMdnn</a>,å®ç°æ·±åº¦å­¦ä¹ æ¨¡å‹ä¹‹é—´çš„ç›¸äº’è½¬æ¢</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/abner2015/tensorflow2caffemodel">tensorflow2caffemodel</a>,tensorflowæ¨¡å‹è½¬caffemodel</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/fengdu78/lihang-code">lihang-code</a>,ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹çš„ä»£ç å®ç°</li>
<li><a target="_blank" rel="noopener" href="https://github.com/DLTcollab/sse2neon">sse2neon</a>,<a
target="_blank" rel="noopener" href="https://github.com/jratcliff63367/sse2neon">sse2neon</a>,SSEè½¬neonï¼ŒåµŒå…¥å¼ç§»æ¤æ—¶å¯èƒ½ä¼šç”¨åˆ°;</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/alirezadir/Production-Level-Deep-Learning">Production-Level-Deep-Learning</a>,æ·±åº¦å­¦ä¹ æ¨¡å‹éƒ¨ç½²æµç¨‹</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/ShusenTang/Dive-into-DL-PyTorch">åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ Dive-into-DL-PyTorch</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/deeplearning-ai/machine-learning-yearning-cn">machine-learning-yearning-cn</a>ï¼ŒMachine
Learning Yearning ä¸­æ–‡ç‰ˆ - ã€Šæœºå™¨å­¦ä¹ è®­ç»ƒç§˜ç±ã€‹ - Andrew Ng è‘—</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/academicpages/academicpages.github.io">academicpages.github.io</a>ï¼Œå­¦æœ¯ä¸»é¡µæ¨¡æ¿</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">Coursera-ML-AndrewNg-Notes</a>,å´æ©è¾¾è€å¸ˆçš„æœºå™¨å­¦ä¹ è¯¾ç¨‹ä¸ªäººç¬”è®°</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/roboticcam/machine-learning-notes">machine-learning-notes</a>,æœºå™¨å­¦ä¹ ï¼Œæ¦‚ç‡æ¨¡å‹å’Œæ·±åº¦å­¦ä¹ çš„è®²ä¹‰(1500+é¡µ)å’Œè§†é¢‘é“¾æ¥</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/scutan90/CNN-Visualization">CNN-Visualization</a>,CNNå¯è§†åŒ–ã€ç†è§£CNN</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/mrgloom/awesome-semantic-segmentation">Awesome
Semantic Segmentation</a>, è¯­ä¹‰åˆ†å‰²é›†åˆ</li>
<li><a target="_blank" rel="noopener" href="https://github.com/mengyuest/iros2018-slam-papers">IROS2018
SLAM Collections</a>, IROS 2018é›†åˆ</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/TerenceCYJ/VP-SLAM-SC-papers">VP-SLAM-SC-papers</a>,Visual
Positioning &amp; SLAM &amp; Spatial Cognition è®ºæ–‡ç»Ÿè®¡ä¸åˆ†æ</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/HuaizhengZhang/Awesome-System-for-Machine-Learning">Awesome
System for Machine Learning</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Thinkgamer/Machine-Learning-With-Python">Machine-Learning-With-Python</a>,
ã€Šæœºå™¨å­¦ä¹ å®æˆ˜ã€‹pythonä»£ç å®ç°</li>
<li><a target="_blank" rel="noopener" href="https://github.com/qqfly/how-to-learn-robotics">How to
learn robotics</a>, å¼€æºæœºå™¨äººå­¦å­¦ä¹ æŒ‡å—</li>
<li><a target="_blank" rel="noopener" href="https://github.com/kjw0612/awesome-deep-vision">Awesome
Deep Vision</a>,DLåœ¨CVé¢†åŸŸçš„åº”ç”¨</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/YapengTian/Single-Image-Super-Resolution">Single-Image-Super-Resolution</a>,
ä¸€ä¸ªæœ‰å…³<strong>å›¾åƒè¶…åˆ†è¾¨</strong>çš„åˆé›†</li>
<li><a target="_blank" rel="noopener" href="https://github.com/wifity/ai-report">ai report</a>,
AIç›¸å…³çš„ç ”ç©¶æŠ¥å‘Š</li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota">State-of-the-art papers
and code</a>,æœé›†äº†ç›®å‰sotaçš„è®ºæ–‡ä»¥åŠä»£ç </li>
<li><a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/cvpr2019">CVPR 2019
(Papers/Codes/Project/Paper reading)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/openMVG/awesome_3DReconstruction_list">A
curated list of papers &amp; resources linked to 3D reconstruction from
images</a>,æœ‰å…³ä¸‰ç»´é‡å»ºçš„è®ºæ–‡æ±‡æ€»</li>
<li><a target="_blank" rel="noopener" href="https://github.com/nebula-beta/SLAM-Jobs">SLAM-Jobs</a>,
SLAM/SFMæ±‚èŒæŒ‡å—</li>
<li><a target="_blank" rel="noopener" href="https://github.com/stevewongv/SPANet">Spatial Attentive
Single-Image Deraining with a High Quality Real Rain Dataset</a>,CVPR
2019,å»é›¨</li>
<li><a target="_blank" rel="noopener" href="https://github.com/hezhangsprinter/DCPDN">Densely Connected
Pyramid Dehazing Network</a>,CVPR 2018,å»é›¾</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsr">MMSR</a>ï¼ŒMMLABæ¨å‡ºçš„è¶…åˆ†è¾¨å·¥å…·ç®±</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Bartzi/stn-ocr">æ·±åº¦å­¦ä¹ OCR</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Vay-keen/Machine-learning-learning-notes">è¥¿ç“œä¹¦ğŸ‰å­¦ä¹ ç¬”è®°</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/wwxFromTju/awesome-reinforcement-learning-zh">awesome-reinforcement-learning-zh</a>,å¼ºåŒ–å­¦ä¹ ä»å…¥é—¨åˆ°æ”¾å¼ƒçš„èµ„æ–™</li>
<li><a target="_blank" rel="noopener" href="https://github.com/cszn/DPSR">Deep Plug-and-Play
Super-Resolution for Arbitrary Blur Kernels</a>,CVPR 2019,è¶…åˆ†è¾¨</li>
<li><a target="_blank" rel="noopener" href="https://github.com/lzhbrian/Cool-Fashion-Papers">Cool
Fashion Papers</a>, Cool resources about Fashion + AI.</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/nbei/Deep-Flow-Guided-Video-Inpainting">Deep
Flow-Guided Video Inpainting</a>,CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.10447.pdf">Paper</a>]</strong>
,å›¾åƒä¿®å¤</li>
<li><a target="_blank" rel="noopener" href="https://github.com/dbolya/yolact">YOLACT: Real-time
Instance Segmentation</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/lyl8213/Plate_Recognition-LPRnet">LPRNet:
License Plate Recognition via Deep Neural Networks</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.10447.pdf">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/xiaofengShi/CHINESE-OCR">CHINESE-OCR</a>,
è¿ç”¨tfå®ç°è‡ªç„¶åœºæ™¯æ–‡å­—æ£€æµ‹</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/PerpetualSmile/BeautyCamera">BeautyCamera</a>,
ç¾é¢œç›¸æœºï¼Œå…·æœ‰äººè„¸æ£€æµ‹ã€ç£¨çš®ç¾ç™½äººè„¸ã€æ»¤é•œã€è°ƒèŠ‚å›¾ç‰‡ã€æ‘„åƒåŠŸèƒ½</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/zhengzhugithub/CV-arXiv-Daily">CV-arXiv-Daily</a>,
åˆ†äº«è®¡ç®—æœºè§†è§‰æ¯å¤©çš„arXivæ–‡ç« </li>
<li><a
target="_blank" rel="noopener" href="https://github.com/lyndonzheng/Pluralistic-Inpainting">Pluralistic-Inpainting</a>,
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.04227">ArXiv</a> | <a
target="_blank" rel="noopener" href="http://www.chuanxiaz.com/publication/pluralistic/">Project
Page</a> | <a
target="_blank" rel="noopener" href="http://www.chuanxiaz.com/project/pluralistic/">Online Demo</a> |
<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=9V7rNoLVmSs">Video(demo)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Jezzamonn/fourier">An Interactive
Introduction to Fourier Transforms</a>, è¶…æ£’çš„å‚…é‡Œå¶å˜æ¢å›¾å½¢åŒ–è§£é‡Š</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/datawhalechina/pumpkin-book">pumpkin-book</a>,
ã€Šæœºå™¨å­¦ä¹ ã€‹ï¼ˆè¥¿ç“œä¹¦ï¼‰å…¬å¼æ¨å¯¼è§£æ</li>
<li><a target="_blank" rel="noopener" href="https://github.com/JuliaLang/julia">Julia</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/alan-turing-institute/MLJ.jl">A Julia
machine learning framework</a>ï¼Œä¸€ç§åŸºäºJuliaçš„æœºå™¨å­¦ä¹ æ¡†æ¶</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/ZhaoJ9014/face.evoLVe.PyTorch">High-Performance
Face Recognition Library on PyTorch</a>ï¼Œäººè„¸è¯†åˆ«åº“</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/enggen/Deep-Learning-Coursera">Deep-Learning-Coursera</a>ï¼Œæ·±åº¦å­¦ä¹ æ•™ç¨‹ï¼ˆdeeplearning.aiï¼‰</li>
<li><a target="_blank" rel="noopener" href="https://github.com/RemoteML/bestofml">The best resources
around Machine Learning</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/cydonia999/VGGFace2-pytorch">VGGFace2: A
dataset for recognising faces across pose and age</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/SmirkCao/Lihang">Statistical learning
methods</a>ï¼Œç»Ÿè®¡å­¦ä¹ æ–¹æ³•</li>
<li><a
target="_blank" rel="noopener" href="https://live.bilibili.com/7332534?visit_id=9ytrx9lpsy80">End-to-end
Adversarial Learning for Generative Conversational
Agents</a>ï¼Œ2017ï¼Œä»‹ç»äº†ä¸€ç§ç«¯åˆ°ç«¯çš„åŸºäºGANçš„èŠå¤©æœºå™¨äºº</li>
<li><a target="_blank" rel="noopener" href="https://github.com/yulunzhang/RNAN">Residual Non-local
Attention Networks for Image Restoration</a>,ICLR 2019.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/HelenMao/MSGAN">MSGAN: Mode Seeking
Generative Adversarial Networks for Diverse Image Synthesis</a>, CVPR
2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.05628">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVlabs/SPADE">SPADE: Semantic Image
Synthesis with Spatially-Adaptive Normalization</a>,CVPR 2019,
<strong>[<a target="_blank" rel="noopener" href="https://nvlabs.github.io/SPADE/">Project
Page</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Oldpan/Faceswap-Deepfake-Pytorch">Faceswap with
Pytorch or DeepFake with Pytorch</a>, æ¢è„¸</li>
<li><a target="_blank" rel="noopener" href="https://github.com/iperov/DeepFaceLab">DeepFaceLab</a>,
æ¢è„¸</li>
</ul>
<!-- ![](https://vincentqin.tech/blog-resources/westlake/westlake-1.jpg) -->

    </div>

    
    
    
      


    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>Post author:  </strong>Vincent Qin
  </li>
  <li class="post-copyright-link">
      <strong>Post link: </strong>
      <a href="https://www.vincentqin.tech/posts/awesome-works/" title="ğŸ”¥Awesome CV Works">https://www.vincentqin.tech/posts/awesome-works/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/SLAM/" rel="tag"># SLAM</a>
              <a href="/tags/pose-tracking/" rel="tag"># pose-tracking</a>
              <a href="/tags/object-tracking/" rel="tag"># object-tracking</a>
              <a href="/tags/disparity/" rel="tag"># disparity</a>
              <a href="/tags/depth-estimation/" rel="tag"># depth-estimation</a>
              <a href="/tags/flow-estimation/" rel="tag"># flow-estimation</a>
              <a href="/tags/3D-graphics/" rel="tag"># 3D-graphics</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/build-ssr-server/" rel="prev" title="å¼€å¯SSRæ¨¡å¼">
                  <i class="fa fa-chevron-left"></i> å¼€å¯SSRæ¨¡å¼
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/first-black-hole/" rel="next" title="Black Hole">
                  Black Hole <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2016 â€“ 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vincent Qin</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.8/pdfobject.min.js","integrity":"sha256-tu9j5pBilBQrWSDePOOajCUdz6hWsid/lBNzK4KgEPM="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.1.6/mermaid.min.js","integrity":"sha256-ZfzwelSToHk5YAcr9wbXAmWgyn9Jyq08fSLrLhZE89w="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"en-US","enable":true,"serverURL":"https://comments.vincentqin.tech","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":true,"locale":{"placeholder":"Welcome to comment"},"emoji":["https://unpkg.com/@waline/emojis@1.1.0/weibo","https://unpkg.com/@waline/emojis@1.1.0/alus","https://unpkg.com/@waline/emojis@1.1.0/bilibili","https://unpkg.com/@waline/emojis@1.1.0/qq","https://unpkg.com/@waline/emojis@1.1.0/tieba","https://unpkg.com/@waline/emojis@1.1.0/tw-emoji"],"meta":["nick","mail","link"],"requiredMeta":["nick","mail"],"wordLimit":0,"login":"enable","el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/posts/awesome-works/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
