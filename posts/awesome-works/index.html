<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/realcat-apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/realcat-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/realcat-32x32.png">
  <link rel="mask-icon" href="/images/realcat-safari-pinned-tab.svg" color="#222">
  <meta name="google-site-verification" content="u46QTaG_Dv3OZLpOBKYtqyuiNtIdnhSG5ASKoNvGBCM">
  <meta name="baidu-site-verification" content="MtcbwE45ft">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.vincentqin.tech","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.13.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"show_result":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":"waline","storage":true,"lazyload":true,"nav":null,"activeClass":"waline"},"stickytabs":true,"motion":{"enable":false,"async":true,"transition":{"post_block":"fadeIn","post_header":"fadeIn","post_body":"fadeIn","coll_header":"fadeIn","sidebar":"fadeIn"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="The post contains papers-with-code about SLAM, Pose&#x2F;Object tracking, Depth&#x2F;Disparity&#x2F;Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc.">
<meta property="og:type" content="article">
<meta property="og:title" content="🔥Awesome CV Works">
<meta property="og:url" content="https://www.vincentqin.tech/posts/awesome-works/index.html">
<meta property="og:site_name" content="RealCat">
<meta property="og:description" content="The post contains papers-with-code about SLAM, Pose&#x2F;Object tracking, Depth&#x2F;Disparity&#x2F;Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://vincentqin.tech/blog-resources/awesome-works/github-star.png">
<meta property="og:image" content="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg">
<meta property="og:image" content="http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019.svg">
<meta property="og:image" content="https://img.shields.io/badge/license-Anti%20996-blue.svg?style=flat-square">
<meta property="article:published_time" content="2019-03-31T12:15:41.000Z">
<meta property="article:modified_time" content="2022-09-04T18:19:11.061Z">
<meta property="article:author" content="Vincent Qin">
<meta property="article:tag" content="SLAM">
<meta property="article:tag" content="pose-tracking">
<meta property="article:tag" content="object-tracking">
<meta property="article:tag" content="disparity">
<meta property="article:tag" content="depth-estimation">
<meta property="article:tag" content="flow-estimation">
<meta property="article:tag" content="3D-graphics">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://vincentqin.tech/blog-resources/awesome-works/github-star.png">


<link rel="canonical" href="https://www.vincentqin.tech/posts/awesome-works/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://www.vincentqin.tech/posts/awesome-works/","path":"posts/awesome-works/","title":"🔥Awesome CV Works"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>🔥Awesome CV Works | RealCat</title>
  
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-97856334-1","only_pageview":true}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>





<link rel="dns-prefetch" href="https://comments.vincentqin.tech">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">RealCat</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Turn on, Tune in, Drop out</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">77</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">14</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">111</span></a></li><li class="menu-item menu-item-collections"><a href="/collections" rel="section"><i class="fa fa-diamond fa-fw"></i>Collections</a></li><li class="menu-item menu-item-guest_comments"><a href="/guestbook" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#slam-related"><span class="nav-number">1.</span> <span class="nav-text">SLAM related</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#poseobject-tracking"><span class="nav-number">2.</span> <span class="nav-text">Pose&#x2F;Object tracking</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#depthdisparity-flow-estimation"><span class="nav-number">3.</span> <span class="nav-text">Depth&#x2F;Disparity &amp; Flow
estimation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#d-graphic"><span class="nav-number">4.</span> <span class="nav-text">3D &amp; Graphic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#other-collections"><span class="nav-number">5.</span> <span class="nav-text">Other Collections</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Vincent Qin"
      src="https://vincentqin.gitee.io/images/qin_small.png">
  <p class="site-author-name" itemprop="name">Vincent Qin</p>
  <div class="site-description" itemprop="description">Keep Your Curiosity</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">77</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">111</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Vincentqyw" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Vincentqyw" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:realcat@126.com" title="Email → mailto:realcat@126.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>Email</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://vincentqin.gitee.io/images/qrcode_realcat.jpg" title="Wechat → https:&#x2F;&#x2F;vincentqin.gitee.io&#x2F;images&#x2F;qrcode_realcat.jpg" rel="noopener" target="_blank"><i class="fab fa-weixin fa-fw"></i>Wechat</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/i_vincent/activities" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;i_vincent&#x2F;activities" rel="noopener" target="_blank"><i class="fab fa-quora fa-fw"></i>Zhihu</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/AlphaRealcat" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;AlphaRealcat" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/18136563" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;18136563" rel="noopener" target="_blank"><i class="fa fa-video-camera fa-fw"></i>Bilibili</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://bafybeic2jt62kpyh6cz2g4ngxs4kazojfw3dhx53mco3wc6f56dejty4xm.ipfs.infura-ipfs.io/" title="Web3.0 → https:&#x2F;&#x2F;bafybeic2jt62kpyh6cz2g4ngxs4kazojfw3dhx53mco3wc6f56dejty4xm.ipfs.infura-ipfs.io" rel="noopener" target="_blank"><i class="link fa-fw"></i>Web3.0</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-dashboard"></i>
      Scholar
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://xxx.itp.ac.cn/" title="http:&#x2F;&#x2F;xxx.itp.ac.cn" rel="noopener" target="_blank">Arxiv-Mirror</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://arxiv-sanity.com/" title="http:&#x2F;&#x2F;arxiv-sanity.com&#x2F;" rel="noopener" target="_blank">Arxiv-sanity</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://openaccess.thecvf.com/menu.py" title="http:&#x2F;&#x2F;openaccess.thecvf.com&#x2F;menu.py" rel="noopener" target="_blank">CVF</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://paperswithcode.com/sota" title="https:&#x2F;&#x2F;paperswithcode.com&#x2F;sota" rel="noopener" target="_blank">Paper&Code</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://scihub.wikicn.top/" title="https:&#x2F;&#x2F;scihub.wikicn.top&#x2F;" rel="noopener" target="_blank">Scihub</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://ras.papercept.net/conferences/scripts/start.pl" title="http:&#x2F;&#x2F;ras.papercept.net&#x2F;conferences&#x2F;scripts&#x2F;start.pl" rel="noopener" target="_blank">RAS</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://openreview.net/" title="https:&#x2F;&#x2F;openreview.net&#x2F;" rel="noopener" target="_blank">OpenReview</a>
        </li>
    </ul>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-battery-three-quarters fa-fw"></i>
      Friends Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.wangpengan.com/" title="http:&#x2F;&#x2F;www.wangpengan.com&#x2F;" rel="noopener" target="_blank">Tensorboy</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://simtalk.cn/" title="http:&#x2F;&#x2F;simtalk.cn&#x2F;" rel="noopener" target="_blank">Simshang</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sttomato.github.io/" title="https:&#x2F;&#x2F;sttomato.github.io" rel="noopener" target="_blank">Tomato</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://dfine.tech/" title="http:&#x2F;&#x2F;dfine.tech&#x2F;" rel="noopener" target="_blank">Newdee</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://cs-people.bu.edu/yfhu/" title="http:&#x2F;&#x2F;cs-people.bu.edu&#x2F;yfhu&#x2F;" rel="noopener" target="_blank">WhoIf</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://yulunzhang.com/" title="http:&#x2F;&#x2F;yulunzhang.com&#x2F;" rel="noopener" target="_blank">Yulun</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sanglongbest.github.io/" title="https:&#x2F;&#x2F;sanglongbest.github.io&#x2F;" rel="noopener" target="_blank">YangLiu</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.erenship.com/" title="https:&#x2F;&#x2F;www.erenship.com&#x2F;" rel="noopener" target="_blank">Eren</a>
        </li>
    </ul>
  </div>

  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-briefcase"></i>
      Common Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://comments.vincentqin.tech/ui" title="https:&#x2F;&#x2F;comments.vincentqin.tech&#x2F;ui" rel="noopener" target="_blank">Comments</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://gitee.com/vincentqin/vincentqin" title="https:&#x2F;&#x2F;gitee.com&#x2F;vincentqin&#x2F;vincentqin" rel="noopener" target="_blank">Source</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.notion.so/realcat" title="https:&#x2F;&#x2F;www.notion.so&#x2F;realcat" rel="noopener" target="_blank">Notion</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.matrixcalculus.org/" title="http:&#x2F;&#x2F;www.matrixcalculus.org&#x2F;" rel="noopener" target="_blank">Calculus</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://emojipedia.org/" title="https:&#x2F;&#x2F;emojipedia.org&#x2F;" rel="noopener" target="_blank">Emoji</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://unstoppabledomains.com/" title="https:&#x2F;&#x2F;unstoppabledomains.com&#x2F;" rel="noopener" target="_blank">UD</a>
        </li>
    </ul>
  </div>




        </div>

      <div class="wechat_QR_code">
      <!-- 二维码 -->
      <img src ="https://vincentqin.tech/blog-resources/qrcode_realcat.jpg">
      <span>Follow Me on Wechat</span>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://www.vincentqin.tech/posts/awesome-works/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://vincentqin.gitee.io/images/qin_small.png">
      <meta itemprop="name" content="Vincent Qin">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RealCat">
      <meta itemprop="description" content="Keep Your Curiosity">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="🔥Awesome CV Works | RealCat">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          🔥Awesome CV Works
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-03-31 20:15:41" itemprop="dateCreated datePublished" datetime="2019-03-31T20:15:41+08:00">2019-03-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-09-05 02:19:11" itemprop="dateModified" datetime="2022-09-05T02:19:11+08:00">2022-09-05</time>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline: </span>
  
    <a title="waline" href="/posts/awesome-works/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/posts/awesome-works/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-item" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="waline-pageview-count" data-path="/posts/awesome-works/"></span>
    </span>
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <script src="/assets/js/DPlayer.min.js"> </script><div class="note success"><p>The post contains papers-with-code about SLAM, Pose/Object tracking,
Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep
Learning etc.</p>
</div>
<!-- [![GitHub stars](https://img.shields.io/github/stars/Vincentqyw/Recent-Stars-2019.svg?logo=github&label=Stars)](https://github.com/Vincentqyw/Recent-Stars-2019) -->
<span id="more"></span>
<p>Here is the <a
target="_blank" rel="noopener" href="https://github.com/Vincentqyw/Recent-Stars-2022">repo</a>: <a
target="_blank" rel="noopener" href="https://github.com/Vincentqyw/Recent-Stars-2022"><img data-src="https://github-readme-stats.vercel.app/api/pin/?username=Vincentqyw&amp;repo=Recent-Stars-2022&amp;show_owner=false&amp;theme=default"
alt="ReadMe Card" /></a></p>
<p>I posted the content of the repo as follows.
<!--# Recent Stars 2020--></p>
<!-- <p align="center">
 <img width="100px" data-src="https://vincentqin.tech/blog-resources/awesome-works/github-star.png" align="center" alt="" />
</p> -->
<!-- <p align="center">
  <a target="_blank" rel="noopener" href="https://github.com/Vincentqyw/Recent-Stars-2020">
    <img alt="Awesome" data-src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" />
  </a>
  <a target="_blank" rel="noopener" href="http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019">
    <img alt="HitCount" data-src="http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019.svg" />
  </a>
  <a target="_blank" rel="noopener" href="https://vincentqin.tech">
    <img alt="LICENSE" data-src="https://img.shields.io/badge/license-Anti%20996-blue.svg?style=flat-square" />
  </a>
</p> -->
<!--
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/Vincentqyw/Recent-Stars-2020)
[![HitCount](http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019.svg)](http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019)
[![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg?style=flat-square)](https://github.com/Vincentqyw/Recent-Stars-2020)
✔ This repo collects some links with papers which I recently starred related on SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, etc.
-->
<h2 id="slam-related">SLAM related</h2>
<ul>
<li>[<strong>SLAM</strong>][ORB-SLAM3: An Accurate Open-Source Library
for Visual, Visual-Inertial and Multi-Map
SLAM](https://github.com/UZ-SLAMLab/ORB_SLAM3), <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.11898">PDF</a>]</strong></li>
<li>[<strong>SLAM</strong>][LIO-SAM](https://github.com/TixiaoShan/LIO-SAM),
激光雷达IMU紧耦合SLAM</li>
<li>[<strong>Tool</strong>][Robotics Toolbox for
Python](https://github.com/petercorke/robotics-toolbox-python), a Python
implementation of the <a
target="_blank" rel="noopener" href="https://github.com/petercorke/robotics-toolbox-matlab">Robotics
Toolbox for MATLAB®</a></li>
<li>[<strong>Matching</strong>][LISRD](https://github.com/rpautrat/LISRD),ECCV
2020, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.08988">PDF</a>]</strong>，在线局部不变特征匹配！重要！</li>
<li>[<strong>Matching</strong>][AdaLAM](https://github.com/cavalli1234/AdaLAM),特征匹配快速滤除外点</li>
<li>[<strong>Calib</strong>][fisheye_pinhole_calib_demo](https://github.com/3DCVer/fisheye_pinhole_calib_demo),
包括鱼眼模型、针孔模型的相机标定，封装了自动编译、库的打包以及外部库的调用测试</li>
<li>[<strong>Calib</strong>][SensorCalibration](https://github.com/FENGChenxi0823/SensorCalibration),
IMU雷达标定</li>
<li>[<strong>VO</strong>][Low-Drift Visual Odometry in Structured
Environments by Decoupling Rotational and Translational
Motion](https://github.com/PyojinKim/LPVO),ICRA 2018, <strong>[<a
target="_blank" rel="noopener" href="http://pyojinkim.com/download/papers/2018_ICRA.pdf">PDF</a>]</strong>,
结构化环境中将旋转量与平移量进行分离优化</li>
<li>[<strong>VIO</strong>][VIO-SLAM](https://github.com/iamwangyabin/VIO-SLAM),
从零开始手写VIO课后作业</li>
<li>[<strong>Matching</strong>][TFMatch: Learning-based image matching
in TensorFlow](https://github.com/lzx551402/tfmatch),TensorFlow 实现的
GeoDesc,ASLFeat以及ContextDesc</li>
<li>[<strong>Tutorial</strong>][SLAM-BOOK](https://github.com/yanyan-li/SLAM-BOOK),
一本关于SLAM的书稿，清楚的介绍SLAM系统中的使用的几何方法和深度学习方法，持续更新中</li>
<li>[<strong>Loop Closing</strong>][OverlapNet - Loop Closing for 3D
LiDAR-based SLAM](https://github.com/PRBonn/OverlapNet), RSS 2020,
<strong>[<a
target="_blank" rel="noopener" href="https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/chen2020rss.pdf">PDF</a>]</strong>,
3D激光雷达SLAM闭环</li>
<li>[<strong>SLAM</strong>][VDO_SLAM](https://github.com/halajun/VDO_SLAM),
RGB-D相机数据作为输入，实现追踪动态物体SLAM的功能, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.11052">PDF</a>]</strong></li>
<li>[<strong>SLAM</strong>][orbslam-map-saving-extension](https://github.com/TUMFTM/orbslam-map-saving-extension)，在ORB-SLAM的基础上增加保存+加载地图功能</li>
<li>[<strong>Tutorial</strong>][Modern Robotics: Mechanics, Planning,
and Control Code Library](https://github.com/NxRLab/ModernRobotics),
现代机器人学, <strong>[<a
target="_blank" rel="noopener" href="http://hades.mech.northwestern.edu/index.php/Modern_Robotics">Homepage</a>]</strong></li>
<li>[<strong>Matching</strong>][image-matching-benchmark-baselines](https://github.com/vcg-uvic/image-matching-benchmark-baselines),
图像特征匹配挑战赛主页</li>
<li>[<strong>Matching</strong>][GraphLineMatching](https://github.com/mameng1/GraphLineMatching)</li>
<li>[<strong>Matching</strong>][Locality Preserving
Matching](https://github.com/jiayi-ma/LPM), IJCAI 2017, <strong>[<a
target="_blank" rel="noopener" href="https://ai.tencent.com/ailab/media/publications/YuanGao_IJCAI2017_LocalityPreservingMatching.pdf">PDF</a>]</strong></li>
<li>[<strong>IMU</strong>][IMUOrientationEstimator](https://github.com/ydsf16/IMUOrientationEstimator)</li>
<li>[<strong>Feature</strong>][BEBLID: Boosted Efficient Binary Local
Image Descriptor](https://github.com/iago-suarez/BEBLID)</li>
<li>[<strong>Relocalization</strong>][KFNet: Learning Temporal Camera
Relocalization using Kalman
Filtering](https://github.com/zlthinker/KFNet),CVPR 2020,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.10629">PDF</a>]</strong></li>
<li>[<strong>Matching</strong>][image-matching-benchmark](https://github.com/vcg-uvic/image-matching-benchmark)</li>
<li>[<strong>Matching</strong>][GMS: Grid-based Motion Statistics for
Fast, Ultra-robust Feature
Correspondence](https://github.com/JiawangBian/GMS-Feature-Matcher),CVPR
17 &amp; IJCV 19,<strong>[<a
target="_blank" rel="noopener" href="http://jwbian.net/Papers/GMS_CVPR17.pdf">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://jwbian.net/gms">Project page</a>]</strong></li>
<li>[<strong>Reloc</strong>][GN-Net-Benchmark](https://github.com/Artisense-ai/GN-Net-Benchmark),
CVPR 2020,GN-Net: The Gauss-Newton Loss for Multi-Weather
Relocalization, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.11932">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://vision.in.tum.de/gn-net">Project page</a>]</strong></li>
<li>[<strong>Matching</strong>][SuperGluePretrainedNetwork](https://github.com/magicleap/SuperGluePretrainedNetwork),
CVPR 2020, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.11763">PDF</a>]</strong>,
划重点！2020年sota超大视角2D特征匹配，<a
href="https://www.vincentqin.tech/posts/superglue/">Blog</a></li>
<li>[<strong>Feature</strong>][D3Feat](https://github.com/XuyangBai/D3Feat),
CVPR 2020, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.03164">PDF</a>]</strong></li>
<li>[<strong>Feature</strong>][ASLFeat](https://github.com/lzx551402/ASLFeat),
CVPR 2020, ASLFeat: Learning Local Features of Accurate Shape and
Localization, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.10071">PDF</a>]</strong></li>
<li>[<strong>Feature</strong>][GMS-Feature-Matcher](https://github.com/XuyangBai/D3Feat),
CVPR 2018, GMS: Grid-based Motion Statistics for Fast, Ultra-robust
Feature Correspondence, <strong>[<a
target="_blank" rel="noopener" href="http://jwbian.net/Papers/GMS_CVPR17.pdf">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://jwbian.net/gms">Project page</a>]</strong></li>
<li>[<strong>Feature</strong>][D3Feat](https://github.com/XuyangBai/D3Feat),
CVPR 2020, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.03164">PDF</a>]</strong></li>
<li>[<strong>Feature</strong>][3DFeatNet](https://github.com/yewzijian/3DFeatNet),
ECCV 2018, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.09413">PDF</a>]</strong></li>
<li>[<strong>Tutorial</strong>][AutonomousDrivingCookbook](https://github.com/microsoft/AutonomousDrivingCookbook)，Scenarios,
tutorials and demos for Autonomous Driving</li>
<li>[<strong>Tutorial</strong>][SLAMPaperReading](https://github.com/PaoPaoRobot/SLAMPaperReading)，泡泡机器人北京线下SLAM论文分享资料</li>
<li>[<strong>Tutorial</strong>][VIO_Tutotial_Course](https://github.com/lishuwei0424/VIO_Tutotial_Course)</li>
<li>[<strong>Tutorial</strong>][VO-SLAM-Review](https://github.com/MichaelBeechan/VO-SLAM-Review)</li>
<li>[<strong>Tutorial</strong>][VINS-Mono-code-annotation](https://github.com/QingSimon/VINS-Mono-code-annotation),VINS-Mono代码注释以及公式推导</li>
<li>[<strong>Tutorial</strong>][VINS-Mono-Learning](https://github.com/ManiiXu/VINS-Mono-Learning),VINS-Mono代码注释</li>
<li>[<strong>Tutorial</strong>][VINS-Course](https://github.com/HeYijia/VINS-Course),VINS-Mono
code without Ceres or ROS</li>
<li>[<strong>Tutorial</strong>][VIO-Doc](https://github.com/StevenCui/VIO-Doc),主流VIO论文推导及代码解析</li>
<li>[<strong>VO</strong>][CNN-DSO](https://github.com/muskie82/CNN-DSO),
Direct Sparse Odometry with CNN Depth Prediction</li>
<li>[<strong>VO</strong>][fisheye-ORB-SLAM](https://github.com/lsyads/fisheye-ORB-SLAM),
A real-time robust monocular visual SLAM system based on ORB-SLAM for
fisheye cameras, without rectifying or cropping the input images</li>
<li>[<strong>VO</strong>][ORB_Line_SLAM](https://github.com/robotseu/ORB_Line_SLAM),
Real-Time SLAM with BoPLW Pairs for Stereo Cameras, with Loop Detection
and Relocalization Capabilities</li>
<li>[<strong>VO</strong>][DeepVO-pytorch](https://github.com/ChiWeiHsiao/DeepVO-pytorch.git),
ICRA 2017 <a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/7989236/">DeepVO: Towards
end-to-end visual odometry with deep Recurrent Convolutional Neural
Networks</a></li>
<li>[<strong>Calib</strong>][CamOdomCalibraTool](https://github.com/MegviiRobot/CamOdomCalibraTool),
The tool to calibrate extrinsic param between camera and wheel.</li>
<li>[<strong>Calib</strong>][lidar_camera_calibration](https://github.com/heethesh/lidar_camera_calibration),<a
target="_blank" rel="noopener" href="https://github.com/ankitdhall/lidar_camera_calibration">another
version</a></li>
<li>[<strong>Calib</strong>][OdomLaserCalibraTool](https://github.com/MegviiRobot/OdomLaserCalibraTool.git)，相机与2D雷达标定</li>
<li>[<strong>Calib</strong>][extrinsic_lidar_camera_calibration](https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration),
LiDARTag: A Real-Time Fiducial Tag using Point Clouds, arXiv 2019,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.10349">PDF</a>]</strong></li>
<li>[<strong>Calib</strong>][velo2cam_calibration](https://github.com/beltransen/velo2cam_calibration),
Automatic Calibration algorithm for Lidar-Stereo camera, <strong>[<a
target="_blank" rel="noopener" href="http://wiki.ros.org/velo2cam_calibration">Project
page</a>]</strong></li>
<li>[<strong>Dataset</strong>][IRS: A Large Synthetic Indoor Robotics
Stereo Dataset for Disparity and Surface Normal
Estimation](https://github.com/HKBU-HPML/IRS.git)</li>
<li>[<strong>Tools</strong>][averaging-quaternions](https://github.com/christophhagen/averaging-quaternions),四元数平均</li>
</ul>
<hr />
<p>分割线，以下是2019年的星标项目，上面是2020年新星标的。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/naver/r2d2">R2D2: Reliable and
Repeatable Detector and Descriptor</a>,NeurIPS 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.06195">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://europe.naverlabs.com/research/publications/r2d2-reliable-and-repeatable-detectors-and-descriptors-for-joint-sparse-local-keypoint-detection-and-feature-extraction/">Project
page</a>]</strong>，深度学习特征点+描述子</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/1989Ryan/Semantic_SLAM">Semantic_SLAM</a>,语义SLAM：ROS
+ ORB SLAM + PSPNet101</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/BAILOOL/PlaceRecognition-LoopDetection">PlaceRecognition-LoopDetection</a>,
Light-weight place recognition and loop detection using road
markings</li>
<li><a target="_blank" rel="noopener" href="https://github.com/MISTLab/DOOR-SLAM">DOOR-SLAM:
Distributed, online, and outlier resilient SLAM for robotic
teams</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.12198">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://mistlab.ca/DOOR-SLAM/">Project
page</a>]</strong>，多机器人协作SLAM，增强了场景的适用性</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/shamangary/awesome-local-global-descriptor">awesome-local-global-descriptor</a>,
超详细深度学习特征点描述子集合，需要重点关注一下这个repo</li>
<li><a target="_blank" rel="noopener" href="https://github.com/zju3dv/GIFT">GIFT: Learning
Transformation-Invariant Dense Visual Descriptors via Group CNNs</a>,
NeurIPS 2019，<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.05932">PDF</a>]</strong>, <strong>[<a
target="_blank" rel="noopener" href="https://zju3dv.github.io/GIFT/">Project
page</a>]</strong>，浙大CAD+商汤联合实验室出品，利用Group
CNN来改进superpoint描述子（仅描述，特征点提取可任意选择），可以大幅度增强视角变化时的特征点复检率与匹配点数</li>
<li><a target="_blank" rel="noopener" href="https://github.com/axelBarroso/Key.Net">Key.Net: Keypoint
Detection by Handcrafted and Learned CNN Filters</a>,ICCV 2019,
<strong>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00889">PDF</a>]</strong>,
深度学习特征点</li>
<li><a target="_blank" rel="noopener" href="https://github.com/TRI-ML/KP3D">Self-Supervised 3D Keypoint
Learning for Ego-motion Estimation</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.03426">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=4hFhSD8QUPM">Youtube</a>]</strong>,
深度学习特征点</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Jichao-Peng/VINS-Mono-Optimization">VINS-Mono-Optimization</a>,
实现点线紧耦合优化的VINS-Mono</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/PetWorm/msckf_vio_zhushi">msckf_vio注释版本</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/lyakaap/NetVLAD-pytorch">NetVLAD-pytorch</a>,
NetVLAD场景识别的pytorch实现</li>
<li><a target="_blank" rel="noopener" href="http://microgps.cs.princeton.edu/">High-Precision
Localization Using Ground Texture (Micro-GPS)</a>,ECCV 2018,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.10687">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://microgps.cs.princeton.edu/">Project
page</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://microgps.cs.princeton.edu/data/micro-gps-cpp-master.zip">code</a>]</strong>，地向（摄像机朝向地面）SLAM，获得高精度重定位效果。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/LRMPUT/PlaneSLAM">PlaneSLAM</a>, Paper:
“On the Representation of Planes for Efficient Graph-based SLAM with
High-level Features”</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ucla-vision/xivo">XIVO: X Inertial-aided
Visual Odometry and Sparse Mapping</a>, an open-source repository for
visual-inertial odometry/mapping.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/lmb-freiburg/deeptam">DeepTAM</a>,ECCV
2018,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.01900.pdf">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://lmb.informatik.uni-freiburg.de/people/zhouh/deeptam/">Project
page</a>]</strong>,a learnt system for keyframe-based dense camera
tracking and mapping.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ajparra/iRotAvg">iRotAvg, Why bundle
adjust?</a>,ICRA 2019,<strong>[<a
target="_blank" rel="noopener" href="https://cs.adelaide.edu.au/~aparra/publication/parra19_icra/">PDF</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Kelym/FAST">Tactical Rewind:
Self-Correction via Backtracking in Vision-and-Language
Navigation</a>,CVPR 2019,<strong>[<a
target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Ke_Tactical_Rewind_Self-Correction_via_Backtracking_in_Vision-And-Language_Navigation_CVPR_2019_paper.html">PDF</a>]</strong>，视觉+语言导航</li>
<li><a target="_blank" rel="noopener" href="https://github.com/MISTLab/DOOR-SLAM">DOOR-SLAM</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/JiawangBian/FM-Bench">An Evaluation of
Feature Matchers for Fundamental Matrix Estimation</a>,BMVC
2019,<strong>[<a
target="_blank" rel="noopener" href="https://jwbian.net/Papers/FM_BMVC19.pdf">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://jwbian.net/fm-bench">Project
Page</a>]</strong>，特征匹配</li>
<li><a target="_blank" rel="noopener" href="https://github.com/hyye/lio-mapping">A Tightly Coupled 3D
Lidar and Inertial Odometry and Mapping Approach</a>,ICRA
2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.06993">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://sites.google.com/view/lio-mapping">Project
Page</a>]</strong>，紧耦合雷达+IMU SLAM</li>
<li><a target="_blank" rel="noopener" href="https://github.com/LRMPUT/PlaneSLAM">On the Representation
of Planes for Efficient Graph-based SLAM with High-level
Features</a>,利用平面信息的SLAM</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Huangying-Zhan/DF-VO">Visual Odometry
Revisited: What Should Be Learnt?</a>,arXiv 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.09803">PDF</a>]</strong>,
深度学习深度+光流进行VO</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Xylon-Sean/rfnet">RF-Net: An End-to-End
Image Matching Network based on Receptive Field</a>,CVPR
2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.00604">PDF</a>]</strong>,
端到端图像匹配</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/HKUST-Aerial-Robotics/Fast-Planner">Fast-Planner</a>,IEEE
Robotics and Automation Letters (RA-L), 2019,<strong>[<a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8758904">PDF</a>]</strong>,
无人机轨迹生成</li>
<li><a target="_blank" rel="noopener" href="https://github.com/dongjing3309/minisam">A general and
flexible factor graph non-linear least square optimization
framework</a>,CoRR 2019,<strong>[<a
target="_blank" rel="noopener" href="http://arxiv.org/abs/1909.00903">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://minisam.readthedocs.io/">Project Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/gao-ouyang/demo_for_kalmanFilter">Demo
for Kalman filter in ranging system</a>,卡尔曼滤波原理演示</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Ahmedest61/CNN-Region-VLAD-VPR">A
Holistic Visual Place Recognition Approach using Lightweight CNNs for
Severe ViewPoint and Appearance
Changes</a>，场景识别（外观与视角变化时）,<a
target="_blank" rel="noopener" href="https://github.com/ethz-asl/hierarchical_loc">训练和部署源码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/uzh-rpg/sips2_open">SIPs: Succinct
Interest Points from Unsupervised Inlierness Probability Learning</a>,3D
Vision (3DV) 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.01358">PDF</a>]</strong>，RPG实验室出品，深度学习特征点（有特征描述子）</li>
<li><a target="_blank" rel="noopener" href="https://github.com/uzh-rpg/imips_open">Matching Features
Without Descriptors: Implicitly Matched Interest Points</a>,BMVC
2019,<strong>[<a
target="_blank" rel="noopener" href="http://rpg.ifi.uzh.ch/docs/BMVC19_Cieslewski.pdf">PDF</a>]</strong>,RPG实验室出品，无需特征描述即可进行特征匹配</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/cardwing/Codes-for-Lane-Detection">Learning
Lightweight Lane Detection CNNs by Self Attention Distillation (ICCV
2019)</a>,ICCV 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.00821">PDF</a>]</strong>，深度学习道路检测</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/youngguncho/awesome-slam-datasets">Awesome SLAM
Datasets</a>,史上最全SLAM数据集， <strong><a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/BzcghUnXTR9RQqA3Pc9MhA">公众号说明:
最全 SLAM 开源数据集</a></strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Aceinna/gnss-ins-sim">GNSS-INS-SIM</a>,惯导融合模拟器，支持IMU数据，轨迹生成等</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/2013fangwentao/Multi-Sensor-Combined-Navigation">Multi-Sensor
Combined Navigation Program(GNSS, IMU, Camera and so on)
多源多传感器融合定位 GPS/INS组合导航</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/scape-research/SOSNet">SOSNet: Second
Order Similarity Regularization for Local Descriptor Learning</a>,CVPR
2019,<strong><a target="_blank" rel="noopener" href="https://research.scape.io/sosnet/">[Project
page]</a></strong> <strong><a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.05019">[Paper]</a></strong> <strong><a
href="imgs/sosnet-poster.pdf">[Poster]</a></strong> <strong><a
href="imgs/sosnet-oral.pdf">[Slides]</a></strong>，一种深度学习特征描述子</li>
<li><a target="_blank" rel="noopener" href="https://github.com/oravus/seq2single">Look No Deeper:
Recognizing Places from Opposing Viewpoints under Varying Scene
Appearance using Single-View Depth Estimation</a>,ICRA 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.07381">PDF</a>]</strong>,利用深度图像实现了大视角长时间的场景识别（根据深度图筛选得到不同深度层次的特征点然后与当前帧进行匹配，提高了场景召回率）</li>
<li><a target="_blank" rel="noopener" href="https://github.com/rpng/calc2.0">CALC2.0</a>,Convolutional
Autoencoder for Loop Closure 2.0,用于闭环检测</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ethz-asl/segmap">SegMap</a>,RSS
2018,<strong>[<a
target="_blank" rel="noopener" href="http://www.roboticsproceedings.org/rss14/p03.pdf">PDF</a>]</strong>,
一种基于3D线段的地图表示，可用于场景识别/机器人定位/环境重建等</li>
<li><a target="_blank" rel="noopener" href="https://github.com/cggos/msckf_vio_cg">MSCKF_VIO</a>, a
stereo version of MSCKF，基于MSCKF的双目VIO</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Relja/netvlad">NetVLAD: CNN architecture
for weakly supervised place recognition</a>，CVPR 2016,
CNN框架弱监督学习场景识别,<strong>[<a
target="_blank" rel="noopener" href="https://www.di.ens.fr/willow/research/netvlad/">Project
Page</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/IFL-CAMP/easy_handeye">easy_handeye</a>,Simple,
straighforward ROS library for hand-eye calibration</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/KinglittleQ/SuperPoint_SLAM">SuperPoint-SLAM</a>,利用SuperPoint替换ORB特征点</li>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/pyrobot">PyRobot: An
Open Source Robotics Research Platform</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ethz-asl/hfnet">From Coarse to Fine:
Robust Hierarchical Localization at Large Scale with
HF-Net</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.03506">PDF</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/mp3guy/ICPCUDA">Super fast
implementation of ICP in CUDA</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ethz-asl/volumetric_mapping">A generic
interface for disparity map and pointcloud insertion</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/tdsuper/SPHORB">SPHORB: A Fast and
Robust Binary Feature on the Sphere</a>,International Journal of
Computer Vision 2015,<strong>[<a
target="_blank" rel="noopener" href="http://scs.tju.edu.cn/~lwan/paper/SPHORB/pdf/SPHORB-final-small.pdf">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://scs.tju.edu.cn/~lwan/paper/SPHORB/SPHORB.html">Project
Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ETH3D/badslam">BADSLAM: Bundle Adjusted
Direct RGB-D SLAM</a>,CVPR 2019,<strong>[<a
target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Schops_BAD_SLAM_Bundle_Adjusted_Direct_RGB-D_SLAM_CVPR_2019_paper.pdf">PDF</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/uzh-rpg/rpg_e2vid">High Speed and High
Dynamic Range Video with an Event Camera</a>,arXiv,<strong>[<a
target="_blank" rel="noopener" href="http://rpg.ifi.uzh.ch/docs/arXiv19_Rebecq.pdf">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="http://rpg.ifi.uzh.ch/E2VID.html">Project Page</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/PaoPaoRobot/Awesome-VIO">Awesome-VIO</a>,Discuss
about VIO in PaoPaoRobot group</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/XinLiGH/GyroAllan">GyroAllan</a>,陀螺仪随机误差的
Allan 方差分析, <a target="_blank" rel="noopener" href="https://github.com/rpng/kalibr_allan">Another
version</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/fangchangma/self-supervised-depth-completion">Self-supervised
Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and
Monocular Camera</a>,ICRA 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1807.00275.pdf">PDF</a>]</strong>,
优化LiDAR以及单目得到的深度图</li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVlabs/planercnn">PlaneRCNN: 3D Plane
Detection and Reconstruction from a Single Image</a>,CVPR
2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.04072.pdf">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://research.nvidia.com/publication/2019-06_PlaneRCNN">Project
Page</a>]</strong>,通过单幅图像进行3D平面检测以及重建</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/kokerf/DBow3">DBow3</a>,注释版的DBow3代码</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/VladyslavUsenko/basalt-mirror">Visual-Inertial
Mapping with Non-Linear Factor Recovery</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.06504">PDF</a>]</strong>,<strong>[<a
target="_blank" rel="noopener" href="https://vision.in.tum.de/research/vslam/basalt">Project
Page</a>]</strong>, 时空联合的VIO优化方案</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/PaoPaoRobot/ICRA2019-paper-list">ICRA2019-paper-list</a>,ICRA
2019论文列表（泡泡机器人出品暂时无链接）</li>
<li><a target="_blank" rel="noopener" href="https://github.com/pedropro/CAPE">Fast Cylinder and Plane
Extraction from Depth Cameras for Visual Odometry</a>, IROS
2018,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.02380">PDF</a>]</strong>,利用深度图进行圆柱检测以及平面检测进行VO</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/kiran-mohan/SLAM-Algorithms-Octave">Solutions
to assignments of Robot Mapping Course WS 2013/14 by Dr. Cyrill
Stachniss at University of Freiburg</a>,SLAM算法学习课后作业答案</li>
<li><a target="_blank" rel="noopener" href="https://github.com/RonaldSun/VI-Stereo-DSO">Direct sparse
odometry combined with stereo cameras and IMU</a>,双目DSO+IMU</li>
<li><a target="_blank" rel="noopener" href="https://github.com/HorizonAD/stereo_dso">Direct Sparse
Odometry with Stereo Cameras</a>,双目DSO</li>
<li><a target="_blank" rel="noopener" href="https://github.com/uoip/g2opy">Python binding of SLAM graph
optimization framework g2o</a>,python版本的g2o实现</li>
<li><a target="_blank" rel="noopener" href="https://github.com/rpautrat/SuperPoint">SuperPoint:
Self-Supervised Interest Point Detection and Description</a>, CVPR 2018,
<strong>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1712.07629">Paper</a>]</strong>,
深度学习描述子+描述</li>
<li><a target="_blank" rel="noopener" href="https://github.com/lzx551402/contextdesc">ContextDesc:
Local Descriptor Augmentation with Cross-Modality Context</a>, CVPR
2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.04084">Paper</a>]</strong>,
深度学习描述子</li>
<li><a target="_blank" rel="noopener" href="https://github.com/mihaidusmanu/d2-net">D2-Net: A Trainable
CNN for Joint Description and Detection of Local Features</a>, CVPR
2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.03561">Paper</a>]</strong>, <strong>[<a
target="_blank" rel="noopener" href="https://dsmn.ml/publications/d2-net.html">Project
Page</a>]</strong>, 深度学习关键点+描述</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ethz-asl/orb_slam_2_ros">ROS interface
for ORBSLAM2</a>,ROS版本的ORBSLAM2</li>
<li><a target="_blank" rel="noopener" href="https://github.com/yan99033/CNN-SVO">CNN-SVO: Improving the
Mapping in Semi-Direct Visual Odometry Using Single-Image Depth
Prediction</a>， <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.01011.pdf">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/ManiiXu/VINS-Mono-Learning">VINS-Mono-Learning</a>，代码注释版VINS-Mono，初学者学习</li>
<li><a target="_blank" rel="noopener" href="https://github.com/xdspacelab/openvslam">OpenVSLAM:
Versatile Visual SLAM Framework</a>, <strong>[<a
target="_blank" rel="noopener" href="https://openvslam.readthedocs.io/">Project Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/fabianschenk/RESLAM">RESLAM: A real-time
robust edge-based SLAM system</a>, ICRA 2019, <strong>[<a
target="_blank" rel="noopener" href="https://github.com/fabianschenk/fabianschenk.github.io/raw/master/files/schenk_icra_2019.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/rubengooj/pl-slam">PL-SLAM: a Stereo
SLAM System through the Combination of Points and Line Segments</a>,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.09479">Paper</a>]</strong>，线特征SLAM</li>
<li><a target="_blank" rel="noopener" href="https://github.com/YipuZhao/GF_PL_SLAM">Good Line Cutting:
towards Accurate Pose Tracking of Line-assisted VO/VSLAM</a>, ECCV 2018,
<strong>[<a
target="_blank" rel="noopener" href="https://sites.google.com/site/zhaoyipu/good-feature-visual-slam">Project
Page</a>]</strong>, 改进的PL-SLAM</li>
<li><a target="_blank" rel="noopener" href="https://github.com/leoshine/Spherical_Regression">Spherical
Regression: Learning Viewpoints, Surface Normals and 3D Rotations on
n-Spheres</a>, CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="http://arxiv.org/abs/1904.05404">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/icsl-Jeon/traj_gen_vis">svo_edgelet</a>,
在线轨迹生成</li>
<li><a target="_blank" rel="noopener" href="https://github.com/TimboKZ/caltech_samaritan">Drone SLAM
project for Caltech's ME 134 Autonomy class</a>, <strong>[<a
target="_blank" rel="noopener" href="https://github.com/TimboKZ/caltech_samaritan/blob/master/CS134_Final_Project_Report.pdf">PDF</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/icsl-Jeon/traj_gen_vis">Online
Trajectory Generation of a MAV for Chasing a Moving Target in 3D Dense
Environments</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.03421.pdf">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/AtsushiSakai/PythonRobotics">PythonRobotics</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.10703">Paper</a>]</strong>, <a
target="_blank" rel="noopener" href="https://github.com/onlytailei/CppRobotics">CppRobotics</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/izhengfan/ba_demo_ceres">Bundle
adjustment demo using Ceres Solver</a>, <strong>[<a
target="_blank" rel="noopener" href="https://fzheng.me/2018/01/23/ba-demo-ceres/">Blog</a>]</strong>,
ceres实现BA</li>
<li><a target="_blank" rel="noopener" href="https://github.com/shichaoy/cube_slam">CubeSLAM: Monocular
3D Object Detection and SLAM</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.00557">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/sshaoshuai/PointRCNN">PointRCNN: 3D
Object Proposal Generation and Detection from Point Cloud</a>, CVPR
2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.04244">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/nrupatunga/GIST-global-Image-Descripor">GIST-Global
Image Descriptor</a>, GIST描述子</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ethz-asl/mav_voxblox_planning">mav
voxblox planning</a>, MAV planning tools using voxblox as the map
representation.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/zziz/kalman-filter">Python Kalman
Filter</a>, 30行实现卡尔曼滤波</li>
<li><a target="_blank" rel="noopener" href="https://github.com/arpg/vicalib">vicalib</a>,
视觉惯导系统标定工具</li>
<li><a target="_blank" rel="noopener" href="https://github.com/simondlevy/BreezySLAM">BreezySLAM</a>,
基于雷达的SLAM，支持Python(&amp;Matlab, C++, and Java)</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Yvon-Shong/Probabilistic-Robotics">Probabilistic-Robotics</a>,
《概率机器人》中文版，书和课后习题</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/emmjaykay/stanford_self_driving_car_code">Stanford
Self Driving Car Code</a>, <strong>[<a
target="_blank" rel="noopener" href="http://robots.stanford.edu/papers/junior08.pdf">Paper</a>]</strong>,
斯坦福自动驾驶车代码</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ndrplz/self-driving-car">Udacity
Self-Driving Car Engineer Nanodegree projects</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/TUMFTM/Lecture_AI_in_Automotive_Technology">Artificial
Intelligence in Automotive Technology</a>,
TUM自动驾驶技术中的人工智能课程</li>
<li><a target="_blank" rel="noopener" href="https://github.com/hlzz/DeepMatchVO">DeepMatchVO: Beyond
Photometric Loss for Self-Supervised Ego-Motion Estimation</a>,ICRA
2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.09103">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/zdzhaoyong/GSLAM">GSLAM: A General SLAM
Framework and Benchmark</a>, CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.07995">Paper</a>]</strong>,
集成了各种传感器输入的SLAM统一框架</li>
<li><a target="_blank" rel="noopener" href="https://github.com/izhengfan/se2lam">Visual-Odometric
Localization and Mapping for Ground Vehicles Using SE(2)-XYZ
Constraints</a>，ICRA 2019,基于SE(2)-XYZ约束的VO系统</li>
<li><a target="_blank" rel="noopener" href="https://github.com/nicolov/simple_slam_loop_closure">Simple
bag-of-words loop closure for visual SLAM</a>, <strong>[<a
target="_blank" rel="noopener" href="https://nicolovaligi.com/bag-of-words-loop-closure-visual-slam.html">Blog</a>]</strong>,
回环</li>
<li><a target="_blank" rel="noopener" href="https://github.com/rmsalinas/fbow">FBOW (Fast Bag of
Words), an extremmely optimized version of the DBow2/DBow3
libraries</a>,优化版本的DBow2/DBow3</li>
<li><a target="_blank" rel="noopener" href="https://github.com/tomas789/tonav">Multi-State Constraint
Kalman Filter (MSCKF) for Vision-aided Inertial Navigation(master's
thesis)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/yuzhou42/MSCKF">MSCKF</a>,
MSCKF中文注释版</li>
<li><a target="_blank" rel="noopener" href="https://github.com/hbtang/calibcamodo">Calibration
algorithm for a camera odometry system</a>, VO系统的标定程序</li>
<li><a target="_blank" rel="noopener" href="https://github.com/cggos/vins_mono_cg">Modified version of
VINS-Mono</a>, 注释版本VINS Mono</li>
<li><a target="_blank" rel="noopener" href="https://github.com/zhenpeiyang/RelativePose">Extreme
Relative Pose Estimation for RGB-D Scans via Scene
Completion</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.00063">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/jessecw/EPnP_Eigen">Implementation of
EPnP algorithm with Eigen</a>,利用Eigen编写的EPnP</li>
<li><a target="_blank" rel="noopener" href="https://github.com/jiexiong2016/GCNv2_SLAM">Real-time SLAM
system with deep features</a>, 深度学习描述子(ORB vs. GCNv2)</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Huangying-Zhan/Depth-VO-Feat">Unsupervised
Learning of Monocular Depth Estimation and Visual Odometry with Deep
Feature Reconstruction</a>, CVPR 2018, 无监督单目深度恢复以及VO</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Phylliida/orbslam-windows">ORB-SLAM-windows</a>,
Windows版本的ORB-SLAM</li>
<li><a target="_blank" rel="noopener" href="https://github.com/danping/structvio">StructVIO :
Visual-inertial Odometry with Structural Regularity of Man-made
Environments</a>,<strong>[<a
target="_blank" rel="noopener" href="http://drone.sjtu.edu.cn/dpzou/project/structvio.html">Project
Page</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/irvingzhang/KalmanFiltering">KalmanFiltering</a>,
各种卡尔曼滤波器的demo</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ZhenghaoFei/visual_odom">Stereo Odometry
based on careful Feature selection and Tracking</a>, <strong>[<a
target="_blank" rel="noopener" href="https://lamor.fer.hr/images/50020776/Cvisic2017.pdf">Paper</a>]</strong>,
C++ OpenCV实现SOFT</li>
<li><a target="_blank" rel="noopener" href="https://github.com/dzunigan/zSLAM">Visual SLAM with RGB-D
Cameras based on Pose Graph Optimization</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/drsrinathsridhar/GRANSAC">Multi-threaded
generic RANSAC implemetation</a>, 多线程RANSAC</li>
<li><a target="_blank" rel="noopener" href="https://github.com/PyojinKim/OPVO">Visual Odometry with
Drift-Free Rotation Estimation Using Indoor Scene Regularities</a>, BMVC
2017, <strong>[<a
target="_blank" rel="noopener" href="http://pyojinkim.me/pub/Visual-Odometry-with-Drift-Free-Rotation-Estimation-Using-Indoor-Scene-Regularities/">Project
Page</a>]</strong>，利用平面正交信息进行VO</li>
<li><a target="_blank" rel="noopener" href="https://github.com/baidu/ICE-BA">ICE-BA</a>, CVPR 2018,
<strong>[<a
target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_ICE-BA_Incremental_Consistent_CVPR_2018_paper.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/AIBluefisher/GraphSfM">GraphSfM: Robust
and Efficient Graph-based Structure from Motion</a>, <strong>[<a
target="_blank" rel="noopener" href="https://aibluefisher.github.io/GraphSfM/">Project
Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/cuitaixiang/LOAM_NOTED">LOAM_NOTED</a>,
loam中文注解版</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Ethan-Zhou/MWO">Divide and Conquer:
Effcient Density-Based Tracking of 3D Sensors in Manhattan
Worlds</a>,ACCV 2016,<strong>[<a
target="_blank" rel="noopener" href="http://users.cecs.anu.edu.au/~u5535909/">Project
Page</a>]</strong>,曼哈顿世界利用深度传感器进行旋转量平移量分离优化</li>
<li><a target="_blank" rel="noopener" href="https://github.com/jstraub/rtmf">Real-time Manhattan World
Rotation Estimation in 3D</a>,IROS 2015,实时曼哈顿世界旋转估计</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/uzh-rpg/event-based_vision_resources">Event-based
Vision Resources</a>，关于事件相机的资源</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/DeepTecher/AutonomousVehiclePaper">AutonomousVehiclePaper</a>，无人驾驶相关论文速递</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/wutianyiRosun/Segmentation.X">Segmentation.X</a>,
Segmentation相关论文&amp;代码</li>
<li><a target="_blank" rel="noopener" href="https://github.com/amusi/CVPR2019-Code">CVPR-2019</a>, CVPR
2019 论文开源项目合集</li>
<li><a target="_blank" rel="noopener" href="https://github.com/kanster/awesome-slam">awesome-slam</a>,
SLAM合集</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/tzutalin/awesome-visual-slam">awesome-visual-slam</a>,
视觉SLAM合集</li>
<li><a target="_blank" rel="noopener" href="https://github.com/zziz/pwc">Papers with code</a>,
周更论文with代码</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/cbsudux/awesome-human-pose-estimation">Awesome
Human Pose Estimation</a>,<a
target="_blank" rel="noopener" href="https://github.com/nkalavak/awesome-object-pose">awesome-object-pose</a>,
位姿估计合集</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Ewenwan/MVision">MVision</a>,
大礼包：机器人视觉 移动机器人 VS-SLAM ORB-SLAM2 深度学习目标检测 yolov3
行为检测 opencv PCL 机器学习 无人驾驶</li>
</ul>
<h2 id="poseobject-tracking">Pose/Object tracking</h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/KovenYu/MAR">Unsupervised person
re-identification by soft multilabel learning</a>,CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://kovenyu.com/papers/2019_CVPR_MAR.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/tianzhi0549/FCOS">FCOS: Fully
Convolutional One-Stage Object Detection</a>,ICCV 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.01355">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/yangli18/hand_detection">Hand Detection
and Orientation Estimation</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Wanggcong/Spatial-Temporal-Re-identification">Spatial-Temporal
Person Re-identification</a>,AAAI 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.03282">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/layumi/Person_reID_baseline_pytorch">A
tiny, friendly, strong pytorch implement of person re-identification
baseline. <strong>Tutorial</strong></a>,CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.07223">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/tengteng95/Pose-Transfer">Progressive
Pose Attention for Person Image Generation</a>,CVPR 2019,<strong>[<a
target="_blank" rel="noopener" href="http://arxiv.org/abs/1904.03349">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/shamangary/FSA-Net">FSA-Net: Learning
Fine-Grained Structure Aggregation for Head Pose Estimation from a
Single Image</a>, CVPR 2019,<strong>[<a
target="_blank" rel="noopener" href="https://github.com/shamangary/FSA-Net/blob/master/0191.pdf">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/yuanyuanli85/Fast_Human_Pose_Estimation_Pytorch">An
unoffical implemention for paper "Fast Human Pose Estimation"</a>, CVPR
2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.05419">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/edvardHua/PoseEstimationForMobile">Real-time
single person pose estimation for Android and
iOS</a>,手机端实现人体位姿估计</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/cbsudux/Human-Pose-Estimation-101">Basics of 2D
and 3D Human Pose Estimation</a>,人体姿态估计入门</li>
<li><a target="_blank" rel="noopener" href="https://github.com/OceanPang/Libra_R-CNN">Libra R-CNN:
Towards Balanced Learning for Object Detection</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/HRNet/HRNet-Object-Detection">High-resolution
networks (HRNets) for object detection</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.04514.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/xiaolonw/TimeCycle">Learning
Correspondence from the Cycle-Consistency of Time</a>, CVPR 2019,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.07593">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/zju3dv/pvnet">PVNet: Pixel-wise Voting
Network for 6DoF Pose Estimation</a>, CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.11788">Paper</a>], [<a
target="_blank" rel="noopener" href="https://zju3dv.github.io/pvnet">Project Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/mkocabas/EpipolarPose">Self-Supervised
Learning of 3D Human Pose using Multi-view Geometry</a>, CVPR 2018,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.02330">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/vita-epfl/openpifpaf">PifPaf: Composite
Fields for Human Pose Estimation</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.06593">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch">Deep
High-Resolution Representation Learning for Human Pose
Estimation</a>,CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.09212.pdf">Paper</a>]</strong>,
<strong>[<a
target="_blank" rel="noopener" href="https://jingdongwang2017.github.io/Projects/HRNet/PoseEstimation.html">Project
Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/YuliangXiu/PoseFlow">PoseFlow: Efficient
Online Pose Tracking)</a>, BMVC 2018, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.00977">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/vana77/Bottom-up-Clustering-Person-Re-identification">A
Bottom-Up Clustering Approach to Unsupervised Person
Re-identification</a>，AAAI 2019, 重定位</li>
<li><a target="_blank" rel="noopener" href="https://github.com/foolwood/SiamMask">Fast Online Object
Tracking and Segmentation: A Unifying Approach</a>,CVPR 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.05050">Paper</a>] [<a
target="_blank" rel="noopener" href="https://youtu.be/I_iOVrcpEBw">Video</a>] [<a
target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~qwang/SiamMask">Project
Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/TuSimple/simpledet">SimpleDet - A Simple
and Versatile Framework for Object Detection and Instance
Recognition</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.05831">Paper</a>]</strong></li>
</ul>
<h2 id="depthdisparity-flow-estimation">Depth/Disparity &amp; Flow
estimation</h2>
<ul>
<li>[<strong>Depth</strong>][SemiGlobalMatching](https://github.com/ethan-li-coding/SemiGlobalMatching),
SGM双目立体匹配算法完整实现，代码规范，注释丰富且清晰，CSDN同步教学</li>
<li><a target="_blank" rel="noopener" href="https://github.com/callmeray/PointMVSNet">PointMVSNet:
Point-based Multi-view Stereo Network</a>,ICCV 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.04422">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/JiaxiongQ/DeepLiDAR">DeepLiDAR</a>,CVPR
2019, <strong>[<a
target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qiu_DeepLiDAR_Deep_Surface_Normal_Guided_Depth_Prediction_for_Outdoor_Scene_CVPR_2019_paper.pdf">Paper</a>]</strong>,
单张RGB图像+稀疏雷达数据进行室外场景深度估计</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/atapour/monocularDepth-Inference">Real-Time
Monocular Depth Estimation using Synthetic Data with Domain Adaptation
via Image Style Transfer</a>,CVPR 2018, <strong>[<a
target="_blank" rel="noopener" href="http://breckon.eu/toby/publications/papers/abarghouei18monocular.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/princeton-vl/YouTube3D">Learning
Single-Image Depth from Videos using Quality Assessment
Networks</a>,CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.09573">Paper</a>]</strong>, <strong>[<a
target="_blank" rel="noopener" href="http://www-personal.umich.edu/~wfchen/youtube3d/">Project
Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/WERush/SCDA">SCDA: Adapting Object
Detectors via Selective Cross-Domain Alignment</a>,CVPR 2019,
<strong>[<a
target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Adapting_Object_Detectors_via_Selective_Cross-Domain_Alignment_CVPR_2019_paper.pdf">Paper</a>]</strong>,
<strong>[<a target="_blank" rel="noopener" href="http://zhuxinge.me/aboutme.html">Project
Page</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/fabiotosi92/monoResMatch-Tensorflow">Learning
monocular depth estimation infusing traditional stereo
knowledge</a>,CVPR 2019,<strong>[<a
target="_blank" rel="noopener" href="https://vision.disi.unibo.it/~ftosi/papers/monoResMatch.pdf">PDF</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/laoreja/HPLFlowNet">HPLFlowNet:
Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on
Large-scale Point Clouds</a>,CVPR 2019,<strong>[<a
href="hhttps://web.cs.ucdavis.edu/~yjlee/projects/cvpr2019-HPLFlowNet.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/feihuzhang/GANet">GA-Net: Guided
Aggregation Net for End-to-end Stereo Matching</a>,CVPR 2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.06587.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/sunghoonim/DPSNet">DPSNet: End-to-end
Deep Plane Sweep Stereo</a>,ICLR 2019,<strong>[<a
target="_blank" rel="noopener" href="https://openreview.net/pdf?id=ryeYHi0ctQ">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/muskie82/AR-Depth-cpp">Fast Depth
Densification for Occlusion-aware Augmented Reality</a>, SIGGRAPH-Asia
2018, <strong>[<a
target="_blank" rel="noopener" href="https://homes.cs.washington.edu/~holynski/publications/occlusion/index.html">Project
Page</a>]</strong>,<a
target="_blank" rel="noopener" href="https://github.com/facebookresearch/AR-Depth">another
version</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/CVLAB-Unibo/Learning2AdaptForStereo">Learning
To Adapt For Stereo</a>, CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.02957">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/JiaRenChang/PSMNet">Pyramid Stereo
Matching Network</a>,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.08669">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/lelimite4444/BridgeDepthFlow">Bridging
Stereo Matching and Optical Flow via Spatiotemporal Correspondence</a>,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.09265">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/wvangansbeke/Sparse-Depth-Completion">Sparse
Depth Completion</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.05356.pdf">Paper</a>]</strong>,
RGB图像辅助雷达深度估计</li>
<li><a target="_blank" rel="noopener" href="https://github.com/sshan-zhao/GASDA">GASDA</a>, CVPR 2019,
<strong>[<a
target="_blank" rel="noopener" href="https://sshan-zhao.github.io/papers/gasda.pdf">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/xy-guo/MVSNet_pytorch">MVSNet: Depth
Inference for Unstructured Multi-view Stereo</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.02505">Paper</a>]</strong>,
非官方实现版本的MVSNet</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/HKUST-Aerial-Robotics/Stereo-RCNN">Stereo R-CNN
based 3D Object Detection for Autonomous Driving</a>, CVPR 2019,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.09738.pdf">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo">Real-time
self-adaptive deep stereo</a>, CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.05424">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ialhashim/DenseDepth">High Quality
Monocular Depth Estimation via Transfer Learning</a>,CVPR 2019,
<strong>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.11941">Paper</a>]</strong>,
<strong>[<a
target="_blank" rel="noopener" href="https://ialhashim.github.io/publications/index.html">Project
Page</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/xy-guo/GwcNet">Group-wise Correlation
Stereo Network</a>,CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.04025">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/phuang17/DeepMVS">DeepMVS: Learning
Multi-View Stereopsis</a>, CVPR 2018,<strong>[<a
target="_blank" rel="noopener" href="https://phuang17.github.io/DeepMVS/index.html">Project
Page</a>]</strong>,多目深度估计</li>
<li><a target="_blank" rel="noopener" href="https://github.com/sampepose/flownet2-tf">FlowNet 2.0:
Evolution of Optical Flow Estimation with Deep Networks</a>, CVPR 2017,
深度学习光流恢复</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/DLuensch/StereoVision-ADCensus">StereoVision-ADCensus</a>,深度恢复代码集合(<strong>ADCensus,
SGBM, BM</strong>)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/yangguorun/SegStereo">SegStereo:
Exploiting Semantic Information for Disparity Estimation</a>,
探究语义信息在深度估计中的作用</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/kuantingchen04/Light-Field-Depth-Estimation">Light
Filed Depth Estimation using GAN</a>，利用GAN进行光场深度恢复</li>
<li><a target="_blank" rel="noopener" href="https://github.com/daniilidis-group/EV-FlowNet">EV-FlowNet:
Self-Supervised Optical Flow for Event-based Cameras</a>,Proceedings of
Robotics 2018,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.06898">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/vt-vl-lab/DF-Net">DF-Net: Unsupervised
Joint Learning of Depth and Flow using Cross-Task Consistency</a>, ECCV
2018, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.01649">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/yzcjtr/GeoNet">GeoNet: Unsupervised
Learning of Dense Depth, Optical Flow and Camera Pose</a>, CVPR 2018,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.02276">Paper</a>]</strong></li>
</ul>
<h2 id="d-graphic">3D &amp; Graphic</h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/WangYueFt/prnet">PRNet: Self-Supervised
Learning for Partial-to-Partial Registration</a>,NeurIPS 2019</li>
<li><a target="_blank" rel="noopener" href="https://github.com/nkolot/SPIN">Learning to Reconstruct 3D
Human Pose and Shape via Model-fitting in the Loop</a>,ICCV 2019,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.12828.pdf">Paper</a>]</strong> ,
<strong>[<a
target="_blank" rel="noopener" href="https://www.seas.upenn.edu/~nkolot/projects/spin/">Project
Page</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/microsoft/multiview-human-pose-estimation-pytorch">Cross
View Fusion for 3D Human Pose Estimation</a>,ICCV 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.01203">Paper</a>]</strong>
,跨视角3D位姿估计</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Fanziapril/mvfnet">MVF-Net: Multi-View
3D Face Morphable Model Regression</a>,多视角3D人脸重建, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.04473">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/saurabheights/KillingFusion">KillingFusion</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/PRBonn/refusion">ReFusion: 3D
Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting
Residuals</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.02082.pdf">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Lotayou/densebody_pytorch">densebody_pytorch</a>,
<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.10153v3">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/svip-lab/PlanarReconstruction">Single-Image
Piece-wise Planar 3D Reconstruction via Associative Embedding</a>,CVPR
2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.09777.pdf">Paper</a>]</strong>,
单目3D重建</li>
<li><a target="_blank" rel="noopener" href="https://github.com/sunset1995/HorizonNet">HorizonNet:
Learning Room Layout with 1D Representation and Pano Stretch Data
Augmentation</a>,CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.03861">Paper</a>]</strong>,
深度学习全景转3D</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Microsoft/O-CNN">Adaptive O-CNN: A
Patch-based Deep Representation of 3D Shapes</a>,SIGGRAPH Asia 2018,
<strong>[<a target="_blank" rel="noopener" href="https://wang-ps.github.io/AO-CNN.html">Project
Page</a>]</strong></li>
</ul>
<h2 id="other-collections">Other Collections</h2>
<ul>
<li><a
target="_blank" rel="noopener" href="https://github.com/timqian/chinese-independent-blogs">chinese-independent-blogs</a>,
中文独立博客集锦</li>
<li><a target="_blank" rel="noopener" href="https://github.com/RenYurui/StructureFlow">StructureFlow:
Image Inpainting via Structure-aware Appearance
Flow</a>,图像inpainting</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/ruanyf/free-books">free-books</a>,互联网上的免费书籍</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>,通用的学术主页模版</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/microsoft/MMdnn">MMdnn</a>,实现深度学习模型之间的相互转换</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/abner2015/tensorflow2caffemodel">tensorflow2caffemodel</a>,tensorflow模型转caffemodel</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/fengdu78/lihang-code">lihang-code</a>,《统计学习方法》的代码实现</li>
<li><a target="_blank" rel="noopener" href="https://github.com/DLTcollab/sse2neon">sse2neon</a>,<a
target="_blank" rel="noopener" href="https://github.com/jratcliff63367/sse2neon">sse2neon</a>,SSE转neon，嵌入式移植时可能会用到;</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/alirezadir/Production-Level-Deep-Learning">Production-Level-Deep-Learning</a>,深度学习模型部署流程</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/ShusenTang/Dive-into-DL-PyTorch">动手学深度学习Dive-into-DL-PyTorch</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/deeplearning-ai/machine-learning-yearning-cn">machine-learning-yearning-cn</a>，Machine
Learning Yearning 中文版 - 《机器学习训练秘籍》 - Andrew Ng 著</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/academicpages/academicpages.github.io">academicpages.github.io</a>，学术主页模板</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">Coursera-ML-AndrewNg-Notes</a>,吴恩达老师的机器学习课程个人笔记</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/roboticcam/machine-learning-notes">machine-learning-notes</a>,机器学习，概率模型和深度学习的讲义(1500+页)和视频链接</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/scutan90/CNN-Visualization">CNN-Visualization</a>,CNN可视化、理解CNN</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/mrgloom/awesome-semantic-segmentation">Awesome
Semantic Segmentation</a>, 语义分割集合</li>
<li><a target="_blank" rel="noopener" href="https://github.com/mengyuest/iros2018-slam-papers">IROS2018
SLAM Collections</a>, IROS 2018集合</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/TerenceCYJ/VP-SLAM-SC-papers">VP-SLAM-SC-papers</a>,Visual
Positioning &amp; SLAM &amp; Spatial Cognition 论文统计与分析</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/HuaizhengZhang/Awesome-System-for-Machine-Learning">Awesome
System for Machine Learning</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Thinkgamer/Machine-Learning-With-Python">Machine-Learning-With-Python</a>,
《机器学习实战》python代码实现</li>
<li><a target="_blank" rel="noopener" href="https://github.com/qqfly/how-to-learn-robotics">How to
learn robotics</a>, 开源机器人学学习指南</li>
<li><a target="_blank" rel="noopener" href="https://github.com/kjw0612/awesome-deep-vision">Awesome
Deep Vision</a>,DL在CV领域的应用</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/YapengTian/Single-Image-Super-Resolution">Single-Image-Super-Resolution</a>,
一个有关<strong>图像超分辨</strong>的合集</li>
<li><a target="_blank" rel="noopener" href="https://github.com/wifity/ai-report">ai report</a>,
AI相关的研究报告</li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota">State-of-the-art papers
and code</a>,搜集了目前sota的论文以及代码</li>
<li><a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/cvpr2019">CVPR 2019
(Papers/Codes/Project/Paper reading)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/openMVG/awesome_3DReconstruction_list">A
curated list of papers &amp; resources linked to 3D reconstruction from
images</a>,有关三维重建的论文汇总</li>
<li><a target="_blank" rel="noopener" href="https://github.com/nebula-beta/SLAM-Jobs">SLAM-Jobs</a>,
SLAM/SFM求职指南</li>
<li><a target="_blank" rel="noopener" href="https://github.com/stevewongv/SPANet">Spatial Attentive
Single-Image Deraining with a High Quality Real Rain Dataset</a>,CVPR
2019,去雨</li>
<li><a target="_blank" rel="noopener" href="https://github.com/hezhangsprinter/DCPDN">Densely Connected
Pyramid Dehazing Network</a>,CVPR 2018,去雾</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsr">MMSR</a>，MMLAB推出的超分辨工具箱</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Bartzi/stn-ocr">深度学习OCR</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Vay-keen/Machine-learning-learning-notes">西瓜书🍉学习笔记</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/wwxFromTju/awesome-reinforcement-learning-zh">awesome-reinforcement-learning-zh</a>,强化学习从入门到放弃的资料</li>
<li><a target="_blank" rel="noopener" href="https://github.com/cszn/DPSR">Deep Plug-and-Play
Super-Resolution for Arbitrary Blur Kernels</a>,CVPR 2019,超分辨</li>
<li><a target="_blank" rel="noopener" href="https://github.com/lzhbrian/Cool-Fashion-Papers">Cool
Fashion Papers</a>, Cool resources about Fashion + AI.</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/nbei/Deep-Flow-Guided-Video-Inpainting">Deep
Flow-Guided Video Inpainting</a>,CVPR 2019, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.10447.pdf">Paper</a>]</strong>
,图像修复</li>
<li><a target="_blank" rel="noopener" href="https://github.com/dbolya/yolact">YOLACT: Real-time
Instance Segmentation</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/lyl8213/Plate_Recognition-LPRnet">LPRNet:
License Plate Recognition via Deep Neural Networks</a>, <strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.10447.pdf">Paper</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/xiaofengShi/CHINESE-OCR">CHINESE-OCR</a>,
运用tf实现自然场景文字检测</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/PerpetualSmile/BeautyCamera">BeautyCamera</a>,
美颜相机，具有人脸检测、磨皮美白人脸、滤镜、调节图片、摄像功能</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/zhengzhugithub/CV-arXiv-Daily">CV-arXiv-Daily</a>,
分享计算机视觉每天的arXiv文章</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/lyndonzheng/Pluralistic-Inpainting">Pluralistic-Inpainting</a>,
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.04227">ArXiv</a> | <a
target="_blank" rel="noopener" href="http://www.chuanxiaz.com/publication/pluralistic/">Project
Page</a> | <a
target="_blank" rel="noopener" href="http://www.chuanxiaz.com/project/pluralistic/">Online Demo</a> |
<a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=9V7rNoLVmSs">Video(demo)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Jezzamonn/fourier">An Interactive
Introduction to Fourier Transforms</a>, 超棒的傅里叶变换图形化解释</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/datawhalechina/pumpkin-book">pumpkin-book</a>,
《机器学习》（西瓜书）公式推导解析</li>
<li><a target="_blank" rel="noopener" href="https://github.com/JuliaLang/julia">Julia</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/alan-turing-institute/MLJ.jl">A Julia
machine learning framework</a>，一种基于Julia的机器学习框架</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/ZhaoJ9014/face.evoLVe.PyTorch">High-Performance
Face Recognition Library on PyTorch</a>，人脸识别库</li>
<li><a
target="_blank" rel="noopener" href="https://github.com/enggen/Deep-Learning-Coursera">Deep-Learning-Coursera</a>，深度学习教程（deeplearning.ai）</li>
<li><a target="_blank" rel="noopener" href="https://github.com/RemoteML/bestofml">The best resources
around Machine Learning</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/cydonia999/VGGFace2-pytorch">VGGFace2: A
dataset for recognising faces across pose and age</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/SmirkCao/Lihang">Statistical learning
methods</a>，统计学习方法</li>
<li><a
target="_blank" rel="noopener" href="https://live.bilibili.com/7332534?visit_id=9ytrx9lpsy80">End-to-end
Adversarial Learning for Generative Conversational
Agents</a>，2017，介绍了一种端到端的基于GAN的聊天机器人</li>
<li><a target="_blank" rel="noopener" href="https://github.com/yulunzhang/RNAN">Residual Non-local
Attention Networks for Image Restoration</a>,ICLR 2019.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/HelenMao/MSGAN">MSGAN: Mode Seeking
Generative Adversarial Networks for Diverse Image Synthesis</a>, CVPR
2019,<strong>[<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.05628">Paper</a>]</strong></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVlabs/SPADE">SPADE: Semantic Image
Synthesis with Spatially-Adaptive Normalization</a>,CVPR 2019,
<strong>[<a target="_blank" rel="noopener" href="https://nvlabs.github.io/SPADE/">Project
Page</a>]</strong></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/Oldpan/Faceswap-Deepfake-Pytorch">Faceswap with
Pytorch or DeepFake with Pytorch</a>, 换脸</li>
<li><a target="_blank" rel="noopener" href="https://github.com/iperov/DeepFaceLab">DeepFaceLab</a>,
换脸</li>
</ul>
<!-- ![](https://vincentqin.tech/blog-resources/westlake/westlake-1.jpg) -->

    </div>

    
    
    
      


    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>Post author:  </strong>Vincent Qin
  </li>
  <li class="post-copyright-link">
      <strong>Post link: </strong>
      <a href="https://www.vincentqin.tech/posts/awesome-works/" title="🔥Awesome CV Works">https://www.vincentqin.tech/posts/awesome-works/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/SLAM/" rel="tag"># SLAM</a>
              <a href="/tags/pose-tracking/" rel="tag"># pose-tracking</a>
              <a href="/tags/object-tracking/" rel="tag"># object-tracking</a>
              <a href="/tags/disparity/" rel="tag"># disparity</a>
              <a href="/tags/depth-estimation/" rel="tag"># depth-estimation</a>
              <a href="/tags/flow-estimation/" rel="tag"># flow-estimation</a>
              <a href="/tags/3D-graphics/" rel="tag"># 3D-graphics</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/build-ssr-server/" rel="prev" title="开启SSR模式">
                  <i class="fa fa-chevron-left"></i> 开启SSR模式
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/first-black-hole/" rel="next" title="Black Hole">
                  Black Hole <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2016 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vincent Qin</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.8/pdfobject.min.js","integrity":"sha256-tu9j5pBilBQrWSDePOOajCUdz6hWsid/lBNzK4KgEPM="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.1.6/mermaid.min.js","integrity":"sha256-ZfzwelSToHk5YAcr9wbXAmWgyn9Jyq08fSLrLhZE89w="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"en-US","enable":true,"serverURL":"https://comments.vincentqin.tech","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":true,"locale":{"placeholder":"Welcome to comment"},"emoji":["https://unpkg.com/@waline/emojis@1.1.0/weibo","https://unpkg.com/@waline/emojis@1.1.0/alus","https://unpkg.com/@waline/emojis@1.1.0/bilibili","https://unpkg.com/@waline/emojis@1.1.0/qq","https://unpkg.com/@waline/emojis@1.1.0/tieba","https://unpkg.com/@waline/emojis@1.1.0/tw-emoji"],"meta":["nick","mail","link"],"requiredMeta":["nick","mail"],"wordLimit":0,"login":"enable","el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/posts/awesome-works/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
