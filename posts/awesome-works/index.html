<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/realcat-apple-touch-icon.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/realcat-32x32.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/realcat-32x32.png?v=7.4.0">
  <link rel="mask-icon" href="/images/realcat-safari-pinned-tab.svg?v=7.4.0" color="#222">
  <link rel="alternate" href="/atom.xml" title="RealCat" type="application/atom+xml">
  <link rel="alternate" href="https://realcat.vercel.app/" title="RealCat" type="application/atom+xml">
  <meta name="google-site-verification" content="u46QTaG_Dv3OZLpOBKYtqyuiNtIdnhSG5ASKoNvGBCM">
  <meta name="baidu-site-verification" content="MtcbwE45ft">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"hide","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":false,"style":"flat"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: true,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":10,"unescape":true,"preload":true},
    path: 'search.xml',
    motion: {"enable":false,"async":true,"transition":{"post_block":"fadeIn","post_header":"fadeIn","post_body":"fadeIn","coll_header":"fadeIn","sidebar":"fadeIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="The post contains papers-with-code about SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc.">
<meta name="keywords" content="SLAM,disparity,pose-tracking,object-tracking,depth-estimation,flow-estimation,3D-graphics">
<meta property="og:type" content="article">
<meta property="og:title" content="🔥Awesome CV Works">
<meta property="og:url" content="https://www.vincentqin.tech/posts/awesome-works/index.html">
<meta property="og:site_name" content="RealCat">
<meta property="og:description" content="The post contains papers-with-code about SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc.">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2020-08-30T13:59:21.496Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="🔥Awesome CV Works">
<meta name="twitter:description" content="The post contains papers-with-code about SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc.">
  <link rel="canonical" href="https://www.vincentqin.tech/posts/awesome-works/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>🔥Awesome CV Works | RealCat</title>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-97856334-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-97856334-1');
    }
  </script>








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">RealCat</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Turn on, Tune in, Drop out</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives<span class="badge">56</span></a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags<span class="badge">106</span></a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories<span class="badge">13</span></a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-collections">
      
    
      
    

    <a href="/collections" rel="section"><i class="menu-item-icon fa fa-fw fa-diamond"></i> <br>Collections</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-top">
      
    
      
    

    <a href="/top/" rel="section"><i class="menu-item-icon fa fa-fw fa-signal"></i> <br>Top</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-guest_comments">
      
    
      
    

    <a href="/guestbook" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://www.vincentqin.tech/posts/awesome-works/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Vincent Qin">
      <meta itemprop="description" content="Keep Your Curiosity">
      <meta itemprop="image" content="https://vincentqin.gitee.io/images/qin_small.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RealCat">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">🔥Awesome CV Works

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-03-31 20:15:41" itemprop="dateCreated datePublished" datetime="2019-03-31T20:15:41+08:00">2019-03-31</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-30 21:59:21" itemprop="dateModified" datetime="2020-08-30T21:59:21+08:00">2020-08-30</time>
              </span>
            
          

          
            <span id="/posts/awesome-works/" class="post-meta-item leancloud_visitors" data-flag-title="🔥Awesome CV Works" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Comments: </span>
    
    <a title="valine" href="/posts/awesome-works/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/awesome-works/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>The post contains papers-with-code about SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc. </p>
<p><a href="https://github.com/Vincentqyw/Recent-Stars-2020" target="_blank" rel="noopener"><img alt="ReadMe Card" data-src="https://github-readme-stats.vercel.app/api/pin/?username=Vincentqyw&amp;repo=Recent-Stars-2020&amp;show_owner=false&amp;theme=default"></a> </p>
<!-- [![GitHub stars](https://img.shields.io/github/stars/Vincentqyw/Recent-Stars-2019.svg?logo=github&label=Stars)](https://github.com/Vincentqyw/Recent-Stars-2019) -->
<a id="more"></a>
<!--# Recent Stars 2020-->
<!-- <p align="center">
 <img width="100px" src="https://cdn.jsdelivr.net/gh/Vincentqyw/blog-resources/awesome-works/github-star.png" align="center" alt="" />
</p> -->
<!-- <p align="center">
  <a href="https://github.com/Vincentqyw/Recent-Stars-2020">
    <img alt="Awesome" src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" />
  </a>
  <a href="http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019">
    <img alt="HitCount" src="http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019.svg" />
  </a>
  <a href="https://vincentqin.tech">
    <img alt="LICENSE" src="https://img.shields.io/badge/license-Anti%20996-blue.svg?style=flat-square" />
  </a>
</p> -->
<!--
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/Vincentqyw/Recent-Stars-2020)
[![HitCount](http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019.svg)](http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019)
[![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg?style=flat-square)](https://github.com/Vincentqyw/Recent-Stars-2020)
✔ This repo collects some links with papers which I recently starred related on SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, etc.
-->
<h2 id="SLAM-related"><a href="#SLAM-related" class="headerlink" title="SLAM related"></a>SLAM related</h2><ul>
<li>[<strong>SLAM</strong>]<a href="https://github.com/UZ-SLAMLab/ORB_SLAM3" target="_blank" rel="noopener">ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM</a>, <strong>[<a href="https://arxiv.org/abs/2007.11898" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>SLAM</strong>]<a href="https://github.com/TixiaoShan/LIO-SAM" target="_blank" rel="noopener">LIO-SAM</a>, 激光雷达IMU紧耦合SLAM</li>
<li>[<strong>Tool</strong>]<a href="https://github.com/petercorke/robotics-toolbox-python" target="_blank" rel="noopener">Robotics Toolbox for Python</a>,  a Python implementation of the <a href="https://github.com/petercorke/robotics-toolbox-matlab" target="_blank" rel="noopener">Robotics Toolbox for MATLAB®</a></li>
<li>[<strong>Matching</strong>]<a href="https://github.com/rpautrat/LISRD" target="_blank" rel="noopener">LISRD</a>,ECCV 2020, <strong>[<a href="https://arxiv.org/abs/2007.08988" target="_blank" rel="noopener">PDF</a>]</strong>，在线局部不变特征匹配！重要！</li>
<li>[<strong>Matching</strong>]<a href="https://github.com/cavalli1234/AdaLAM" target="_blank" rel="noopener">AdaLAM</a>,特征匹配快速滤除外点</li>
<li>[<strong>Calib</strong>]<a href="https://github.com/3DCVer/fisheye_pinhole_calib_demo" target="_blank" rel="noopener">fisheye_pinhole_calib_demo</a>, 包括鱼眼模型、针孔模型的相机标定，封装了自动编译、库的打包以及外部库的调用测试</li>
<li>[<strong>Calib</strong>]<a href="https://github.com/FENGChenxi0823/SensorCalibration" target="_blank" rel="noopener">SensorCalibration</a>, IMU雷达标定</li>
<li>[<strong>VO</strong>]<a href="https://github.com/PyojinKim/LPVO" target="_blank" rel="noopener">Low-Drift Visual Odometry in Structured Environments by Decoupling Rotational and Translational Motion</a>,ICRA 2018, <strong>[<a href="http://pyojinkim.com/download/papers/2018_ICRA.pdf" target="_blank" rel="noopener">PDF</a>]</strong>, 结构化环境中将旋转量与平移量进行分离优化</li>
<li>[<strong>VIO</strong>]<a href="https://github.com/iamwangyabin/VIO-SLAM" target="_blank" rel="noopener">VIO-SLAM</a>, 从零开始手写VIO课后作业</li>
<li>[<strong>Matching</strong>]<a href="https://github.com/lzx551402/tfmatch" target="_blank" rel="noopener">TFMatch: Learning-based image matching in TensorFlow</a>,TensorFlow 实现的 GeoDesc,ASLFeat以及ContextDesc</li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/yanyan-li/SLAM-BOOK" target="_blank" rel="noopener">SLAM-BOOK</a>, 一本关于SLAM的书稿，清楚的介绍SLAM系统中的使用的几何方法和深度学习方法，持续更新中</li>
<li>[<strong>Loop Closing</strong>]<a href="https://github.com/PRBonn/OverlapNet" target="_blank" rel="noopener">OverlapNet - Loop Closing for 3D LiDAR-based SLAM</a>, RSS 2020, <strong>[<a href="https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/chen2020rss.pdf" target="_blank" rel="noopener">PDF</a>]</strong>, 3D激光雷达SLAM闭环</li>
<li>[<strong>SLAM</strong>]<a href="https://github.com/halajun/VDO_SLAM" target="_blank" rel="noopener">VDO_SLAM</a>, RGB-D相机数据作为输入，实现追踪动态物体SLAM的功能, <strong>[<a href="https://arxiv.org/abs/2005.11052" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>SLAM</strong>]<a href="https://github.com/TUMFTM/orbslam-map-saving-extension" target="_blank" rel="noopener">orbslam-map-saving-extension</a>，在ORB-SLAM的基础上增加保存+加载地图功能</li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/NxRLab/ModernRobotics" target="_blank" rel="noopener">Modern Robotics: Mechanics, Planning, and Control Code Library</a>, 现代机器人学, <strong>[<a href="http://hades.mech.northwestern.edu/index.php/Modern_Robotics" target="_blank" rel="noopener">Homepage</a>]</strong></li>
<li>[<strong>Matching</strong>]<a href="https://github.com/vcg-uvic/image-matching-benchmark-baselines" target="_blank" rel="noopener">image-matching-benchmark-baselines</a>, 图像特征匹配挑战赛主页</li>
<li>[<strong>Matching</strong>]<a href="https://github.com/mameng1/GraphLineMatching" target="_blank" rel="noopener">GraphLineMatching</a></li>
<li>[<strong>Matching</strong>]<a href="https://github.com/jiayi-ma/LPM" target="_blank" rel="noopener">Locality Preserving Matching</a>, IJCAI 2017, <strong>[<a href="https://ai.tencent.com/ailab/media/publications/YuanGao_IJCAI2017_LocalityPreservingMatching.pdf" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>IMU</strong>]<a href="https://github.com/ydsf16/IMUOrientationEstimator" target="_blank" rel="noopener">IMUOrientationEstimator</a></li>
<li>[<strong>Feature</strong>]<a href="https://github.com/iago-suarez/BEBLID" target="_blank" rel="noopener">BEBLID: Boosted Efficient Binary Local Image Descriptor</a></li>
<li>[<strong>Relocalization</strong>]<a href="https://github.com/zlthinker/KFNet" target="_blank" rel="noopener">KFNet: Learning Temporal Camera Relocalization using Kalman Filtering</a>,CVPR 2020,<strong>[<a href="https://arxiv.org/abs/2003.10629" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>Matching</strong>]<a href="https://github.com/vcg-uvic/image-matching-benchmark" target="_blank" rel="noopener">image-matching-benchmark</a></li>
<li>[<strong>Matching</strong>]<a href="https://github.com/JiawangBian/GMS-Feature-Matcher" target="_blank" rel="noopener">GMS: Grid-based Motion Statistics for Fast, Ultra-robust Feature Correspondence</a>,CVPR 17 &amp; IJCV 19,<strong>[<a href="http://jwbian.net/Papers/GMS_CVPR17.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://jwbian.net/gms" target="_blank" rel="noopener">Project page</a>]</strong></li>
<li>[<strong>Reloc</strong>]<a href="https://github.com/Artisense-ai/GN-Net-Benchmark" target="_blank" rel="noopener">GN-Net-Benchmark</a>, CVPR 2020,GN-Net: The Gauss-Newton Loss for Multi-Weather Relocalization, <strong>[<a href="https://arxiv.org/abs/1904.11932" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://vision.in.tum.de/gn-net" target="_blank" rel="noopener">Project page</a>]</strong></li>
<li>[<strong>Matching</strong>]<a href="https://github.com/magicleap/SuperGluePretrainedNetwork" target="_blank" rel="noopener">SuperGluePretrainedNetwork</a>, CVPR 2020, <strong>[<a href="https://arxiv.org/abs/1911.11763" target="_blank" rel="noopener">PDF</a>]</strong>, 划重点！2020年sota超大视角2D特征匹配，<a href="https://www.vincentqin.tech/posts/superglue/">Blog</a></li>
<li>[<strong>Feature</strong>]<a href="https://github.com/XuyangBai/D3Feat" target="_blank" rel="noopener">D3Feat</a>, CVPR 2020, <strong>[<a href="https://arxiv.org/abs/2003.03164" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>Feature</strong>]<a href="https://github.com/lzx551402/ASLFeat" target="_blank" rel="noopener">ASLFeat</a>, CVPR 2020, ASLFeat: Learning Local Features of Accurate Shape and Localization, <strong>[<a href="https://arxiv.org/abs/2003.10071" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>Feature</strong>]<a href="https://github.com/XuyangBai/D3Feat" target="_blank" rel="noopener">GMS-Feature-Matcher</a>, CVPR 2018, GMS: Grid-based Motion Statistics for Fast, Ultra-robust Feature Correspondence, <strong>[<a href="http://jwbian.net/Papers/GMS_CVPR17.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://jwbian.net/gms" target="_blank" rel="noopener">Project page</a>]</strong></li>
<li>[<strong>Feature</strong>]<a href="https://github.com/XuyangBai/D3Feat" target="_blank" rel="noopener">D3Feat</a>, CVPR 2020, <strong>[<a href="https://arxiv.org/abs/2003.03164" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>Feature</strong>]<a href="https://github.com/yewzijian/3DFeatNet" target="_blank" rel="noopener">3DFeatNet</a>, ECCV 2018, <strong>[<a href="https://arxiv.org/abs/1807.09413" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/microsoft/AutonomousDrivingCookbook" target="_blank" rel="noopener">AutonomousDrivingCookbook</a>，Scenarios, tutorials and demos for Autonomous Driving</li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/PaoPaoRobot/SLAMPaperReading" target="_blank" rel="noopener">SLAMPaperReading</a>，泡泡机器人北京线下SLAM论文分享资料</li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/lishuwei0424/VIO_Tutotial_Course" target="_blank" rel="noopener">VIO_Tutotial_Course</a></li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/MichaelBeechan/VO-SLAM-Review" target="_blank" rel="noopener">VO-SLAM-Review</a></li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/QingSimon/VINS-Mono-code-annotation" target="_blank" rel="noopener">VINS-Mono-code-annotation</a>,VINS-Mono代码注释以及公式推导</li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/ManiiXu/VINS-Mono-Learning" target="_blank" rel="noopener">VINS-Mono-Learning</a>,VINS-Mono代码注释</li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/HeYijia/VINS-Course" target="_blank" rel="noopener">VINS-Course</a>,VINS-Mono code without Ceres or ROS</li>
<li>[<strong>Tutorial</strong>]<a href="https://github.com/StevenCui/VIO-Doc" target="_blank" rel="noopener">VIO-Doc</a>,主流VIO论文推导及代码解析</li>
<li>[<strong>VO</strong>]<a href="https://github.com/muskie82/CNN-DSO" target="_blank" rel="noopener">CNN-DSO</a>, Direct Sparse Odometry with CNN Depth Prediction</li>
<li>[<strong>VO</strong>]<a href="https://github.com/lsyads/fisheye-ORB-SLAM" target="_blank" rel="noopener">fisheye-ORB-SLAM</a>, A real-time robust monocular visual SLAM system based on ORB-SLAM for fisheye cameras, without rectifying or cropping the input images</li>
<li>[<strong>VO</strong>]<a href="https://github.com/robotseu/ORB_Line_SLAM" target="_blank" rel="noopener">ORB_Line_SLAM</a>, Real-Time SLAM with BoPLW Pairs for Stereo Cameras, with Loop Detection and Relocalization Capabilities</li>
<li>[<strong>VO</strong>]<a href="https://github.com/ChiWeiHsiao/DeepVO-pytorch.git" target="_blank" rel="noopener">DeepVO-pytorch</a>, ICRA 2017 <a href="https://ieeexplore.ieee.org/document/7989236/" target="_blank" rel="noopener">DeepVO: Towards end-to-end visual odometry with deep Recurrent Convolutional Neural Networks</a></li>
<li>[<strong>Calib</strong>]<a href="https://github.com/MegviiRobot/CamOdomCalibraTool" target="_blank" rel="noopener">CamOdomCalibraTool</a>, The tool to calibrate extrinsic param between camera and wheel.</li>
<li>[<strong>Calib</strong>]<a href="https://github.com/heethesh/lidar_camera_calibration" target="_blank" rel="noopener">lidar_camera_calibration</a>,<a href="https://github.com/ankitdhall/lidar_camera_calibration" target="_blank" rel="noopener">another version</a></li>
<li>[<strong>Calib</strong>]<a href="https://github.com/MegviiRobot/OdomLaserCalibraTool.git" target="_blank" rel="noopener">OdomLaserCalibraTool</a>，相机与2D雷达标定</li>
<li>[<strong>Calib</strong>]<a href="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration" target="_blank" rel="noopener">extrinsic_lidar_camera_calibration</a>, LiDARTag: A Real-Time Fiducial Tag using Point Clouds, arXiv 2019, <strong>[<a href="https://arxiv.org/abs/1908.10349" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li>[<strong>Calib</strong>]<a href="https://github.com/beltransen/velo2cam_calibration" target="_blank" rel="noopener">velo2cam_calibration</a>, Automatic Calibration algorithm for Lidar-Stereo camera, <strong>[<a href="http://wiki.ros.org/velo2cam_calibration" target="_blank" rel="noopener">Project page</a>]</strong></li>
<li>[<strong>Dataset</strong>]<a href="https://github.com/HKBU-HPML/IRS.git" target="_blank" rel="noopener">IRS: A Large Synthetic Indoor Robotics Stereo Dataset for Disparity and Surface Normal Estimation</a></li>
<li>[<strong>Tools</strong>]<a href="https://github.com/christophhagen/averaging-quaternions" target="_blank" rel="noopener">averaging-quaternions</a>,四元数平均</li>
</ul>
<hr>
<p>分割线，以下是2019年的星标项目，上面是2020年新星标的。</p>
<ul>
<li><a href="https://github.com/naver/r2d2" target="_blank" rel="noopener">R2D2: Reliable and Repeatable Detector and Descriptor</a>,NeurIPS 2019,<strong>[<a href="https://arxiv.org/abs/1906.06195" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://europe.naverlabs.com/research/publications/r2d2-reliable-and-repeatable-detectors-and-descriptors-for-joint-sparse-local-keypoint-detection-and-feature-extraction/" target="_blank" rel="noopener">Project page</a>]</strong>，深度学习特征点+描述子</li>
<li><a href="https://github.com/1989Ryan/Semantic_SLAM" target="_blank" rel="noopener">Semantic_SLAM</a>,语义SLAM：ROS + ORB SLAM + PSPNet101</li>
<li><a href="https://github.com/BAILOOL/PlaceRecognition-LoopDetection" target="_blank" rel="noopener">PlaceRecognition-LoopDetection</a>, Light-weight place recognition and loop detection using road markings</li>
<li><a href="https://github.com/MISTLab/DOOR-SLAM" target="_blank" rel="noopener">DOOR-SLAM: Distributed, online, and outlier resilient SLAM for robotic teams</a>,<strong>[<a href="https://arxiv.org/abs/1909.12198" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://mistlab.ca/DOOR-SLAM/" target="_blank" rel="noopener">Project page</a>]</strong>，多机器人协作SLAM，增强了场景的适用性</li>
<li><a href="https://github.com/shamangary/awesome-local-global-descriptor" target="_blank" rel="noopener">awesome-local-global-descriptor</a>, 超详细深度学习特征点描述子集合，需要重点关注一下这个repo</li>
<li><a href="https://github.com/zju3dv/GIFT" target="_blank" rel="noopener">GIFT: Learning Transformation-Invariant Dense Visual Descriptors via Group CNNs</a>, NeurIPS 2019，<strong>[<a href="https://arxiv.org/abs/1911.05932" target="_blank" rel="noopener">PDF</a>]</strong>, <strong>[<a href="https://zju3dv.github.io/GIFT/" target="_blank" rel="noopener">Project page</a>]</strong>，浙大CAD+商汤联合实验室出品，利用Group CNN来改进superpoint描述子（仅描述，特征点提取可任意选择），可以大幅度增强视角变化时的特征点复检率与匹配点数</li>
<li><a href="https://github.com/axelBarroso/Key.Net" target="_blank" rel="noopener">Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters</a>,ICCV 2019, <strong>[<a href="https://arxiv.org/abs/1904.00889" target="_blank" rel="noopener">PDF</a>]</strong>, 深度学习特征点</li>
<li><a href="https://github.com/TRI-ML/KP3D" target="_blank" rel="noopener">Self-Supervised 3D Keypoint Learning for Ego-motion Estimation</a>,<strong>[<a href="https://arxiv.org/abs/1912.03426" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://www.youtube.com/watch?v=4hFhSD8QUPM" target="_blank" rel="noopener">Youtube</a>]</strong>, 深度学习特征点</li>
<li><a href="https://github.com/Jichao-Peng/VINS-Mono-Optimization" target="_blank" rel="noopener">VINS-Mono-Optimization</a>, 实现点线紧耦合优化的VINS-Mono</li>
<li><a href="https://github.com/PetWorm/msckf_vio_zhushi" target="_blank" rel="noopener">msckf_vio注释版本</a></li>
<li><a href="https://github.com/lyakaap/NetVLAD-pytorch" target="_blank" rel="noopener">NetVLAD-pytorch</a>, NetVLAD场景识别的pytorch实现</li>
<li><a href="http://microgps.cs.princeton.edu/" target="_blank" rel="noopener">High-Precision Localization Using Ground Texture (Micro-GPS)</a>,ECCV 2018,<strong>[<a href="https://arxiv.org/abs/1710.10687" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://microgps.cs.princeton.edu/" target="_blank" rel="noopener">Project page</a>]</strong>,<strong>[<a href="http://microgps.cs.princeton.edu/data/micro-gps-cpp-master.zip" target="_blank" rel="noopener">code</a>]</strong>，地向（摄像机朝向地面）SLAM，获得高精度重定位效果。</li>
<li><a href="https://github.com/LRMPUT/PlaneSLAM" target="_blank" rel="noopener">PlaneSLAM</a>, Paper: “On the Representation of Planes for Efficient Graph-based SLAM with High-level Features”</li>
<li><a href="https://github.com/ucla-vision/xivo" target="_blank" rel="noopener">XIVO: X Inertial-aided Visual Odometry and Sparse Mapping</a>, an open-source repository for visual-inertial odometry/mapping. </li>
<li><a href="https://github.com/lmb-freiburg/deeptam" target="_blank" rel="noopener">DeepTAM</a>,ECCV 2018,<strong>[<a href="https://arxiv.org/pdf/1808.01900.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://lmb.informatik.uni-freiburg.de/people/zhouh/deeptam/" target="_blank" rel="noopener">Project page</a>]</strong>,a learnt system for keyframe-based dense camera tracking and mapping.</li>
<li><a href="https://github.com/ajparra/iRotAvg" target="_blank" rel="noopener">iRotAvg, Why bundle adjust?</a>,ICRA 2019,<strong>[<a href="https://cs.adelaide.edu.au/~aparra/publication/parra19_icra/" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li><a href="https://github.com/Kelym/FAST" target="_blank" rel="noopener">Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation</a>,CVPR 2019,<strong>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Ke_Tactical_Rewind_Self-Correction_via_Backtracking_in_Vision-And-Language_Navigation_CVPR_2019_paper.html" target="_blank" rel="noopener">PDF</a>]</strong>，视觉+语言导航</li>
<li><a href="https://github.com/MISTLab/DOOR-SLAM" target="_blank" rel="noopener">DOOR-SLAM</a></li>
<li><a href="https://github.com/JiawangBian/FM-Bench" target="_blank" rel="noopener">An Evaluation of Feature Matchers for Fundamental Matrix Estimation</a>,BMVC 2019,<strong>[<a href="https://jwbian.net/Papers/FM_BMVC19.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://jwbian.net/fm-bench" target="_blank" rel="noopener">Project Page</a>]</strong>，特征匹配</li>
<li><a href="https://github.com/hyye/lio-mapping" target="_blank" rel="noopener">A Tightly Coupled 3D Lidar and Inertial Odometry and Mapping Approach</a>,ICRA 2019,<strong>[<a href="https://arxiv.org/abs/1904.06993" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://sites.google.com/view/lio-mapping" target="_blank" rel="noopener">Project Page</a>]</strong>，紧耦合雷达+IMU SLAM</li>
<li><a href="https://github.com/LRMPUT/PlaneSLAM" target="_blank" rel="noopener">On the Representation of Planes for Efficient Graph-based SLAM with High-level Features</a>,利用平面信息的SLAM</li>
<li><a href="https://github.com/Huangying-Zhan/DF-VO" target="_blank" rel="noopener">Visual Odometry Revisited: What Should Be Learnt?</a>,arXiv 2019,<strong>[<a href="https://arxiv.org/abs/1909.09803" target="_blank" rel="noopener">PDF</a>]</strong>, 深度学习深度+光流进行VO</li>
<li><a href="https://github.com/Xylon-Sean/rfnet" target="_blank" rel="noopener">RF-Net: An End-to-End Image Matching Network based on Receptive Field</a>,CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1906.00604" target="_blank" rel="noopener">PDF</a>]</strong>, 端到端图像匹配</li>
<li><a href="https://github.com/HKUST-Aerial-Robotics/Fast-Planner" target="_blank" rel="noopener">Fast-Planner</a>,IEEE Robotics and Automation Letters (RA-L), 2019,<strong>[<a href="https://ieeexplore.ieee.org/document/8758904" target="_blank" rel="noopener">PDF</a>]</strong>, 无人机轨迹生成</li>
<li><a href="https://github.com/dongjing3309/minisam" target="_blank" rel="noopener">A general and flexible factor graph non-linear least square optimization framework</a>,CoRR 2019,<strong>[<a href="http://arxiv.org/abs/1909.00903" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://minisam.readthedocs.io/" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/gao-ouyang/demo_for_kalmanFilter" target="_blank" rel="noopener">Demo for Kalman filter in ranging system</a>,卡尔曼滤波原理演示</li>
<li><a href="https://github.com/Ahmedest61/CNN-Region-VLAD-VPR" target="_blank" rel="noopener">A Holistic Visual Place Recognition Approach using Lightweight CNNs for Severe ViewPoint and Appearance Changes</a>，场景识别（外观与视角变化时）,<a href="https://github.com/ethz-asl/hierarchical_loc" target="_blank" rel="noopener">训练和部署源码</a></li>
<li><a href="https://github.com/uzh-rpg/sips2_open" target="_blank" rel="noopener">SIPs: Succinct Interest Points from Unsupervised Inlierness Probability Learning</a>,3D Vision (3DV) 2019,<strong>[<a href="https://arxiv.org/abs/1805.01358" target="_blank" rel="noopener">PDF</a>]</strong>，RPG实验室出品，深度学习特征点（有特征描述子）</li>
<li><a href="https://github.com/uzh-rpg/imips_open" target="_blank" rel="noopener">Matching Features Without Descriptors: Implicitly Matched Interest Points</a>,BMVC 2019,<strong>[<a href="http://rpg.ifi.uzh.ch/docs/BMVC19_Cieslewski.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,RPG实验室出品，无需特征描述即可进行特征匹配</li>
<li><a href="https://github.com/cardwing/Codes-for-Lane-Detection" target="_blank" rel="noopener">Learning Lightweight Lane Detection CNNs by Self Attention Distillation (ICCV 2019)</a>,ICCV 2019,<strong>[<a href="https://arxiv.org/abs/1908.00821" target="_blank" rel="noopener">PDF</a>]</strong>，深度学习道路检测</li>
<li><a href="https://github.com/youngguncho/awesome-slam-datasets" target="_blank" rel="noopener">Awesome SLAM Datasets</a>,史上最全SLAM数据集， <strong><a href="https://mp.weixin.qq.com/s/BzcghUnXTR9RQqA3Pc9MhA" target="_blank" rel="noopener">公众号说明: 最全 SLAM 开源数据集</a></strong></li>
<li><a href="https://github.com/Aceinna/gnss-ins-sim" target="_blank" rel="noopener">GNSS-INS-SIM</a>,惯导融合模拟器，支持IMU数据，轨迹生成等</li>
<li><a href="https://github.com/2013fangwentao/Multi-Sensor-Combined-Navigation" target="_blank" rel="noopener">Multi-Sensor Combined Navigation Program(GNSS, IMU, Camera and so on) 多源多传感器融合定位 GPS/INS组合导航</a></li>
<li><a href="https://github.com/scape-research/SOSNet" target="_blank" rel="noopener">SOSNet: Second Order Similarity Regularization for Local Descriptor Learning</a>,CVPR 2019,<strong><a href="https://research.scape.io/sosnet/" target="_blank" rel="noopener">[Project page]</a></strong> <strong><a href="https://arxiv.org/abs/1904.05019" target="_blank" rel="noopener">[Paper]</a></strong> <strong><a href="imgs/sosnet-poster.pdf">[Poster]</a></strong> <strong><a href="imgs/sosnet-oral.pdf">[Slides]</a></strong>，一种深度学习特征描述子</li>
<li><a href="https://github.com/oravus/seq2single" target="_blank" rel="noopener">Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation</a>,ICRA 2019,<strong>[<a href="https://arxiv.org/abs/1902.07381" target="_blank" rel="noopener">PDF</a>]</strong>,利用深度图像实现了大视角长时间的场景识别（根据深度图筛选得到不同深度层次的特征点然后与当前帧进行匹配，提高了场景召回率）</li>
<li><a href="https://github.com/rpng/calc2.0" target="_blank" rel="noopener">CALC2.0</a>,Convolutional Autoencoder for Loop Closure 2.0,用于闭环检测</li>
<li><a href="https://github.com/ethz-asl/segmap" target="_blank" rel="noopener">SegMap</a>,RSS 2018,<strong>[<a href="http://www.roboticsproceedings.org/rss14/p03.pdf" target="_blank" rel="noopener">PDF</a>]</strong>, 一种基于3D线段的地图表示，可用于场景识别/机器人定位/环境重建等</li>
<li><a href="https://github.com/cggos/msckf_vio_cg" target="_blank" rel="noopener">MSCKF_VIO</a>, a stereo version of MSCKF，基于MSCKF的双目VIO</li>
<li><a href="https://github.com/Relja/netvlad" target="_blank" rel="noopener">NetVLAD: CNN architecture for weakly supervised place recognition</a>，CVPR 2016, CNN框架弱监督学习场景识别,<strong>[<a href="https://www.di.ens.fr/willow/research/netvlad/" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/IFL-CAMP/easy_handeye" target="_blank" rel="noopener">easy_handeye</a>,Simple, straighforward ROS library for hand-eye calibration</li>
<li><a href="https://github.com/KinglittleQ/SuperPoint_SLAM" target="_blank" rel="noopener">SuperPoint-SLAM</a>,利用SuperPoint替换ORB特征点</li>
<li><a href="https://github.com/facebookresearch/pyrobot" target="_blank" rel="noopener">PyRobot: An Open Source Robotics Research Platform</a></li>
<li><a href="https://github.com/ethz-asl/hfnet" target="_blank" rel="noopener">From Coarse to Fine: Robust Hierarchical Localization at Large Scale with HF-Net</a>,<strong>[<a href="https://arxiv.org/abs/1812.03506" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li><a href="https://github.com/mp3guy/ICPCUDA" target="_blank" rel="noopener">Super fast implementation of ICP in CUDA</a></li>
<li><a href="https://github.com/ethz-asl/volumetric_mapping" target="_blank" rel="noopener"> A generic interface for disparity map and pointcloud insertion</a></li>
<li><a href="https://github.com/tdsuper/SPHORB" target="_blank" rel="noopener">SPHORB: A Fast and Robust Binary Feature on the Sphere</a>,International Journal of Computer Vision 2015,<strong>[<a href="http://scs.tju.edu.cn/~lwan/paper/SPHORB/pdf/SPHORB-final-small.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://scs.tju.edu.cn/~lwan/paper/SPHORB/SPHORB.html" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/ETH3D/badslam" target="_blank" rel="noopener">BADSLAM: Bundle Adjusted Direct RGB-D SLAM</a>,CVPR 2019,<strong>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Schops_BAD_SLAM_Bundle_Adjusted_Direct_RGB-D_SLAM_CVPR_2019_paper.pdf" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li><a href="https://github.com/uzh-rpg/rpg_e2vid" target="_blank" rel="noopener">High Speed and High Dynamic Range Video with an Event Camera</a>,arXiv,<strong>[<a href="http://rpg.ifi.uzh.ch/docs/arXiv19_Rebecq.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://rpg.ifi.uzh.ch/E2VID.html" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/PaoPaoRobot/Awesome-VIO" target="_blank" rel="noopener">Awesome-VIO</a>,Discuss about VIO in PaoPaoRobot group</li>
<li><a href="https://github.com/XinLiGH/GyroAllan" target="_blank" rel="noopener">GyroAllan</a>,陀螺仪随机误差的 Allan 方差分析, <a href="https://github.com/rpng/kalibr_allan" target="_blank" rel="noopener">Another version</a></li>
<li><a href="https://github.com/fangchangma/self-supervised-depth-completion" target="_blank" rel="noopener">Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and Monocular Camera</a>,ICRA 2019,<strong>[<a href="https://arxiv.org/pdf/1807.00275.pdf" target="_blank" rel="noopener">PDF</a>]</strong>, 优化LiDAR以及单目得到的深度图</li>
<li><a href="https://github.com/NVlabs/planercnn" target="_blank" rel="noopener">PlaneRCNN: 3D Plane Detection and Reconstruction from a Single Image</a>,CVPR 2019,<strong>[<a href="https://arxiv.org/pdf/1812.04072.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://research.nvidia.com/publication/2019-06_PlaneRCNN" target="_blank" rel="noopener">Project Page</a>]</strong>,通过单幅图像进行3D平面检测以及重建</li>
<li><a href="https://github.com/kokerf/DBow3" target="_blank" rel="noopener">DBow3</a>,注释版的DBow3代码</li>
<li><a href="https://github.com/VladyslavUsenko/basalt-mirror" target="_blank" rel="noopener">Visual-Inertial Mapping with Non-Linear Factor Recovery</a>,<strong>[<a href="https://arxiv.org/abs/1904.06504" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://vision.in.tum.de/research/vslam/basalt" target="_blank" rel="noopener">Project Page</a>]</strong>, 时空联合的VIO优化方案</li>
<li><a href="https://github.com/PaoPaoRobot/ICRA2019-paper-list" target="_blank" rel="noopener">ICRA2019-paper-list</a>,ICRA 2019论文列表（泡泡机器人出品暂时无链接）</li>
<li><a href="https://github.com/pedropro/CAPE" target="_blank" rel="noopener">Fast Cylinder and Plane Extraction from Depth Cameras for Visual Odometry</a>, IROS 2018,<strong>[<a href="https://arxiv.org/abs/1803.02380" target="_blank" rel="noopener">PDF</a>]</strong>,利用深度图进行圆柱检测以及平面检测进行VO</li>
<li><a href="https://github.com/kiran-mohan/SLAM-Algorithms-Octave" target="_blank" rel="noopener">Solutions to assignments of Robot Mapping Course WS 2013/14 by Dr. Cyrill Stachniss at University of Freiburg</a>,SLAM算法学习课后作业答案</li>
<li><a href="https://github.com/RonaldSun/VI-Stereo-DSO" target="_blank" rel="noopener">Direct sparse odometry combined with stereo cameras and IMU</a>,双目DSO+IMU</li>
<li><a href="https://github.com/HorizonAD/stereo_dso" target="_blank" rel="noopener">Direct Sparse Odometry with Stereo Cameras</a>,双目DSO</li>
<li><a href="https://github.com/uoip/g2opy" target="_blank" rel="noopener">Python binding of SLAM graph optimization framework g2o</a>,python版本的g2o实现</li>
<li><a href="https://github.com/rpautrat/SuperPoint" target="_blank" rel="noopener">SuperPoint: Self-Supervised Interest Point Detection and Description</a>, CVPR 2018, <strong>[<a href="https://arxiv.org/abs/1712.07629" target="_blank" rel="noopener">Paper</a>]</strong>, 深度学习描述子+描述</li>
<li><a href="https://github.com/lzx551402/contextdesc" target="_blank" rel="noopener">ContextDesc: Local Descriptor Augmentation with Cross-Modality Context</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1904.04084" target="_blank" rel="noopener">Paper</a>]</strong>, 深度学习描述子</li>
<li><a href="https://github.com/mihaidusmanu/d2-net" target="_blank" rel="noopener">D2-Net: A Trainable CNN for Joint Description and Detection of Local Features</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1905.03561" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://dsmn.ml/publications/d2-net.html" target="_blank" rel="noopener">Project Page</a>]</strong>, 深度学习关键点+描述</li>
<li><a href="https://github.com/ethz-asl/orb_slam_2_ros" target="_blank" rel="noopener">ROS interface for ORBSLAM2</a>,ROS版本的ORBSLAM2</li>
<li><a href="https://github.com/yan99033/CNN-SVO" target="_blank" rel="noopener">CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction</a>， <strong>[<a href="https://arxiv.org/pdf/1810.01011.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/ManiiXu/VINS-Mono-Learning" target="_blank" rel="noopener">VINS-Mono-Learning</a>，代码注释版VINS-Mono，初学者学习</li>
<li><a href="https://github.com/xdspacelab/openvslam" target="_blank" rel="noopener">OpenVSLAM: Versatile Visual SLAM Framework</a>, <strong>[<a href="https://openvslam.readthedocs.io/" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/fabianschenk/RESLAM" target="_blank" rel="noopener">RESLAM: A real-time robust edge-based SLAM system</a>, ICRA 2019, <strong>[<a href="https://github.com/fabianschenk/fabianschenk.github.io/raw/master/files/schenk_icra_2019.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/rubengooj/pl-slam" target="_blank" rel="noopener">PL-SLAM: a Stereo SLAM System through the Combination of Points and Line Segments</a>, <strong>[<a href="https://arxiv.org/abs/1705.09479" target="_blank" rel="noopener">Paper</a>]</strong>，线特征SLAM</li>
<li><a href="https://github.com/YipuZhao/GF_PL_SLAM" target="_blank" rel="noopener">Good Line Cutting: towards Accurate Pose Tracking of Line-assisted VO/VSLAM</a>, ECCV 2018, <strong>[<a href="https://sites.google.com/site/zhaoyipu/good-feature-visual-slam" target="_blank" rel="noopener">Project Page</a>]</strong>, 改进的PL-SLAM</li>
<li><a href="https://github.com/leoshine/Spherical_Regression" target="_blank" rel="noopener">Spherical Regression: Learning Viewpoints, Surface Normals and 3D Rotations on n-Spheres</a>, CVPR 2019, <strong>[<a href="http://arxiv.org/abs/1904.05404" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/icsl-Jeon/traj_gen_vis" target="_blank" rel="noopener">svo_edgelet</a>, 在线轨迹生成</li>
<li><a href="https://github.com/TimboKZ/caltech_samaritan" target="_blank" rel="noopener">Drone SLAM project for Caltech’s ME 134 Autonomy class</a>, <strong>[<a href="https://github.com/TimboKZ/caltech_samaritan/blob/master/CS134_Final_Project_Report.pdf" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li><a href="https://github.com/icsl-Jeon/traj_gen_vis" target="_blank" rel="noopener">Online Trajectory Generation of a MAV for Chasing a Moving Target in 3D Dense Environments</a>, <strong>[<a href="https://arxiv.org/pdf/1904.03421.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/AtsushiSakai/PythonRobotics" target="_blank" rel="noopener">PythonRobotics</a>,<strong>[<a href="https://arxiv.org/abs/1808.10703" target="_blank" rel="noopener">Paper</a>]</strong>, <a href="https://github.com/onlytailei/CppRobotics" target="_blank" rel="noopener">CppRobotics</a></li>
<li><a href="https://github.com/izhengfan/ba_demo_ceres" target="_blank" rel="noopener">Bundle adjustment demo using Ceres Solver</a>,  <strong>[<a href="https://fzheng.me/2018/01/23/ba-demo-ceres/" target="_blank" rel="noopener">Blog</a>]</strong>, ceres实现BA</li>
<li><a href="https://github.com/shichaoy/cube_slam" target="_blank" rel="noopener">CubeSLAM: Monocular 3D Object Detection and SLAM</a>, <strong>[<a href="https://arxiv.org/abs/1806.00557" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/sshaoshuai/PointRCNN" target="_blank" rel="noopener">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.04244" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/nrupatunga/GIST-global-Image-Descripor" target="_blank" rel="noopener">GIST-Global Image Descriptor</a>, GIST描述子</li>
<li><a href="https://github.com/ethz-asl/mav_voxblox_planning" target="_blank" rel="noopener">mav voxblox planning</a>, MAV planning tools using voxblox as the map representation.</li>
<li><a href="https://github.com/zziz/kalman-filter" target="_blank" rel="noopener">Python Kalman Filter</a>, 30行实现卡尔曼滤波</li>
<li><a href="https://github.com/arpg/vicalib" target="_blank" rel="noopener">vicalib</a>, 视觉惯导系统标定工具</li>
<li><a href="https://github.com/simondlevy/BreezySLAM" target="_blank" rel="noopener">BreezySLAM</a>, 基于雷达的SLAM，支持Python(&amp;Matlab, C++, and Java)</li>
<li><a href="https://github.com/Yvon-Shong/Probabilistic-Robotics" target="_blank" rel="noopener">Probabilistic-Robotics</a>, 《概率机器人》中文版，书和课后习题</li>
<li><a href="https://github.com/emmjaykay/stanford_self_driving_car_code" target="_blank" rel="noopener">Stanford Self Driving Car Code</a>, <strong>[<a href="http://robots.stanford.edu/papers/junior08.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, 斯坦福自动驾驶车代码</li>
<li><a href="https://github.com/ndrplz/self-driving-car" target="_blank" rel="noopener">Udacity Self-Driving Car Engineer Nanodegree projects</a></li>
<li><a href="https://github.com/TUMFTM/Lecture_AI_in_Automotive_Technology" target="_blank" rel="noopener">Artificial Intelligence in Automotive Technology</a>, TUM自动驾驶技术中的人工智能课程</li>
<li><a href="https://github.com/hlzz/DeepMatchVO" target="_blank" rel="noopener">DeepMatchVO: Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation</a>,ICRA 2019, <strong>[<a href="https://arxiv.org/abs/1902.09103" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/zdzhaoyong/GSLAM" target="_blank" rel="noopener">GSLAM: A General SLAM Framework and Benchmark</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1902.07995" target="_blank" rel="noopener">Paper</a>]</strong>, 集成了各种传感器输入的SLAM统一框架</li>
<li><a href="https://github.com/izhengfan/se2lam" target="_blank" rel="noopener">Visual-Odometric Localization and Mapping for Ground Vehicles Using SE(2)-XYZ Constraints</a>，ICRA 2019,基于SE(2)-XYZ约束的VO系统</li>
<li><a href="https://github.com/nicolov/simple_slam_loop_closure" target="_blank" rel="noopener">Simple bag-of-words loop closure for visual SLAM</a>, <strong>[<a href="https://nicolovaligi.com/bag-of-words-loop-closure-visual-slam.html" target="_blank" rel="noopener">Blog</a>]</strong>, 回环</li>
<li><a href="https://github.com/rmsalinas/fbow" target="_blank" rel="noopener">FBOW (Fast Bag of Words), an extremmely optimized version of the DBow2/DBow3 libraries</a>,优化版本的DBow2/DBow3</li>
<li><a href="https://github.com/tomas789/tonav" target="_blank" rel="noopener">Multi-State Constraint Kalman Filter (MSCKF) for Vision-aided Inertial Navigation(master’s thesis)</a></li>
<li><a href="https://github.com/yuzhou42/MSCKF" target="_blank" rel="noopener">MSCKF</a>, MSCKF中文注释版</li>
<li><a href="https://github.com/hbtang/calibcamodo" target="_blank" rel="noopener">Calibration algorithm for a camera odometry system</a>, VO系统的标定程序</li>
<li><a href="https://github.com/cggos/vins_mono_cg" target="_blank" rel="noopener">Modified version of VINS-Mono</a>, 注释版本VINS Mono</li>
<li><a href="https://github.com/zhenpeiyang/RelativePose" target="_blank" rel="noopener">Extreme Relative Pose Estimation for RGB-D Scans via Scene Completion</a>,<strong>[<a href="https://arxiv.org/abs/1901.00063" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/jessecw/EPnP_Eigen" target="_blank" rel="noopener">Implementation of EPnP algorithm with Eigen</a>,利用Eigen编写的EPnP</li>
<li><a href="https://github.com/jiexiong2016/GCNv2_SLAM" target="_blank" rel="noopener">Real-time SLAM system with deep features</a>, 深度学习描述子(ORB vs. GCNv2)</li>
<li><a href="https://github.com/Huangying-Zhan/Depth-VO-Feat" target="_blank" rel="noopener">Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction</a>, CVPR 2018, 无监督单目深度恢复以及VO</li>
<li><a href="https://github.com/Phylliida/orbslam-windows" target="_blank" rel="noopener">ORB-SLAM-windows</a>, Windows版本的ORB-SLAM</li>
<li><a href="https://github.com/danping/structvio" target="_blank" rel="noopener">StructVIO : Visual-inertial Odometry with Structural Regularity of Man-made Environments</a>,<strong>[<a href="http://drone.sjtu.edu.cn/dpzou/project/structvio.html" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/irvingzhang/KalmanFiltering" target="_blank" rel="noopener">KalmanFiltering</a>, 各种卡尔曼滤波器的demo</li>
<li><a href="https://github.com/ZhenghaoFei/visual_odom" target="_blank" rel="noopener">Stereo Odometry based on careful Feature selection and Tracking</a>, <strong>[<a href="https://lamor.fer.hr/images/50020776/Cvisic2017.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, C++ OpenCV实现SOFT</li>
<li><a href="https://github.com/dzunigan/zSLAM" target="_blank" rel="noopener">Visual SLAM with RGB-D Cameras based on Pose Graph Optimization</a></li>
<li><a href="https://github.com/drsrinathsridhar/GRANSAC" target="_blank" rel="noopener">Multi-threaded generic RANSAC implemetation</a>, 多线程RANSAC</li>
<li><a href="https://github.com/PyojinKim/OPVO" target="_blank" rel="noopener">Visual Odometry with Drift-Free Rotation Estimation Using Indoor Scene Regularities</a>, BMVC 2017, <strong>[<a href="http://pyojinkim.me/pub/Visual-Odometry-with-Drift-Free-Rotation-Estimation-Using-Indoor-Scene-Regularities/" target="_blank" rel="noopener">Project Page</a>]</strong>，利用平面正交信息进行VO</li>
<li><a href="https://github.com/baidu/ICE-BA" target="_blank" rel="noopener">ICE-BA</a>, CVPR 2018, <strong>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_ICE-BA_Incremental_Consistent_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/AIBluefisher/GraphSfM" target="_blank" rel="noopener">GraphSfM: Robust and Efficient Graph-based Structure from Motion</a>, <strong>[<a href="https://aibluefisher.github.io/GraphSfM/" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/cuitaixiang/LOAM_NOTED" target="_blank" rel="noopener">LOAM_NOTED</a>, loam中文注解版</li>
<li><a href="https://github.com/Ethan-Zhou/MWO" target="_blank" rel="noopener">Divide and Conquer: Effcient Density-Based Tracking of 3D Sensors in Manhattan Worlds</a>,ACCV 2016,<strong>[<a href="http://users.cecs.anu.edu.au/~u5535909/" target="_blank" rel="noopener">Project Page</a>]</strong>,曼哈顿世界利用深度传感器进行旋转量平移量分离优化</li>
<li><a href="https://github.com/jstraub/rtmf" target="_blank" rel="noopener">Real-time Manhattan World Rotation Estimation in 3D</a>,IROS 2015,实时曼哈顿世界旋转估计</li>
<li><a href="https://github.com/uzh-rpg/event-based_vision_resources" target="_blank" rel="noopener">Event-based Vision Resources</a>，关于事件相机的资源</li>
<li><a href="https://github.com/DeepTecher/AutonomousVehiclePaper" target="_blank" rel="noopener">AutonomousVehiclePaper</a>，无人驾驶相关论文速递</li>
<li><a href="https://github.com/wutianyiRosun/Segmentation.X" target="_blank" rel="noopener">Segmentation.X</a>, Segmentation相关论文&amp;代码</li>
<li><a href="https://github.com/amusi/CVPR2019-Code" target="_blank" rel="noopener">CVPR-2019</a>, CVPR 2019 论文开源项目合集</li>
<li><a href="https://github.com/kanster/awesome-slam" target="_blank" rel="noopener">awesome-slam</a>, SLAM合集</li>
<li><a href="https://github.com/tzutalin/awesome-visual-slam" target="_blank" rel="noopener">awesome-visual-slam</a>, 视觉SLAM合集</li>
<li><a href="https://github.com/zziz/pwc" target="_blank" rel="noopener">Papers with code</a>, 周更论文with代码</li>
<li><a href="https://github.com/cbsudux/awesome-human-pose-estimation" target="_blank" rel="noopener">Awesome Human Pose Estimation</a>,<a href="https://github.com/nkalavak/awesome-object-pose" target="_blank" rel="noopener">awesome-object-pose</a>, 位姿估计合集</li>
<li><a href="https://github.com/Ewenwan/MVision" target="_blank" rel="noopener">MVision</a>, 大礼包：机器人视觉 移动机器人 VS-SLAM ORB-SLAM2 深度学习目标检测 yolov3 行为检测 opencv PCL 机器学习 无人驾驶</li>
</ul>
<h2 id="Pose-Object-tracking"><a href="#Pose-Object-tracking" class="headerlink" title="Pose/Object tracking"></a>Pose/Object tracking</h2><ul>
<li><a href="https://github.com/KovenYu/MAR" target="_blank" rel="noopener">Unsupervised person re-identification by soft multilabel learning</a>,CVPR 2019,  <strong>[<a href="https://kovenyu.com/papers/2019_CVPR_MAR.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/tianzhi0549/FCOS" target="_blank" rel="noopener">FCOS: Fully Convolutional One-Stage Object Detection</a>,ICCV 2019,  <strong>[<a href="https://arxiv.org/abs/1904.01355" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/yangli18/hand_detection" target="_blank" rel="noopener">Hand Detection and Orientation Estimation</a></li>
<li><a href="https://github.com/Wanggcong/Spatial-Temporal-Re-identification" target="_blank" rel="noopener">Spatial-Temporal Person Re-identification</a>,AAAI 2019,<strong>[<a href="https://arxiv.org/abs/1812.03282" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/layumi/Person_reID_baseline_pytorch" target="_blank" rel="noopener">A tiny, friendly, strong pytorch implement of person re-identification baseline. <strong>Tutorial</strong></a>,CVPR 2019,  <strong>[<a href="https://arxiv.org/abs/1904.07223" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/tengteng95/Pose-Transfer" target="_blank" rel="noopener">Progressive Pose Attention for Person Image Generation</a>,CVPR 2019,<strong>[<a href="http://arxiv.org/abs/1904.03349" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/shamangary/FSA-Net" target="_blank" rel="noopener">FSA-Net: Learning Fine-Grained Structure Aggregation for Head Pose Estimation from a Single Image</a>, CVPR 2019,<strong>[<a href="https://github.com/shamangary/FSA-Net/blob/master/0191.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/yuanyuanli85/Fast_Human_Pose_Estimation_Pytorch" target="_blank" rel="noopener">An unoffical implemention for paper “Fast Human Pose Estimation”</a>, CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1811.05419" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/edvardHua/PoseEstimationForMobile" target="_blank" rel="noopener">Real-time single person pose estimation for Android and iOS</a>,手机端实现人体位姿估计</li>
<li><a href="https://github.com/cbsudux/Human-Pose-Estimation-101" target="_blank" rel="noopener">Basics of 2D and 3D Human Pose Estimation</a>,人体姿态估计入门</li>
<li><a href="https://github.com/OceanPang/Libra_R-CNN" target="_blank" rel="noopener">Libra R-CNN: Towards Balanced Learning for Object Detection</a></li>
<li><a href="https://github.com/HRNet/HRNet-Object-Detection" target="_blank" rel="noopener">High-resolution networks (HRNets) for object detection</a>, <strong>[<a href="https://arxiv.org/pdf/1904.04514.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/xiaolonw/TimeCycle" target="_blank" rel="noopener">Learning Correspondence from the Cycle-Consistency of Time</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1903.07593" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/zju3dv/pvnet" target="_blank" rel="noopener">PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.11788" target="_blank" rel="noopener">Paper</a>], [<a href="https://zju3dv.github.io/pvnet" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/mkocabas/EpipolarPose" target="_blank" rel="noopener">Self-Supervised Learning of 3D Human Pose using Multi-view Geometry</a>, CVPR 2018, <strong>[<a href="https://arxiv.org/abs/1903.02330" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/vita-epfl/openpifpaf" target="_blank" rel="noopener">PifPaf: Composite Fields for Human Pose Estimation</a>, <strong>[<a href="https://arxiv.org/abs/1903.06593" target="_blank" rel="noopener">Paper</a>]</strong> </li>
<li><a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" target="_blank" rel="noopener">Deep High-Resolution Representation Learning for Human Pose Estimation</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09212.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://jingdongwang2017.github.io/Projects/HRNet/PoseEstimation.html" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/YuliangXiu/PoseFlow" target="_blank" rel="noopener">PoseFlow: Efficient Online Pose Tracking)</a>, BMVC 2018, <strong>[<a href="https://arxiv.org/abs/1802.00977" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/vana77/Bottom-up-Clustering-Person-Re-identification" target="_blank" rel="noopener">A Bottom-Up Clustering Approach to Unsupervised Person Re-identification</a>，AAAI 2019, 重定位</li>
<li><a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener">Fast Online Object Tracking and Segmentation: A Unifying Approach</a>,CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1812.05050" target="_blank" rel="noopener">Paper</a>] [<a href="https://youtu.be/I_iOVrcpEBw" target="_blank" rel="noopener">Video</a>] [<a href="http://www.robots.ox.ac.uk/~qwang/SiamMask" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/TuSimple/simpledet" target="_blank" rel="noopener">SimpleDet - A Simple and Versatile Framework for Object Detection and Instance Recognition</a>,<strong>[<a href="https://arxiv.org/abs/1903.05831" target="_blank" rel="noopener">Paper</a>]</strong> </li>
</ul>
<h2 id="Depth-Disparity-amp-Flow-estimation"><a href="#Depth-Disparity-amp-Flow-estimation" class="headerlink" title="Depth/Disparity &amp; Flow estimation"></a>Depth/Disparity &amp; Flow estimation</h2><ul>
<li>[<strong>Depth</strong>]<a href="https://github.com/ethan-li-coding/SemiGlobalMatching" target="_blank" rel="noopener">SemiGlobalMatching</a>, SGM双目立体匹配算法完整实现，代码规范，注释丰富且清晰，CSDN同步教学</li>
<li><a href="https://github.com/callmeray/PointMVSNet" target="_blank" rel="noopener">PointMVSNet: Point-based Multi-view Stereo Network</a>,ICCV 2019,<strong>[<a href="https://arxiv.org/abs/1908.04422" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/JiaxiongQ/DeepLiDAR" target="_blank" rel="noopener">DeepLiDAR</a>,CVPR 2019, <strong>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qiu_DeepLiDAR_Deep_Surface_Normal_Guided_Depth_Prediction_for_Outdoor_Scene_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, 单张RGB图像+稀疏雷达数据进行室外场景深度估计</li>
<li><a href="https://github.com/atapour/monocularDepth-Inference" target="_blank" rel="noopener">Real-Time Monocular Depth Estimation using Synthetic Data with Domain Adaptation via Image Style Transfer</a>,CVPR 2018, <strong>[<a href="http://breckon.eu/toby/publications/papers/abarghouei18monocular.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/princeton-vl/YouTube3D" target="_blank" rel="noopener">Learning Single-Image Depth from Videos using Quality Assessment Networks</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1806.09573" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="http://www-personal.umich.edu/~wfchen/youtube3d/" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/WERush/SCDA" target="_blank" rel="noopener">SCDA: Adapting Object Detectors via Selective Cross-Domain Alignment</a>,CVPR 2019, <strong>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Adapting_Object_Detectors_via_Selective_Cross-Domain_Alignment_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="http://zhuxinge.me/aboutme.html" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/fabiotosi92/monoResMatch-Tensorflow" target="_blank" rel="noopener">Learning monocular depth estimation infusing traditional stereo knowledge</a>,CVPR 2019,<strong>[<a href="https://vision.disi.unibo.it/~ftosi/papers/monoResMatch.pdf" target="_blank" rel="noopener">PDF</a>]</strong></li>
<li><a href="https://github.com/laoreja/HPLFlowNet" target="_blank" rel="noopener">HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on Large-scale Point Clouds</a>,CVPR 2019,<strong>[<a href="hhttps://web.cs.ucdavis.edu/~yjlee/projects/cvpr2019-HPLFlowNet.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/feihuzhang/GANet" target="_blank" rel="noopener">GA-Net: Guided Aggregation Net for End-to-end Stereo Matching</a>,CVPR 2019,<strong>[<a href="https://arxiv.org/pdf/1904.06587.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/sunghoonim/DPSNet" target="_blank" rel="noopener">DPSNet: End-to-end Deep Plane Sweep Stereo</a>,ICLR 2019,<strong>[<a href="https://openreview.net/pdf?id=ryeYHi0ctQ" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/muskie82/AR-Depth-cpp" target="_blank" rel="noopener">Fast Depth Densification for Occlusion-aware Augmented Reality</a>, SIGGRAPH-Asia 2018, <strong>[<a href="https://homes.cs.washington.edu/~holynski/publications/occlusion/index.html" target="_blank" rel="noopener">Project Page</a>]</strong>,<a href="https://github.com/facebookresearch/AR-Depth" target="_blank" rel="noopener">another version</a></li>
<li><a href="https://github.com/CVLAB-Unibo/Learning2AdaptForStereo" target="_blank" rel="noopener">Learning To Adapt For Stereo</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1904.02957" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/JiaRenChang/PSMNet" target="_blank" rel="noopener">Pyramid Stereo Matching Network</a>,<strong>[<a href="https://arxiv.org/abs/1803.08669" target="_blank" rel="noopener">Paper</a>]</strong> </li>
<li><a href="https://github.com/lelimite4444/BridgeDepthFlow" target="_blank" rel="noopener">Bridging Stereo Matching and Optical Flow via Spatiotemporal Correspondence</a>, <strong>[<a href="https://arxiv.org/abs/1905.09265" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/wvangansbeke/Sparse-Depth-Completion" target="_blank" rel="noopener">Sparse Depth Completion</a>, <strong>[<a href="https://arxiv.org/pdf/1902.05356.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, RGB图像辅助雷达深度估计</li>
<li><a href="https://github.com/sshan-zhao/GASDA" target="_blank" rel="noopener">GASDA</a>, CVPR 2019, <strong>[<a href="https://sshan-zhao.github.io/papers/gasda.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/xy-guo/MVSNet_pytorch" target="_blank" rel="noopener">MVSNet: Depth Inference for Unstructured Multi-view Stereo</a>, <strong>[<a href="https://arxiv.org/abs/1804.02505" target="_blank" rel="noopener">Paper</a>]</strong>, 非官方实现版本的MVSNet</li>
<li><a href="https://github.com/HKUST-Aerial-Robotics/Stereo-RCNN" target="_blank" rel="noopener">Stereo R-CNN based 3D Object Detection for Autonomous Driving</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09738.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo" target="_blank" rel="noopener">Real-time self-adaptive deep stereo</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1810.05424" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/ialhashim/DenseDepth" target="_blank" rel="noopener">High Quality Monocular Depth Estimation via Transfer Learning</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.11941" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://ialhashim.github.io/publications/index.html" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/xy-guo/GwcNet" target="_blank" rel="noopener">Group-wise Correlation Stereo Network</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1903.04025" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/phuang17/DeepMVS" target="_blank" rel="noopener">DeepMVS: Learning Multi-View Stereopsis</a>, CVPR 2018,<strong>[<a href="https://phuang17.github.io/DeepMVS/index.html" target="_blank" rel="noopener">Project Page</a>]</strong>,多目深度估计</li>
<li><a href="https://github.com/sampepose/flownet2-tf" target="_blank" rel="noopener">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</a>, CVPR 2017, 深度学习光流恢复</li>
<li><a href="https://github.com/DLuensch/StereoVision-ADCensus" target="_blank" rel="noopener">StereoVision-ADCensus</a>,深度恢复代码集合(<strong>ADCensus, SGBM, BM</strong>)</li>
<li><a href="https://github.com/yangguorun/SegStereo" target="_blank" rel="noopener">SegStereo: Exploiting Semantic Information for Disparity Estimation</a>, 探究语义信息在深度估计中的作用</li>
<li><a href="https://github.com/kuantingchen04/Light-Field-Depth-Estimation" target="_blank" rel="noopener">Light Filed Depth Estimation using GAN</a>，利用GAN进行光场深度恢复</li>
<li><a href="https://github.com/daniilidis-group/EV-FlowNet" target="_blank" rel="noopener">EV-FlowNet: Self-Supervised Optical Flow for Event-based Cameras</a>,Proceedings of Robotics 2018,<strong>[<a href="https://arxiv.org/abs/1802.06898" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/vt-vl-lab/DF-Net" target="_blank" rel="noopener">DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency</a>, ECCV 2018, <strong>[<a href="https://arxiv.org/abs/1809.01649" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/yzcjtr/GeoNet" target="_blank" rel="noopener">GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose</a>, CVPR 2018, <strong>[<a href="https://arxiv.org/abs/1803.02276" target="_blank" rel="noopener">Paper</a>]</strong></li>
</ul>
<h2 id="3D-amp-Graphic"><a href="#3D-amp-Graphic" class="headerlink" title="3D &amp; Graphic"></a>3D &amp; Graphic</h2><ul>
<li><a href="https://github.com/WangYueFt/prnet" target="_blank" rel="noopener">PRNet: Self-Supervised Learning for Partial-to-Partial Registration</a>,NeurIPS 2019</li>
<li><a href="https://github.com/nkolot/SPIN" target="_blank" rel="noopener">Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop</a>,ICCV 2019, <strong>[<a href="https://arxiv.org/pdf/1909.12828.pdf" target="_blank" rel="noopener">Paper</a>]</strong> , <strong>[<a href="https://www.seas.upenn.edu/~nkolot/projects/spin/" target="_blank" rel="noopener">Project Page</a>]</strong> </li>
<li><a href="https://github.com/microsoft/multiview-human-pose-estimation-pytorch" target="_blank" rel="noopener">Cross View Fusion for 3D Human Pose Estimation</a>,ICCV 2019, <strong>[<a href="https://arxiv.org/abs/1909.01203" target="_blank" rel="noopener">Paper</a>]</strong> ,跨视角3D位姿估计</li>
<li><a href="https://github.com/Fanziapril/mvfnet" target="_blank" rel="noopener">MVF-Net: Multi-View 3D Face Morphable Model Regression</a>,多视角3D人脸重建, <strong>[<a href="https://arxiv.org/abs/1904.04473" target="_blank" rel="noopener">Paper</a>]</strong> </li>
<li><a href="https://github.com/saurabheights/KillingFusion" target="_blank" rel="noopener">KillingFusion</a></li>
<li><a href="https://github.com/PRBonn/refusion" target="_blank" rel="noopener">ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals</a>, <strong>[<a href="https://arxiv.org/pdf/1905.02082.pdf" target="_blank" rel="noopener">Paper</a>]</strong> </li>
<li><a href="https://github.com/Lotayou/densebody_pytorch" target="_blank" rel="noopener">densebody_pytorch</a>, <strong>[<a href="https://arxiv.org/abs/1903.10153v3" target="_blank" rel="noopener">Paper</a>]</strong> </li>
<li><a href="https://github.com/svip-lab/PlanarReconstruction" target="_blank" rel="noopener">Single-Image Piece-wise Planar 3D Reconstruction via Associative Embedding</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09777.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, 单目3D重建</li>
<li><a href="https://github.com/sunset1995/HorizonNet" target="_blank" rel="noopener">HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch Data Augmentation</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1901.03861" target="_blank" rel="noopener">Paper</a>]</strong>, 深度学习全景转3D</li>
<li><a href="https://github.com/Microsoft/O-CNN" target="_blank" rel="noopener">Adaptive O-CNN: A Patch-based Deep Representation of 3D Shapes</a>,SIGGRAPH Asia 2018, <strong>[<a href="https://wang-ps.github.io/AO-CNN.html" target="_blank" rel="noopener">Project Page</a>]</strong></li>
</ul>
<h2 id="Other-Collections"><a href="#Other-Collections" class="headerlink" title="Other Collections"></a>Other Collections</h2><ul>
<li><a href="https://github.com/timqian/chinese-independent-blogs" target="_blank" rel="noopener">chinese-independent-blogs</a>, 中文独立博客集锦</li>
<li><a href="https://github.com/RenYurui/StructureFlow" target="_blank" rel="noopener">StructureFlow: Image Inpainting via Structure-aware Appearance Flow</a>,图像inpainting</li>
<li><a href="https://github.com/ruanyf/free-books" target="_blank" rel="noopener">free-books</a>,互联网上的免费书籍</li>
<li><a href="https://github.com/academicpages/academicpages.github.io" target="_blank" rel="noopener">AcademicPages</a>,通用的学术主页模版</li>
<li><a href="https://github.com/microsoft/MMdnn" target="_blank" rel="noopener">MMdnn</a>,实现深度学习模型之间的相互转换</li>
<li><a href="https://github.com/abner2015/tensorflow2caffemodel" target="_blank" rel="noopener">tensorflow2caffemodel</a>,tensorflow模型转caffemodel</li>
<li><a href="https://github.com/fengdu78/lihang-code" target="_blank" rel="noopener">lihang-code</a>,《统计学习方法》的代码实现</li>
<li><a href="https://github.com/DLTcollab/sse2neon" target="_blank" rel="noopener">sse2neon</a>,<a href="https://github.com/jratcliff63367/sse2neon" target="_blank" rel="noopener">sse2neon</a>,SSE转neon，嵌入式移植时可能会用到;</li>
<li><a href="https://github.com/alirezadir/Production-Level-Deep-Learning" target="_blank" rel="noopener">Production-Level-Deep-Learning</a>,深度学习模型部署流程</li>
<li><a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch" target="_blank" rel="noopener">动手学深度学习Dive-into-DL-PyTorch</a></li>
<li><a href="https://github.com/deeplearning-ai/machine-learning-yearning-cn" target="_blank" rel="noopener">machine-learning-yearning-cn</a>，Machine Learning Yearning 中文版 - 《机器学习训练秘籍》 - Andrew Ng 著</li>
<li><a href="https://github.com/academicpages/academicpages.github.io" target="_blank" rel="noopener">academicpages.github.io</a>，学术主页模板</li>
<li><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">Coursera-ML-AndrewNg-Notes</a>,吴恩达老师的机器学习课程个人笔记</li>
<li><a href="https://github.com/roboticcam/machine-learning-notes" target="_blank" rel="noopener">machine-learning-notes</a>,机器学习，概率模型和深度学习的讲义(1500+页)和视频链接</li>
<li><a href="https://github.com/scutan90/CNN-Visualization" target="_blank" rel="noopener">CNN-Visualization</a>,CNN可视化、理解CNN</li>
<li><a href="https://github.com/mrgloom/awesome-semantic-segmentation" target="_blank" rel="noopener">Awesome Semantic Segmentation</a>, 语义分割集合</li>
<li><a href="https://github.com/mengyuest/iros2018-slam-papers" target="_blank" rel="noopener">IROS2018 SLAM Collections</a>, IROS 2018集合</li>
<li><a href="https://github.com/TerenceCYJ/VP-SLAM-SC-papers" target="_blank" rel="noopener">VP-SLAM-SC-papers</a>,Visual Positioning &amp; SLAM &amp; Spatial Cognition 论文统计与分析</li>
<li><a href="https://github.com/HuaizhengZhang/Awesome-System-for-Machine-Learning" target="_blank" rel="noopener">Awesome System for Machine Learning</a></li>
<li><a href="https://github.com/Thinkgamer/Machine-Learning-With-Python" target="_blank" rel="noopener">Machine-Learning-With-Python</a>, 《机器学习实战》python代码实现</li>
<li><a href="https://github.com/qqfly/how-to-learn-robotics" target="_blank" rel="noopener">How to learn robotics</a>, 开源机器人学学习指南</li>
<li><a href="https://github.com/kjw0612/awesome-deep-vision" target="_blank" rel="noopener">Awesome Deep Vision</a>,DL在CV领域的应用</li>
<li><a href="https://github.com/YapengTian/Single-Image-Super-Resolution" target="_blank" rel="noopener">Single-Image-Super-Resolution</a>, 一个有关<strong>图像超分辨</strong>的合集</li>
<li><a href="https://github.com/wifity/ai-report" target="_blank" rel="noopener">ai report</a>, AI相关的研究报告</li>
<li><a href="https://paperswithcode.com/sota" target="_blank" rel="noopener">State-of-the-art papers and code</a>,搜集了目前sota的论文以及代码</li>
<li><a href="https://github.com/extreme-assistant/cvpr2019" target="_blank" rel="noopener">CVPR 2019 (Papers/Codes/Project/Paper reading)</a></li>
<li><a href="https://github.com/openMVG/awesome_3DReconstruction_list" target="_blank" rel="noopener">A curated list of papers &amp; resources linked to 3D reconstruction from images</a>,有关三维重建的论文汇总</li>
<li><a href="https://github.com/nebula-beta/SLAM-Jobs" target="_blank" rel="noopener">SLAM-Jobs</a>, SLAM/SFM求职指南</li>
<li><a href="https://github.com/stevewongv/SPANet" target="_blank" rel="noopener">Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset</a>,CVPR 2019,去雨</li>
<li><a href="https://github.com/hezhangsprinter/DCPDN" target="_blank" rel="noopener">Densely Connected Pyramid Dehazing Network</a>,CVPR 2018,去雾</li>
<li><a href="https://github.com/open-mmlab/mmsr" target="_blank" rel="noopener">MMSR</a>，MMLAB推出的超分辨工具箱</li>
<li><a href="https://github.com/Bartzi/stn-ocr" target="_blank" rel="noopener">深度学习OCR</a></li>
<li><a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">西瓜书🍉学习笔记</a></li>
<li><a href="https://github.com/wwxFromTju/awesome-reinforcement-learning-zh" target="_blank" rel="noopener">awesome-reinforcement-learning-zh</a>,强化学习从入门到放弃的资料</li>
<li><a href="https://github.com/cszn/DPSR" target="_blank" rel="noopener">Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels</a>,CVPR 2019,超分辨</li>
<li><a href="https://github.com/lzhbrian/Cool-Fashion-Papers" target="_blank" rel="noopener">Cool Fashion Papers</a>, Cool resources about Fashion + AI.</li>
<li><a href="https://github.com/nbei/Deep-Flow-Guided-Video-Inpainting" target="_blank" rel="noopener">Deep Flow-Guided Video Inpainting</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1806.10447.pdf" target="_blank" rel="noopener">Paper</a>]</strong> ,图像修复</li>
<li><a href="https://github.com/dbolya/yolact" target="_blank" rel="noopener">YOLACT: Real-time Instance Segmentation</a></li>
<li><a href="https://github.com/lyl8213/Plate_Recognition-LPRnet" target="_blank" rel="noopener">LPRNet: License Plate Recognition via Deep Neural Networks</a>, <strong>[<a href="https://arxiv.org/pdf/1806.10447.pdf" target="_blank" rel="noopener">Paper</a>]</strong> </li>
<li><a href="https://github.com/xiaofengShi/CHINESE-OCR" target="_blank" rel="noopener">CHINESE-OCR</a>, 运用tf实现自然场景文字检测</li>
<li><a href="https://github.com/PerpetualSmile/BeautyCamera" target="_blank" rel="noopener">BeautyCamera</a>, 美颜相机，具有人脸检测、磨皮美白人脸、滤镜、调节图片、摄像功能</li>
<li><a href="https://github.com/zhengzhugithub/CV-arXiv-Daily" target="_blank" rel="noopener">CV-arXiv-Daily</a>, 分享计算机视觉每天的arXiv文章</li>
<li><a href="https://github.com/lyndonzheng/Pluralistic-Inpainting" target="_blank" rel="noopener">Pluralistic-Inpainting</a>, <a href="https://arxiv.org/abs/1903.04227" target="_blank" rel="noopener">ArXiv</a> | <a href="http://www.chuanxiaz.com/publication/pluralistic/" target="_blank" rel="noopener">Project Page</a> | <a href="http://www.chuanxiaz.com/project/pluralistic/" target="_blank" rel="noopener">Online Demo</a> | <a href="https://www.youtube.com/watch?v=9V7rNoLVmSs" target="_blank" rel="noopener">Video(demo)</a></li>
<li><a href="https://github.com/Jezzamonn/fourier" target="_blank" rel="noopener">An Interactive Introduction to Fourier Transforms</a>, 超棒的傅里叶变换图形化解释</li>
<li><a href="https://github.com/datawhalechina/pumpkin-book" target="_blank" rel="noopener">pumpkin-book</a>, 《机器学习》（西瓜书）公式推导解析</li>
<li><a href="https://github.com/JuliaLang/julia" target="_blank" rel="noopener">Julia</a></li>
<li><a href="https://github.com/alan-turing-institute/MLJ.jl" target="_blank" rel="noopener">A Julia machine learning framework</a>，一种基于Julia的机器学习框架</li>
<li><a href="https://github.com/ZhaoJ9014/face.evoLVe.PyTorch" target="_blank" rel="noopener">High-Performance Face Recognition Library on PyTorch</a>，人脸识别库</li>
<li><a href="https://github.com/enggen/Deep-Learning-Coursera" target="_blank" rel="noopener">Deep-Learning-Coursera</a>，深度学习教程（deeplearning.ai）</li>
<li><a href="https://github.com/RemoteML/bestofml" target="_blank" rel="noopener">The best resources around Machine Learning</a></li>
<li><a href="https://github.com/cydonia999/VGGFace2-pytorch" target="_blank" rel="noopener">VGGFace2: A dataset for recognising faces across pose and age</a></li>
<li><a href="https://github.com/SmirkCao/Lihang" target="_blank" rel="noopener">Statistical learning methods</a>，统计学习方法</li>
<li><a href="https://live.bilibili.com/7332534?visit_id=9ytrx9lpsy80" target="_blank" rel="noopener">End-to-end Adversarial Learning for Generative Conversational Agents</a>，2017，介绍了一种端到端的基于GAN的聊天机器人</li>
<li><a href="https://github.com/yulunzhang/RNAN" target="_blank" rel="noopener">Residual Non-local Attention Networks for Image Restoration</a>,ICLR 2019.</li>
<li><a href="https://github.com/HelenMao/MSGAN" target="_blank" rel="noopener">MSGAN: Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis</a>, CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1903.05628" target="_blank" rel="noopener">Paper</a>]</strong></li>
<li><a href="https://github.com/NVlabs/SPADE" target="_blank" rel="noopener">SPADE: Semantic Image Synthesis with Spatially-Adaptive Normalization</a>,CVPR 2019, <strong>[<a href="https://nvlabs.github.io/SPADE/" target="_blank" rel="noopener">Project Page</a>]</strong></li>
<li><a href="https://github.com/Oldpan/Faceswap-Deepfake-Pytorch" target="_blank" rel="noopener">Faceswap with Pytorch or DeepFake with Pytorch</a>, 换脸</li>
<li><a href="https://github.com/iperov/DeepFaceLab" target="_blank" rel="noopener">DeepFaceLab</a>, 换脸</li>
</ul>
<!-- ![](https://cdn.jsdelivr.net/gh/Vincentqyw/blog-resources/westlake/westlake-1.jpg) -->
    </div>

    
    
    
        
      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Vincent Qin</li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://www.vincentqin.tech/posts/awesome-works/" title="🔥Awesome CV Works">https://www.vincentqin.tech/posts/awesome-works/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.</li>
</ul>
</div>

      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/SLAM/" rel="tag"># SLAM</a>
            
              <a href="/tags/disparity/" rel="tag"># disparity</a>
            
              <a href="/tags/pose-tracking/" rel="tag"># pose-tracking</a>
            
              <a href="/tags/object-tracking/" rel="tag"># object-tracking</a>
            
              <a href="/tags/depth-estimation/" rel="tag"># depth-estimation</a>
            
              <a href="/tags/flow-estimation/" rel="tag"># flow-estimation</a>
            
              <a href="/tags/3D-graphics/" rel="tag"># 3D-graphics</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/posts/build-ssr-server/" rel="next" title="开启SSR模式">
                  <i class="fa fa-chevron-left"></i> 开启SSR模式
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/posts/first-black-hole/" rel="prev" title="Black Hole">
                  Black Hole <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="comments"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#SLAM-related"><span class="nav-number">1.</span> <span class="nav-text">SLAM related</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pose-Object-tracking"><span class="nav-number">2.</span> <span class="nav-text">Pose/Object tracking</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Depth-Disparity-amp-Flow-estimation"><span class="nav-number">3.</span> <span class="nav-text">Depth/Disparity &amp; Flow estimation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3D-amp-Graphic"><span class="nav-number">4.</span> <span class="nav-text">3D &amp; Graphic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Other-Collections"><span class="nav-number">5.</span> <span class="nav-text">Other Collections</span></a></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="https://vincentqin.gitee.io/images/qin_small.png"
      alt="Vincent Qin">
  <p class="site-author-name" itemprop="name">Vincent Qin</p>
  <div class="site-description" itemprop="description">Keep Your Curiosity</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">56</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">106</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/Vincentqyw" title="GitHub &rarr; https://github.com/Vincentqyw" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:realcat@126.com" title="Email &rarr; mailto:realcat@126.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>Email</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://vincentqin.gitee.io/images/qrcode_realcat.jpg" title="Wechat &rarr; https://vincentqin.gitee.io/images/qrcode_realcat.jpg" rel="noopener" target="_blank"><i class="fa fa-fw fa-weixin"></i>Wechat</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.zhihu.com/people/i_vincent/activities" title="Zhihu &rarr; https://www.zhihu.com/people/i_vincent/activities" rel="noopener" target="_blank"><i class="fa fa-fw fa-quora"></i>Zhihu</a>
      </span>
    
  </div>



  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-dashboard"></i>
      Scholar
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://xxx.itp.ac.cn" title="http://xxx.itp.ac.cn" rel="noopener" target="_blank">arxiv</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://arxiv-sanity.com/" title="http://arxiv-sanity.com/" rel="noopener" target="_blank">arxiv-sanity</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://openaccess.thecvf.com/menu.py" title="http://openaccess.thecvf.com/menu.py" rel="noopener" target="_blank">CVF</a>
        </li>
      
    </ul>
  </div>



  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-battery-three-quarters"></i>
      Friends Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.wangpengan.com/" title="http://www.wangpengan.com/" rel="noopener" target="_blank">Tensorboy</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://simtalk.cn/" title="http://simtalk.cn/" rel="noopener" target="_blank">Simshang</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://sttomato.github.io" title="https://sttomato.github.io" rel="noopener" target="_blank">Tomato</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://newdee.cf/" title="https://newdee.cf/" rel="noopener" target="_blank">Newdee</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://cs-people.bu.edu/yfhu/" title="http://cs-people.bu.edu/yfhu/" rel="noopener" target="_blank">WhoIf</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://yulunzhang.com/" title="http://yulunzhang.com/" rel="noopener" target="_blank">Yulun</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://sanglongbest.github.io/" title="https://sanglongbest.github.io/" rel="noopener" target="_blank">YangLiu</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://erenship.com/" title="https://erenship.com/" rel="noopener" target="_blank">Eren</a>
        </li>
      
    </ul>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-briefcase"></i>
      常用链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://realcat.avosapps.us" title="https://realcat.avosapps.us" rel="noopener" target="_blank">评论管理</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://gitee.com/vincentqin/vincentqin" title="https://gitee.com/vincentqin/vincentqin" rel="noopener" target="_blank">网站源码</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://xyz.realcat.icu:65432" title="http://xyz.realcat.icu:65432" rel="noopener" target="_blank">不可描述</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://www.notion.so/realcat" title="https://www.notion.so/realcat" rel="noopener" target="_blank">Notion</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="http://www.matrixcalculus.org/" title="http://www.matrixcalculus.org/" rel="noopener" target="_blank">矩阵求导</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://emojipedia.org/" title="https://emojipedia.org/" rel="noopener" target="_blank">Emoji</a>
        </li>
      
    </ul>
  </div>


  <div class="feed-link motion-element">
    <a href="https://realcat.vercel.app/" rel="alternate">
       <i class="fa fa-home"></i>Homepage Backup
    </a>
  </div>


      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2016 – <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vincent Qin</span>
</div>

  <script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
  <script>
    function timer() {
      var ages = moment.preciseDiff(moment(),moment(20160701,"YYYYMMDD"));
      ages = ages.replace(/years?/, "年");
      ages = ages.replace(/months?/, "月");
      ages = ages.replace(/days?/, "天");
      ages = ages.replace(/hours?/, "小时");
      ages = ages.replace(/minutes?/, "分");
      ages = ages.replace(/seconds?/, "秒");
      ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
      div.innerHTML = `已运行 ${ages}`;
    }
    var div = document.createElement("div");
    //插入到copyright之后
    var copyright = document.querySelector(".copyright");
    document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
    timer();
    setInterval("timer()",1000)
  </script>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="Total Visitors">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="Total Views">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>


  <div style="display: none;">
    <script src="//s95.cnzz.com/z_stat.php?id=1273219530&web_id=1273219530"></script>
  </div>






  <script>
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=65489609";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
<script src="/js/utils.js?v=7.4.0"></script>
<script src="/js/schemes/pisces.js?v=7.4.0"></script>

<script src="/js/next-boot.js?v=7.4.0"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>















<script>
if (document.querySelectorAll('div.pdf').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/pdfobject@2/pdfobject.min.js', () => {
    document.querySelectorAll('div.pdf').forEach(element => {
      PDFObject.embed(element.getAttribute('target'), element, {
        pdfOpenParams: {
          navpanes: 0,
          toolbar: 0,
          statusbar: 0,
          pagemode: 'thumbs',
          view: 'FitH'
        },
        PDFJS_URL: '/lib/pdf/web/viewer.html',
        height: element.getAttribute('height') || '500px'
      });
    });
  }, window.PDFObject);
}
</script>


<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme: 'forest',
      logLevel: 3,
      flowchart: { curve: 'linear' },
      gantt: { axisFormat: '%m/%d/%Y' },
      sequence: { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>




  

  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: true,
    notify: false,
    appId: 'E2yzANt8H4UiIuw4c95dTaXH-MdYXbMMI',
    appKey: 'NF1yxeki6kw4KM5glHkwjvKc',
    placeholder: 'Just go go',
    avatar: 'wavatar',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn',
    path: location.pathname
  });
}, window.Valine);
</script>


  





  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>



</body>
</html>
