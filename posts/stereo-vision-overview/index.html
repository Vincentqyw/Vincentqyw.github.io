<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/realcat-apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/realcat-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/realcat-32x32.png">
  <link rel="mask-icon" href="/images/realcat-safari-pinned-tab.svg" color="#222">
  <meta name="google-site-verification" content="u46QTaG_Dv3OZLpOBKYtqyuiNtIdnhSG5ASKoNvGBCM">
  <meta name="baidu-site-verification" content="MtcbwE45ft">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.vincentqin.tech","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.13.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"show_result":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":"waline","storage":true,"lazyload":true,"nav":null,"activeClass":"waline"},"stickytabs":true,"motion":{"enable":false,"async":true,"transition":{"post_block":"fadeIn","post_header":"fadeIn","post_body":"fadeIn","coll_header":"fadeIn","sidebar":"fadeIn"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本文主要翻译自Mattoccia的双目视差估计综述，对于刚刚接触立体深度估计方向的小伙伴会有帮助；如果你是专家也可以做一下复习，如有错误请在评论中指出。">
<meta property="og:type" content="article">
<meta property="og:title" content="立体视觉综述：Stereo Vision Overview">
<meta property="og:url" content="https://www.vincentqin.tech/posts/stereo-vision-overview/index.html">
<meta property="og:site_name" content="RealCat">
<meta property="og:description" content="本文主要翻译自Mattoccia的双目视差估计综述，对于刚刚接触立体深度估计方向的小伙伴会有帮助；如果你是专家也可以做一下复习，如有错误请在评论中指出。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/depth-overview-cover.jpg">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p6.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p8.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p9.png">
<meta property="og:image" content="https://vincentqin.tech/blog-resources/stereo-vision-overview/Epipolar_geometry.svg">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p10.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p11.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p12.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p13.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p14-1.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p14-2.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p15.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p16.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p17.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p18.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p21.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p22.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p23.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p25.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p26.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p27.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p28.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p29.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p37-1.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p37-2.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p38.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p39-1.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p39-2.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p40.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p41-1.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p41-2.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p42.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p44.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p47.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p48.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p50.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p52.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p53.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p57.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p58.png">
<meta property="og:image" content="https://vincentqin.tech/blog-resources/stereo-vision-overview/p59.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p60-1.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p60-2.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p61.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p62.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p64.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p65.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p66.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p87.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p88.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p89.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p90.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p91.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p92.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p94.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p95.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p98-1.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p98-2.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p99.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p101.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p102.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p103.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p109.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p111.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p115.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p122.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p127.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p135.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p139.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p166.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p167.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p168.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p169.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p170.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p181.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p183.png">
<meta property="article:published_time" content="2018-03-26T07:03:55.000Z">
<meta property="article:modified_time" content="2022-09-04T16:29:35.823Z">
<meta property="article:author" content="Vincent Qin">
<meta property="article:tag" content="computer vision">
<meta property="article:tag" content="stereo matching">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/depth-overview-cover.jpg">


<link rel="canonical" href="https://www.vincentqin.tech/posts/stereo-vision-overview/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://www.vincentqin.tech/posts/stereo-vision-overview/","path":"posts/stereo-vision-overview/","title":"立体视觉综述：Stereo Vision Overview"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>立体视觉综述：Stereo Vision Overview | RealCat</title>
  
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-97856334-1","only_pageview":true}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>





<link rel="dns-prefetch" href="https://comments.vincentqin.tech">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">RealCat</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Turn on, Tune in, Drop out</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">76</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">14</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">111</span></a></li><li class="menu-item menu-item-collections"><a href="/collections" rel="section"><i class="fa fa-diamond fa-fw"></i>Collections</a></li><li class="menu-item menu-item-guest_comments"><a href="/guestbook" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89"><span class="nav-number">1.</span> <span class="nav-text">什么是立体视觉</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E7%9B%AE%E7%9B%B8%E6%9C%BA"><span class="nav-number">2.</span> <span class="nav-text">单目相机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8C%E7%9B%AE%E7%9B%B8%E6%9C%BA"><span class="nav-number">3.</span> <span class="nav-text">双目相机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%81%E7%BA%BF%E7%BA%A6%E6%9D%9F%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95"><span class="nav-number">3.1.</span> <span class="nav-text">极线约束(对极几何)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E4%B8%8E%E8%A7%86%E5%B7%AE"><span class="nav-number">3.2.</span> <span class="nav-text">深度与视差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%86%E7%95%8C"><span class="nav-number">3.3.</span> <span class="nav-text">视界</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1"><span class="nav-number">4.</span> <span class="nav-text">深度估计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A6%BB%E7%BA%BF%E6%A0%87%E5%AE%9A"><span class="nav-number">4.1.</span> <span class="nav-text">离线标定</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D"><span class="nav-number">4.2.</span> <span class="nav-text">立体匹配</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%89%E8%A7%92%E6%B5%8B%E9%87%8F"><span class="nav-number">4.3.</span> <span class="nav-text">三角测量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D%E7%9A%84%E6%8C%91%E6%88%98%E6%80%A7"><span class="nav-number">5.</span> <span class="nav-text">立体匹配的挑战性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%89%E5%BA%A6%E5%A4%B1%E7%9C%9F%E4%BB%A5%E5%8F%8A%E5%99%AA%E5%A3%B0"><span class="nav-number">5.1.</span> <span class="nav-text">光度失真以及噪声</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%8F%E8%A7%86%E6%94%B6%E7%BC%A9"><span class="nav-number">5.2.</span> <span class="nav-text">透视收缩</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%8F%E8%A7%86%E5%8F%98%E5%BD%A2"><span class="nav-number">5.3.</span> <span class="nav-text">透视变形</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A0%E7%BA%B9%E7%90%86%E5%8C%BA%E5%9F%9F"><span class="nav-number">5.4.</span> <span class="nav-text">无纹理区域</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E5%A4%8D%E6%B7%B7%E6%B7%86%E5%8C%BA%E5%9F%9F"><span class="nav-number">5.5.</span> <span class="nav-text">重复&#x2F;混淆区域</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%8F%E6%98%8E%E7%89%A9%E4%BD%93"><span class="nav-number">5.6.</span> <span class="nav-text">透明物体</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%81%AE%E6%8C%A1%E5%8C%BA%E4%BB%A5%E5%8F%8A%E4%B8%8D%E8%BF%9E%E7%BB%AD%E5%8C%BA%E5%9F%9F1"><span class="nav-number">5.7.</span> <span class="nav-text">遮挡区以及不连续区域（1）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%81%AE%E6%8C%A1%E5%8C%BA%E4%BB%A5%E5%8F%8A%E4%B8%8D%E8%BF%9E%E7%BB%AD%E5%8C%BA%E5%9F%9F2"><span class="nav-number">5.8.</span> <span class="nav-text">遮挡区以及不连续区域（2）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#middlebury%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">6.</span> <span class="nav-text">Middlebury数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8C%B9%E9%85%8D%E9%97%AE%E9%A2%98"><span class="nav-number">7.</span> <span class="nav-text">匹配问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">7.1.</span> <span class="nav-text">数据预处理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E9%87%8F%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="nav-number">8.</span> <span class="nav-text">损失量的计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%90%E5%83%8F%E7%B4%A0%E7%9A%84%E5%8C%B9%E9%85%8D%E8%AF%AF%E5%B7%AE"><span class="nav-number">8.1.</span> <span class="nav-text">逐像素的匹配误差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8C%BA%E5%9F%9F%E5%8C%B9%E9%85%8D%E6%8D%9F%E5%A4%B1"><span class="nav-number">8.2.</span> <span class="nav-text">区域匹配损失</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E8%81%9A%E5%90%88"><span class="nav-number">9.</span> <span class="nav-text">损失聚合</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A7%AF%E5%88%86%E5%9B%BE%E5%83%8F"><span class="nav-number">9.1.</span> <span class="nav-text">积分图像</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%B1%E6%BB%A4%E6%B3%A2%E5%99%A8"><span class="nav-number">9.2.</span> <span class="nav-text">箱滤波器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D%E4%B8%AD%E6%8D%9F%E5%A4%B1%E8%81%9A%E5%90%88%E7%AD%96%E7%95%A5%E7%9A%84%E5%88%86%E7%B1%BB%E5%8F%8A%E8%AF%84%E4%BC%B0"><span class="nav-number">10.</span> <span class="nav-text">立体匹配中损失聚合策略的分类及评估</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BA%E5%AE%9A%E7%AA%97%E5%8F%A3"><span class="nav-number">10.1.</span> <span class="nav-text">固定窗口</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E7%A7%BB%E5%8A%A8%E7%AA%97%E5%8F%A311"><span class="nav-number">10.2.</span> <span class="nav-text">可移动窗口11</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E7%AA%97%E5%8F%A37"><span class="nav-number">10.3.</span> <span class="nav-text">多窗口7</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E5%8F%98%E7%AA%97%E5%8F%A312"><span class="nav-number">10.4.</span> <span class="nav-text">可变窗口12</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%88%86%E5%89%B2%E7%9A%84%E7%AA%97%E5%8F%A35"><span class="nav-number">10.5.</span> <span class="nav-text">基于分割的窗口5</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E9%80%82%E5%BA%94%E5%8A%A0%E6%9D%8314-51"><span class="nav-number">10.6.</span> <span class="nav-text">自适应加权14
51</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%9F%9F10"><span class="nav-number">10.7.</span> <span class="nav-text">可分支持域10</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BF%AB%E9%80%9F%E8%81%9A%E5%90%8864"><span class="nav-number">10.8.</span> <span class="nav-text">快速聚合64</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BF%AB%E9%80%9F%E5%8F%8C%E8%BE%B9%E6%BB%A4%E6%B3%A265"><span class="nav-number">10.9.</span> <span class="nav-text">快速双边滤波65</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">10.10.</span> <span class="nav-text">局部一致性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#o1-adaptive-cost-aggregationspan-classhint--top-hint--error-hint--medium-hint--rounded-hint--bounce-aria-labell.-de-maeztu-s.-mattoccia-a.-villanueva-r.-cabeza-linear-stereo-matching-international-conference-on-computer-vision-iccv-2011-november-6-13-2011-barcelona-spain75"><span class="nav-number">10.11.</span> <span class="nav-text">O(1)
adaptive cost
aggregation&lt;span
class&#x3D;&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot;
aria-label&#x3D;&quot;L. De-Maeztu, S. Mattoccia, A. Villanueva, R. Cabeza,
&quot;Linear stereo matching&quot;, International Conference on Computer Vision
(ICCV 2011), November 6-13, 2011, Barcelona,
Spain&quot;&gt;75</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%86%E5%B7%AE%E6%B7%B1%E5%BA%A6%E8%AE%A1%E7%AE%97%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96"><span class="nav-number">11.</span> <span class="nav-text">视差&#x2F;深度计算以及优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92"><span class="nav-number">11.1.</span> <span class="nav-text">动态规划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%AB%E6%8F%8F%E7%BA%BF%E4%BC%98%E5%8C%96scanline-optimizationso30"><span class="nav-number">11.2.</span> <span class="nav-text">扫描线优化（Scanline
Optimization，SO30）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%86%E5%B7%AE%E7%B2%BE%E5%8C%96"><span class="nav-number">12.</span> <span class="nav-text">视差精化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">13.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Vincent Qin"
      src="https://vincentqin.gitee.io/images/qin_small.png">
  <p class="site-author-name" itemprop="name">Vincent Qin</p>
  <div class="site-description" itemprop="description">Keep Your Curiosity</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">76</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">111</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Vincentqyw" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Vincentqyw" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:realcat@126.com" title="Email → mailto:realcat@126.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>Email</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://vincentqin.gitee.io/images/qrcode_realcat.jpg" title="Wechat → https:&#x2F;&#x2F;vincentqin.gitee.io&#x2F;images&#x2F;qrcode_realcat.jpg" rel="noopener" target="_blank"><i class="fab fa-weixin fa-fw"></i>Wechat</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/i_vincent/activities" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;i_vincent&#x2F;activities" rel="noopener" target="_blank"><i class="fab fa-quora fa-fw"></i>Zhihu</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/AlphaRealcat" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;AlphaRealcat" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/18136563" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;18136563" rel="noopener" target="_blank"><i class="fa fa-video-camera fa-fw"></i>Bilibili</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://bafybeic2jt62kpyh6cz2g4ngxs4kazojfw3dhx53mco3wc6f56dejty4xm.ipfs.infura-ipfs.io/" title="Web3.0 → https:&#x2F;&#x2F;bafybeic2jt62kpyh6cz2g4ngxs4kazojfw3dhx53mco3wc6f56dejty4xm.ipfs.infura-ipfs.io" rel="noopener" target="_blank"><i class="link fa-fw"></i>Web3.0</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-dashboard"></i>
      Scholar
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://xxx.itp.ac.cn/" title="http:&#x2F;&#x2F;xxx.itp.ac.cn" rel="noopener" target="_blank">Arxiv-Mirror</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://arxiv-sanity.com/" title="http:&#x2F;&#x2F;arxiv-sanity.com&#x2F;" rel="noopener" target="_blank">Arxiv-sanity</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://openaccess.thecvf.com/menu.py" title="http:&#x2F;&#x2F;openaccess.thecvf.com&#x2F;menu.py" rel="noopener" target="_blank">CVF</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://paperswithcode.com/sota" title="https:&#x2F;&#x2F;paperswithcode.com&#x2F;sota" rel="noopener" target="_blank">Paper&Code</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://scihub.wikicn.top/" title="https:&#x2F;&#x2F;scihub.wikicn.top&#x2F;" rel="noopener" target="_blank">Scihub</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://ras.papercept.net/conferences/scripts/start.pl" title="http:&#x2F;&#x2F;ras.papercept.net&#x2F;conferences&#x2F;scripts&#x2F;start.pl" rel="noopener" target="_blank">RAS</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://openreview.net/" title="https:&#x2F;&#x2F;openreview.net&#x2F;" rel="noopener" target="_blank">OpenReview</a>
        </li>
    </ul>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-battery-three-quarters fa-fw"></i>
      Friends Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.wangpengan.com/" title="http:&#x2F;&#x2F;www.wangpengan.com&#x2F;" rel="noopener" target="_blank">Tensorboy</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://simtalk.cn/" title="http:&#x2F;&#x2F;simtalk.cn&#x2F;" rel="noopener" target="_blank">Simshang</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sttomato.github.io/" title="https:&#x2F;&#x2F;sttomato.github.io" rel="noopener" target="_blank">Tomato</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://dfine.tech/" title="http:&#x2F;&#x2F;dfine.tech&#x2F;" rel="noopener" target="_blank">Newdee</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://cs-people.bu.edu/yfhu/" title="http:&#x2F;&#x2F;cs-people.bu.edu&#x2F;yfhu&#x2F;" rel="noopener" target="_blank">WhoIf</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://yulunzhang.com/" title="http:&#x2F;&#x2F;yulunzhang.com&#x2F;" rel="noopener" target="_blank">Yulun</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sanglongbest.github.io/" title="https:&#x2F;&#x2F;sanglongbest.github.io&#x2F;" rel="noopener" target="_blank">YangLiu</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.erenship.com/" title="https:&#x2F;&#x2F;www.erenship.com&#x2F;" rel="noopener" target="_blank">Eren</a>
        </li>
    </ul>
  </div>

  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-briefcase"></i>
      Common Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://comments.vincentqin.tech/ui" title="https:&#x2F;&#x2F;comments.vincentqin.tech&#x2F;ui" rel="noopener" target="_blank">Comments</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://gitee.com/vincentqin/vincentqin" title="https:&#x2F;&#x2F;gitee.com&#x2F;vincentqin&#x2F;vincentqin" rel="noopener" target="_blank">Source</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.notion.so/realcat" title="https:&#x2F;&#x2F;www.notion.so&#x2F;realcat" rel="noopener" target="_blank">Notion</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.matrixcalculus.org/" title="http:&#x2F;&#x2F;www.matrixcalculus.org&#x2F;" rel="noopener" target="_blank">Calculus</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://emojipedia.org/" title="https:&#x2F;&#x2F;emojipedia.org&#x2F;" rel="noopener" target="_blank">Emoji</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://unstoppabledomains.com/" title="https:&#x2F;&#x2F;unstoppabledomains.com&#x2F;" rel="noopener" target="_blank">UD</a>
        </li>
    </ul>
  </div>




        </div>

      <div class="wechat_QR_code">
      <!-- 二维码 -->
      <img src ="https://vincentqin.tech/blog-resources/qrcode_realcat.jpg">
      <span>Follow Me on Wechat</span>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://www.vincentqin.tech/posts/stereo-vision-overview/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://vincentqin.gitee.io/images/qin_small.png">
      <meta itemprop="name" content="Vincent Qin">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RealCat">
      <meta itemprop="description" content="Keep Your Curiosity">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="立体视觉综述：Stereo Vision Overview | RealCat">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          立体视觉综述：Stereo Vision Overview
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-03-26 15:03:55" itemprop="dateCreated datePublished" datetime="2018-03-26T15:03:55+08:00">2018-03-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-09-05 00:29:35" itemprop="dateModified" datetime="2022-09-05T00:29:35+08:00">2022-09-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Depth-Estimation/" itemprop="url" rel="index"><span itemprop="name">Depth Estimation</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline: </span>
  
    <a title="waline" href="/posts/stereo-vision-overview/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/posts/stereo-vision-overview/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-item" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="waline-pageview-count" data-path="/posts/stereo-vision-overview/"></span>
    </span>
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <script src="/assets/js/DPlayer.min.js"> </script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><!-- [//]: ![](https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/cat_girl_cut.jpg) -->
<blockquote>
<p>本文主要翻译自<a
href="www.vision.deis.unibo.it/smatt">Mattoccia</a>的双目视差估计综述，对于刚刚接触立体深度估计方向的小伙伴会有帮助；如果你是专家也可以做一下复习，如有错误请在评论中指出。</p>
</blockquote>
<span id="more"></span>
<p>文中主要介绍以下几个方面的内容：</p>
<ul>
<li>Introduction to stereo vision</li>
<li>Overview of a stereo vision system</li>
<li>Algorithms for visual correspondence</li>
<li>Computational optimizations</li>
<li>Hardware implementation</li>
<li>Applications</li>
</ul>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/depth-overview-cover.jpg" width=1600px></p>
<h2 id="什么是立体视觉">什么是立体视觉</h2>
<ul>
<li>是一个能够从双目或者多目相机中提取深度图像的技术</li>
<li>在计算机视觉领域很火爆的研究话题</li>
<li>这与以下几个方面的相关：双目立体视觉系统、稠密立体算法、立体视觉应用</li>
<li>偏好能够实时或者硬件实现</li>
</ul>
<h2 id="单目相机">单目相机</h2>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p6.png" width=1200px></p>
<p>如图所示的是单目摄像机的拍摄原理，右侧实际场景可以抽象成左侧的模型。可以发现场景中的P点与Q点会同时汇聚在成像平面中的一点，同样遮挡问题出现在PQ连线的所有点。</p>
<h2 id="双目相机">双目相机</h2>
<p>对于双目相机，<span class="math inline">\(O_R\)</span>和<span
class="math inline">\(O_T\)</span>分别是左右相机的光学中心，对于在参考相机像平面上被汇聚的两点（p和q），在目标相机像平面上会被区分开来，那么我们可以找到双目或者多目相机中匹配的点利用三角相似原理来估计深度。那么我们怎么寻找相对应的点呢？一个直观的想法就是固定两幅图中的一幅，然后在另外一幅图中
进行2D范围的搜索匹配点。
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p8.png" width=1200px></p>
<p>但实际情况这样做的代价非常大。不过多亏了有<strong>极线约束</strong>，我们可以在图像的<strong>1D</strong>范围上进行搜索。以下将要对极线约束进行解释。</p>
<h3 id="极线约束对极几何">极线约束(对极几何)</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p9.png" width=1200px>
- 对于参考图像R而言，现实场景中的P与Q点在其像平面<span
class="math inline">\({\pi}_R\)</span>上被投影成为一个点p=q。 -
极线约束规定，属于（红色）视线的点对应位于目标图像T的图像平面<span
class="math inline">\({\pi}_T\)</span>上的绿线上。</p>
<p>我们可以在维基百科上找到更为详细的<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Epipolar_geometry">介绍</a>，具体描述可见下图。特别感谢<a
target="_blank" rel="noopener" href="https://blog.csdn.net/lin453701006/article/details/55096777"><span
class="citation"
data-cites="岳麓吹雪同学">@岳麓吹雪同学</span></a>的帮忙，以下是他已经整理好的译文。下图是针孔相机模型图。两个针孔相机看向空间点，实际相机的像面位于焦点中心后面，生成了一幅关于透镜的焦点中心对称的图像。<strong>这个问题可以简化为在焦点中心前方放置一个虚拟像面来生成正立图像，而不需要对称变换得到</strong>。<span
class="math inline">\(O_L\)</span>和<span
class="math inline">\(O_R\)</span>表示两个相机透镜中心，<span
class="math inline">\(X\)</span>表示两个相机共同的目标点，<span
class="math inline">\(X_L\)</span>和<span
class="math inline">\(X_R\)</span>是点<span
class="math inline">\(X\)</span>在两像面上的投影。
<img data-src="https://vincentqin.tech/blog-resources/stereo-vision-overview/Epipolar_geometry.svg" width=1600px></p>
<ul>
<li><p><strong>epipolar points极点</strong>
每一个相机的透镜中心是不同的，会投射到另一个相机像面的不同点上。这两个像点用<span
class="math inline">\(e_L\)</span>和<span
class="math inline">\(e_R\)</span>表示，被称为<strong>epipolar
points极点</strong>。两个极点<span
class="math inline">\(e_L\)</span>、<span
class="math inline">\(e_R\)</span>分别与透镜中心<span
class="math inline">\(O_L\)</span>、<span
class="math inline">\(O_R\)</span>在空间中位于一条直线上。</p></li>
<li><p><strong>epipolar plane极面</strong> 将<span
class="math inline">\(X\)</span>、<span
class="math inline">\(O_L\)</span>和<span
class="math inline">\(O_R\)</span>三点形成的面称为epipolar
plane极面。</p></li>
<li><p><strong>epipolar line极线</strong> 直线<span
class="math inline">\(O_LX\)</span>被左侧相机看做一个点，因为它和透镜中心位于一条线上。然而，从右相机看直线<span
class="math inline">\(O_LX\)</span>，则是像面上的一条线直线<span
class="math inline">\(e_RX_R\)</span>，被称为epipolar
line极线。从另一个角度看，极面<span
class="math inline">\(XO_LO_R\)</span>与相机像面相交形成极线。极线是3D空间中点X的位置函数，随<span
class="math inline">\(X\)</span>变化，两幅图像会生成一组极线。直线<span
class="math inline">\(O_LX\)</span>通过透镜中心<span
class="math inline">\(O_L\)</span>，右像面中对应的极线必然通过极点<span
class="math inline">\(e_R\)</span>。一幅图像中的所有极线包含了该图像的所有极点。实际上，任意一条包含极点的线都是由空间中某一点<span
class="math inline">\(X\)</span>推导出的一条极线。</p></li>
</ul>
<p>如果两个相机位置已知，则： 1.如果投影点<span
class="math inline">\(X_L\)</span>已知，则极线<span
class="math inline">\(e_RX_R\)</span>已知，点X必定投影到右像面极线上的<span
class="math inline">\(X_R\)</span>处。这就意味着，在一个图像中观察到的每个点，在已知的极线上观察到该点的其他图像。这就是Epipolar
constraint极线约束：<strong><span
class="math inline">\(X\)</span>在右像面上的投影<span
class="math inline">\(X_R\)</span>必然被约束在<span
class="math inline">\(e_RX_R\)</span>极线上</strong>。对于<span
class="math inline">\(O_LX_L\)</span>上的<span
class="math inline">\(X\)</span>，<span
class="math inline">\(X_1\)</span>，<span
class="math inline">\(X_2\)</span>，<span
class="math inline">\(X_3\)</span>都受该约束。极线约束可以用于测试两点是否对应同一3D点。极线约束也可以用两相机间的基本矩阵来描述。
2.如果<span class="math inline">\(X_L\)</span>和<span
class="math inline">\(X_R\)</span>已知，他们的投影线已知。如果两幅图像上的点对应同一点X，则投影线必然交于<span
class="math inline">\(X\)</span>。这就意味着<span
class="math inline">\(X\)</span>可以用两个像点的坐标计算得到。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p10.png" width=1200px>
由极线约束可知，我们可以将原来的匹配点搜索范围由2D转换成1D，这样做可以很大程度上减少计算量。我们将左右视图摆放成更容易理解的形式，可以发现对应点的匹配问题转换成了在同一条扫描线上（极线）的匹配问题。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p11.png" width=1200px>
可以发现相机的摆放姿势影响着扫描线的方向。在上图A中，相机与水平呈一定角度地摆放，其扫描线为右图所示，同样是与水平倾斜的扫描线。假如两个相机平行摆放的话，其拍出来匹配对是扫描线已经对齐了的。</p>
<h3 id="深度与视差">深度与视差</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p12.png" width=1200px>
如上图所示为扫描线已经对齐了的匹配图像对（以下简称<strong>匹配对</strong>）。可以发现：<span
class="math inline">\(PO_RO_T\)</span>与<span
class="math inline">\(Ppp&#39;\)</span>是相似三角形，由于相似三角形原理，我们可以很容易知道：
<span
class="math display">\[\frac{b}{Z}=\frac{(b+x_T)-x_R}{Z-f}\]</span></p>
<p>其中，<span
class="math inline">\(x_R-x_T\)</span>就是视差，Z表示深度，B为基线，f是焦距。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p13.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p14-1.png" width=1200px></p>
<p>所谓<strong>视差就是匹配对中对应点之间x方向上的差异</strong>，我们可以将这种差异转换成为灰度图（越近越白），如上最后一个图所示。
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p14-2.png" width=1200px></p>
<p>上图展示了物体距离相机越近的话，视差就越大。其实很容易理解，将人的双眼比作成双目相机，对比将手指放在双眼前方近处与远处晃动的区别，可以发现在近处的话人眼感知到手指的晃动是比远处晃动的“程度”明显的，那么这种程度就是视差在人脑中的反映。</p>
<h3 id="视界">视界</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p15.png" width=1200px></p>
<p>图为双摄装置，基线为b，焦距为f，那么双摄的视界被视差范围所限定{<span
class="math inline">\(d_{min},d_{max}\)</span>}，如图中绿色包裹的区域。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p16.png" width=1200px></p>
<ul>
<li>深度是通过利用立体匹配系统将视差离散成一系列平行的平面来测量的；每一层平面对应着一个视差。</li>
<li>可以通过超像素的方法得到效果更好的深度图。</li>
</ul>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p17.png" width=1200px>
图为5个视差{<span
class="math inline">\(d_{min},d_{min}+4\)</span>}组成的视场。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p18.png" width=1200px>
- 图为5个视差{<span
class="math inline">\(\Delta+d_{\min},\Delta+d_{\min}+4\)</span>}组成的视场
- <span
class="math inline">\(\Delta&gt;0\)</span>时，视场收缩并向相机靠近</p>
<h2 id="深度估计">深度估计</h2>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p21.png" width=1200px>
图中为传统算法以及ICCV2011当时最好的结果。可以发现，能够达到较好的视差是具有挑战性的。下面将要展示视差估计的基本流程。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p22.png" width=1200px>
通过双摄设备采集图像，此时图像是存在镜头畸变的，在进行扫描线对齐之前要进行离线标定以消除镜头畸变。扫描线对齐的过程叫做<strong>镜头矫正</strong>（rectificaition），经过这步之后就可以进行1D的匹配点搜索（stereo
correspondence）了。随后通过三角形相似原理得到相应的深度/视差图。</p>
<h3 id="离线标定">离线标定</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p23.png" width=1200px>
标定的目标是寻找： - 相机内参：焦距、图像中心、镜头畸变参数 -
相机外参：排列相机使其对齐的参数</p>
<p>注意的是，相机标定的话一般需要10对以上的图像（通常拍摄棋盘格图像，利用张氏标定法进行标定）。
-
标定程序可以见Opencv<sup id="fnref:39"><a href="#fn:39" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="OpenCV Computer Vision Library, http://sourceforge.net/projects/opencvlibrary/
">39</span></a></sup>和Matlab<sup id="fnref:40"><a href="#fn:40" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Jean-Yves Bouguet , Camera Calibration Toolbox for Matlab, http://www.vision.caltech.edu/bouguetj/calib_doc/
">40</span></a></sup>。 -
更为详细的介绍参见<sup id="fnref:20"><a href="#fn:20" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="E. Trucco, A. Verri, Introductory Techniques for 3-D Computer Vision, Prentice Hall, 1998
">20</span></a></sup>
<sup id="fnref:21"><a href="#fn:21" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="R.I.Hartley, A. Zisserman, Multiple View Geometry in Computer Vision, Cambridge University Press, 2000
">21</span></a></sup>
<sup id="fnref:22"><a href="#fn:22" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="G. Bradsky, A. Kaehler, Learning Opencv, O’Reilly, 2008
">22</span></a></sup>。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p25.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p26.png" width=1200px>
### 匹配矫正</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p27.png" width=1200px>
利用标定步骤得到的相机的内参对相机镜头畸变进行校正，同时对其扫描线。</p>
<h3 id="立体匹配">立体匹配</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p28.png" width=1200px>
目标：从匹配对中寻找对应的点，反映在图像中就是视差图像。</p>
<h3 id="三角测量">三角测量</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p29.png" width=1200px>
给定视差图像，基线长度以及焦距可以通过三角计算得到当前位置对应的3D位置。</p>
<h2 id="立体匹配的挑战性">立体匹配的挑战性</h2>
<h3 id="光度失真以及噪声">光度失真以及噪声</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p37-1.png" width=1200px>
### 高光表面</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p37-2.png" width=1200px></p>
<h3 id="透视收缩">透视收缩</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p38.png" width=1200px></p>
<h3 id="透视变形">透视变形</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p39-1.png" width=1200px></p>
<h3 id="无纹理区域">无纹理区域</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p39-2.png" width=1200px></p>
<h3 id="重复混淆区域">重复/混淆区域</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p40.png" width=1200px></p>
<h3 id="透明物体">透明物体</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p41-1.png" width=1200px></p>
<h3 id="遮挡区以及不连续区域1">遮挡区以及不连续区域（1）</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p41-2.png" width=1200px></p>
<h3 id="遮挡区以及不连续区域2">遮挡区以及不连续区域（2）</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p42.png" width=1200px></p>
<h2 id="middlebury数据集">Middlebury数据集</h2>
<p><a
target="_blank" rel="noopener" href="http://vision.middlebury.edu/stereo/eval3/">Middlebury数据集</a>提供了一套可供深度估计的数据集以及评价系统，深度估计算法可在该数据集上进行测试性能。2003年的数据集提供了Tsukuba,
Venus, Teddy and Cones这几个场景的匹配对。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p44.png" width=1200px></p>
<h2 id="匹配问题">匹配问题</h2>
<p>立体匹配的算法可以分成以下几个步骤：</p>
<ol type="1">
<li>匹配量/损失计算</li>
<li>损失聚合</li>
<li>视差计算/优化</li>
<li>视差精化</li>
</ol>
<ul>
<li>局部算法包括： 1-&gt;2-&gt;3（简单的WTA算法）</li>
<li>全局算法包括： 1（-&gt;2）-&gt;3（全局或者半全局算法）</li>
</ul>
<h3 id="数据预处理">数据预处理</h3>
<p>数据预处理是为了消除图像的光度失真。常见的操作有： -
LoG滤波器<sup id="fnref:41"><a href="#fn:41" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="T. Kanade, H. Kato, S. Kimura, A. Yoshida, and K. Oda, Development of a Video-Rate Stereo Machine International Robotics and Systems Conference (IROS &#39;95), Human Robot Interaction and Cooperative Robots, 1995
">41</span></a></sup> -
消减附近像素中计算的平均值<sup id="fnref:42"><a href="#fn:42" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="O. Faugeras, B. Hotz, H. Mathieu, T. Viville, Z. Zhang, P. Fua, E. Thron, L. Moll, G. Berry, Real-time correlation-based stereo: Algorithm. Implementation and Applications, INRIA TR n. 2013, 1993
">42</span></a></sup> - 双边滤波 - 统计变换</p>
<p>最简单的立体匹配算法如下图所示，逐像素地计算SAD匹配损失；然后通过WTA得到初始视差，但是此时得到的视差质量是很差的。那么如何提高深度图像的质量呢？通常来说有两种不同类别的策略。
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p47.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p48.png" width=1200px></p>
<ul>
<li>局部算法。同样是利用到了简单的WTA提取到初始视差，但是通过计算窗口内的损失量提高了信噪比。有时需会加入平滑项。Steps
1+2 (+ WTA)
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p50.png" width=1200px></li>
<li>全局/半全局算法。寻找能够使得能量函数的最小值的视差以得到逐点视差。Steps
1+ Step3。（有时，损失函数需要聚合）</li>
</ul>
<p>两种算法都假设了匹配对是平滑的，但有时，该假设并不成立。这个假设在局部算法中隐晦地提及，却在全局算法中明确地建模，如下形式。
<span class="math display">\[E(d)=E_{data}(d)+E_{smooth}(d)\]</span></p>
<h2 id="损失量的计算">损失量的计算</h2>
<h3 id="逐像素的匹配误差">逐像素的匹配误差</h3>
<ul>
<li><p>绝对值误差 <span
class="math display">\[e(x,y,d)=|I_R(x,y)-I_T(x+d,y)|\]</span></p></li>
<li><p>平方误差 <span
class="math display">\[e(x,y,d)=(I_R(x,y)-I_T(x+d,y))^2\]</span></p></li>
<li><p>鲁棒匹配子（M-estimators） 如截断绝对误差（truncated absolute
differences (TAD)）可以减少离群点的干扰： <span
class="math display">\[e(x,y,d)=min\{|I_R(x,y)-I_T(x+d,y),T\}\]</span></p></li>
<li><p>相异性测量对于图像噪声不敏感（Birchfield and
Tomasi<sup id="fnref:27"><a href="#fn:27" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="S. Birchfield and C. Tomasi. A pixel dissimilarity measure that is insensitive to image sampling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(4):401-406, April 1998
">27</span></a></sup>）</p></li>
</ul>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p52.png" width=1200px>
视差空间图像（DSI）是一个如下图所示张量<span
class="math inline">\(W\times
H\times(d_{max}-d_{min})\)</span>，其中的每一个元素<span
class="math inline">\(C(x,y,d)\)</span>表示<span
class="math inline">\(I_R(x_R,y)\)</span>与<span
class="math inline">\(I_T(x_R+d,y)\)</span>之间的匹配度。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p53.png" width=1200px></p>
<h3 id="区域匹配损失">区域匹配损失</h3>
<ul>
<li>绝对误差和（Sum of Absolute differences (SAD)） <span
class="math display">\[C(x,y,d)=\sum_{x\in
S}|I_R(x,y)-I_T(x+d,y)|\]</span></li>
<li>绝对平方和（Sum of Squared differences (SSD)） <span
class="math display">\[C(x,y,d)=\sum_{x\in
S}\left(I_R(x,y)-I_T(x+d,y)\right)^2\]</span></li>
<li>截断绝对误差和（Sum of truncated absolute differences (STAD)） <span
class="math display">\[C(x,y,d)=\sum_{x\in
S}\{|I_R(x,y)-I_T(x+d,y),T\}\]</span></li>
<li>Normalized Cross Correlation
<sup id="fnref:57"><a href="#fn:57" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="S. Mattoccia, F. Tombari, L. Di Stefano, Fast full-search equivalent template matching by Enhanced Bounded Correlation, IEEE Transactions on Image Processing, 17(4), pp 528-538, April 2008
">57</span></a></sup></li>
<li>Zero mean Normalized Cross Correlation
<sup id="fnref:58"><a href="#fn:58" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="L. Di Stefano, S. Mattoccia, F. Tombari, ZNCC-based template matching using Bounded Partial Correlation Pattern Recognition Letters, 16(14), pp 2129-2134, October 2005
">58</span></a></sup></li>
<li>Gradient based MF
<sup id="fnref:59"><a href="#fn:59" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="F. Tombari, L. Di Stefano, S. Mattoccia, A. Galanti, Performance evaluation of robust matching measures 3rd International Conference on Computer Vision Theory and Applications (VISAPP 2008)
">59</span></a></sup></li>
<li>Non parametric
<sup id="fnref:60"><a href="#fn:60" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="R. Zabih, J John Woodll Non-parametric Local Transforms for Computing Visual Correspondence, ECCV 1994
">60</span></a></sup>
<sup id="fnref:61"><a href="#fn:61" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="D. N. Bhat, S. K. Nayar, Ordinal measures for visual correspondence, CVPR 1996
">61</span></a></sup></li>
<li>Mutual Information
<sup id="fnref:30"><a href="#fn:30" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="H. Hirschmüller. Stereo vision in structured environments by consistent semi-global matching. CVPR 2006, PAMI 30(2):328-341, 2008
">30</span></a></sup></li>
<li>Combination of matching costs</li>
</ul>
<h2 id="损失聚合">损失聚合</h2>
<p>那么从最简单的固定窗口（FW）损失聚合开始，以下为利用FW聚合的TAD损失然后利用WTA得到的深度图。理想是完美的，但现实是骨感的，可以看到下图给出的结果并不佳，这是什么原因呢？
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p57.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p58.png" width=1200px>
a. 隐含地假设前额表面处于同一视差 b. 忽略了深度的非连续性 c.
平坦区域的处理不佳 d. 重复的区域</p>
<p>对于a.
隐含地假设前额表面处于同一视差，很多即使是当前最好的损失聚合算法也会假设：在一个小的支持域里面的所有点所处的视差是相同的。但实际情况并非如此，可以观察以上两图，人体头像模型的面部是不规则的表面，展现出来的是视差的不断变化；下面的图是桌子平面，它表面是倾斜的，同样表现出来的是视差的变化。</p>
<p><img data-src="https://vincentqin.tech/blog-resources/stereo-vision-overview/p59.png" width=1200px></p>
<p>对于b.
忽略了深度的非连续性，原本假设真实场景中的正面平行表面在支持域内深度不会变化，但是这个假设在深度不连续处的附近被打破。可以看到下图中在台灯灯罩的边界处出现了深度的间断，这样经过损失聚合之后得到的深度就会出现边界误匹配的现象，表现在图中为边界没有很好的对齐。不过利用TAD可以在一定程度上减少这种现象。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p60-1.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p60-2.png" width=1200px></p>
<p>目前最好的损失聚合算法都在想方设法地改变支持域的形状以适应在仅在相同的已知视差上做匹配。对于FW而言，就是减小其窗口大小，来减少边界定位问题。但是与此同时，这个改变使得匹配问题变得含糊不清，特别是对于有重复区域以及平滑区域的情形。
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p61.png" width=1200px></p>
<p>对于c与d，FW并不能很好地处理。在这两种情况下，损失聚合算法应该不断地加大支持域的尺寸以获得更多的相同深度上的点。
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p62.png" width=1200px></p>
<p>以上为FW所面对的诸多问题。令人吃惊的是，虽然FW看起来如此不堪一击，但是其应用却是如此广泛。原因可能有以下几点：
1. 容易实现； 2. 快！(特别感谢增量计算框架)； 3.
可以在传统的处理器上实时完成计算； 4. 仅需要很小的内存； 5.
可硬件（FPGA）实时实现，且功率小（&lt;1W）</p>
<p>在介绍更加复杂的算法之前，我们首先介绍积分图像（Integral Images
(II)）以及箱滤波（Box-Filtering (BF)）。</p>
<h3 id="积分图像">积分图像</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p64.png" width=1200px></p>
<h3 id="箱滤波器">箱滤波器</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p65.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p66.png" width=1200px></p>
<p>可以总结出积分图与箱滤波器的关系： 1. 每个点需要4个运算 2.
积分图可以支持不同的支持域尺寸 3. 积分图有溢出风险 4.
积分图对内存消耗较大</p>
<p>在实际应用当中，积分图对于可变支持域的情况会有帮助。</p>
<h2
id="立体匹配中损失聚合策略的分类及评估">立体匹配中损失聚合策略的分类及评估</h2>
<p>在文献<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="F. Tombari, S. Mattoccia, L. Di Stefano, E. Addimanda, Classification and evaluation of cost aggregation methods for stereo correspondence, IEEE International Conference on Computer Vision and Pattern Recognition (CVPR 2008)">1</span></a></sup>中，作者实现、分类以及评估了超过10种损失聚合算法。这些损失聚合的策略包含几种方式：
- 位置 - 方向 - 位置与方向 - 权重</p>
<p>接下来就对文中但不限于文中提到的诸多算法进行介绍 (i.e. Fast
Aggregation <sup id="fnref:64"><a href="#fn:64" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="F. Tombari, S. Mattoccia, L. Di Stefano, E. Addimanda, Near real-time stereo based on effective cost aggregation International Conference on Pattern Recognition (ICPR 2008)
">64</span></a></sup>, Fast Bilateral Stereo (FBS)
<sup id="fnref:65"><a href="#fn:65" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="S. Mattoccia, S. Giardino,A. Gambini, Accurate and efficient cost aggregation strategy for stereo correspondence based on approximated joint bilateral filtering, Asian Conference on Computer Vision (ACCV 2009), September 23-27 2009, Xiang, China
">65</span></a></sup> and the Locally Consistent (LC) methodology
<sup id="fnref:66"><a href="#fn:66" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="S. Mattoccia, A locally global approach to stereo correspondence, 3D Digital Imaging and Modeling (3DIM 2009), pp 1763-1770, October 3-4, 2009, Kyoto, Japan
">66</span></a></sup>)。</p>
<h3 id="固定窗口">固定窗口</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p87.png" width=1200px></p>
<h3
id="可移动窗口11">可移动窗口<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="D. Scharstein and R. Szeliski, A taxonomy and evaluation of dense two-frame stereo correspondence algorithms Int. Jour. Computer Vision, 47(1/2/3):7–42, 2002.
">11</span></a></sup></h3>
<p>这种方法是为了应对场景边界定位问题，这种算法不限制当前位置位于支持域中心。
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p88.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p89.png" width=1200px></p>
<h3
id="多窗口7">多窗口<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="H. Hirschmuller, P. Innocent, and J. Garibaldi, Real-time correlation-based stereo vision with reduced border errors Int. Journ. of Computer Vision, 47:1–3, 2002
">7</span></a></sup></h3>
<p>支持域内元素个数为常数；支持域的形状不限于为矩形；支持域大小可为5、9、25（5W,9W,25W）。下图所示的为9W：
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p90.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p91.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p92.png" width=1200px></p>
<h3
id="可变窗口12">可变窗口<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="O. Veksler. Fast variable window for stereo correspondence using integral images, In Proc. Conf. on Computer Vision and Pattern Recognition (CVPR 2003), pages 556–561, 2003
">12</span></a></sup></h3>
<p>这种方式，支持域的形状是固定的但是尺寸是变化的。支持域的位置是可变的。
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p94.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p95.png" width=1200px></p>
<h3
id="基于分割的窗口5">基于分割的窗口<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="M. Gerrits and P. Bekaert. Local Stereo Matching with Segmentation-based Outlier Rejection In Proc. Canadian Conf. on Computer and Robot Vision (CRV 2006), pages 66-66, 2006.
">5</span></a></sup></h3>
<p>这种方式根据图像的颜色相似性将其分割成一系列图像块，这对于损失聚合、深度图像优化以及离群点检测都有帮助。这种算法假设：每个分割块内深度平滑变化。由于涉及到图像分割此时要求分割的精度很高，并且分割后的每个支持域的形状也是不规则的。如下图所示，对于一个可允许的最大支持域范围内，包含支持域中心点所在的分割所覆盖支持域权重赋值为1，支持域的其余部分赋值为<span
class="math inline">\(\lambda\)</span>，其中<span
class="math inline">\(\lambda&lt;&lt;1\)</span>。
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p98-1.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p98-2.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p99.png" width=1200px></p>
<h3
id="自适应加权14-51">自适应加权<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="K. Yoon and I. Kweon, Adaptive support-weight approach for correspondence search, IEEE Trans. PAMI, 28(4):650–656,2006
">14</span></a></sup>
<sup id="fnref:51"><a href="#fn:51" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="C. Tomasi and R. Manduchi. Bilateral filtering for gray and color images. In ICCV98, pages 839–846, 1998
">51</span></a></sup></h3>
<p>首先介绍双边滤波，双边滤波是一种边缘保持滤波器，它是根据图像的邻域的颜色以及空间相关性对每个中心点进行加权。类似于双边滤波，自适应加权对其进行了简化，只考虑颜色的相关性。每一个损失都被乘以了一个这样的权重可以得到<span
class="math inline">\(C(p_c,q_c)\)</span>。
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p101.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p102.png" width=1200px>
以下是自适应加权的结果：
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p103.png" width=1200px></p>
<h3
id="可分支持域10">可分支持域<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="F. Tombari, S. Mattoccia, and L. Di Stefano, Segmentation-based adaptive support for accurate stereo correspondence PSIVT 2007
">10</span></a></sup></h3>
<h3
id="快速聚合64">快速聚合<sup id="fnref:64"><a href="#fn:64" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="F. Tombari, S. Mattoccia, L. Di Stefano, E. Addimanda, Near real-time stereo based on effective cost aggregation International Conference on Pattern Recognition (ICPR 2008)
">64</span></a></sup></h3>
<ul>
<li>假设：在每个分割块内的深度变化平缓；</li>
<li>损失量：TAD；</li>
<li>只对参考图像进行聚合；</li>
<li>对称的支持域</li>
<li>支持域覆盖整个分割块
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p109.png" width=1200px>
<span
class="math display">\[C_{agg}(p,q,d)=\frac{C_S(p,q,d)}{|S_p|}+\frac{C_W(p,q,d)}{|r^2|}\]</span>
<span class="math display">\[C_S(p,q,d)=\sum_{p_i \in
S_p}{TAD(p_i,q_{i+d})}\]</span> <span
class="math display">\[C_W(p,q,d)=\sum_{p_i \in
W_p}{TAD(p_i,q_{i+d})}\]</span> 其中<span
class="math inline">\(C_W\)</span>为了消除“分割锁定”，这一项可能在纹理稠密的区域很有帮助，但是这一项有可能带来深度的不连续性。
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p111.png" width=1200px></li>
</ul>
<h3 id="快速双边滤波65"><a
target="_blank" rel="noopener" href="http://vision.deis.unibo.it/~smatt/fast_bilateral_stereo.htm">快速双边滤波</a><sup id="fnref:65"><a href="#fn:65" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="S. Mattoccia, S. Giardino,A. Gambini, Accurate and efficient cost aggregation strategy for stereo correspondence based on approximated joint bilateral filtering, Asian Conference on Computer Vision (ACCV 2009), September 23-27 2009, Xiang, China
">65</span></a></sup></h3>
<p>该方法兼顾了自适应加权的精度以及传统方法的效率。通过逐块的利用双边滤波对损失进行规范化，通过这种方式可以增加对噪声的鲁棒性。可以利用前面提及的积分图或者箱滤波的方式快速计算。由于双边滤波的局部计算特性，可以利用GPU进行加速，<a
target="_blank" rel="noopener" href="http://vision.deis.unibo.it/~smatt/FBS_GPU.html">GPU加速版本</a>。结果如下所示：
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p115.png" width=1200px></p>
<h3 id="局部一致性"><a
target="_blank" rel="noopener" href="http://vision.deis.unibo.it/~smatt/lc_stereo.htm">局部一致性</a></h3>
<p>通过对像素之间的一致性约束进行建模寻找像素之间的相关关系，这种方法对于目前最好的方法具有显著的效果提升。
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p122.png" width=1200px></p>
<h3
id="o1-adaptive-cost-aggregationspan-classhint--top-hint--error-hint--medium-hint--rounded-hint--bounce-aria-labell.-de-maeztu-s.-mattoccia-a.-villanueva-r.-cabeza-linear-stereo-matching-international-conference-on-computer-vision-iccv-2011-november-6-13-2011-barcelona-spain75">O(1)
adaptive cost
aggregation<sup id="fnref:75"><a href="#fn:75" rel="footnote">&lt;span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="L. De-Maeztu, S. Mattoccia, A. Villanueva, R. Cabeza,
"Linear stereo matching", International Conference on Computer Vision
(ICCV 2011), November 6-13, 2011, Barcelona,
Spain"&gt;75</span></a></sup></h3>
<p>该方法受引导滤波的启发，效果还不错，可与最佳效果相媲美。
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p127.png" width=1200px></p>
<h2 id="视差深度计算以及优化">视差/深度计算以及优化</h2>
<p>该步骤是为了寻找可最小化损失函数的视差（或者得到DSI图像的最佳路径以最小化能量函数）。通常情况下，能量函数可以表示为以下形式
<span class="math display">\[E(d)=E_{data}(d)+E_{smooth}(d)\]</span>
其中的数据项<span
class="math inline">\(E_{data}(d)\)</span>为了衡量目前假定的视差能够以何等程度接近真实视差。目前已经有不少逐像素的损失构造策略，但是目前也涌现了许多基于支持域的数据项。
另外一项是平滑项或者叫做正则项<span
class="math inline">\(E_{smooth}(d)\)</span>，它可以对像素之间的连续性或者相似性进行约束：这一项对大的视差给予大的惩罚，同时对于边界处的大视差变化以及小的惩罚。也就是说，视差的变化在边界处是被允许的。</p>
<p>以上模型的求解是个NP-hard问题，在这里可以借助几种常用的策略对其进行求解。
- Graph Cuts <sup id="fnref:52"><a href="#fn:52" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="V. Kolmogorov and R. Zabih, Computing visual correspondence with occlusions using graph cuts, ICCV 2001
">52</span></a></sup> - Belief Propagation
<sup id="fnref:53"><a href="#fn:53" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="A. Klaus, M. Sormann and K. Karner, Segment-based stereo matching using belief propagation and a self-adapting dissimilarity measure, ICPR 2006
">53</span></a></sup> - Cooperative optimization
<sup id="fnref:54"><a href="#fn:54" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Z. Wang and Z. Zheng, A region based stereo matching algorithm using cooperative optimization, CVPR 2008
">54</span></a></sup></p>
<p>这些方法的比较在[63]中进行了详述。有意思的是，上述问题的解决可以由动态规划以及扫描线优化来解决。</p>
<h3 id="动态规划">动态规划</h3>
<ul>
<li>高效 (polynomial time) ≈ 1 sec</li>
<li>边界以及纹理稀疏区域有所帮助</li>
<li>条带现象
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p135.png" width=1200px></li>
</ul>
<h3 id="扫描线优化scanline-optimizationso30">扫描线优化（Scanline
Optimization，SO<sup id="fnref:30"><a href="#fn:30" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="H. Hirschmüller. Stereo vision in structured environments by consistent semi-global matching. CVPR 2006, PAMI 30(2):328-341, 2008
">30</span></a></sup>）</h3>
<ul>
<li>高效 (polynomial time) ≈ few seconds</li>
<li>边界以及纹理稀疏区域有所帮助</li>
<li>条带现象</li>
<li>高内存消耗
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p139.png" width=1200px></li>
</ul>
<h2 id="视差精化">视差精化</h2>
<p>原始视差中包含的离群点需要被检测以及被移除；同时，视差是离散的数据点，有时需要更高的精度；以下将会介绍亚像素插值、图像滤波、双向验证。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p166.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p167.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p168.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p169.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p170.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p181.png" width=1200px>
<img data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p183.png" width=1200px></p>
<p><font color="F08080" size=5>未完待续...</font></p>
<p><strong>注：</strong>特此感谢Stefano
Mattoccia给出如此良心的立体视觉综述，<a
target="_blank" rel="noopener" href="http://www.vision.deis.unibo.it/smatt/Seminars/StereoVision.pdf">本文最新版本在此</a>(速度较慢)或者<a
target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/StereoVision.pdf">这里</a>(较快，大小51.61M)。</p>
<h2 id="参考文献">参考文献</h2>
<div id="footnotes">
<hr>
<div id="footnotelist">
<ol style="list-style: none; padding-left: 0; margin-left: 40px">
<li id="fn:1">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">F.
Tombari, S. Mattoccia, L. Di Stefano, E. Addimanda, Classification and
evaluation of cost aggregation methods for stereo correspondence, IEEE
International Conference on Computer Vision and Pattern Recognition
(CVPR 2008)<a href="#fnref:1" rev="footnote">↩︎</a></span>
</li>
<li id="fn:2">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Y.
Boykov, O. Veksler, and R. Zabih, A variable window approach to early
vision IEEE Trans. PAMI, 20(12):1283–1294,
1998<a href="#fnref:2" rev="footnote">↩︎</a></span>
</li>
<li id="fn:3">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">S.
Chan, Y. Wong, and J. Daniel, Dense stereo correspondence based on
recursive adaptive size multi-windowing In Proc. Image and Vision
Computing New Zealand (IVCNZ’03), volume 1, pages 256–260,
2003<a href="#fnref:3" rev="footnote">↩︎</a></span>
</li>
<li id="fn:4">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">C.
Demoulin and M. Van Droogenbroeck, A method based on multiple adaptive
windows to improve the determination of disparity maps. In Proc. IEEE
Workshop on Circuit, Systems and Signal Processing, pages 615–618,
2005<a href="#fnref:4" rev="footnote">↩︎</a></span>
</li>
<li id="fn:5">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">M.
Gerrits and P. Bekaert. Local Stereo Matching with Segmentation-based
Outlier Rejection In Proc. Canadian Conf. on Computer and Robot Vision
(CRV 2006), pages 66-66,
2006.<a href="#fnref:5" rev="footnote">↩︎</a></span>
</li>
<li id="fn:6">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">M.
Gong and R. Yang. Image-gradient-guided real-time stereo on graphics
hardware In Proc. Int. Conf. 3D Digital Imaging and Modeling (3DIM),
pages 548–555, 2005<a href="#fnref:6" rev="footnote">↩︎</a></span>
</li>
<li id="fn:7">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">H.
Hirschmuller, P. Innocent, and J. Garibaldi, Real-time correlation-based
stereo vision with reduced border errors Int. Journ. of Computer Vision,
47:1–3, 2002<a href="#fnref:7" rev="footnote">↩︎</a></span>
</li>
<li id="fn:8">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">S.
Kang, R. Szeliski, and J. Chai, Handling occlusions in dense multi-view
stereo In Proc. Conf. on Computer Vision and Pattern Recognition (CVPR
2001), pages 103–110, 2001<a href="#fnref:8" rev="footnote">↩︎</a></span>
</li>
<li id="fn:9">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">J.
Kim, K. Lee, B. Choi, and S. Lee. A dense stereo matching using two-pass
dynamic programming with generalized ground control points, In Proc.
Conf. on Computer Vision and Pattern Recognition (CVPR 2005), pages
1075–1082, 2005<a href="#fnref:9" rev="footnote">↩︎</a></span>
</li>
<li id="fn:10">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">F.
Tombari, S. Mattoccia, and L. Di Stefano, Segmentation-based adaptive
support for accurate stereo correspondence PSIVT
2007<a href="#fnref:10" rev="footnote">↩︎</a></span>
</li>
<li id="fn:11">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">D.
Scharstein and R. Szeliski, A taxonomy and evaluation of dense two-frame
stereo correspondence algorithms Int. Jour. Computer Vision,
47(1/2/3):7–42, 2002.<a href="#fnref:11" rev="footnote">↩︎</a></span>
</li>
<li id="fn:12">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">O.
Veksler. Fast variable window for stereo correspondence using integral
images, In Proc. Conf. on Computer Vision and Pattern Recognition (CVPR
2003), pages 556–561,
2003<a href="#fnref:12" rev="footnote">↩︎</a></span>
</li>
<li id="fn:13">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Y.
Xu, D. Wang, T. Feng, and H. Shum, Stereo computation using radial
adaptive windows, In Proc. Int. Conf. on Pattern Recognition (ICPR
2002), volume 3, pages 595–598,
2002<a href="#fnref:13" rev="footnote">↩︎</a></span>
</li>
<li id="fn:14">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">K.
Yoon and I. Kweon, Adaptive support-weight approach for correspondence
search, IEEE Trans. PAMI,
28(4):650–656,2006<a href="#fnref:14" rev="footnote">↩︎</a></span>
</li>
<li id="fn:15">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">D.
Scharstein and R. Szeliski,
http://vision.middlebury.edu/stereo/eval/<a href="#fnref:15" rev="footnote">↩︎</a></span>
</li>
<li id="fn:16">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">A.
Ansar, A. Castano, L. Matthies, Enhanced real-time stereo using
bilateral filtering IEEE Conference on Computer Vision and Pattern
Recognition 2004<a href="#fnref:16" rev="footnote">↩︎</a></span>
</li>
<li id="fn:17">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">D.
Scharstein and R. Szeliski, 􀀃High-accuracy stereo depth maps using
structured light. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR 2003), volume 1, pages
195-202<a href="#fnref:17" rev="footnote">↩︎</a></span>
</li>
<li id="fn:18">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">D.
Scharstein and C. Pal. Learning conditional random fields for stereo.In
IEEE Conference on Computer Vision and Pattern Recognition (CVPR
2007)<a href="#fnref:18" rev="footnote">↩︎</a></span>
</li>
<li id="fn:19">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">H.
Hirschmüller and D. Scharstein. Evaluation of cost functions for stereo
matching.In IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR
2007)<a href="#fnref:19" rev="footnote">↩︎</a></span>
</li>
<li id="fn:20">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">20.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">E.
Trucco, A. Verri, Introductory Techniques for 3-D Computer Vision,
Prentice Hall, 1998<a href="#fnref:20" rev="footnote">↩︎</a></span>
</li>
<li id="fn:21">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">21.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">R.I.Hartley,
A. Zisserman, Multiple View Geometry in Computer Vision, Cambridge
University Press, 2000<a href="#fnref:21" rev="footnote">↩︎</a></span>
</li>
<li id="fn:22">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">22.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">G.
Bradsky, A. Kaehler, Learning Opencv, O’Reilly,
2008<a href="#fnref:22" rev="footnote">↩︎</a></span>
</li>
<li id="fn:23">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">23.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">OpenCV
Computer Vision Library,
http://sourceforge.net/projects/opencvlibrary/<a href="#fnref:23" rev="footnote">↩︎</a></span>
</li>
<li id="fn:24">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">24.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Jean-Yves
Bouguet , Camera Calibration Toolbox for Matlab,
http://www.vision.caltech.edu/bouguetj/calib_doc/<a href="#fnref:24" rev="footnote">↩︎</a></span>
</li>
<li id="fn:25">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">25.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">M.
A. Fischler and R. C. Bolles, Random Sample Consensus: A Paradigm for
Model Fitting with Applications to Image Analysis and Automated
Cartography, Comm. of the ACM 24: 381–395, June
1981<a href="#fnref:25" rev="footnote">↩︎</a></span>
</li>
<li id="fn:26">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">26.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Z.
Wang and Z. Zheng, A region based stereo matching algorithm using
cooperative optimization IEEE CVPR
2008<a href="#fnref:26" rev="footnote">↩︎</a></span>
</li>
<li id="fn:27">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">27.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">S.
Birchfield and C. Tomasi. A pixel dissimilarity measure that is
insensitive to image sampling. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20(4):401-406, April
1998<a href="#fnref:27" rev="footnote">↩︎</a></span>
</li>
<li id="fn:28">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">28.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">J.
Zabih, J. Woodfill, Non-parametric local transforms for computing visual
correspondence. European Conf. on Computer Vision, Stockholm, Sweden,
151–158<a href="#fnref:28" rev="footnote">↩︎</a></span>
</li>
<li id="fn:29">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">29.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">S.
Mattoccia, F. Tombari, and L. Di Stefano, Stereo vision enabling precise
border localization within a scanline optimization framework, ACCV
2007<a href="#fnref:29" rev="footnote">↩︎</a></span>
</li>
<li id="fn:30">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">30.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">H.
Hirschmüller. Stereo vision in structured environments by consistent
semi-global matching. CVPR 2006, PAMI 30(2):328-341,
2008<a href="#fnref:30" rev="footnote">↩︎</a></span>
</li>
<li id="fn:31">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">31.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">F.
Tombari, S. Mattoccia, L. Di Stefano, F. Tonelli, Detecting motion by
means of 2D and 3D information ACCV'07 Workshop on Multi-dimensional and
Multi-view Image Processing (ACCV 2007
WS)<a href="#fnref:31" rev="footnote">↩︎</a></span>
</li>
<li id="fn:32">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">32.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">P.
Azzari, L. Di Stefano, F. Tombari, S. Mattoccia, Markerless augmented
reality using image mosaics International Conference on Image and Signal
Processing (ICISP 2008)<a href="#fnref:32" rev="footnote">↩︎</a></span>
</li>
<li id="fn:33">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">33.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Li
Zhang, Brian Curless, and Steven M. Seitz Spacetime Stereo: Shape
Recovery for Dynamic Scenes IEEE Computer Society Conference on Computer
Vision and Pattern Recognition (CVPR 2003), pp.
367-374<a href="#fnref:33" rev="footnote">↩︎</a></span>
</li>
<li id="fn:34">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">34.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">J.
Davis, D. Nehab, R. Ramamoothi, S. Rusinkiewicz. Spacetime Stereo : A
Unifying Framework for Depth from Triangulation, IEEE Trans. On Pattern
Analysis and Machine Intelligence (PAMI), vol. 27, no. 2, Feb
2005<a href="#fnref:34" rev="footnote">↩︎</a></span>
</li>
<li id="fn:35">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">35.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">F.
Tombari, L. Di Stefano, S. Mattoccia, A. Zanetti, Graffiti detection
using a Time-Of-Flight camera Advanced Concepts for Intelligent Vision
Systems (ACIVS 2008)<a href="#fnref:35" rev="footnote">↩︎</a></span>
</li>
<li id="fn:36">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">36.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">L.
Di Stefano, F. Tombari, A. Lanza, S. Mattoccia, S. Monti, Graffiti
detection using two views ECCV 2008 - 8th International Workshop on
Visual Surveillance (VS
2008)<a href="#fnref:36" rev="footnote">↩︎</a></span>
</li>
<li id="fn:37">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">37.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">T.
Darrell, D. Demirdijan, N. Checka, P. Felzenszwalb, Plan-view trajectory
estimation with dense stereo background models, International Conference
on Computer Vision (ICCV 2001),
2001<a href="#fnref:37" rev="footnote">↩︎</a></span>
</li>
<li id="fn:38">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">38.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">M.
Harville, Stereo person tracking with adaptive plan-view templates of
height and occupancy statistics Image and Vision Computing 22(2) pp
127-142, February 2004<a href="#fnref:38" rev="footnote">↩︎</a></span>
</li>
<li id="fn:39">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">39.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">OpenCV
Computer Vision Library,
http://sourceforge.net/projects/opencvlibrary/<a href="#fnref:39" rev="footnote">↩︎</a></span>
</li>
<li id="fn:40">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">40.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Jean-Yves
Bouguet , Camera Calibration Toolbox for Matlab,
http://www.vision.caltech.edu/bouguetj/calib_doc/<a href="#fnref:40" rev="footnote">↩︎</a></span>
</li>
<li id="fn:41">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">41.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">T.
Kanade, H. Kato, S. Kimura, A. Yoshida, and K. Oda, Development of a
Video-Rate Stereo Machine International Robotics and Systems Conference
(IROS '95), Human Robot Interaction and Cooperative Robots,
1995<a href="#fnref:41" rev="footnote">↩︎</a></span>
</li>
<li id="fn:42">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">42.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">O.
Faugeras, B. Hotz, H. Mathieu, T. Viville, Z. Zhang, P. Fua, E. Thron,
L. Moll, G. Berry, Real-time correlation-based stereo: Algorithm.
Implementation and Applications, INRIA TR n. 2013,
1993<a href="#fnref:42" rev="footnote">↩︎</a></span>
</li>
<li id="fn:43">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">43.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">F.
Crow, Summed-area tables for texture mapping, Computer Graphics,
18(3):207–212, 1984<a href="#fnref:43" rev="footnote">↩︎</a></span>
</li>
<li id="fn:44">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">44.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">M.
Mc Donnel. Box-filtering techniques, Computer Graphics and Image
Processing, 17:65–70,
1981<a href="#fnref:44" rev="footnote">↩︎</a></span>
</li>
<li id="fn:45">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">45.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">A.
Goshtasby, 2-D and 3-D Image Registration for Medical, Remote Sensing
and Industrial Applications New York: Wiley,
2005<a href="#fnref:45" rev="footnote">↩︎</a></span>
</li>
<li id="fn:46">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">46.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">B.
Zitova and J. Flusser, Image registration methods:A survey, Image Vision
Computing, vol. 21, no. 11, pp. 977–1000,
2003<a href="#fnref:46" rev="footnote">↩︎</a></span>
</li>
<li id="fn:47">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">47.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Changming
Sun, Recursive Algorithms for Diamond, Hexagon and General Polygonal
Shaped Window Operations Pattern Recognition Letters, 27(6):556-566,
April 2006<a href="#fnref:47" rev="footnote">↩︎</a></span>
</li>
<li id="fn:48">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">48.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">L.
Di Stefano, M. Marchionni, S. Mattoccia, A fast area-based stereo
matching algorithm, Image and Vision Computing, 22(12), pp 983-1005,
October 2004<a href="#fnref:48" rev="footnote">↩︎</a></span>
</li>
<li id="fn:49">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">49.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">L.
Di Stefano, M. Marchionni, S. Mattoccia, A PC-based real-time stereo
vision system, Machine Graphics &amp; Vision, 13(3), pp. 197-220,
January 2004<a href="#fnref:49" rev="footnote">↩︎</a></span>
</li>
<li id="fn:50">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">50.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">D.
Comaniciu and P. Meer, Mean shift: A robust approach toward feature
space analysis, IEEE Transactions on Pattern Analysis and Machine
Intelligence, 24:603–619,
2002<a href="#fnref:50" rev="footnote">↩︎</a></span>
</li>
<li id="fn:51">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">51.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">C.
Tomasi and R. Manduchi. Bilateral filtering for gray and color images.
In ICCV98, pages 839–846,
1998<a href="#fnref:51" rev="footnote">↩︎</a></span>
</li>
<li id="fn:52">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">52.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">V.
Kolmogorov and R. Zabih, Computing visual correspondence with occlusions
using graph cuts, ICCV
2001<a href="#fnref:52" rev="footnote">↩︎</a></span>
</li>
<li id="fn:53">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">53.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">A.
Klaus, M. Sormann and K. Karner, Segment-based stereo matching using
belief propagation and a self-adapting dissimilarity measure, ICPR
2006<a href="#fnref:53" rev="footnote">↩︎</a></span>
</li>
<li id="fn:54">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">54.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Z.
Wang and Z. Zheng, A region based stereo matching algorithm using
cooperative optimization, CVPR
2008<a href="#fnref:54" rev="footnote">↩︎</a></span>
</li>
<li id="fn:55">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">55.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">L.
Di Stefano, S. Mattoccia, Real-time stereo within the VIDET project
Real-Time Imaging, 8(5), pp. 439-453, Oct.
2002<a href="#fnref:55" rev="footnote">↩︎</a></span>
</li>
<li id="fn:56">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">56.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">F.
Tombari, S. Mattoccia, L. Di Stefano, Full search-equivalent pattern
matching with Incremental Dissimilarity Approximations, IEEE
Transactions on Pattern Analysis and Machine Intelligence, 31(1), pp
129-141, January 2009<a href="#fnref:56" rev="footnote">↩︎</a></span>
</li>
<li id="fn:57">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">57.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">S.
Mattoccia, F. Tombari, L. Di Stefano, Fast full-search equivalent
template matching by Enhanced Bounded Correlation, IEEE Transactions on
Image Processing, 17(4), pp 528-538, April
2008<a href="#fnref:57" rev="footnote">↩︎</a></span>
</li>
<li id="fn:58">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">58.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">L.
Di Stefano, S. Mattoccia, F. Tombari, ZNCC-based template matching using
Bounded Partial Correlation Pattern Recognition Letters, 16(14), pp
2129-2134, October 2005<a href="#fnref:58" rev="footnote">↩︎</a></span>
</li>
<li id="fn:59">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">59.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">F.
Tombari, L. Di Stefano, S. Mattoccia, A. Galanti, Performance evaluation
of robust matching measures 3rd International Conference on Computer
Vision Theory and Applications (VISAPP
2008)<a href="#fnref:59" rev="footnote">↩︎</a></span>
</li>
<li id="fn:60">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">60.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">R.
Zabih, J John Woodll Non-parametric Local Transforms for Computing
Visual Correspondence, ECCV
1994<a href="#fnref:60" rev="footnote">↩︎</a></span>
</li>
<li id="fn:61">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">61.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">D.
N. Bhat, S. K. Nayar, Ordinal measures for visual correspondence, CVPR
1996<a href="#fnref:61" rev="footnote">↩︎</a></span>
</li>
<li id="fn:62">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">62.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">D.
G. Lowe, Distinctive image features from scale-invariant keypoints,
International Journal of Computer Vision, 60, 2 (2004), pp.
91-110<a href="#fnref:62" rev="footnote">↩︎</a></span>
</li>
<li id="fn:63">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">63.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">R.Szeliski,
R. Zabih, D. Scharstein, O. Veksler, V. Kolmogorov, A. Agarwala, M.
Tappen, C. Rother, A Comparative Study of Energy Minimization Methods
for Markov Random Fields with Smoothness-Based Priors, IEEE Transactions
on Pattern Analysis and Machine Intelligence, 30, 6, June 2008, pp
1068-1080<a href="#fnref:63" rev="footnote">↩︎</a></span>
</li>
<li id="fn:64">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">64.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">F.
Tombari, S. Mattoccia, L. Di Stefano, E. Addimanda, Near real-time
stereo based on effective cost aggregation International Conference on
Pattern Recognition (ICPR
2008)<a href="#fnref:64" rev="footnote">↩︎</a></span>
</li>
<li id="fn:65">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">65.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">S.
Mattoccia, S. Giardino,A. Gambini, Accurate and efficient cost
aggregation strategy for stereo correspondence based on approximated
joint bilateral filtering, Asian Conference on Computer Vision (ACCV
2009), September 23-27 2009, Xiang,
China<a href="#fnref:65" rev="footnote">↩︎</a></span>
</li>
<li id="fn:66">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">66.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">S.
Mattoccia, A locally global approach to stereo correspondence, 3D
Digital Imaging and Modeling (3DIM 2009), pp 1763-1770, October 3-4,
2009, Kyoto, Japan<a href="#fnref:66" rev="footnote">↩︎</a></span>
</li>
<li id="fn:67">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">67.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">S.
Mattoccia, Improving the accuracy of fast dense stereo correspondence
algorithms by enforcing local consistency of disparity fields, 3D Data
Processing, Visualization, and Transmission (3DPVT 2010), 17-20 May
2010, Paris, France<a href="#fnref:67" rev="footnote">↩︎</a></span>
</li>
<li id="fn:68">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">68.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">S.
Mattoccia, Fast locally consistent dense stereo on multicore, Sixth IEEE
Embedded Computer Vision Workshop (ECVW2010), CVPR workshop, June 13,
2010, San Francisco, USA<a href="#fnref:68" rev="footnote">↩︎</a></span>
</li>
<li id="fn:69">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">69.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">S.
Mattoccia, Accurate dense stereo by constraining local consistency on
superpixels, 20th International Conference on Pattern Recognition
(ICPR2010), August 23-26, 2010, Istanbul,
Turkey<a href="#fnref:69" rev="footnote">↩︎</a></span>
</li>
<li id="fn:70">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">70.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">L.
Wang, M. Liao, M. Gong, R. Yang, and D. Nistér. High-quality real-time
stereo using adaptive cost aggregation and dynamic programming. 3DPVT
2006<a href="#fnref:70" rev="footnote">↩︎</a></span>
</li>
<li id="fn:71">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">71.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">S.
Mattoccia, M. Viti, F. Ries,. Near real-time Fast Bilateral Stereo on
the GPU, 7th IEEE Workshop on Embedded Computer Vision (ECVW20011), CVPR
Workshop, June 20, 2011, Colorado Springs (CO),
USA<a href="#fnref:71" rev="footnote">↩︎</a></span>
</li>
<li id="fn:72">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">72.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">S.
Mattoccia, L. De-Maeztu, "A fast segmentation-driven algorithm for
stereo correspondence", International Conference on 3D (IC3D 2011),
December 7-8, 2011, Liege,
Belgium<a href="#fnref:72" rev="footnote">↩︎</a></span>
</li>
<li id="fn:73">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">73.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">L.
De-Maeztu, S. Mattoccia, A. Villanueva, R. Cabeza, "Efficient
aggregation via iterative block-based adapting support
weight",International Conference on 3D (IC3D 2011), December 7-8, 2011,
Liege, Belgium<a href="#fnref:73" rev="footnote">↩︎</a></span>
</li>
<li id="fn:74">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">74.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">D.
Min, J. Lu, and M. Do, A revisit to cost aggregation in stereo matching:
how far can we reduce its computational redundancy?, ICCV
2011<a href="#fnref:74" rev="footnote">↩︎</a></span>
</li>
<li id="fn:75">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">75.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">L.
De-Maeztu, S. Mattoccia, A. Villanueva, R. Cabeza, "Linear stereo
matching", International Conference on Computer Vision (ICCV 2011),
November 6-13, 2011, Barcelona,
Spain<a href="#fnref:75" rev="footnote">↩︎</a></span>
</li>
</ol>
</div>
</div>

    </div>

    
    
    
      


    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>Post author:  </strong>Vincent Qin
  </li>
  <li class="post-copyright-link">
      <strong>Post link: </strong>
      <a href="https://www.vincentqin.tech/posts/stereo-vision-overview/" title="立体视觉综述：Stereo Vision Overview">https://www.vincentqin.tech/posts/stereo-vision-overview/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/computer-vision/" rel="tag"># computer vision</a>
              <a href="/tags/stereo-matching/" rel="tag"># stereo matching</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/lytro-light-field/" rel="prev" title="Lytro的光场AR之路：从巅峰到死亡">
                  <i class="fa fa-chevron-left"></i> Lytro的光场AR之路：从巅峰到死亡
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/cv-books/" rel="next" title="CV Related References">
                  CV Related References <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2016 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vincent Qin</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.8/pdfobject.min.js","integrity":"sha256-tu9j5pBilBQrWSDePOOajCUdz6hWsid/lBNzK4KgEPM="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.1.6/mermaid.min.js","integrity":"sha256-ZfzwelSToHk5YAcr9wbXAmWgyn9Jyq08fSLrLhZE89w="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"en-US","enable":true,"serverURL":"https://comments.vincentqin.tech","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":true,"locale":{"placeholder":"Welcome to comment"},"emoji":["https://unpkg.com/@waline/emojis@1.1.0/weibo","https://unpkg.com/@waline/emojis@1.1.0/alus","https://unpkg.com/@waline/emojis@1.1.0/bilibili","https://unpkg.com/@waline/emojis@1.1.0/qq","https://unpkg.com/@waline/emojis@1.1.0/tieba","https://unpkg.com/@waline/emojis@1.1.0/tw-emoji"],"meta":["nick","mail","link"],"requiredMeta":["nick","mail"],"wordLimit":0,"login":"enable","el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/posts/stereo-vision-overview/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
