<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/realcat-apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/realcat-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/realcat-32x32.png">
  <link rel="mask-icon" href="/images/realcat-safari-pinned-tab.svg" color="#222">
  <meta name="google-site-verification" content="u46QTaG_Dv3OZLpOBKYtqyuiNtIdnhSG5ASKoNvGBCM">
  <meta name="baidu-site-verification" content="MtcbwE45ft">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.vincentqin.tech","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.13.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"show_result":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":"waline","storage":true,"lazyload":true,"nav":null,"activeClass":"waline"},"stickytabs":true,"motion":{"enable":false,"async":true,"transition":{"post_block":"fadeIn","post_header":"fadeIn","post_body":"fadeIn","coll_header":"fadeIn","sidebar":"fadeIn"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本文将介绍光场领域进行深度估计的相关研究。 In this post, I&#39;ll introduce some depth estimation algorithms using Light field information. Here is some of the code. 研究生阶段的研究方向是光场深度信息的恢复。再此做一些总结，以便于让大家了解光场数据处理的一般步骤以及深度估计">
<meta property="og:type" content="article">
<meta property="og:title" content="Light Field Depth Estimation">
<meta property="og:url" content="https://www.vincentqin.tech/posts/light-field-depth-estimation/index.html">
<meta property="og:site_name" content="RealCat">
<meta property="og:description" content="本文将介绍光场领域进行深度估计的相关研究。 In this post, I&#39;ll introduce some depth estimation algorithms using Light field information. Here is some of the code. 研究生阶段的研究方向是光场深度信息的恢复。再此做一些总结，以便于让大家了解光场数据处理的一般步骤以及深度估计">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/uvst2images.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/2pp-epi-depth.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/epi-depth.png">
<meta property="og:image" content="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/viewpoints-effect.png">
<meta property="article:published_time" content="2018-05-16T05:35:54.000Z">
<meta property="article:modified_time" content="2022-09-04T16:15:46.262Z">
<meta property="article:author" content="Vincent Qin">
<meta property="article:tag" content="depth estimation">
<meta property="article:tag" content="light field">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/uvst2images.png">


<link rel="canonical" href="https://www.vincentqin.tech/posts/light-field-depth-estimation/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://www.vincentqin.tech/posts/light-field-depth-estimation/","path":"posts/light-field-depth-estimation/","title":"Light Field Depth Estimation"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Light Field Depth Estimation | RealCat</title>
  
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-97856334-1","only_pageview":true}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>





<link rel="dns-prefetch" href="https://comments.vincentqin.tech">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">RealCat</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Turn on, Tune in, Drop out</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">77</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">14</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">111</span></a></li><li class="menu-item menu-item-collections"><a href="/collections" rel="section"><i class="fa fa-diamond fa-fw"></i>Collections</a></li><li class="menu-item menu-item-guest_comments"><a href="/guestbook" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%85%89%E5%9C%BA"><span class="nav-number">1.</span> <span class="nav-text">什么是光场？</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%89%E5%9C%BA%E5%8F%82%E6%95%B0%E5%8C%96%E8%A1%A8%E7%A4%BA"><span class="nav-number">1.1.</span> <span class="nav-text">光场参数化表示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%89%E5%9C%BA%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number">1.2.</span> <span class="nav-text">光场的可视化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%89%E5%9C%BA%E7%9A%84%E8%8E%B7%E5%8F%96"><span class="nav-number">2.</span> <span class="nav-text">光场的获取</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%89%E5%9C%BA%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB"><span class="nav-number">3.</span> <span class="nav-text">光场深度估计算法分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E8%A7%86%E8%A7%92%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D"><span class="nav-number">3.1.</span> <span class="nav-text">多视角立体匹配</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E7%A7%BB%E7%90%86%E8%AE%BA"><span class="nav-number">3.1.1.</span> <span class="nav-text">相移理论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8C%B9%E9%85%8D%E4%BB%A3%E4%BB%B7%E6%9E%84%E5%BB%BA"><span class="nav-number">3.1.2.</span> <span class="nav-text">匹配代价构建</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8Eepi%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">3.2.</span> <span class="nav-text">基于EPI的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%A3%E7%84%A6%E5%8F%8A%E8%9E%8D%E5%90%88%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">3.3.</span> <span class="nav-text">散焦及融合的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%AE%E4%BF%A1%E5%BA%A6%E5%88%86%E6%9E%90%E5%8F%8A%E6%B7%B1%E5%BA%A6%E8%9E%8D%E5%90%88"><span class="nav-number">3.3.1.</span> <span class="nav-text">置信度分析及深度融合</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">3.4.</span> <span class="nav-text">学习的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%89%E5%9C%BA%E5%9B%BE%E5%83%8F%E5%87%A0%E4%BD%95%E7%89%B9%E5%BE%81"><span class="nav-number">3.4.1.</span> <span class="nav-text">光场图像几何特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1"><span class="nav-number">3.4.2.</span> <span class="nav-text">网络设计</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#references"><span class="nav-number">4.</span> <span class="nav-text">References</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Vincent Qin"
      src="https://vincentqin.gitee.io/images/qin_small.png">
  <p class="site-author-name" itemprop="name">Vincent Qin</p>
  <div class="site-description" itemprop="description">Keep Your Curiosity</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">77</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">111</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Vincentqyw" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Vincentqyw" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:realcat@126.com" title="Email → mailto:realcat@126.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>Email</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://vincentqin.gitee.io/images/qrcode_realcat.jpg" title="Wechat → https:&#x2F;&#x2F;vincentqin.gitee.io&#x2F;images&#x2F;qrcode_realcat.jpg" rel="noopener" target="_blank"><i class="fab fa-weixin fa-fw"></i>Wechat</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/i_vincent/activities" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;i_vincent&#x2F;activities" rel="noopener" target="_blank"><i class="fab fa-quora fa-fw"></i>Zhihu</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/AlphaRealcat" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;AlphaRealcat" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/18136563" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;18136563" rel="noopener" target="_blank"><i class="fa fa-video-camera fa-fw"></i>Bilibili</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://bafybeic2jt62kpyh6cz2g4ngxs4kazojfw3dhx53mco3wc6f56dejty4xm.ipfs.infura-ipfs.io/" title="Web3.0 → https:&#x2F;&#x2F;bafybeic2jt62kpyh6cz2g4ngxs4kazojfw3dhx53mco3wc6f56dejty4xm.ipfs.infura-ipfs.io" rel="noopener" target="_blank"><i class="link fa-fw"></i>Web3.0</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-dashboard"></i>
      Scholar
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://xxx.itp.ac.cn/" title="http:&#x2F;&#x2F;xxx.itp.ac.cn" rel="noopener" target="_blank">Arxiv-Mirror</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://arxiv-sanity.com/" title="http:&#x2F;&#x2F;arxiv-sanity.com&#x2F;" rel="noopener" target="_blank">Arxiv-sanity</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://openaccess.thecvf.com/menu.py" title="http:&#x2F;&#x2F;openaccess.thecvf.com&#x2F;menu.py" rel="noopener" target="_blank">CVF</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://paperswithcode.com/sota" title="https:&#x2F;&#x2F;paperswithcode.com&#x2F;sota" rel="noopener" target="_blank">Paper&Code</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://scihub.wikicn.top/" title="https:&#x2F;&#x2F;scihub.wikicn.top&#x2F;" rel="noopener" target="_blank">Scihub</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://ras.papercept.net/conferences/scripts/start.pl" title="http:&#x2F;&#x2F;ras.papercept.net&#x2F;conferences&#x2F;scripts&#x2F;start.pl" rel="noopener" target="_blank">RAS</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://openreview.net/" title="https:&#x2F;&#x2F;openreview.net&#x2F;" rel="noopener" target="_blank">OpenReview</a>
        </li>
    </ul>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-battery-three-quarters fa-fw"></i>
      Friends Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.wangpengan.com/" title="http:&#x2F;&#x2F;www.wangpengan.com&#x2F;" rel="noopener" target="_blank">Tensorboy</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://simtalk.cn/" title="http:&#x2F;&#x2F;simtalk.cn&#x2F;" rel="noopener" target="_blank">Simshang</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sttomato.github.io/" title="https:&#x2F;&#x2F;sttomato.github.io" rel="noopener" target="_blank">Tomato</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://dfine.tech/" title="http:&#x2F;&#x2F;dfine.tech&#x2F;" rel="noopener" target="_blank">Newdee</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://cs-people.bu.edu/yfhu/" title="http:&#x2F;&#x2F;cs-people.bu.edu&#x2F;yfhu&#x2F;" rel="noopener" target="_blank">WhoIf</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://yulunzhang.com/" title="http:&#x2F;&#x2F;yulunzhang.com&#x2F;" rel="noopener" target="_blank">Yulun</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sanglongbest.github.io/" title="https:&#x2F;&#x2F;sanglongbest.github.io&#x2F;" rel="noopener" target="_blank">YangLiu</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.erenship.com/" title="https:&#x2F;&#x2F;www.erenship.com&#x2F;" rel="noopener" target="_blank">Eren</a>
        </li>
    </ul>
  </div>

  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-briefcase"></i>
      Common Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://comments.vincentqin.tech/ui" title="https:&#x2F;&#x2F;comments.vincentqin.tech&#x2F;ui" rel="noopener" target="_blank">Comments</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://gitee.com/vincentqin/vincentqin" title="https:&#x2F;&#x2F;gitee.com&#x2F;vincentqin&#x2F;vincentqin" rel="noopener" target="_blank">Source</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.notion.so/realcat" title="https:&#x2F;&#x2F;www.notion.so&#x2F;realcat" rel="noopener" target="_blank">Notion</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.matrixcalculus.org/" title="http:&#x2F;&#x2F;www.matrixcalculus.org&#x2F;" rel="noopener" target="_blank">Calculus</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://emojipedia.org/" title="https:&#x2F;&#x2F;emojipedia.org&#x2F;" rel="noopener" target="_blank">Emoji</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://unstoppabledomains.com/" title="https:&#x2F;&#x2F;unstoppabledomains.com&#x2F;" rel="noopener" target="_blank">UD</a>
        </li>
    </ul>
  </div>




        </div>

      <div class="wechat_QR_code">
      <!-- 二维码 -->
      <img src ="https://vincentqin.tech/blog-resources/qrcode_realcat.jpg">
      <span>Follow Me on Wechat</span>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://www.vincentqin.tech/posts/light-field-depth-estimation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://vincentqin.gitee.io/images/qin_small.png">
      <meta itemprop="name" content="Vincent Qin">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RealCat">
      <meta itemprop="description" content="Keep Your Curiosity">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Light Field Depth Estimation | RealCat">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Light Field Depth Estimation
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-05-16 13:35:54" itemprop="dateCreated datePublished" datetime="2018-05-16T13:35:54+08:00">2018-05-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-09-05 00:15:46" itemprop="dateModified" datetime="2022-09-05T00:15:46+08:00">2022-09-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Light-Field/" itemprop="url" rel="index"><span itemprop="name">Light Field</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline: </span>
  
    <a title="waline" href="/posts/light-field-depth-estimation/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/posts/light-field-depth-estimation/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-item" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="waline-pageview-count" data-path="/posts/light-field-depth-estimation/"></span>
    </span>
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <script src="/assets/js/DPlayer.min.js"> </script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/street.jpg" /></p>
<div class="note success"><p>本文将介绍光场领域进行深度估计的相关研究。 In this post, I'll
introduce some depth estimation algorithms using Light field
information. Here is some of the <a
target="_blank" rel="noopener" href="https://github.com/Vincentqyw/Depth-Estimation-Light-Field">code</a>.
研究生阶段的研究方向是光场深度信息的恢复。再此做一些总结，以便于让大家了解光场数据处理的一般步骤以及深度估计的相关的知识，光场可视化部分代码见<a
target="_blank" rel="noopener" href="https://github.com/Vincentqyw/light-field-Processing">light-field-Processing</a>，收集的部分光场深度估计代码见<a
target="_blank" rel="noopener" href="https://github.com/Vincentqyw/Depth-Estimation-Light-Field">Depth-Estimation-Light-Field</a>。如有任何疑问或者建议，请大家在评论区提出。</p>
</div>
<span id="more"></span>
<h1 id="什么是光场">什么是光场？</h1>
<p>提到光场，很多人对它的解释模糊不清，在此我对它的概念进行统一表述。它的故事可以追溯到1936年，那是一个春天，Gershun写了一本名为<strong>The
Light
Field</strong><sup id="fnref:1"><a href="#fn:1" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Gershun, A. [The Light Field](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/1.Gershun-1939-Journal_of_Mathematics_and_Physics.pdf). Studies in Applied Mathematics 18.1-4(1939):51–151.">1</span></a></sup>的鸿篇巨著（感兴趣的同学可以看看那个年代的论文），于是光场的概念就此诞生，但它并没有因此被世人熟知。经过了近六十年的沉寂，1991年Adelson<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Adelson, Edward H, and J. R. Bergen. [The plenoptic function and the elements of early vision](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/2.The%20plenoptic%20function%20and%20the%20elements%20of%20early%20vision.pdf).  Computational Models of Visual Processing (1991):3-20.
">2</span></a></sup>等一帮帅小伙将光场表示成了如下的7维函数：</p>
<p><span class="math display">\[
P(\theta,\phi,\lambda,t,V_x,V_y,V_z). \tag{1}
\]</span></p>
<p>其中<span
class="math inline">\((\theta,\phi)\)</span>表示球面坐标，<span
class="math inline">\(\lambda\)</span>表示光线的波长，<span
class="math inline">\(t\)</span>表示时间，<span
class="math inline">\((V_x,V_y,V_z)\)</span>表示观察者的位置。
可以想象假如有这样一张由针孔相机拍摄的黑白照片，它表示：我们从<strong>某个时刻</strong>、<strong>单一视角</strong>观察到的<strong>可见光谱</strong>中某个<strong>波长</strong>的光线的平均。也就是说，它记录了通过<span
class="math inline">\(P\)</span>点的光强分布，光线方向可以由球面坐标<span
class="math inline">\(P(\theta,\phi)\)</span>或者笛卡尔坐标<span
class="math inline">\(P(x,y)\)</span>来表示。对于彩色图片而言，我们要添加光线的波长<span
class="math inline">\(\lambda\)</span>信息即变为<span
class="math inline">\(P(\theta,\phi,\lambda)\)</span>。按照同样的思路，彩色电影也就是增加了时间维度<span
class="math inline">\(t\)</span>，因此<span
class="math inline">\(P(\theta,\phi,\lambda,t)\)</span>。对于彩色全息电影而言，我们可以从任意空间位置<span
class="math inline">\((V_x,V_y,V_z)\)</span>进行观看，于是其可以表达为最终的形式<span
class="math inline">\(P(\theta,\phi,\lambda,t,V_x,V_y,V_z)\)</span>。这个函数又被成为全光函数（Plenoptic
Function）。
但是以上的七维的全光函数过于复杂，难以记录以及编程实现。所以在实际应用中我们对其进行简化处理。第一个简化是单色光以及时不变。可分别记录3原色以简化掉波长<span
class="math inline">\(\lambda\)</span>，可以通过记录不同帧以简化<span
class="math inline">\(t\)</span>，这样全光函数就变成了5D。第二个简化是Levoy<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Levoy, Marc. [Light field rendering](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/3.Light_Field_Rendering.pdf). Conference on Computer Graphics and Interactive Techniques ACM, 1996:31-42.
">3</span></a></sup>等人（1996年）认为5D光场中还有一定的冗余，可以在自由空间（光线在传播过程中能量保持不变）中简化成4D。</p>
<h2 id="光场参数化表示">光场参数化表示</h2>
<p>参数化表示要解决的问题包括：1. 计算高效；2. 光线可控；3.
光线均匀采样。目前比较常用的表示方式是双平面法（<span
class="math inline">\(2PP\)</span>）<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Levoy, Marc. [Light field rendering](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/3.Light_Field_Rendering.pdf). Conference on Computer Graphics and Interactive Techniques ACM, 1996:31-42.
">3</span></a></sup>，利用双平面法可以将光场表示为：<span
class="math inline">\(L(u,v,s,t)\)</span>。其中<span
class="math inline">\((u,v)\)</span>为第一个平面，<span
class="math inline">\((s,t)\)</span>是第二个平面。那么一条有方向的直线可以表示为连接<span
class="math inline">\(uv\)</span>以及<span
class="math inline">\(st\)</span>平面上任意两点确定的线，如下图所示：</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/2pp-v1.png" /></p>
<p>【注】：Levoy<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Levoy, Marc. [Light field rendering](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/3.Light_Field_Rendering.pdf). Conference on Computer Graphics and Interactive Techniques ACM, 1996:31-42.
">3</span></a></sup>首先利用双平面法对光场进行表示，光线首先通过<span
class="math inline">\(uv\)</span>平面，然后再通过<span
class="math inline">\(st\)</span>平面。但是后来（同年）Gortler<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Gortler, Steven J., et al. [The Lumigraph](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/4.The%20lumigraph.pdf). Proc Siggraph 96(1996):43-54.
">4</span></a></sup>等人将其传播方向反了过来，导致后续研究者对此表述并不一致。与此同时，也有不少文献中也引入了<span
class="math inline">\(xy\)</span>坐标，例如著名的光场相机的缔造者N.G.博士的毕业论文。通常情况下，这指的是像平面坐标，即指的是由传感器得到的图像中像素的位置坐标。由于后续处理中都是针对图像而言，而对于光学结构以及光线的传播过程并不感兴趣。所以为了方便起见，我们在本文中统一采用Levoy<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Levoy, Marc. [Light field rendering](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/3.Light_Field_Rendering.pdf). Conference on Computer Graphics and Interactive Techniques ACM, 1996:31-42.
">3</span></a></sup>的方式对<strong>光场图像</strong>进行表示，即<span
class="math inline">\(uv\)</span>表示角度分辨率，<span
class="math inline">\(xy\)</span>表示空间分辨率，即<span
class="math inline">\(L(u,v,x,y)\)</span>。同时在表示<strong>光场</strong>时用<span
class="math inline">\(L(u,v,s,t)\)</span>。有时候二者不做区分，注意即可。</p>
<h2 id="光场的可视化">光场的可视化</h2>
<p>虽然光场由<span class="math inline">\(7D\)</span>全光函数降维到<span
class="math inline">\(4D\)</span>，但是其结构还是很难直观想象。通过固定4D光场参数化表示<span
class="math inline">\(L(u,v,s,t)\)</span>中的某些变量，我们可以很容易地对光场进行可视化。我们通常认为<span
class="math inline">\((u,v)\)</span>控制着某个视角的位置，即相机平面；而<span
class="math inline">\((s,t)\)</span>控制着从某个视角观察到的图像。说简单点：<span
class="math inline">\(uv\)</span>控制角度分辨率，<span
class="math inline">\(st\)</span>控制空间分辨率（视野）。注意上式描述的是光线的表示方法，并没有涉及图像处理，所以没有<span
class="math inline">\(xy\)</span>。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/uvst2images.png" width="600" alt="uvst2images"></p>
<p>接下来讲解，几种常见的可视化方式（图片来源<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Wu, Gaochang, et al. [Light Field Image Processing: An Overview](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/5.Light-Field-Image-Processing-An%20Overview.pdf). IEEE Journal of Selected Topics in Signal Processing PP.99(2017):1-1.
">5</span></a></sup>）。首先是<strong>多视图法</strong>。很容易理解，对于最简单的情况，首先固定<span
class="math inline">\(u=u^*,v=v^*\)</span>，我们可以得到多视角的某个视图<span
class="math inline">\(L(u^*,v^*,s,t)\)</span>，如下图所示：</p>
<p><img data-src="https://vincentqin.tech/blog-resources/light-field-depth-estimation/allviews.png" /></p>
<p>第二种表示方法是<strong>角度域法</strong>，通过固定<span
class="math inline">\(s=s^*,t=t^*\)</span>可以得到某个宏像素<span
class="math inline">\(L(u,v,s^*,t^*)\)</span>，如下图所示：</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/angular-patch.png" /></p>
<p>第三种表示方法是<strong>极线图法</strong>，通过固定<span
class="math inline">\(v=v^*,t=t^*\)</span>可以得到极线图：<span
class="math inline">\(L(u,v^*,s,t^*)\)</span>，如下图中水平方向的图所示；同理固定<span
class="math inline">\(u=u^*,s=s^*\)</span>可以得到极线图：<span
class="math inline">\(L(u^*,v,s^*,t)\)</span>，如下图中竖直方向的图所示：</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/epi.png" /></p>
<p>最后，给出这几种方式的对应关系图（注意图中，<span
class="math inline">\(xy\)</span>对应于以上<span
class="math inline">\(st\)</span>，<span
class="math inline">\(st\)</span>对应于<span
class="math inline">\(uv\)</span>）。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/lf-all-view.png" /></p>
<h1 id="光场的获取">光场的获取</h1>
<p>我们知道传统的相机只能采集来自场景某个方向的<span
class="math inline">\(2D\)</span>信息，那怎么才能够采集到光场信息呢？试想一下，当多个相机在多个不同视角同时拍摄时，这样我们就可以得到一个光场的采样（多视角图像）了。当然，这是容易想到的方法，目前已有多种获得光场的方式，如下表格中列举了其中具有代表性的方式<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Wu, Gaochang, et al. [Light Field Image Processing: An Overview](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/5.Light-Field-Image-Processing-An%20Overview.pdf). IEEE Journal of Selected Topics in Signal Processing PP.99(2017):1-1.
">5</span></a></sup>。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/lf-acquisition.png" /></p>
<h1 id="光场深度估计算法分类">光场深度估计算法分类</h1>
<p>由上可知，光场图像中包含来自场景的多视角信息，这使得深度估计成为可能。相较于传统的多视角深度估计算法而言，基于光场的深度估计算法无需进行相机标定，这大大简化的深度估计的流程。但是由于光场图像巨大导致了深度估计过程占用大量的计算资源。同时这些所谓的多个视角之间虚拟相机的基线过短，从而可能导致误匹配的问题。以下将对多种深度估计算法进行分类并挑选具有代表性的算法进行介绍。</p>
<h2 id="多视角立体匹配">多视角立体匹配</h2>
<p>根据光场相机的成像原理，我们可以将光场图像想像成为多个虚拟相机在多个不同视角拍摄同一场景得到图像的集合，那么此时的深度估计问题就转换成为多视角立体匹配问题。以下列举几种基于多视角立体匹配算法的深度估计算法<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Jeon, Hae Gon, et al. [Accurate depth map estimation from a lenslet light field camera](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/8.Accurate%20Depth%20Map%20Estimation%20from%20a%20Lenslet%20Light%20Field%20Camera.pdf). Computer Vision and Pattern Recognition IEEE, 2015:1547-1555.
">8</span></a></sup>
<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Yu, Zhan, et al. [Line Assisted Light Field Triangulation and Stereo Matching](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/9.Line%20Assisted%20Light%20Field%20Triangulation%20and%20Stereo%20Matching.pdf). IEEE International Conference on Computer Vision IEEE, 2014:2792-2799.
">9</span></a></sup>
<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Heber, Stefan, and T. Pock. [Shape from Light Field Meets Robust PCA](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/10.Shape%20from%20Light%20Field%20meets%20Robust%20PCA.pdf). Computer Vision – ECCV 2014. 2014:751-767.
">10</span></a></sup>
<sup id="fnref:20"><a href="#fn:20" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Heber, Stefan, R. Ranftl, and T. Pock. [Variational Shape from Light Field](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/20.Variational%20Shape%20from%20Light%20Field.pdf). Energy Minimization Methods in Computer Vision and Pattern Recognition. Springer Berlin Heidelberg, 2013:66-79.
">20</span></a></sup>
<sup id="fnref:21"><a href="#fn:21" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Chen, Can, et al. [Light Field Stereo Matching Using Bilateral Statistics of Surface Cameras](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/21.Light%20Field%20Stereo%20Matching%20Using%20Bilateral%20Statistics%20of%20Surface%20Cameras-Can_CVPR14_stereo.pdf). IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society, 2014:1518-1525.
">21</span></a></sup>。</p>
<table>
<tr>
<td rowspan="6">
MVS-based<br/>
<td>
<b>Approach</b>
</td>
<td>
<b>Main Feature</b>
</td>
</tr>
<tr>
<td>
Jeon et al. <sup><a href="#fn_8" id="reffn_8">8</a></sup>
</td>
<td>
Phase shift Sub-pixel
</td>
</tr>
<tr>
<td>
Yu et al. <sup><a href="#fn_9" id="reffn_9">9</a></sup>
</td>
<td>
Line-assisted graph cut
</td>
</tr>
<tr>
<td>
Heber et al. <sup><a href="#fn_10" id="reffn_10">10</a></sup>
<sup><a href="#fn_2" id="reffn_20">20</a></sup>
</td>
<td>
PCA matching term
</td>
</tr>
<tr>
<td>
Chen et al. <sup><a href="#fn_21" id="reffn_21">21</a></sup>
</td>
<td>
Scam: Bilateral Consistency Metric
</td>
</tr>
</table>
<p>在这里介绍Jeon等人<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Jeon, Hae Gon, et al. [Accurate depth map estimation from a lenslet light field camera](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/8.Accurate%20Depth%20Map%20Estimation%20from%20a%20Lenslet%20Light%20Field%20Camera.pdf). Computer Vision and Pattern Recognition IEEE, 2015:1547-1555.
">8</span></a></sup>提出的基于相移的亚像素多视角立体匹配算法。</p>
<h3 id="相移理论">相移理论</h3>
<p>该算法的核心就是用到了相移理论，即空域的一个小的位移在频域为原始信号的频域表达与位移的指数的幂乘积，即如下公式：</p>
<p><span class="math display">\[
\mathcal{F}\left\{I(x+\Delta x)\right\} =
\mathcal{F}\left\{I(x)\right\}\exp^{2\pi j\Delta x}. \tag{2}
\]</span></p>
<p>所以，经过位移后图像可以表示为：</p>
<p><span class="math display">\[
I&#39;(x)=I(x+\Delta
x)={\mathcal{F}^{-1}\left\{\mathcal{F}\left\{I(x)\right\}\exp^{2 \pi j
\Delta x}\right\}},\tag{3}
\]</span></p>
<p>面对Lytro相机窄基线的难点，通过相移的思想能够实现亚像素精度的匹配，在一定程度上解决了基线短的问题。那么大家可能好奇的是，如何将这个理论用在多视角立体匹配中呢？带着这样的疑问，继续介绍该算法。</p>
<h3 id="匹配代价构建">匹配代价构建</h3>
<p>为了能够使子视角图像之间进行匹配，作者设计了2中不同的代价量：Sum of
Absolute Differences (SAD)以及Sum of Gradient Differences
(GRAD)，最终通过加权的方式获得最终的匹配量<span
class="math inline">\(C\)</span>，它是位点<span
class="math inline">\(x\)</span>以及损失编号（可以理解成深度/视差层）<span
class="math inline">\(l\)</span>的函数，具体形式如下公式所示：</p>
<p><span class="math display">\[
C(x,l) = \alpha C_A(x,l)+(1-\alpha)C_G(x,l),\tag{4}
\]</span></p>
<p>其中<span class="math inline">\(\alpha \in
[0,1]\)</span>表示SAD损失量<span
class="math inline">\(C_A\)</span>以及SGD损失量<span
class="math inline">\(C_G\)</span>之间的权重。同时其中的<span
class="math inline">\(C_A\)</span>被定义为如下形式：</p>
<p><span class="math display">\[
C_A(x,l) = \sum_{u \in V}\sum_{x \in R_x}{\min\left( |
I(u_c,x)-I(u,x+\Delta x(u,l))|,\tau _1\right)},\tag{5}
\]</span></p>
<p>其中的<span class="math inline">\(R_x\)</span>表示在<span
class="math inline">\(x\)</span>点邻域的矩形区域；<span
class="math inline">\(\tau
_1\)</span>是代价的截断值（为了增加算法鲁棒性）；<span
class="math inline">\(V\)</span>表示除了中心视角<span
class="math inline">\(u_c\)</span>之外的其余视角。上述公式通过比较中心视角图像<span
class="math inline">\(I(u_c,x)\)</span>与其余视角<span
class="math inline">\(I(u,x)\)</span>的差异来构建损失量，具体而言就是通过不断地在某个视角<span
class="math inline">\(I(u_i,x)\)</span>上<span
class="math inline">\(x\)</span>点的周围移动一个<strong>小的距离</strong>并于中心视角做差；重复这个过程直到比较完所有的视角(i=1...视角数目N)为止。此时会用到上节提及的相移理论以得到移动后的像素强度，注意上面提到的<strong>小的距离</strong>实际上就是公式中的<span
class="math inline">\(\Delta x\)</span>，它被定义为如下形式：</p>
<p><span class="math display">\[
\Delta x(u,l) = lk(u-u_c),\tag{6}
\]</span></p>
<p>其中k表示深度/视差层的单位（像素），<span
class="math inline">\(\Delta
x\)</span>会随着任意视角与中心视角之间距离的增大而线性增加。同理，可以构造出第二个匹配代价量SGD，其基本形式如下所示：</p>
<p><span class="math display">\[
\begin{equation} \tag{7}
\begin{aligned}
C_G(x,l) = &amp;\sum_{u \in V}\sum_{x \in R_x}\beta (u){\min\left(
Diff_x(u_c,u,x,l),\tau _2\right)}+ \\ &amp;(1-\beta (u)){\min\left(
Diff_y(u_c,u,x,l),\tau _2\right)}
\end{aligned}
\end{equation}
\]</span></p>
<p>其中的<span
class="math inline">\(Diff_x(u_c,u,x,l)=|I_x(u_c,x)-I_x(u,x+\Delta
x(u,l))|\)</span>表示子视角图像在x方向的上的梯度，同理<span
class="math inline">\(Diff_y\)</span>表示子孔径图像在y方向上的梯度；<span
class="math inline">\(\beta
(u)\)</span>控制着这两个方向代价量的权重，它由任意视角与中心视角之间的相对距离表示：</p>
<p><span class="math display">\[
\beta (u) = \frac{|u-u_c|}{|u-u_c|+|v-v_c|}.\tag{8}
\]</span></p>
<p>至此，代价函数构建完毕。随后对于该代价函数利用边缘保持滤波器进行损失聚合，得到优化后的代价量。紧接着作者建立了一个多标签优化模型（GC求解）以及迭代优化模型对深度图进行优化，再此不做详细介绍。下面是其算法的分部结果：</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/jeon-depth-step.png" /></p>
<h2 id="基于epi的方法">基于EPI的方法</h2>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/2pp-epi-depth.png" width="100%"></p>
<p>不同于多视角立体匹配的方式，EPI的方式是通过分析光场数据结构的从而进行深度估计的方式。EPI图像中斜线的斜率就能够反映出场景的深度。上图中点P为空间点，平面<span
class="math inline">\(\Pi\)</span>为相机平面，平面<span
class="math inline">\(\Omega\)</span>为像平面。图中<span
class="math inline">\(\Delta u\)</span>与<span
class="math inline">\(\Delta
x\)</span>的关系可以表示为如下公式<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Wanner, Sven, and B. Goldluecke. [Variational Light Field Analysis for Disparity Estimation and Super-Resolution](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/6.Variational%20Light%20Field%20Analysis%20for%20Disparity%20Estimation%20and%20Super-Resolution.pdf). IEEE Transactions on Pattern Analysis &amp; Machine Intelligence 36.3(2013):1.
">6</span></a></sup>：</p>
<p><span class="math display">\[
\Delta x=- \frac{f}{Z}\Delta u,\tag{9}
\]</span></p>
<p>假如固定相同的<span class="math inline">\(\Delta
u\)</span>，水平方向位移较大的EPI图中斜线所对应的视差就越大，即深度就越小。如下图所示，<span
class="math inline">\(\Delta x_2\)</span>&gt;<span
class="math inline">\(\Delta
x_1\)</span>，那么绿色线所对应的空间点要比红色线所对应的空间点深度小。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/epi-depth.png" width="100%"></p>
<p>以下列举几种基于EPI的深度估计算法<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Kim, Changil, et al. [Scene reconstruction from high spatio-angular resolution light fields](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/11.scene-reconstruction-from-high-spatio-angular-resolution-light-fields-siggraph-2013-compressed-kim-et-al.pdf). Acm Transactions on Graphics 32.4(2017):1-12.
">11</span></a></sup>
<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Li, J., M. Lu, and Z. N. Li. [Continuous Depth Map Reconstruction From Light Fields](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/12.Continuous%20Depth%20Map%20Reconstruction%20From%20Light%20Fields.pdf). IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society 24.11(2015):3257.
">12</span></a></sup>
<sup id="fnref:13"><a href="#fn:13" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Krolla, Bernd, et al. [Spherical Light Fields](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/13.Spherical%20light%20field.pdf). British Machine Vision Conference 2014.
">13</span></a></sup>
<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Wanner, Sven, C. Straehle, and B. Goldluecke. [Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/14.Globally%20consistent%20multi-label%20assignment%20on%20the%20ray%20space%20of%204D%20light%20field.pdf). IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society, 2013:1011-1018.
">14</span></a></sup>
<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Diebold, Maximilian, B. Jahne, and A. Gatto. [Heterogeneous Light Fields](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/15.Heterogeneous%20Light%20Fields.pdf). Computer Vision and Pattern Recognition IEEE, 2016:1745-1753.
">15</span></a></sup>
<sup id="fnref:24"><a href="#fn:24" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Zhang S, Sheng H, Li C, et al. [Robust depth estimation for light field via spinning parallelogram operator](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/24.Robust%20Depth%20Estimation%20for%20Light%20Field%20via%20Spinning%20Parallelogram%20Operator.pdf). Computer Vision and Image Understanding, 2016, 145:148-159.
">24</span></a></sup>。</p>
<table>
<tr>
<td rowspan="9">
EPI-based<br/><br />

<td>
<b>Approach</b>
</td>
<td>
<b>Main Feature</b>
</td>
</tr>
<tr>
<td>
Kim et al. <sup><a href="#fn_11" id="reffn_11">11</a></sup>
</td>
<td>
Large scene reconstruction
</td>
</tr>
<tr>
<td>
Li et al. <sup><a href="#fn_12" id="reffn_12">12</a></sup>
</td>
<td>
Sparse linear optimization
</td>
</tr>
<tr>
<td>
Krolla et al. <sup><a href="#fn_13" id="reffn_13">13</a></sup>
</td>
<td>
Spherical light field
</td>
</tr>
<tr>
<td>
Wanner et al. <sup><a href="#fn_14" id="reffn_14">14</a></sup>
<sup><a href="#fn_33" id="reffn_33">33</a></sup>
<sup><a href="#fn_26" id="reffn_26">26</a></sup>
</td>
<td>
Total variation(TV)
</td>
</tr>
<tr>
<td>
Diebode et al. <sup><a href="#fn_15" id="reffn_15">15</a></sup>
</td>
<td>
Modified structure tensor
</td>
</tr>
<tr>
<td>
Zhang et al. <sup><a href="#fn_24" id="reffn_24">24</a></sup>
</td>
<td>
Spinning Parallelogram Operator(SPO)
</td>
</tr>
</table>
<p>在以上表格中最具代表性的算法是由wanner<sup id="fnref:33"><a href="#fn:33" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="S. Wanner and B. Goldluecke. [Globally Consistent Depth Labeling of 4D Light Fields](http://publications.lightfield-analysis.net/WG12_cvpr.pdf). 2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, 2012, pp. 41-48, doi: 10.1109/CVPR.2012.6247656. ">33</span></a></sup>提出的结构张量法得到EPI图中线的斜率，如下公式所示：</p>
<p><span class="math display">\[
J=
\left[
\begin{matrix}
   G_{\sigma}*(S_xS_x) &amp; G_{\sigma}*(S_xS_y)  \\
   G_{\sigma}*(S_xS_y) &amp; G_{\sigma}*(S_yS_y)
  \end{matrix}
  \right]=
  \left[
\begin{matrix}
   J_{xx} &amp; J_{xy}\\
   J_{xy} &amp; J_{yy}
  \end{matrix}
  \right],
  \tag{10}
\]</span></p>
<p>其中<span class="math inline">\(S=S_{y^*,v^*}\)</span>为极线图。<span
class="math inline">\(S_x\)</span>以及<span
class="math inline">\(S_y\)</span>表示极线图在x以及y方向上的梯度，<span
class="math inline">\(G_{\sigma}\)</span>表示高斯平滑算子。最终极线图中局部斜线的斜率可以表示成如下形式：
<span class="math display">\[
J=\left[
\begin{matrix}
   \Delta x  \\
    \Delta v
  \end{matrix}
  \right]=
  \left[
\begin{matrix}
  \sin \varphi\\
   \cos \varphi
  \end{matrix}
  \right],
  \tag{11}
\]</span> 其中<span class="math display">\[\varphi =
\frac{1}{2}\arctan\left(\frac{J_{yy}-J_{xx}}{2J_{xy}}\right)\]</span>。因此深度可以由公式（9）推出：</p>
<p><span class="math display">\[
Z=-f\frac{\Delta v}{\Delta x},  \tag{12}
\]</span></p>
<p>通常情况下，可以用一种更加简单的形式，如视差对其进行表示：</p>
<p><span class="math display">\[
d_{y^*,v^*}=-f/Z=\frac{\Delta x}{\Delta v}=\tan \phi .  \tag{13}
\]</span></p>
<p>至此，利用上述公式可以从EPI中估计出视差。</p>
<h2 id="散焦及融合的方法">散焦及融合的方法</h2>
<p>光场相机一个很重要的卖点是先拍照后对焦，这其实是根据光场剪切原理<sup id="fnref:31"><a href="#fn:31" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Ng, Ren. [Digital light field photography](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/31.Digital%20light%20field%20photography.pdf). 2006, 115(3):38-39.
">31</span></a></sup>得到的。通过衡量像素在不同焦栈处的“模糊度”可以得到其对应的深度。以下列举几种基于散焦的深度估计算法<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Wang, Ting Chun, A. A. Efros, and R. Ramamoorthi. [Occlusion-Aware Depth Estimation Using Light-Field Cameras](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/7.Occlusion-aware%20Depth%20Estimation%20Using%20Light-field%20Cameras.pdf). IEEE International Conference on Computer Vision IEEE, 2016:3487-3495.
">7</span></a></sup>
<sup id="fnref:16"><a href="#fn:16" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Tao, M. W, et al. [Depth from Combining Defocus and Correspondence Using Light-Field Cameras](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/16.Depth%20from%20Combining%20Defocus%20and%20Correspondence%20Using%20Light-Field%20Cameras.pdf). IEEE International Conference on Computer Vision IEEE Computer Society, 2013:673-680.
">16</span></a></sup>
<sup id="fnref:17"><a href="#fn:17" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Tao, Michael W., et al. [Depth from shading, defocus, and correspondence using light-field angular coherence](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/17.Depth%20from%20Shading,%20Defocus,%20and%20Correspondence%20Using%20Light-Field%20Angular%20Coherence.pdf). Computer Vision and Pattern Recognition IEEE, 2015:1940-1948.
">17</span></a></sup>
<sup id="fnref:22"><a href="#fn:22" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Williem W, Kyu P I. [Robust light field depth estimation for noisy scene with occlusion](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/22.Williem_Robust_Light_Field_CVPR_2016_paper.pdf). Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016:4396-4404.
">22</span></a></sup>
<sup id="fnref:23"><a href="#fn:23" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Williem W, Park I K, Lee K M. [Robust light field depth estimation using occlusion-noise aware data costs](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/23.TPAMI2017_Williem.pdf). IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017(99):1-1.
">23</span></a></sup>。</p>
<table>
<tr>
<td rowspan="5">
Defocus-based<br/>
<td>
<b>Approach</b>
</td>
<td>
<b>Main Feature</b>
</td>
</tr>
<tr>
<td>
Wang et al. <sup><a href="#fn_7" id="reffn_7">7</a></sup>
</td>
<td>
Occlusion-aware
</td>
</tr>
<tr>
<td>
Tao et al. <sup><a href="#fn_16" id="reffn_16">16</a></sup>
</td>
<td>
Defocus cues &amp; Correspondence cues
</td>
</tr>
<tr>
<td>
Tao et al. <sup><a href="#fn_17" id="reffn_17">17</a></sup>
</td>
<td>
Angular Coherence
</td>
</tr>
<tr>
<td>
Williem et al. <sup><a href="#fn_22" id="reffn_22">22</a></sup>
<sup><a href="#fn_23" id="reffn_23">23</a></sup>
</td>
<td>
Angular Entropy(AD, AE, CAD, CAE)
</td>
</tr>
</table>
<p>这里介绍一个最具代表性的工作，由Tao等人<sup id="fnref:16"><a href="#fn:16" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Tao, M. W, et al. [Depth from Combining Defocus and Correspondence Using Light-Field Cameras](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/16.Depth%20from%20Combining%20Defocus%20and%20Correspondence%20Using%20Light-Field%20Cameras.pdf). IEEE International Conference on Computer Vision IEEE Computer Society, 2013:673-680.
">16</span></a></sup>在2013年提出，下图为其算法框架以及分部结果。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/tao2013.png" />
<img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/tao2013-results-step-by-step.png" /></p>
<p>该工作其实就做了2件事情：1. 设计两种深度线索并估计原始深度；2.
置信度分析及MRF融合。以下对其进行具体介绍。 ### 双线索提取</p>
<p>首先对光场图像进行重聚焦，然后得到一系列具有不同深度的焦栈。然后对该焦栈分别提取2个线索：散焦量以及匹配量。其中散焦量被定义为：</p>
<p><span class="math display">\[
D_{\alpha}(x)=\frac{1}{|W_{D}|}{\sum _{x&#39; \in W_D} {|\Delta
_x{L}_{\alpha}(x&#39;)|}},\tag{14}
\]</span></p>
<p>其中，<span
class="math inline">\(W_D\)</span>表示为当前像素领域窗口大小，<span
class="math inline">\(\Delta _x\)</span>表示水平方向拉式算子，<span
class="math inline">\(\overline{L}_{\alpha}(x)\)</span>为每个经过平均化后的重聚焦后光场图像，其表达式如下：</p>
<p><span class="math display">\[
\overline{L}_{\alpha}(x)=\frac{1}{N_{u}}\sum _{u&#39;}
{L}_{\alpha}(x,u&#39;),\tag{15}
\]</span></p>
<p>其中<span
class="math inline">\(N_{u}\)</span>表示每一个角度域内像素的数目。然后匹配量被定义成如下形式：</p>
<p><span class="math display">\[
{C}_{\alpha}(x)=\frac{1}{|W_{C}|}\sum _{x&#39; \in W_C}
{\sigma}_{\alpha}(x&#39;),\tag{16}
\]</span></p>
<p>其中，<span
class="math inline">\(W_C\)</span>表示为当前像素领域窗口大小，<span
class="math inline">\({\sigma}_{\alpha}(x)\)</span>表示每个宏像素强度的标准差，其表达式为：</p>
<p><span class="math display">\[
{\sigma}_{\alpha}(x)^2=\frac{1}{N_{u}}\sum _{u&#39;}
\left({L}_{\alpha}(x,u&#39;)-\overline{L}_{\alpha}(x)\right)^2.\tag{17}
\]</span></p>
<p>经过以上两个线索可以通过赢者通吃（Winner Takes
All，WTA）得到两张原始深度图。注意：对这两个线索使用WTA时略有不同，通过最大化空间对比度可以得到散焦线索对应的深度，最小化角度域方差能够获得匹配量对应的深度。因此二者深度可以分别表示为如下公式：</p>
<p><span class="math display">\[
\alpha ^{*}_D(x)=\mathop{\arg\max}_{\alpha} \ \ {D}_{\alpha}(x).\tag{18}
\]</span></p>
<p><span class="math display">\[
\alpha ^{*}_C(x)=\mathop{\arg\min}_{\alpha} \ \ {C}_{\alpha}(x).\tag{19}
\]</span></p>
<h3 id="置信度分析及深度融合">置信度分析及深度融合</h3>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/tao2013-confidence-analysis.png" /></p>
<p>上图中显示了两个线索随着深度层次而变化的曲线。接下来的置信度分析用<strong>主次峰值比例</strong>（Peak
Ratio）来定义每种线索的置信度，可表示为如下公式，其中的<span
class="math inline">\(\alpha ^{*2}_D(x)\)</span>以及<span
class="math inline">\(\alpha
^{*2}_C(x)\)</span>分别表示曲线的次峰值对应的深度层次。</p>
<p><span class="math display">\[
D_{conf}(x)=\frac{D_{\alpha ^{*}_D}(x)}{D_{\alpha ^{*2}_D}(x)}.\tag{20}
\]</span></p>
<p><span class="math display">\[
C_{conf}(x)=\frac{C_{\alpha ^{*}_C}(x)}{C_{\alpha ^{*2}_C}(x)}.\tag{21}
\]</span></p>
<p>接下来对原始深度进行MRF置信度融合：</p>
<p><span class="math display">\[
\begin{equation} \tag{22}
\begin{aligned}
\mathop{minimize}_{Z} \ \ &amp;\sum_{source}\lambda _{source} \sum _i
W_i|Z_i-Z_i^{source}|
\\ &amp;+\lambda _{flat} \sum _{(x,y)}\left( \left |\frac{\partial
Z_i}{\partial x}\right|_{(x,y)}+\left|\frac{\partial Z_i}{\partial
y}\right|_{(x,y)}\right)
\\ &amp;+ \lambda _{smooth} \sum _{(x,y)}|\Delta Z_i|_{(x,y)}.
\end{aligned}
\end{equation}
\]</span></p>
<p>其中，<span
class="math inline">\(source\)</span>控制着数据项，即优化后的深度要与原始深度尽量保持一致。第二项与第三项分别控制着平坦性（flatness）与平滑性（smoothness）。注意：<strong>平坦</strong>的意思是物体表面没有凹凸变化的沟壑，例如魔方任一侧面，无论是否拼好（忽略中间黑线）。而<strong>平滑</strong>则表示在平坦的基础上物体表面没有花纹，如拼好的魔方的一个侧面。另外的<span
class="math inline">\(W\)</span>是权重量，此处选用的是每个线索的置信度。</p>
<p><span class="math display">\[
\{Z_1^{source},Z_2^{source}\}=\{\alpha_C^{*},\alpha_D^{*}\}.\tag{23}
\]</span></p>
<p><span class="math display">\[
\{W_1^{source},W_2^{source}\}=\{C_{conf},D_{conf}\}.\tag{24}
\]</span></p>
<p>至此，该算法介绍完毕，其代码已经放在我的<a
target="_blank" rel="noopener" href="https://github.com/Vincentqyw/Depth-Estimation-Light-Field/tree/master/LF_DC">Github</a>。</p>
<h2 id="学习的方法">学习的方法</h2>
<p>目前而言，将深度学习应用于从双目或者单目中恢复深度已经不再新鲜，我在之前的<a
href="https://www.vincentqin.tech/posts/depth-estimation-using-deeplearning-1/">博文1</a>&amp;<a
href="https://www.vincentqin.tech/posts/depth-estimation-using-deeplearning-2/">博文2</a>中有过对这类算法的介绍。但是将其应用于光场领域进行深度估计的算法还真是寥寥无几。不过总有一些勇敢的践行者去探索如何将二者结合，以下列举几种基于学习的深度估计算法<sup id="fnref:18"><a href="#fn:18" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Johannsen, Ole, A. Sulc, and B. Goldluecke. [Variational Separation of Light Field Layers](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/18.Variational%20separation%20of%20light%20field%20layers.pdf). (2015).
">18</span></a></sup>
<sup id="fnref:19"><a href="#fn:19" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Heber, Stefan, and T. Pock. [Convolutional Networks for Shape from Light Field](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/19.Heber_Convolutional_Networks_for_CVPR_2016_paper.pdf). Computer Vision and Pattern Recognition IEEE, 2016:3746-3754.
">19</span></a></sup>
<sup id="fnref:25"><a href="#fn:25" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Johannsen O, Sulc A, Goldluecke B. [What sparse light field coding reveals about scene structure](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/25.What%20Sparse%20Light%20Field%20Coding%20Reveals%20about%20Scene%20Structure.pdf). In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016(1/3/4):3262-3270.
">25</span></a></sup>
<sup id="fnref:27"><a href="#fn:27" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Heber S, Yu W, Pock T. [U-shaped networks for shape from light field](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/27.U-shaped%20Networks%20for%20Shape%20from%20Light%20Field.pdf). British Machine Vision Conference, 2016, 37:1-12.
">27</span></a></sup>
<sup id="fnref:28"><a href="#fn:28" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Heber S, Yu W, Pock T. [Neural EPI-Volume networks for shape from light field](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/28.Neural%20EPI-volume%20Networks%20for%20Shape%20from%20Light%20Field.pdf). IEEE International Conference on Computer Vision (ICCV), IEEE Computer Society, 2017:2271-2279.
">28</span></a></sup>
<sup id="fnref:29"><a href="#fn:29" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Jeon H G, Park J, Choe G, et.al. [Depth from a Light Field Image with Learning-based Matching Costs](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/29.Depth%20from%20a%20Light%20Field%20Image%20with%20Learning-based%20Matching%20Costs.pdf). IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018.
">29</span></a></sup>
<sup id="fnref:30"><a href="#fn:30" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Shin C, Jeon H G, Yoon Y. [EPINET: A Fully-Convolutional Neural Network for Light Field Depth Estimation Using Epipolar Geometry](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/30.EPINET%20A%20fully-Convolutional%20Neural%20Network%20Using%20Epipolar%20Geometry%20for%20Depth%20from%20Light%20Field%20Images.pdf). Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
">30</span></a></sup>。</p>
<table>
<tr>
<td rowspan="5">
Learning-based<br/>
<td>
<b>Approach</b>
</td>
<td>
<b>Main Feature</b>
</td>
</tr>
<tr>
<td>
Johannsen et al. <sup><a href="#fn_18" id="reffn_18">18</a></sup>
<sup><a href="#fn_25" id="reffn_25">25</a></sup>
</td>
<td>
Sparse coding
</td>
</tr>
<tr>
<td>
Heber et al. <sup><a href="#fn_19" id="reffn_19">19</a></sup>
<sup><a href="#fn_27" id="reffn_27">27</a></sup>
<sup><a href="#fn_28" id="reffn_28">28</a></sup>
</td>
<td>
CNN-based
</td>
</tr>
<tr>
<td>
Jeon et al. <sup><a href="#fn_29" id="reffn_29">29</a></sup>
</td>
<td>
SAD, SGD, ZNCC, CT, Random Forests
</td>
</tr>
<tr>
<td>
Shin et al. <sup><a href="#fn_30" id="reffn_30">30</a></sup>
</td>
<td>
4-Directions EPIs &amp; CNN-based
</td>
</tr>
</table>
<p>在此，我将对截止目前（2018年5月29日）而言，在HCI新数据集上表现最好的<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.02379">EPINET: A Fully-Convolutional
Neural Network Using Epipolar Geometry for Depth from Light Field
Images</a><sup id="fnref:30"><a href="#fn:30" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Shin C, Jeon H G, Yoon Y. [EPINET: A Fully-Convolutional Neural Network for Light Field Depth Estimation Using Epipolar Geometry](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/30.EPINET%20A%20fully-Convolutional%20Neural%20Network%20Using%20Epipolar%20Geometry%20for%20Depth%20from%20Light%20Field%20Images.pdf). Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
">30</span></a></sup>算法进行介绍，下图为该算法在各个指标上的表现情况。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/epinet-architecture.png" /></p>
<p>算法摘要：光场相机能够同时采集空间光线的空域以及角度域信息，因此可以根据这种特性恢复出空间场景的涉深度。在本文中，作者提出了一种基于CNN的快速准确的光场深度估计算法。作者在设计网络时将光场的几何结构加入考虑，同时提出了一种新的数据增强算法以克服训练数据不足的缺陷。作者提出的算法能够在HCI
4D-LFB上在多个指标上取得Top1的成绩。作者指出，光场相机存在优势的同时也有诸多缺点，例如：基线超级短且空间&amp;角度分辨率有一定的权衡关系。目前已有很多工作去克服这些问题，这样一来，深度图像的精度提升了，但是带来的后果就是计算量超级大，无法快速地估计出深度。因此作者为了解决精度以及速度之间权衡关系设计了该算法（感觉很有意义吧）。</p>
<p>上面表格中提到的诸如Johannsen<sup id="fnref:18"><a href="#fn:18" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Johannsen, Ole, A. Sulc, and B. Goldluecke. [Variational Separation of Light Field Layers](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/18.Variational%20separation%20of%20light%20field%20layers.pdf). (2015).
">18</span></a></sup>
<sup id="fnref:25"><a href="#fn:25" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Johannsen O, Sulc A, Goldluecke B. [What sparse light field coding reveals about scene structure](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/25.What%20Sparse%20Light%20Field%20Coding%20Reveals%20about%20Scene%20Structure.pdf). In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016(1/3/4):3262-3270.
">25</span></a></sup>以及Heber<sup id="fnref:19"><a href="#fn:19" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Heber, Stefan, and T. Pock. [Convolutional Networks for Shape from Light Field](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/19.Heber_Convolutional_Networks_for_CVPR_2016_paper.pdf). Computer Vision and Pattern Recognition IEEE, 2016:3746-3754.
">19</span></a></sup>
<sup id="fnref:27"><a href="#fn:27" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Heber S, Yu W, Pock T. [U-shaped networks for shape from light field](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/27.U-shaped%20Networks%20for%20Shape%20from%20Light%20Field.pdf). British Machine Vision Conference, 2016, 37:1-12.
">27</span></a></sup>
<sup id="fnref:28"><a href="#fn:28" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Heber S, Yu W, Pock T. [Neural EPI-Volume networks for shape from light field](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/28.Neural%20EPI-volume%20Networks%20for%20Shape%20from%20Light%20Field.pdf). IEEE International Conference on Computer Vision (ICCV), IEEE Computer Society, 2017:2271-2279.
">28</span></a></sup>等人设计的算法仅仅考虑到了一个极线方向，从而容易导致低置信度的深度估计。为了解决他们算法中存在的问题，作者通过一种多流网络将不同的极线图像分别进行编码去预测深度。因为，每个极线图都有属于自己的集合特征，将这些极线图放入网络训练能够充分地利用其提供的信息。</p>
<h3 id="光场图像几何特征">光场图像几何特征</h3>
<p>由于光场图像可以等效成多个视角图像的集合，这里的视角数目通常要比传统的立体匹配算法需要的视角数目多得多。所以，如果利用全部的视角做深度估计将会相当耗时，所以在实际情况下并不需要用到全部的视角。作者的思路就是想办法尽量减少实际要使用的视角数目，所以作者探究了不同角度域方向光场图像的特征。中心视角图像与其余视角的关系可以表示成如下公式：</p>
<p><span class="math display">\[
L(x,y,0,0)=L(x+d(x,y)*u,y+d(x,y)*v,u,v),\tag{25}
\]</span></p>
<p>其中<span
class="math inline">\(d(x,y)\)</span>表示中心视角到其相应相邻视角之间的视差（disparity）。令角度方向为<span
class="math inline">\(\theta\)</span>（<span class="math inline">\(\tan
\theta=v/u\)</span>），我们可以将上式改写成如下公式：</p>
<p><span class="math display">\[
L(x,y,0,0)=L(x+d(x,y)*u,y+d(x,y)*u \tan \theta,u,u \tan \theta).\tag{26}
\]</span></p>
<p>作者选择了四个方向<span class="math inline">\(\theta\)</span>:
0<sup>o</sup>，45<sup>o</sup>，90<sup>o</sup>，135<sup>o</sup>，同时假设光场图像总视角数为<span
class="math inline">\((2N+1)\times(2N+1)\)</span>。</p>
<h3 id="网络设计">网络设计</h3>
<p>如本节开始的图所示的网络结构，该网络的开始为多路编码网络（类似于Flownet以及<a
target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~urtasun/publications/luo_etal_cvpr16.pdf">Efficient
Deep Learning for Stereo
Matching</a><sup id="fnref:32"><a href="#fn:32" rel="footnote"><span
class="hint--top hint--error hint--medium hint--rounded hint--bounce"
aria-label="Luo, Wenjie, A. G. Schwing, and R. Urtasun. [Efficient Deep Learning for Stereo Matching](https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/32.Efficient%20deep%20learning%20for%20stereo%20matching.pdf). IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society, 2016:5695-5703.
">32</span></a></sup>），其输入为4个不同方向视角图像集合，每个方向对应于一路网络，每一路都可以对其对应方向上图像进行编码提取特征。每一路网络都由3个全卷积模块组成，因为全卷积层对逐点稠密预测问题卓有成效，所以作者将每一个全卷积模块定义为这样的卷积层的集合：<strong>Conv-ReLU-Conv-BN-ReLU</strong>，这样的话就可以在局部块中预逐点预测视差。为了解决基线短的问题，作者设计了非常小的卷积核：<span
class="math inline">\(2\times 2\)</span>，同时stride =
1，这样的话就可以测量<span class="math inline">\(\pm
4\)</span>的视差。为了验证这种多路网络的有效性，作者同单路的网络做了对比试验，其结果如下表所示，可见多路网络相对于单路网络有10%的误差降低。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/viewpoints-effect.png" width="60%"></p>
<p>在完成多路编码之后，网络将这些特征串联起来组成更维度更高的特征。后面的融合网络包含8个卷积块，其目的是寻找经多路编码之后特征之间的相关性。注意除了最后一个卷积块之外，其余的卷积块全部相同。为了推断得到亚像素精度的视差图，作者将最后一个卷积块设计为<strong>Conv-ReLU-Conv</strong>结构。</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/data-augmentation.png" /></p>
<p>最后，图像增强方式包括视角偏移（从9<em>9视角中选7</em>7，可扩展3*3倍数据），图像旋转（90<sup>o</sup>，180<sup>o</sup>，270<sup>o</sup>），图像缩放（[0.25,1]），色彩值域变化（[0.5,2]），随机灰度变化，gamma变换（[0.8,1.2]）以及翻转，最终扩充了288倍。</p>
<p>以下为其各个指标上的性能表现：</p>
<p><img data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/benchmark-ranking.png" /></p>
<p>以上介绍了目前已有的深度估计算法不同类别中具有代表性的算法，它们不一定是最优的，但绝对是最容易理解其精髓的。到目前为止，光场领域已经有一大波人做深度估计的工作，利用传统的方式其精度很难再往上提高。随着深度学习的大热，已经有一批先驱开始用深度学习做深度估计，虽然在仿真数据上可以表现得很好，但实际场景千变万化，即使是深度学习的策略也不敢保证对所有的场景都有效。路漫漫其修远兮，深度估计道路阻且长。我认为以后的趋势应该是从EPI图像下手，然后利用CNN提feature（或者响应）；此时可供选择的工具有<a
target="_blank" rel="noopener" href="http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo">KITTI
Stereo</a>/<a
target="_blank" rel="noopener" href="http://hci-lightfield.iwr.uni-heidelberg.de/">HCI新数据集算法比较</a>/<a
target="_blank" rel="noopener" href="http://vision.middlebury.edu/stereo/">Middlebury
Stereo</a>中较好的几种算法，我们需要总结其算法优势并迁移到光场领域中来。GPU这个Powerful的计算工具一定要用到光场领域中来，发挥出多线程的优势。否则传统的CPU对于动辄上百兆的数据有心无力。这样一来，深度图像不仅仅可以从精度上得以提高，而且深度估计的速度也会更快。至此，本文介绍到此结束。</p>
<h1 id="references">References</h1>
<div id="footnotes">
<hr>
<div id="footnotelist">
<ol style="list-style: none; padding-left: 0; margin-left: 40px">
<li id="fn:1">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Gershun,
A.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/1.Gershun-1939-Journal_of_Mathematics_and_Physics.pdf">The
Light Field</a>. Studies in Applied Mathematics
18.1-4(1939):51–151.<a href="#fnref:1" rev="footnote">↩︎</a></span>
</li>
<li id="fn:2">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Adelson,
Edward H, and J. R. Bergen.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/2.The%20plenoptic%20function%20and%20the%20elements%20of%20early%20vision.pdf">The
plenoptic function and the elements of early vision</a>. Computational
Models of Visual Processing
(1991):3-20.<a href="#fnref:2" rev="footnote">↩︎</a></span>
</li>
<li id="fn:3">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Levoy,
Marc.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/3.Light_Field_Rendering.pdf">Light
field rendering</a>. Conference on Computer Graphics and Interactive
Techniques ACM,
1996:31-42.<a href="#fnref:3" rev="footnote">↩︎</a></span>
</li>
<li id="fn:4">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Gortler,
Steven J., et al.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/4.The%20lumigraph.pdf">The
Lumigraph</a>. Proc Siggraph
96(1996):43-54.<a href="#fnref:4" rev="footnote">↩︎</a></span>
</li>
<li id="fn:5">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Wu,
Gaochang, et al.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/5.Light-Field-Image-Processing-An%20Overview.pdf">Light
Field Image Processing: An Overview</a>. IEEE Journal of Selected Topics
in Signal Processing
PP.99(2017):1-1.<a href="#fnref:5" rev="footnote">↩︎</a></span>
</li>
<li id="fn:6">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Wanner,
Sven, and B. Goldluecke.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/6.Variational%20Light%20Field%20Analysis%20for%20Disparity%20Estimation%20and%20Super-Resolution.pdf">Variational
Light Field Analysis for Disparity Estimation and Super-Resolution</a>.
IEEE Transactions on Pattern Analysis &amp; Machine Intelligence
36.3(2013):1.<a href="#fnref:6" rev="footnote">↩︎</a></span>
</li>
<li id="fn:7">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Wang,
Ting Chun, A. A. Efros, and R. Ramamoorthi.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/7.Occlusion-aware%20Depth%20Estimation%20Using%20Light-field%20Cameras.pdf">Occlusion-Aware
Depth Estimation Using Light-Field Cameras</a>. IEEE International
Conference on Computer Vision IEEE,
2016:3487-3495.<a href="#fnref:7" rev="footnote">↩︎</a></span>
</li>
<li id="fn:8">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Jeon,
Hae Gon, et al.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/8.Accurate%20Depth%20Map%20Estimation%20from%20a%20Lenslet%20Light%20Field%20Camera.pdf">Accurate
depth map estimation from a lenslet light field camera</a>. Computer
Vision and Pattern Recognition IEEE,
2015:1547-1555.<a href="#fnref:8" rev="footnote">↩︎</a></span>
</li>
<li id="fn:9">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Yu,
Zhan, et al.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/9.Line%20Assisted%20Light%20Field%20Triangulation%20and%20Stereo%20Matching.pdf">Line
Assisted Light Field Triangulation and Stereo Matching</a>. IEEE
International Conference on Computer Vision IEEE,
2014:2792-2799.<a href="#fnref:9" rev="footnote">↩︎</a></span>
</li>
<li id="fn:10">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Heber,
Stefan, and T. Pock.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/10.Shape%20from%20Light%20Field%20meets%20Robust%20PCA.pdf">Shape
from Light Field Meets Robust PCA</a>. Computer Vision – ECCV 2014.
2014:751-767.<a href="#fnref:10" rev="footnote">↩︎</a></span>
</li>
<li id="fn:11">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Kim,
Changil, et al.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/11.scene-reconstruction-from-high-spatio-angular-resolution-light-fields-siggraph-2013-compressed-kim-et-al.pdf">Scene
reconstruction from high spatio-angular resolution light fields</a>. Acm
Transactions on Graphics
32.4(2017):1-12.<a href="#fnref:11" rev="footnote">↩︎</a></span>
</li>
<li id="fn:12">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Li,
J., M. Lu, and Z. N. Li.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/12.Continuous%20Depth%20Map%20Reconstruction%20From%20Light%20Fields.pdf">Continuous
Depth Map Reconstruction From Light Fields</a>. IEEE Transactions on
Image Processing A Publication of the IEEE Signal Processing Society
24.11(2015):3257.<a href="#fnref:12" rev="footnote">↩︎</a></span>
</li>
<li id="fn:13">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Krolla,
Bernd, et al.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/13.Spherical%20light%20field.pdf">Spherical
Light Fields</a>. British Machine Vision Conference
2014.<a href="#fnref:13" rev="footnote">↩︎</a></span>
</li>
<li id="fn:14">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Wanner,
Sven, C. Straehle, and B. Goldluecke.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/14.Globally%20consistent%20multi-label%20assignment%20on%20the%20ray%20space%20of%204D%20light%20field.pdf">Globally
Consistent Multi-label Assignment on the Ray Space of 4D Light
Fields</a>. IEEE Conference on Computer Vision and Pattern Recognition
IEEE Computer Society,
2013:1011-1018.<a href="#fnref:14" rev="footnote">↩︎</a></span>
</li>
<li id="fn:15">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Diebold,
Maximilian, B. Jahne, and A. Gatto.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/15.Heterogeneous%20Light%20Fields.pdf">Heterogeneous
Light Fields</a>. Computer Vision and Pattern Recognition IEEE,
2016:1745-1753.<a href="#fnref:15" rev="footnote">↩︎</a></span>
</li>
<li id="fn:16">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">16.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Tao,
M. W, et al.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/16.Depth%20from%20Combining%20Defocus%20and%20Correspondence%20Using%20Light-Field%20Cameras.pdf">Depth
from Combining Defocus and Correspondence Using Light-Field Cameras</a>.
IEEE International Conference on Computer Vision IEEE Computer Society,
2013:673-680.<a href="#fnref:16" rev="footnote">↩︎</a></span>
</li>
<li id="fn:17">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">17.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Tao,
Michael W., et al.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/17.Depth%20from%20Shading,%20Defocus,%20and%20Correspondence%20Using%20Light-Field%20Angular%20Coherence.pdf">Depth
from shading, defocus, and correspondence using light-field angular
coherence</a>. Computer Vision and Pattern Recognition IEEE,
2015:1940-1948.<a href="#fnref:17" rev="footnote">↩︎</a></span>
</li>
<li id="fn:18">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">18.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Johannsen,
Ole, A. Sulc, and B. Goldluecke.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/18.Variational%20separation%20of%20light%20field%20layers.pdf">Variational
Separation of Light Field Layers</a>.
(2015).<a href="#fnref:18" rev="footnote">↩︎</a></span>
</li>
<li id="fn:19">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">19.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Heber,
Stefan, and T. Pock.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/19.Heber_Convolutional_Networks_for_CVPR_2016_paper.pdf">Convolutional
Networks for Shape from Light Field</a>. Computer Vision and Pattern
Recognition IEEE,
2016:3746-3754.<a href="#fnref:19" rev="footnote">↩︎</a></span>
</li>
<li id="fn:20">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">20.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Heber,
Stefan, R. Ranftl, and T. Pock.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/20.Variational%20Shape%20from%20Light%20Field.pdf">Variational
Shape from Light Field</a>. Energy Minimization Methods in Computer
Vision and Pattern Recognition. Springer Berlin Heidelberg,
2013:66-79.<a href="#fnref:20" rev="footnote">↩︎</a></span>
</li>
<li id="fn:21">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">21.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Chen,
Can, et al.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/21.Light%20Field%20Stereo%20Matching%20Using%20Bilateral%20Statistics%20of%20Surface%20Cameras-Can_CVPR14_stereo.pdf">Light
Field Stereo Matching Using Bilateral Statistics of Surface Cameras</a>.
IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer
Society, 2014:1518-1525.<a href="#fnref:21" rev="footnote">↩︎</a></span>
</li>
<li id="fn:22">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">22.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Williem
W, Kyu P I.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/22.Williem_Robust_Light_Field_CVPR_2016_paper.pdf">Robust
light field depth estimation for noisy scene with occlusion</a>.
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR),
2016:4396-4404.<a href="#fnref:22" rev="footnote">↩︎</a></span>
</li>
<li id="fn:23">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">23.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Williem
W, Park I K, Lee K M.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/23.TPAMI2017_Williem.pdf">Robust
light field depth estimation using occlusion-noise aware data costs</a>.
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),
2017(99):1-1.<a href="#fnref:23" rev="footnote">↩︎</a></span>
</li>
<li id="fn:24">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">24.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Zhang
S, Sheng H, Li C, et al.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/24.Robust%20Depth%20Estimation%20for%20Light%20Field%20via%20Spinning%20Parallelogram%20Operator.pdf">Robust
depth estimation for light field via spinning parallelogram
operator</a>. Computer Vision and Image Understanding, 2016,
145:148-159.<a href="#fnref:24" rev="footnote">↩︎</a></span>
</li>
<li id="fn:25">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">25.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Johannsen
O, Sulc A, Goldluecke B.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/25.What%20Sparse%20Light%20Field%20Coding%20Reveals%20about%20Scene%20Structure.pdf">What
sparse light field coding reveals about scene structure</a>. In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR),
2016(1/3/4):3262-3270.<a href="#fnref:25" rev="footnote">↩︎</a></span>
</li>
<li id="fn:26">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">26.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Wanner
S, Goldluecke B.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/26.Reconstructing%20reflective%20and%20transparent%20surfaces%20from%20epipolar%20plane%20images.pdf">Reconstructing
reflective and transparent surfaces from epipolar plane images</a>. In
German Conference on Pattern Recognition (Proc. GCPR),
2013:1-10.<a href="#fnref:26" rev="footnote">↩︎</a></span>
</li>
<li id="fn:27">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">27.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Heber
S, Yu W, Pock T.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/27.U-shaped%20Networks%20for%20Shape%20from%20Light%20Field.pdf">U-shaped
networks for shape from light field</a>. British Machine Vision
Conference, 2016,
37:1-12.<a href="#fnref:27" rev="footnote">↩︎</a></span>
</li>
<li id="fn:28">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">28.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Heber
S, Yu W, Pock T.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/28.Neural%20EPI-volume%20Networks%20for%20Shape%20from%20Light%20Field.pdf">Neural
EPI-Volume networks for shape from light field</a>. IEEE International
Conference on Computer Vision (ICCV), IEEE Computer Society,
2017:2271-2279.<a href="#fnref:28" rev="footnote">↩︎</a></span>
</li>
<li id="fn:29">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">29.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Jeon
H G, Park J, Choe G, et.al.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/29.Depth%20from%20a%20Light%20Field%20Image%20with%20Learning-based%20Matching%20Costs.pdf">Depth
from a Light Field Image with Learning-based Matching Costs</a>. IEEE
Transactions on Pattern Analysis and Machine Intelligence (TPAMI),
2018.<a href="#fnref:29" rev="footnote">↩︎</a></span>
</li>
<li id="fn:30">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">30.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Shin
C, Jeon H G, Yoon Y.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/30.EPINET%20A%20fully-Convolutional%20Neural%20Network%20Using%20Epipolar%20Geometry%20for%20Depth%20from%20Light%20Field%20Images.pdf">EPINET:
A Fully-Convolutional Neural Network for Light Field Depth Estimation
Using Epipolar Geometry</a>. Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR),
2018.<a href="#fnref:30" rev="footnote">↩︎</a></span>
</li>
<li id="fn:31">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">31.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Ng,
Ren.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/31.Digital%20light%20field%20photography.pdf">Digital
light field photography</a>. 2006,
115(3):38-39.<a href="#fnref:31" rev="footnote">↩︎</a></span>
</li>
<li id="fn:32">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">32.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">Luo,
Wenjie, A. G. Schwing, and R. Urtasun.
<a target="_blank" rel="noopener" href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/32.Efficient%20deep%20learning%20for%20stereo%20matching.pdf">Efficient
Deep Learning for Stereo Matching</a>. IEEE Conference on Computer
Vision and Pattern Recognition IEEE Computer Society,
2016:5695-5703.<a href="#fnref:32" rev="footnote">↩︎</a></span>
</li>
<li id="fn:33">
<span
style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">33.</span><span
style="display: inline-block; vertical-align: top; margin-left: 10px;">S.
Wanner and B. Goldluecke.
<a target="_blank" rel="noopener" href="http://publications.lightfield-analysis.net/WG12_cvpr.pdf">Globally
Consistent Depth Labeling of 4D Light Fields</a>. 2012 IEEE Conference
on Computer Vision and Pattern Recognition, Providence, RI, USA, 2012,
pp. 41-48, doi:
10.1109/CVPR.2012.6247656.<a href="#fnref:33" rev="footnote">↩︎</a></span>
</li>
</ol>
</div>
</div>

    </div>

    
    
    
      


    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/depth-estimation/" rel="tag"># depth estimation</a>
              <a href="/tags/light-field/" rel="tag"># light field</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/cv-books/" rel="prev" title="CV Related References">
                  <i class="fa fa-chevron-left"></i> CV Related References
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/news/" rel="next" title="Think Different">
                  Think Different <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2016 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vincent Qin</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.8/pdfobject.min.js","integrity":"sha256-tu9j5pBilBQrWSDePOOajCUdz6hWsid/lBNzK4KgEPM="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.1.6/mermaid.min.js","integrity":"sha256-ZfzwelSToHk5YAcr9wbXAmWgyn9Jyq08fSLrLhZE89w="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"en-US","enable":true,"serverURL":"https://comments.vincentqin.tech","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":true,"locale":{"placeholder":"Welcome to comment"},"emoji":["https://unpkg.com/@waline/emojis@1.1.0/weibo","https://unpkg.com/@waline/emojis@1.1.0/alus","https://unpkg.com/@waline/emojis@1.1.0/bilibili","https://unpkg.com/@waline/emojis@1.1.0/qq","https://unpkg.com/@waline/emojis@1.1.0/tieba","https://unpkg.com/@waline/emojis@1.1.0/tw-emoji"],"meta":["nick","mail","link"],"requiredMeta":["nick","mail"],"wordLimit":0,"login":"enable","el":"#waline","comment":true,"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","path":"/posts/light-field-depth-estimation/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>
