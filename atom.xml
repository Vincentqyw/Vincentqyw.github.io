<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RealCat</title>
  
  <subtitle>Turn on, Tune in, Drop out</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.vincentqin.tech/"/>
  <updated>2020-04-18T10:46:13.379Z</updated>
  <id>https://www.vincentqin.tech/</id>
  
  <author>
    <name>Vincent Qin</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SuperGlue:Learning Feature Matching with Graph Neural Networks</title>
    <link href="https://www.vincentqin.tech/posts/superglue/"/>
    <id>https://www.vincentqin.tech/posts/superglue/</id>
    <published>2020-04-17T17:21:56.000Z</published>
    <updated>2020-04-18T10:46:13.379Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>ETHZ ASL与Magicleap联名之作，CVPR 2020 Oral（论文见文末），一作是来自ETHZ的实习生，二作是当年CVPR2018 SuperPoint的作者Daniel DeTone。</p><p><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/freiburg_matches.gif" alt></p><a id="more"></a><p>注：</p><ol><li>SuperPoint参见另外一篇文章<a href="https://www.vincentqin.tech/posts/superpoint/">《SuperPoint: Self-Supervised Interest Point Detection and Description》</a>，<a href="https://vincentqin.gitee.io/posts/superpoint/" target="_blank" rel="noopener">备用链接</a>。</li><li>后文中反复提到的self-attention/cross-attention，我暂时翻译成自我注意力/交叉注意力。</li><li>本人知识水平有限，如有错误请在评论区指出。当然，没有问题也可刷刷评论。</li></ol><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文提出了一种能够同时进行特征匹配以及滤除外点的网络。其中特征匹配是通过求解可微分最优化转移问题（ optimal transport problem）来解决，损失函数由GNN来构建；本文基于注意力机制提出了一种灵活的内容聚合机制，这使得SuperGlue能够同时感知潜在的3D场景以及进行特征匹配。该算法与传统的，手工设计的特征相比，能够在室内外环境中位姿估计任务中取得最好的结果，该网络能够在GPU上达到实时，预期能够集成到sfm以及slam算法中。</p><p><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_front.png" alt="superglue_front"></p><p>SuperGlue是一种特征匹配网络，它的输入是2张图像中特征点以及描述子（手工特征或者深度学习特征均可），输出是图像特征之间的匹配关系。</p><p>作者认为学习特征匹配可以被视为找到两簇点的局部分配关系。作者受到了 Transformer的启发，同时将self-以及cross-attention利用特征点位置以及其视觉外观进行匹配。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="局部特征匹配"><a href="#局部特征匹配" class="headerlink" title="局部特征匹配"></a>局部特征匹配</h3><p>传统的特征可分5步走：1)提取特征点；2)计算描述子；3)最近邻匹配；4)滤除外点；5)求解几何约束；其中滤除外点一步包括点方法有：计算最优次优比，RANSAC，交叉验证以及neighborhood consensus。</p><p>最近的一些工作主要集中在设计特异性更好的稀疏特征上，而它们的匹配仍然依赖于NN等策略，在做匹配时并没有考虑特征的结构相似性以及外观相似性。</p><h3 id="图匹配"><a href="#图匹配" class="headerlink" title="图匹配"></a>图匹配</h3><p>将特征的匹配问题描述成为“quadratic assignment problems”，这是一个NP-hard问题，求解这类问题需要复杂不切实际的算子。后来的研究者将这个问题化简成“linear assignment problems”，但仅仅用了一个浅层模型，相比之下SuperGlue利用深度神经网络构建了一种合适的代价进行求解。此处需要说明的是图匹配问题可以认为是一种“<em>optimal transport</em>”问题，<strong>它是一种有效但简单的近似解的广义线性分配，即Sinkhorn算法</strong>。</p><h3 id="Deep-learning-for-sets"><a href="#Deep-learning-for-sets" class="headerlink" title="Deep learning for sets"></a>Deep learning for sets</h3><p>类似于点云匹配的目的是通过在元素之间聚集信息来设计置换等价或不变函数。一些算法同等的对待这些元素，还有一些算法主要关注于元素的局部坐标或者特征空间。注意力机制可以通过关注特定的元素和属性来实现全局和依赖于数据的局部聚合，因而更加全面和灵活。SuperGlue借鉴了这种注意力机制。</p><h2 id="框架以及原理"><a href="#框架以及原理" class="headerlink" title="框架以及原理"></a>框架以及原理</h2><p>特征匹配必须满足的硬性要求是：i)至多有1个匹配点；ii)有些点由于遮挡等原因并没有匹配点。一个成熟的特征匹配模型应该做到：既能够找到特征之间的正确匹配，又可以鉴别错误匹配。</p><p><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_arch.png" alt="superglue_arch"></p><p>整个框架由两个主要模块组成：注意力GNN以及最优匹配层。其中注意力GNN将特征点以及描述子编码成为一个向量（该向量可以理解为特征匹配向量），随后利用自我注意力以及交叉注意力来回增强（重复$L$次）这个向量$\mathbf{f}$的特征匹配性能；随后进入最优匹配层，通过计算特征匹配向量的内积得到匹配度得分矩阵，然后通过Sinkhorn算法（迭代$T$次）解算出最优特征分配矩阵。 </p><h3 id="公式化"><a href="#公式化" class="headerlink" title="公式化"></a>公式化</h3><p>该部分对特征匹配问题建模。给定两张图片$A,B$，每张图片上都有特征点位置$\mathbf{p}$以及对应的描述子$\mathbf{d}$，所以我们经常用$(\mathbf{p},\mathbf{d})$来表示图像特征。第$i$个特征可以表示为$\mathbf{p}_i:=(x,y,c)$，其中$c$表示特征点提取置信度，$(x,y)$表示特征坐标；描述子可以表示为$\mathbf{d}_i \in \mathbb{R}^{D}$，其中$D$表示特征维度，这里的特征可以是CNN特征，如SuperPoint，或者是传统特征SIFT。假设图像$A,B$分别有$M,N$个特征，可以表示为$\mathcal{A}:=\{1, \ldots, M\}$以及$\mathcal{B}:=\{1, \ldots, N\}$。</p><p><strong>部分分配</strong>：约束i）和ii）意味着对应关系来自两组关键点之间的部分分配。我们给出一个软分配矩阵$\mathbf{P} \in[0,1]^{M \times N}$，根据上述约束，我们有如下关系：</p><script type="math/tex; mode=display">\mathbf{P} \mathbf{1}_{N} \leq \mathbf{1}_{M} \quad \text { and } \quad \mathbf{P}^{\top} \mathbf{1}_{M} \leq \mathbf{1}_{N}</script><p>那我们设计网络的目标就是解算这个分配矩阵$\mathbf{P}$。</p><h3 id="注意力GNN"><a href="#注意力GNN" class="headerlink" title="注意力GNN"></a>注意力GNN</h3><p>这里有个有意思的说法：特征点的位置以及视觉外观能够提高其特异性。另外一个具有启发性的观点是人类在寻找匹配过程是具有参考价值的。想一下人类是怎样进行特征匹配的，人类通过来回浏览两个图像试探性筛选匹配关键点，并进行来回检查（如果不是匹配的，观察以下周围有没有匹配的更好的点，直到找到匹配点/或没有匹配）。上述过程人们通过主动寻找上下文来增加自己特异性，这样可以排除一些具有奇异性的匹配。然后，本文的核心就是利用基于注意力机制的GNN实现上述过程，即模拟了人类进行特征匹配。</p><h4 id="特征点Encode"><a href="#特征点Encode" class="headerlink" title="特征点Encode"></a>特征点Encode</h4><p>首先根据上述说法，特征点位置+描述会获得更强的特征匹配特异性，所以这里将特征点的位置以及描述子合并成每个特征点$i$的初始表示$^{(0)} \mathbf{x}_{i}$，</p><script type="math/tex; mode=display">^{(0)} \mathbf{x}_{i}=\mathbf{d}_{i}+\mathbf{M L P}_{\mathrm{enc}}\left(\mathbf{p}_{i}\right)</script><p>其中MLP表示多层感知机（Multilayer Perceptron ，MLP）此处用于对低维特征升维，上式实际上是将视觉外观以及特征点位置进行了耦合，正因如此，这使得该Encode形式使得后续的注意力机制能够充分考虑到特征的外观以及位置相似度。</p><h4 id="多层GNN"><a href="#多层GNN" class="headerlink" title="多层GNN"></a>多层GNN</h4><p>考虑一个单一的完全图，它的节点是图像中每个特征点，这个图包括两种不同的无向边：一种是“Intra-image edges”（self edge）$\mathcal{E}_{\text {self }}$，它连接了来自图像内部特征点；另外一种是“Inter-image edges”（cross edge）$\mathcal{E}_{\text {cross }}$，它连接本图特征点$i$与另外一张图所有特征点（构成了该边）。</p><p>令$^{(\ell)} \mathbf{x}_{i}^{A}$表示为图像$A$上第$i$个元素在第$\ell$层的中间表达形式。信息（message）$\mathbf{m}_{\mathcal{E} \rightarrow i}$是聚合了所有特征点$\{j:(i, j) \in \mathcal{E}\}$之后点结果（它的具体形式后面的Attentional Aggregation会介绍，一句话来说就是将自我注意力以及互注意力进行聚合），其中$\mathcal{E} \in \{\mathcal{E}_{\text {self }},\mathcal{E}_{\text {self }}\}$，所以图像$A$中所有特征$i$传递更新的残差信息（residual message？）是：</p><script type="math/tex; mode=display">^{(\ell+1)} \mathbf{x}_{i}^{A}=^{(\ell)} \mathbf{x}_{i}^{A}+\operatorname{MLP}\left(\left[^{(\ell)} \mathbf{x}_{i}^{A} \| \mathbf{m}_{\mathcal{E} \rightarrow i}\right]\right)</script><p>其中$[\cdot | \cdot]$表示串联操作，同样的对图像$B$上所有特征有类似的更新形式。可以看到self 以及cross edges绑在一起并交替进行更新，先self后cross，作者提到共有固定数量的$L$层。</p><p>需要说明的是，这里的self-/cross-attention实际上就是模拟了人类来回浏览匹配的过程，其中self-attention是为了使得特征更加具有匹配特异性，而cross-attention是为了用这些具有特异性的点做图像间特征的相似度比较。</p><h4 id="Attentional-Aggregation"><a href="#Attentional-Aggregation" class="headerlink" title="Attentional Aggregation"></a>Attentional Aggregation</h4><p>文章的亮点之一就是将注意力机制用于特征匹配，这到底是如何实现的呢？作者提到，注意力机制将self以及cross信息聚合得到$\mathbf{m}_{\mathcal{E} \rightarrow i}$。其中self edge利用了self-attention[58]，cross edge利用了cross-attention。类似于数据库检索，我们想要查询$\mathbf{q}_i$基于元素的属性即键$\mathbf{k}_i$，检索到了某些元素的值$\mathbf{v}_j$。</p><script type="math/tex; mode=display">\mathbf{m}_{\mathcal{E} \rightarrow i}=\sum_{j:(i, j) \in \mathcal{E}} \alpha_{i j} \mathbf{v}_{j}</script><p>其中注意力权重${\alpha}_{ij}$是查询与检索到对象键值相似度的$\operatorname{Softmax}$即，$\alpha_{i j}=\operatorname{Softmax}_{j}\left(\mathbf{q}_{i}^{\top} \mathbf{k}_{j}\right)$。</p><p>这里需要解释一下键（key），query以及值（value）。令待查询点特征点$i$位于查询图像$Q$上，所有的源特征点位于图像$S$上，其中$(Q, S) \in\{A, B\}^{2}$，于是我们可以将key，query以及value写成下述形式：</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{q}_{i} &=\mathbf{W}_{1}^{(\ell)} \mathbf{x}_{i}^{Q}+\mathbf{b}_{1} \\\left[\begin{array}{l}\mathbf{k}_{j} \\ \mathbf{v}_{j}\end{array}\right] &=\left[\begin{array}{l}\mathbf{W}_{2} \\ \mathbf{W}_{3}\end{array}\right](\ell) \mathbf{x}_{i}^{S}+\left[\begin{array}{l}\mathbf{b}_{2} \\ \mathbf{b}_{3}\end{array}\right] \end{aligned}</script><p>每一层$\ell$都有其对应的一套投影参数，这些参数被所有的特征点共享。理解一下：此处的$\mathbf{q}_i$对应于待查询图像上某个特征点$i$的一种表示（self-attention映射），$\mathbf{k}_j$以及$\mathbf{v}_j$都是来自于召回的图像特征点$j$的一种表示（映射）；$\alpha_{i j}$表示这两个特征相似度，它是由$\mathbf{q}_i$以及$\mathbf{k}_j$计算得到（在这里体现了cross-attention的思想？），越大就表示这两个特征越相似，然后利用该相似度对$\mathbf{v}_j$加权求和得到$\mathbf{m}_{\mathcal{E} \rightarrow i}$，这就是所谓的<strong>特征聚合</strong>。</p><p>上面提到的这些概念有些难以理解，作者特意对上述过程进行了可视化，self-attention就是一张图像内部的边相连进行聚合，它能够更加关注具有特异性的所有点，且并不仅局限于其邻域位置特征（心心相依，何惧千里，逃…）；cross-attention做的就是匹配那些外观相似的两张图像见的特征。</p><p><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_4.png" alt="superglue_fig_4"></p><p><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_7.jpg" alt="superglue_fig_7"></p><p>下图展示了每层self-attention以及across-attention中权重${\alpha_{i j}}$的结果。按照匹配从难到易，文中画出了3个不同的特征点作为演示，绿色特征点（容易），蓝色特征点（中等）以及红色特征点（困难）。对于self-attention，初始时它（某个特征）关联了图像上所有的点（首行），然后逐渐地关注在与该特征相邻近的特征点（尾行）。同样地，cross-attention主要关注去匹配可能的特征点，随着层的增加，它逐渐减少匹配点集直到收敛。绿色特征点在第9层就已经趋近收敛，而红色特征直到最后才能趋紧收敛（匹配）。可以看到无论是self还是cross，它们关注的区域会随着网络层深度的增加而逐渐缩小。</p><p><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_15_1.jpg" alt="superglue_fig_15"></p><p>经过了$L$次self/cross-attention后就可以得到注意力GNN的输出，对于图像$A$我们有：</p><script type="math/tex; mode=display">\mathbf{f}_{i}^{A}=\mathbf{W} \cdot^{(L)} \mathbf{x}_{i}^{A}+\mathbf{b}, \quad \forall i \in \mathcal{A}</script><p>我们可以把$\mathbf{f}_{i}^{A}$理解为<strong>匹配描述子</strong>（类比特征描述子），专门为特征匹配服务，对于图像$B$具有类似的形式。</p><h3 id="匹配层（Optimal-matching-layer）"><a href="#匹配层（Optimal-matching-layer）" class="headerlink" title="匹配层（Optimal matching layer）"></a>匹配层（Optimal matching layer）</h3><p>接下来的任务就是去构建软分配矩阵$\mathbf{P}$。对于一般的图匹配流程，这个分配矩阵可以通过计算一个得分矩阵$\mathbf{S} \in \mathbb{R}^{M \times N}$（用来表示一些潜在的匹配）来实现。具体而言，通过最大化总体得分$\sum_{i, j} \mathbf{S}_{i, j} \mathbf{P}_{i, j}$即可得到这个分配矩阵$\mathbf{P}$，其中要注意的是$\mathbf{P}$是有约束的。</p><h4 id="匹配得分预测"><a href="#匹配得分预测" class="headerlink" title="匹配得分预测"></a>匹配得分预测</h4><p>去计算$M\times N$个潜在匹配得分是不可取的，于是作者就用GNN聚合得到的$\mathbf{f}_{i}^{A}$以及$\mathbf{f}_{i}^{B}$计算内积得到得分：</p><script type="math/tex; mode=display">\mathbf{S}_{i, j}=<\mathbf{f}_{i}^{A}, \mathbf{f}_{j}^{B}>, \forall(i, j) \in \mathcal{A} \times \mathcal{B}</script><h4 id="遮挡以及可见性"><a href="#遮挡以及可见性" class="headerlink" title="遮挡以及可见性"></a>遮挡以及可见性</h4><p>类似于SuperPoint在提取特征点时增加了一层dustbin通道，专门为了应对图像中没有特征点情况。本文借鉴了该思想，在得分矩阵$\mathbf{S}$的最后一列/行设置为dustbins可以得到$\overline{\mathbf{S}}$，这样做的作用在于可以滤出错误的匹配点。</p><script type="math/tex; mode=display">\overline{\mathbf{S}}_{i, N+1}=\overline{\mathbf{S}}_{M+1, j}=\overline{\mathbf{S}}_{M+1, N+1}=z \in \mathbb{R}</script><p>图像$A$上的特征点被分配到图像$B$上某个特征匹配或者被分配到dustbin，这就意味着每个dustbin有$N,M$个匹配，因此软分配矩阵有如下约束：</p><script type="math/tex; mode=display">\overline{\mathbf{P}} \mathbf{1}_{N+1}=\mathbf{a} \quad\text {  and } \quad \overline{\mathbf{P}}^{\top} \mathbf{1}_{M+1}=\mathbf{b}</script><p>其中$\mathbf{a}=\left[\begin{array}{ll}\mathbf{1}_{M}^{\top} &amp; N\end{array}\right]^{\top}$，$\mathbf{b}=\left[\begin{array}{ll}\mathbf{1}_{N}^{\top} &amp; M\end{array}\right]^{\top}$。（<font color="red">此处不太理解，最后一维为何为N？</font>）</p><h4 id="Sinkhorn-Algorithm"><a href="#Sinkhorn-Algorithm" class="headerlink" title="Sinkhorn Algorithm"></a>Sinkhorn Algorithm</h4><p>求解最大化总体得分可由“Sinkhorn Algorithm”[52,12]进行求解，此处并不作为重点讲解。</p><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>GNN网络以及最优匹配层都是可微的，这使得反向传播训练成为可能。网络训练使用了一种监督学习的方式，即有了匹配的真值$\mathcal{M}=\{(i, j)\} \subset \mathcal{A} \times \mathcal{B}$（如，由真值相对位姿变换得到的匹配关系），当然也可以获得有些有些没有匹配的特征点$\mathcal{I} \subseteq \mathcal{A}$以及$ \mathcal{J} \subseteq \mathcal{B}$。当给定真值标签，就可以去最小化分配矩阵$\overline{\mathbf{P}}$ 负对数似然函数：</p><script type="math/tex; mode=display">\begin{aligned} \operatorname{Loss}=&-\sum_{(i, j) \in \mathcal{M}} \log \overline{\mathbf{P}}_{i, j} \\ &-\sum_{i \in \mathcal{I}} \log \overline{\mathbf{P}}_{i, N+1}-\sum_{j \in \mathcal{J}} \log \overline{\mathbf{P}}_{M+1, j} \end{aligned}</script><p>这个监督学习的目标是同时最大化精度以及匹配的召回率，接下来的训练过程略过，直接开始实验阶段的介绍。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>特征匹配的目的是为了解算出两帧之间的相对位姿，所以实验对比的一个指标就是<strong>单应矩阵</strong>估计，另外还有室内外的位姿估计。只能说SuperGlue的效果太好了，直接放结果吧（本来论文7页就写完了，作者放了10页附录大招）。</p><h3 id="单应矩阵估计"><a href="#单应矩阵估计" class="headerlink" title="单应矩阵估计"></a>单应矩阵估计</h3><p>能够获得非常高的匹配召回率（98.3%）同时获得超高的精度，比传统的暴力匹配都好了一大截。</p><p><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_tb_1.png" alt="superglue_tb_1"></p><h3 id="室内外位姿估计"><a href="#室内外位姿估计" class="headerlink" title="室内外位姿估计"></a>室内外位姿估计</h3><p>下表看来，大基线室内位姿估计也是相当棒，完胜传统算法。</p><p><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_tb_2.png" alt="superglue_tb_2"></p><p><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_tb_3.png" alt="superglue_tb_3"></p><h3 id="网络耗时"><a href="#网络耗时" class="headerlink" title="网络耗时"></a>网络耗时</h3><p>接下来放出大家比较关心的网络耗时，下图是在NVIDIA GeForce GTX 1080 GPU跑了500次的结果，512个点69ms（14.5fps），1024个点87ms（11.5fps）。</p><p><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_11.png" alt="superglue_tb_3"></p><h3 id="更多匹配结果"><a href="#更多匹配结果" class="headerlink" title="更多匹配结果"></a>更多匹配结果</h3><p>第一列是SuperPoint+暴力匹配结果，第二列是SuperPoint+OAnet（ICCV 2019）结果，第三列是SuperPoint+SuperGlue结果。能看到SuperGlue惊人的特征匹配能力，尤其是在大视角变化时优势明显（红线表示错误匹配，绿线表示正确匹配）。</p><p><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_res_3.jpg" alt></p><p><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_res_1.jpg" alt></p><p><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_res_2.jpg" alt></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文展示了基于注意力的图神经网络对局部特征匹配的强大功能。 SuperGlue的框架使用两种注意力：（i）自我注意力，可以增强局部描述符的接受力；以及（ii）交叉注意力，可以实现跨图像交流，并受到人类来回观察方式的启发进行匹配图像。文中方法通过解决<strong>最优运输问题</strong>，优雅地处理了特征分配问题以及遮挡点。实验表明，SuperGlue与现有方法相比有了显着改进，可以在极宽的基线室内和室外图像对上进行高精度的相对姿势估计。此外，SuperGlue可以实时运行，并且可以同时使用经典功能和学习功能。</p><p>总而言之，论文提出的可学习的中后端（middle-end）算法以功能强大的神经网络模型替代了手工启发式技术，该模型同时在单个统一体系结构中执行上下文聚合，匹配和过滤外点。作者最后提到：若与深度学习前端结合使用，SuperGlue是迈向端到端深度学习SLAM的重要里程碑。（when combined with a deep front-end, SuperGlue is a major milestone towards end-to-end deep SLAM）</p><p>这真是鼓舞SLAM研究人员的士气！</p><h2 id="附件"><a href="#附件" class="headerlink" title="附件"></a>附件</h2><ul><li><a href="https://github.com/magicleap/SuperGluePretrainedNetwork" target="_blank" rel="noopener">SuperGlue Github地址</a></li><li><a href="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/SuperGlue.pdf" target="_blank" rel="noopener">SuperGlue Paper</a></li><li>*<a href="https://image-matching-workshop.github.io/" target="_blank" rel="noopener">Image Matching: Local Features &amp; Beyond CVPR 2020 Workshop</a></li></ul><hr><p><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_01.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_02.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_03.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_04.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_05.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_06.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_07.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_08.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_09.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_10.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_11.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_12.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_13.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_14.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_15.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_16.png" alt><br><img src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_17.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ETHZ ASL与Magicleap联名之作，CVPR 2020 Oral（论文见文末），一作是来自ETHZ的实习生，二作是当年CVPR2018 SuperPoint的作者Daniel DeTone。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/freiburg_matches.gif&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="CV" scheme="https://www.vincentqin.tech/categories/CV/"/>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="Deep Learning" scheme="https://www.vincentqin.tech/tags/Deep-Learning/"/>
    
      <category term="特征提取" scheme="https://www.vincentqin.tech/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
      <category term="MagicLeap" scheme="https://www.vincentqin.tech/tags/MagicLeap/"/>
    
      <category term="深度学习" scheme="https://www.vincentqin.tech/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SuperGlue" scheme="https://www.vincentqin.tech/tags/SuperGlue/"/>
    
  </entry>
  
  <entry>
    <title>SLAM常见问题(五)：Singular Value Decomposition（SVD）分解</title>
    <link href="https://www.vincentqin.tech/posts/slam-common-issues-SVD/"/>
    <id>https://www.vincentqin.tech/posts/slam-common-issues-SVD/</id>
    <published>2019-08-18T11:12:28.000Z</published>
    <updated>2020-03-31T14:57:52.667Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>SVD分解就是一种矩阵拆解术，它能够把<strong>任意</strong>矩阵$A \in \mathbb{R}^{m \times n}$拆解成3个矩阵的乘积形式，即：</p><script type="math/tex; mode=display">A = U \Sigma V^T</script><p>其中，$U \in \mathbb{R}^{m \times m}$，$V \in \mathbb{R}^{n \times n}$都是正交矩阵，即列向量是正交的单位向量，$\Sigma \in \mathbb{R}^{m \times n}$的对角阵（奇异值）。搬运了来自MIT OpenCourseWare的在线课程并放在了B站，讲解得很清晰。</p><a id="more"></a><!-- <div id="dplayer2" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer2"),"loop":true,"video":{"url":"http://45.76.197.98:888/api/public/dl/0hkAga3Z/Singular-Value-Decomposition.mp4"},"danmaku":{"id":"bbe4286bf164ef6w1497f18a7b42ff944e6r4b821","api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> --><iframe src="//player.bilibili.com/player.html?aid=93275447&cid=159253510&page=15" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>刚才说了矩阵$U, \Sigma, V$的形式，视频中还提到了这三个矩阵的物理意义，即SVD分解可以理解为：任意矩阵都可以分解为<strong>(rotation)*(Stretch)*(rotation)</strong>的形式。接下来说明一下这三个矩阵是如何来的。</p><h2 id="计算-A-TA"><a href="#计算-A-TA" class="headerlink" title="计算 $A^TA$"></a>计算 $A^TA$</h2><script type="math/tex; mode=display">A^TA = (U \Sigma V^T)^TU \Sigma V^T = V{\Sigma}^TU^TU \Sigma V^T = V{\Sigma}^T \Sigma V^T</script><p>可见，$V$正是矩阵$A^TA$的特征向量，而${\Sigma}^T \Sigma $为矩阵$A^TA$的特征值。</p><h2 id="计算-AA-T"><a href="#计算-AA-T" class="headerlink" title="计算 $AA^T$"></a>计算 $AA^T$</h2><script type="math/tex; mode=display">AA^T = U \Sigma V^T(U \Sigma V^T)^T = U \Sigma V^TV{\Sigma}^TU^TU \Sigma V^T = U{\Sigma}^T \Sigma U^T</script><p>可见，$U$正是矩阵$AA^T$的特征向量，而${\Sigma}^T \Sigma $为矩阵$A^TA$的特征值。</p><p>所以$U, \Sigma, V$都可以通过上述方式来计算。</p><h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><script type="math/tex; mode=display">\Sigma = \left[    \begin{array}    {cccc|cccc}    {\sigma_{1}} & {0} & {\dots} & {0} & {0} & {0} & {\dots} & {0} \\     {0} & {\sigma_{2}} & {\dots} & {0} & {0} & {0} & {\dots} & {0} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {0} & {0} & {\dots} & {0} \\    {0} & {0} & {} & {\sigma_{k}} & {0} & {0} & {\dots} & {0} \\    \hline     {0} & {0} & {\dots} & {0} & {0} & {0} & {\dots} & {0} \\    {\vdots} & {\vdots} & {} & {\vdots} & {\vdots} & {\vdots} & {} & {\vdots} \\    {0} & {0} & {\dots} & {0} & {0} & {0} & {\dots} & {0}    \end{array}\right]_{m \times n}\Rightarrow\left[    \begin{array}    {cccc}    {\sigma_{1}} & {0} & {\dots} & {0} \\     {0} & {\sigma_{2}} & {\dots} & {0}  \\     {\vdots} & {\vdots} & {\ddots} & {\vdots}  \\    {0} & {0} & {} & {\sigma_{k}}    \end{array}\right]_{k \times k}</script><p>其中${\sigma}_1 \geq {\sigma}_2 \geq … {\sigma}_k &gt; 0 $，将$\Sigma$中主对角线为0的部分删去，同样的$U,V$对应的部分删去，SVD分解就变成了下图的形式。<br><img src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-SVD/svd.png" alt></p><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><h3 id="数字例子"><a href="#数字例子" class="headerlink" title="数字例子"></a>数字例子</h3><p>有矩阵A，对其进行SVD分解，已知：</p><script type="math/tex; mode=display">A = \left[\begin{matrix}​    1 & 4 & 3 & 5 & 6  \cr ​    2 & 3 & {4} & 5 & 0  \cr ​    7 & 4 & 0 & 9 & 1  \cr  \end{matrix}\right]</script><p>计算$A^TA$以及$AA^T$：</p><script type="math/tex; mode=display">A^TA = \left[\begin{matrix}​    {54} & {38} & {11} & {78} & {13}  \cr ​    {38} & {41} & {24} & {71} & {28}  \cr ​    {11} & {24} & {25} & {35} & {18}  \cr ​    {78} & {71} & {35} & {131} & {39}  \cr ​    {13} & {28} & {18} & {39} & {37} \end{matrix}\right]\\A^TA = \left[\begin{matrix}​    {87} & {51} & {74}   \cr ​    {51} & {54} & {71}   \cr ​    {74} & {71} & {147}   \cr \end{matrix}\right]</script><p>对以上两式做特征值分解得到：</p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">V <span class="built_in">=</span></span><br><span class="line">    <span class="number">0.4269</span>    <span class="number">0.5222</span>    <span class="number">0.1760</span>   -<span class="number">0.5292</span>   -<span class="number">0.4839</span></span><br><span class="line">    <span class="number">0.4087</span>   -<span class="number">0.1757</span>   -<span class="number">0.0655</span>   -<span class="number">0.5258</span>    <span class="number">0.7221</span></span><br><span class="line">    <span class="number">0.2100</span>   -<span class="number">0.4474</span>   -<span class="number">0.7536</span>   -<span class="number">0.1512</span>   -<span class="number">0.4062</span></span><br><span class="line">    <span class="number">0.7389</span>    <span class="number">0.1520</span>   -<span class="number">0.0603</span>    <span class="number">0.6481</span>    <span class="number">0.0853</span></span><br><span class="line">    <span class="number">0.2464</span>   -<span class="number">0.6879</span>    <span class="number">0.6271</span>   -<span class="number">0.0258</span>   -<span class="number">0.2687</span></span><br><span class="line"></span><br><span class="line">U <span class="built_in">=</span></span><br><span class="line">    <span class="number">0.5095</span>    <span class="number">0.7999</span>    <span class="number">0.3171</span></span><br><span class="line">    <span class="number">0.4285</span>    <span class="number">0.0838</span>   -<span class="number">0.8997</span></span><br><span class="line">    <span class="number">0.7462</span>   -<span class="number">0.5942</span>    <span class="number">0.3001</span></span><br></pre></td></tr></table></figure><p>奇异值$\Sigma ^T \Sigma = \text{Diag}(238.2878, 37.3715, 12.3407) \Rightarrow \Sigma = \text{Diag}(15.4366, 6.1132, 3.5129)$</p><p>这与直接调用<code>svd(A)</code>结果是一致的（可能差个正负号）。</p><h3 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h3><p>祭上亲爱的Battle Angel Alita。</p><p><img src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-SVD/alita_origin.jpg" alt></p><p>原始图像尺寸$1440\times 2560 $，我们可以对该图像做SVD分解，然后仅保留奇异值的前10，50，100重构图像，比较重构图像与原始图像的质量差异。可见仅仅保留其前10个奇异值时，图像质量遭到了极大破坏（此时仅保留原始图像信息的58.864%），随着奇异值数量的增多，图像质量也会逐渐提升，可以看到当奇异值个数为100时，基本上已经看不出与原图的差异（此时仅保留原始图像信息的87.37%）。由此，我们实现了图像压缩。</p><p><img src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-SVD/alita_svd1.jpg" alt></p><p>下图是保留的奇异值数量与图像质量的关系图，保留的奇异值越多，图像质量越高，图像压缩效果越不明显；反之，奇异值越少，图像质量越差，图像压缩效果越明显。这只是一种非常简单的图像压缩算法，仅作原理验证使用，在实际中用到的概率不是很大。</p><p><img src="//www.vincentqin.tech/posts/slam-common-issues-SVD/alita_svd_quality.svg" alt></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% simple test of using SVD decomposistion</span></span><br><span class="line">clear <span class="built_in">all</span>;</span><br><span class="line">close <span class="built_in">all</span>;</span><br><span class="line">clc;</span><br><span class="line"></span><br><span class="line"><span class="comment">% A = [1 4 3 5 6;</span></span><br><span class="line"><span class="comment">%          2 3 4 5 0;</span></span><br><span class="line"><span class="comment">%          7 4 0 9 1]</span></span><br><span class="line"><span class="comment">% A'*A;</span></span><br><span class="line"><span class="comment">% A*A';</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% [V,Dv] = eig(A'*A);</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% lambda = wrev(diag(Dv));</span></span><br><span class="line"><span class="comment">% V = fliplr(V)</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% [U,Du] = eig(A*A');</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% lambda = wrev(diag(Du));</span></span><br><span class="line"><span class="comment">% U = fliplr(U)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% LOADING IMAGE</span></span><br><span class="line">img <span class="built_in">=</span> imread(<span class="string">'alita_origin.png'</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ENERGE <span class="built_in">=</span> <span class="number">0</span>;</span><br><span class="line">for i <span class="built_in">=</span> <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">    [U(:,:,i) D(:,:,i) V(:,:,i)] <span class="built_in">=</span> svd(double(img(:,:,i)))  ;</span><br><span class="line">    ENERGE <span class="built_in">=</span> ENERGE +sum(diag(D(:,:,i)));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"><span class="comment">%% 10</span></span><br><span class="line">DIM <span class="built_in">=</span> <span class="number">10</span>;</span><br><span class="line">ENERGE10 <span class="built_in">=</span> <span class="number">0</span>;</span><br><span class="line">for i <span class="built_in">=</span> <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">    img_recons10(:,:,i) <span class="built_in">=</span> U(:,<span class="number">1</span>:DIM,i)*D(<span class="number">1</span>:DIM,<span class="number">1</span>:DIM,i)*V(:,<span class="number">1</span>:DIM,i)<span class="string">';</span></span><br><span class="line"><span class="string">     ENERGE10 = ENERGE10 +sum(diag(D(1:DIM,1:DIM,i)));</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">% figure;</span></span><br><span class="line"><span class="string">% imshow(mat2gray(img_recons10))</span></span><br><span class="line"><span class="string">% imwrite(mat2gray(img_recons10),'</span>alita_10.png<span class="string">');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">%% 50</span></span><br><span class="line"><span class="string">DIM = 50;</span></span><br><span class="line"><span class="string">ENERGE50 = 0;</span></span><br><span class="line"><span class="string">for i = 1:3</span></span><br><span class="line"><span class="string">    img_recons50(:,:,i) = U(:,1:DIM,i)*D(1:DIM,1:DIM,i)*V(:,1:DIM,i)'</span>;</span><br><span class="line">     ENERGE50 <span class="built_in">=</span> ENERGE50 +sum(diag(D(<span class="number">1</span>:DIM,<span class="number">1</span>:DIM,i)));</span><br><span class="line">end</span><br><span class="line"><span class="comment">% figure;</span></span><br><span class="line"><span class="comment">% imshow(mat2gray(img_recons50))</span></span><br><span class="line"><span class="comment">% imwrite(mat2gray(img_recons50),'alita_50.png');</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 100</span></span><br><span class="line">DIM <span class="built_in">=</span> <span class="number">100</span>;</span><br><span class="line">ENERGE100 <span class="built_in">=</span> <span class="number">0</span>;</span><br><span class="line">for i <span class="built_in">=</span> <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">    img_recons100(:,:,i) <span class="built_in">=</span> U(:,<span class="number">1</span>:DIM,i)*D(<span class="number">1</span>:DIM,<span class="number">1</span>:DIM,i)*V(:,<span class="number">1</span>:DIM,i)<span class="string">';</span></span><br><span class="line"><span class="string">    ENERGE100 = ENERGE100 +sum(diag(D(1:DIM,1:DIM,i)));</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">% figure;</span></span><br><span class="line"><span class="string">% imshow(mat2gray(img_recons100))</span></span><br><span class="line"><span class="string">% imwrite(mat2gray(img_recons100),'</span>alita_100.png<span class="string">');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">figure;</span></span><br><span class="line"><span class="string">set(gcf,'</span>pos<span class="string">',[ 986 414 1274 826])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">FONTSIZE = 15;</span></span><br><span class="line"><span class="string">h(1) = subplot(221);imshow(mat2gray(img)); </span></span><br><span class="line"><span class="string">xlabel('</span>origin Alita<span class="string">');set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">h(2) = subplot(222);imshow(mat2gray(img_recons10));</span></span><br><span class="line"><span class="string">xlabel(['</span>Using <span class="number">10</span> singular values: <span class="string">' num2str(ENERGE10/ENERGE)]);set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">h(3) = subplot(223);imshow(mat2gray(img_recons50));</span></span><br><span class="line"><span class="string">xlabel(['</span>Using <span class="number">50</span> singular values: <span class="string">' num2str(ENERGE50/ENERGE)]);set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">h(4) = subplot(224);imshow(mat2gray(img_recons100));</span></span><br><span class="line"><span class="string">xlabel(['</span>Using <span class="number">100</span> singular values: <span class="string">' num2str(ENERGE100/ENERGE)]);set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string">set(gcf,'</span>color<span class="string">',[1 1 1])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">%% SHOW ENERGY</span></span><br><span class="line"><span class="string">ENERGY_tmp = zeros(size(img,1),1);</span></span><br><span class="line"><span class="string">for DIM_ = 1:size(img,1)</span></span><br><span class="line"><span class="string">   for i = 1:3</span></span><br><span class="line"><span class="string">     ENERGY_tmp(DIM_,1) = ENERGY_tmp(DIM_,1) +sum(diag(D(1:DIM_,1:DIM_,i)));</span></span><br><span class="line"><span class="string">   end</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">figure;</span></span><br><span class="line"><span class="string">FONTSIZE = 30;</span></span><br><span class="line"><span class="string">ratio = ENERGY_tmp/ENERGE;</span></span><br><span class="line"><span class="string">X =   1:size(img,1);</span></span><br><span class="line"><span class="string">plot(X,ratio,'</span>linewidth<span class="string">',5,'</span>color<span class="string">','</span>r<span class="string">');</span></span><br><span class="line"><span class="string">set(gcf,'</span>color<span class="string">',[1 1 1])</span></span><br><span class="line"><span class="string">xlabel('</span>Number of Singular values<span class="string">');</span></span><br><span class="line"><span class="string">ylabel('</span>Image Quality<span class="string">');</span></span><br><span class="line"><span class="string">set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string">set(gcf,'</span>pos<span class="string">',[ 986 414 1274 826])</span></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="http://www-users.math.umn.edu/~lerman/math5467/svd.pdf" target="_blank" rel="noopener">A Singularly Valuable Decomposition The SVD of a Matrix</a></li><li>李宏毅关于SVD的介绍，<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/LA_2018/Lecture/SVD.pdf" target="_blank" rel="noopener">PPT</a>,<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_LA18.html" target="_blank" rel="noopener">课程列表</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SVD分解就是一种矩阵拆解术，它能够把&lt;strong&gt;任意&lt;/strong&gt;矩阵$A \in \mathbb{R}^{m \times n}$拆解成3个矩阵的乘积形式，即：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
A = U \Sigma V^T&lt;/script&gt;&lt;p&gt;其中，$U \in \mathbb{R}^{m \times m}$，$V \in \mathbb{R}^{n \times n}$都是正交矩阵，即列向量是正交的单位向量，$\Sigma \in \mathbb{R}^{m \times n}$的对角阵（奇异值）。搬运了来自MIT OpenCourseWare的在线课程并放在了B站，讲解得很清晰。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="SVD" scheme="https://www.vincentqin.tech/tags/SVD/"/>
    
      <category term="位姿" scheme="https://www.vincentqin.tech/tags/%E4%BD%8D%E5%A7%BF/"/>
    
  </entry>
  
  <entry>
    <title>SLAM常见问题(四)：求解ICP，利用SVD分解得到旋转矩阵</title>
    <link href="https://www.vincentqin.tech/posts/slam-common-issues-ICP/"/>
    <id>https://www.vincentqin.tech/posts/slam-common-issues-ICP/</id>
    <published>2019-08-18T03:43:04.000Z</published>
    <updated>2020-03-31T15:05:16.626Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>今天讲一篇关于利用<code>SVD</code>方法求解<code>ICP</code>问题的文献<a href="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-ICP/svd_rot.pdf" target="_blank" rel="noopener">《Least-Squares Rigid Motion Using SVD》</a>，这篇文章非常精彩地推导出将$3D$点对齐问题的解析解，同时总结了求解该问题的统一范式。</p><a id="more"></a><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>已知<script type="math/tex">{\mathcal{P}=\left\{\mathbf{p}_{1}, \mathbf{p}_{2}, \ldots, \mathbf{p}_{n}\right\}}</script>以及<script type="math/tex">{\mathcal{Q}=\left\{\mathbf{q}_{1}, \mathbf{q}_{2}, \ldots, \mathbf{q}_{n}\right\}}</script>是空间中（文中说的更加普适，<script type="math/tex">\mathbf{p}_i , \mathbf{q}_i \in \mathbb{R}^{d}</script>，可以表示$d$维空间）的匹配点集，我们试图找到这样的旋转矩阵$R$和平移向量$\mathbf{t}$最小化如下对齐误差（即<code>ICP</code>问题的形式）：</p><script type="math/tex; mode=display">(R, \mathbf{t})=\underset{R \in S O(d), \mathbf{t} \in \mathbb{R}^{d}}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left\|\left(R \mathbf{p}_{i}+\mathbf{t}\right)-\mathbf{q}_{i}\right\|^{2} \tag{1}</script><p>接下来文章分别推导了平移向量$\mathbf{t}$以及旋转矩阵$R$的解析解。</p><h2 id="计算平移量"><a href="#计算平移量" class="headerlink" title="计算平移量"></a>计算平移量</h2><p>此时假定旋转矩阵$R$是固定的，令<script type="math/tex">F(\mathbf{t}) = \sum_{i=1}^{n} w_{i}\left\|\left(R \mathbf{p}_{i}+\mathbf{t}\right)-\mathbf{q}_{i}\right\|^{2}</script>，我们可以通过$F$对$\mathbf{t}$求导的方式得到平移量的最优解，如下：</p><script type="math/tex; mode=display">\begin{aligned} 0 &=\frac{\partial F}{\partial \mathbf{t}}=\sum_{i=1}^{n} 2 w_{i}\left(R \mathbf{p}_{i}+\mathbf{t}-\mathbf{q}_{i}\right)=\\ &=2 \mathbf{t}\left(\sum_{i=1}^{n} w_{i}\right)+2 R\left(\sum_{i=1}^{n} w_{i} \mathbf{p}_{i}\right)-2 \sum_{i=1}^{n} w_{i} \mathbf{q}_{i}  \end{aligned} \tag{2}</script><p>令：</p><script type="math/tex; mode=display">\overline{\mathbf{p}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{p}_{i}}{\sum_{i=1}^{n} w_{i}},  \overline{\mathbf{q}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{q}_{i}}{\sum_{i=1}^{n} w_{i}} \tag{3}</script><p>于是我们得到$\mathbf{t}$的解：</p><script type="math/tex; mode=display">\mathbf{t} = \overline{\mathbf{q}} - R\overline{\mathbf{p}} \tag{4}</script><p>从上式看出最优的平移量$\mathbf{t}$将$\mathcal{P}$点集的加权中心映射到了$\mathcal{Q}$点集的中心。接下来将上式带入优化方程，得：</p><script type="math/tex; mode=display">\begin{aligned}\sum_{i=1}^{n} w_{i}\left\|\left(R \mathbf{p}_{i}+\mathbf{t}\right)-\mathbf{q}_{i}\right\|^{2} &= \sum_{i=1}^{n} w_{i}\left\| R \mathbf{p}_{i}+ \overline{\mathbf{q}} - R\overline{\mathbf{p}} -\mathbf{q}_{i}\right\|^{2}  \\ &= \sum_{i=1}^{n} w_{i}\left\|R (\mathbf{p}_{i} -\overline{\mathbf{p}}) - (\mathbf{q}_{i} - \overline{\mathbf{q}} ) \right\|^{2}\end{aligned} \tag{5}</script><p>由此我们将原问题转换成了无平移量的优化问题，令：</p><script type="math/tex; mode=display">\mathbf{x}_i := \mathbf{p}_{i} -\overline{\mathbf{p}}，\mathbf{y}_i := \mathbf{q}_{i} -\overline{\mathbf{q}}，\tag{6}</script><p>我们把问题简写成如下形式：</p><script type="math/tex; mode=display">R = \underset{R \in S O(d)}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2} \tag{7}</script><h2 id="计算旋转量"><a href="#计算旋转量" class="headerlink" title="计算旋转量"></a>计算旋转量</h2><p>简化上式：</p><script type="math/tex; mode=display">\begin{aligned}\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2} &= \left( R\mathbf{x}_i - \mathbf{y}_i\right)^T\left( R\mathbf{x}_i - \mathbf{y}_i\right)  = \left( \mathbf{x}_i^TR^T - \mathbf{y}_i^T\right)\left( R\mathbf{x}_i - \mathbf{y}_i\right)  \\&= \mathbf{x}_i^TR^TR\mathbf{x}_i - \mathbf{x}_i^TR^T\mathbf{y}_i - \mathbf{y}_i^TR\mathbf{x}_i +\mathbf{y}_i^T\mathbf{y}_i \end{aligned}\tag{8}</script><p>又因为旋转矩阵的正交性：$R^TR=I$；另外$ \mathbf{x}_i^TR^T\mathbf{y}_i$是标量：$\mathbf{x}_i$维度为$1 \times d$，$R^T$维度为$d \times d$，$\mathbf{y}_i$维度为$d \times 1$。于是有下式：</p><script type="math/tex; mode=display"> \mathbf{x}_i^TR^T\mathbf{y}_i = (\mathbf{x}_i^TR^T\mathbf{y}_i)^T = \mathbf{y}_i^TR\mathbf{x}_i \tag{9}</script><p>得：</p><script type="math/tex; mode=display">\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2} = \mathbf{x}_i^T\mathbf{x}_i -  2\mathbf{y}_i^TR\mathbf{x}_i +\mathbf{y}_i^T\mathbf{y}_i  \tag{10}</script><p>将整理好的上式带入简化后的$R$优化问题，得：</p><script type="math/tex; mode=display">\begin{aligned} & \underset{R \in S O(d)}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2}=\underset{R \in S O(d)}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left(\mathbf{x}_{i}^{\top} \mathbf{x}_{i}-2 \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}+\mathbf{y}_{i}^{\top} \mathbf{y}_{i}\right)=\\=& \underset{R \in S O(d)}{\operatorname{argmin}}\left(\sum_{i=1}^{n} w_{i} \mathbf{x}_{i}^{\top} \mathbf{x}_{i}-2 \sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}+\sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} \mathbf{y}_{i}\right)=\\=& \operatorname{argmin}_{R \in S O(d)}\left(-2 \sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}\right) \end{aligned}\tag{11}</script><p>接下来将要利用到如下关于迹的技巧:</p><script type="math/tex; mode=display">\begin{aligned}\left[\begin{array}{cccc}{w_1} \\ {} & {w_1} & {} &{} \\ {} & {} & {\ddots} & {}\\ {} & {} & {} & {w_n}\end{array}\right]\left[\begin{array}{ccc}{—}& {\mathbf{y}_1^T}&{—}   \\ {—}& {\mathbf{y}_2^T}&{—}   \\ {—}  & {\vdots} & {—}\\ {—}& {\mathbf{y}_n^T}&{—}\end{array}\right]\left[\begin{array}{ccc}{}& {} &{}   \\ {}& {R} &{}   \\ {}& {} &{}   \\ \end{array}\right]\left[\begin{array}{cccc}{|}& {|} &{|} &{|}  \\ {\mathbf{x}_1}& {\mathbf{x}_2} &{\dots} &{\mathbf{x}_n}  \\ {|}& {|} &{|} &{|}  \\ \end{array}\right]  \\=\left[\begin{array}{ccc}{—}& {w_1\mathbf{y}_1^T}&{—}   \\ {—}& {w_2\mathbf{y}_2^T}&{—}   \\ {—}  & {\vdots} & {—}\\ {—}& {w_n\mathbf{y}_n^T}&{—}\end{array}\right]\left[\begin{array}{cccc}{|}& {|} &{|} &{|}  \\ {R\mathbf{x}_1}& {R\mathbf{x}_2} &{\dots} &{R\mathbf{x}_n}  \\ {|}& {|} &{|} &{|}  \\ \end{array}\right] \\=\left[\begin{array}{cccc}{w_1\mathbf{y}_1^TR\mathbf{x}_1}& {} &{} &{*}  \\ {}& {w_2\mathbf{y}_2^TR\mathbf{x}_2} &{} &{}  \\ {}& {} &{\ddots} &{}  \\ {*}& {} &{} &{w_n\mathbf{y}_n^TR\mathbf{x}_n} \\ \end{array}\right]\end{aligned}</script><p>上式就是对<script type="math/tex">\sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i} =  \operatorname{tr}\left( WY^TRX\right)</script>的完美解释。</p><p>利用上式，式$(11)$可以整理得：</p><script type="math/tex; mode=display">\begin{aligned}\underset{R \in S O(d)}{\operatorname{argmin}}\left(-2 \sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}\right) &= \underset{R \in S O(d)}{\operatorname{argmax}}\left(\sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}\right) \\&= \underset{R \in S O(d)}{\operatorname{argmax}} \operatorname{tr}\left( WY^TRX\right)\end{aligned}\tag{12}</script><p>这里说明一下维度：$W = diag(w_1,w_2,…,w_n)$维度为$n \times n$，$Y^T$维度为$n \times d$，$R$维度为$d \times d$，$X$维度为$d \times n$。</p><p>接下来回顾一下迹的性质：$\operatorname{tr}(AB) = \operatorname{tr}(BA)$，因此有下式：</p><script type="math/tex; mode=display">\operatorname{tr}\left( WY^TRX\right) = \operatorname{tr}\left( (WY^T)(RX)\right) =\operatorname{tr}\left( RXWY^T\right) \tag{13}</script><p>令$d\times d$的“covariance”矩阵$S = XWY^T$，求$S$的<code>SVD</code>分解：</p><script type="math/tex; mode=display">S= U\Sigma V^T.\tag{14}</script><p>于是式$(13)$变为：</p><script type="math/tex; mode=display">\operatorname{tr}\left( WY^TRX\right) =\operatorname{tr}\left( RS\right) =\operatorname{tr}\left( RU\Sigma V^T\right)=\operatorname{tr}\left( \Sigma V^TRU\right) \tag{15}</script><p>由于$V,T,R$均为正交矩阵，因此$M = V^TRU$也是正交阵，也就是说$M$的列向量$\mathbf{m}_j$是互相正交的单位向量，即$\mathbf{m}_j^T\mathbf{m}_j=1$，于是：</p><script type="math/tex; mode=display">1=\mathbf{m}_{j}^{\top} \mathbf{m}_{j}=\sum_{i=1}^{d} m_{i j}^{2} \Rightarrow m_{i j}^{2} \leq 1 \Rightarrow\left|m_{i j}\right| \leq 1 \tag{16}</script><p>由于<code>SVD</code>分解的性质可知$\sigma$的元素均为非负数：${\sigma}_1,{\sigma}_2,{\sigma}_d \geq 0$，于是式$(18)$变为如下形式：</p><script type="math/tex; mode=display">\operatorname{tr}(\Sigma M)=\left(\begin{array}{ccccc}{\sigma_{1}} & {} & {} & {} & {} \\ {} & {\sigma_{2}} & {} & {} & {} \\ {} & {} & {\ddots} & {} & {} \\ {} & {} & {} & {} & {\sigma_{d}}\end{array}\right)\left(\begin{array}{cccc}{m_{11}} & {m_{12}} & {\dots} & {m_{1 d}} \\ {m_{21}} & {m_{22}} & {\dots} & {m_{2 d}} \\ {\vdots} & {\vdots} & {\vdots} & {\vdots} \\ {m_{d 1}} & {m_{d 2}} & {\dots} & {m_{d d}}\end{array}\right)=\sum_{i=1}^{d} \sigma_{i} m_{i i} \leq \sum_{i=1}^{d} \sigma_{i} \tag{17}</script><p>可见，当迹最大时$m_{ii} = 1 $，又由于$M$是正交阵，这使得$M$为单位阵！</p><script type="math/tex; mode=display">I = M = V^TRU \Rightarrow R = VU^T \tag{18}</script><p>看到没，R的解析解竟然如此简单，并且与<code>SVD</code>分解产生了联系，让人感觉到了数学的美妙。不过到这里还没完，后面作者进行了一步方向矫正，大意是这样的：利用公式$(18)$得到的矩阵并不一定是一个旋转矩阵，也可能为<code>反射矩阵</code>，此时可以通过验证$VU^T$的行列式来判断到底是旋转（行列式 = 1）还是反射（行列式 = -1）。但我们要求的是旋转矩阵，这时需要对公式$(18)$进行一步处理。</p><p>假设$\operatorname{det}(VU^T) = -1$，则限制$R$为旋转就意味着$M = V^TRU $为<code>反射矩阵</code>， 于是我们试图找到一个<code>反射矩阵</code>$M$最大化下式：</p><script type="math/tex; mode=display">\operatorname{tr}(\Sigma M) = {\sigma}_1 m_{11} + {\sigma}_2 m_{22} +...+ {\sigma}_d m_{dd} := f(m_{11},m_{11},...,m_{dd})  \tag{19}</script><p>即$f$是以<script type="math/tex">m_{11},m_{11},...,m_{dd}</script>为变量的线性函数，由于<script type="math/tex">m_{ii} \in \left[ -1,1\right]</script>，其极大值肯定在其定义域的边界处。于是当<script type="math/tex">{\forall} i, m_{ii} = 1</script>时，$f$取得极大值，但是此时的$R$为<code>反射矩阵</code>，所以并不能这样取值。然后我们看第二个极大值点$(1,1,…,-1)$，有：</p><script type="math/tex; mode=display">f = \operatorname{tr}(\Sigma M) = {\sigma}_1 + {\sigma}_2+...+ {\sigma}_{d-1} -  {\sigma}_d \tag{20}</script><p>这个值大于任何其它的自变量取值$(\pm 1,\pm 1,…,\pm 1)$的组合（除了$( 1, 1,…, 1)$），因为奇异值是经过排序的，${\sigma}_d$是最小的一个奇异值。</p><p>综上，为了将解转换为旋转矩阵要进行如下处理：</p><script type="math/tex; mode=display">R=V\left(\begin{array}{cccc}{1} \\ {} & {\ddots} & {} &{} \\ {} & {} & 1 & {}\\ {} & {} & {} & {\operatorname{det}\left(V U^{\top}\right)}\end{array}\right) U^{\top} \tag{21}</script><h2 id="可以总结的套路"><a href="#可以总结的套路" class="headerlink" title="可以总结的套路"></a>可以总结的套路</h2><p>为了得到<code>ICP</code>问题的最优解，我们可以采取如下套路：</p><p><strong>step1</strong>. 计算两组匹配点的加权中心：</p><script type="math/tex; mode=display">\overline{\mathbf{p}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{p}_{i}}{\sum_{i=1}^{n} w_{i}},  \overline{\mathbf{q}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{q}_{i}}{\sum_{i=1}^{n} w_{i}}</script><p><strong>step2</strong>. 得到去中心化的点集：</p><script type="math/tex; mode=display">\mathbf{x}_i := \mathbf{p}_{i} -\overline{\mathbf{p}}，\mathbf{y}_i := \mathbf{q}_{i} -\overline{\mathbf{q}}, i = 1,2...n</script><p><strong>step3</strong>. 计算$d \times d$的covariance矩阵：</p><script type="math/tex; mode=display">S = XWY^T</script><p>其中，$X,Y$为$d \times n$的矩阵，$\mathbf{x}_i,\mathbf{y}_i$分别是它们的列元素，另外$W = diag(w_1,w_2,…,w_n)$。</p><p><strong>step4</strong>. 对$S$进行<code>SVD</code>分解$S = U\Sigma V^T$，得到旋转矩阵：</p><script type="math/tex; mode=display">R=V\left(\begin{array}{cccc}{1} \\ {} & {\ddots} & {} &{} \\ {} & {} & 1 & {}\\ {} & {} & {} & {\operatorname{det}\left(V U^{\top}\right)}\end{array}\right) U^{\top}</script><p><strong>step5</strong>. 计算平移量：</p><script type="math/tex; mode=display">\mathbf{t} = \overline{\mathbf{q}} - R\overline{\mathbf{p}}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天讲一篇关于利用&lt;code&gt;SVD&lt;/code&gt;方法求解&lt;code&gt;ICP&lt;/code&gt;问题的文献&lt;a href=&quot;https://vincentqin.gitee.io/blogresource-3/slam-common-issues-ICP/svd_rot.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《Least-Squares Rigid Motion Using SVD》&lt;/a&gt;，这篇文章非常精彩地推导出将$3D$点对齐问题的解析解，同时总结了求解该问题的统一范式。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="位姿" scheme="https://www.vincentqin.tech/tags/%E4%BD%8D%E5%A7%BF/"/>
    
      <category term="ICP" scheme="https://www.vincentqin.tech/tags/ICP/"/>
    
  </entry>
  
  <entry>
    <title>SLAM常见问题(三)：PNP</title>
    <link href="https://www.vincentqin.tech/posts/slam-common-issues-PNP/"/>
    <id>https://www.vincentqin.tech/posts/slam-common-issues-PNP/</id>
    <published>2019-08-11T13:29:24.000Z</published>
    <updated>2020-03-31T15:31:47.349Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><code>PNP</code>即“Perspective-N-Points”，是求解 3D 到 2D 点对运动的方法。它描述了当我们知道n个3D空间点以及它们在图像上的位置时，如何估计相机所在的位姿。PnP 问题有很多种求解方法，例如用三对点估计位姿的 <code>P3P</code>（通常需要额外一个点进行验证结果），直接线性变换（<code>DLT</code>），<code>EPnP</code>（Efficient PnP，已知内参时用），<code>UPnP</code>（内参未知时用） 等等）。此外，还能用非线性优化的方式，构建最小二乘问题并迭代求解，也就是万金油式的 <code>Bundle Adjustment</code>。</p><a id="more"></a><h2 id="P3P"><a href="#P3P" class="headerlink" title="P3P"></a>P3P</h2><p>已知：$3D-2D$匹配点，$3D$点的<strong>世界坐标</strong>记为$A, B, C$，图像上的2D点记为$a, b, c$。</p><p>未知：<strong>相机系下3D点的坐标是未知的</strong>，即$OA,OB,OC$，一旦$ 3D$ 点在相机坐标系下的坐标能够算出，我们就得到了$3D-3D$的对应点，把<code>PnP</code>问题转换为了<code>ICP</code>问题。</p><p>我们的目标就是通过<strong>纯几何的方法</strong>求出上述未知量，过程如下。</p><p><img src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-PNP/p3p.png" alt></p><p>由于余弦定理可知：</p><script type="math/tex; mode=display">\begin{array}{l}{O A^{2}+O B^{2}-2 O A \cdot O B \cdot \cos \langle a, b\rangle= A B^{2}} \\ {O B^{2}+O C^{2}-2 O B \cdot O C \cdot \cos \langle b, c\rangle= B C^{2}} \\ {O A^{2}+O C^{2}-2 O A \cdot O C \cdot \cos \langle a, c\rangle= A C^{2}}\end{array}</script><p>对上面三式全体除以$OC^{2}$，记$x=O A / O C, y=O B / O C$，得：</p><script type="math/tex; mode=display">\begin{array}{l}{x^{2}+y^{2}-2 x y \cos \langle a, b\rangle= A B^{2} / O C^{2}} \\ {y^{2}+1^{2}-2 y \cos \langle b, c\rangle= B C^{2} / O C^{2}} \\ {x^{2}+1^{2}-2 x \cos \langle a, c\rangle= A C^{2} / O C^{2}}\end{array}</script><p>记$v=A B^{2} / O C^{2}, u v=B C^{2} / O C^{2}, w v=A C^{2} / O C^{2}$，得：</p><script type="math/tex; mode=display">\begin{array}{l}{x^{2}+y^{2}-2 x y \cos \langle a, b\rangle- v=0} \\ {y^{2}+1^{2}-2 y \cos \langle b, c\rangle- u v=0} \\ {x^{2}+1^{2}-2 x \cos \langle a, c\rangle- w v=0}\end{array}</script><p>将第一个式子中$v = x^{2}+y^{2}-2 x y \cos \langle a, b\rangle$带入后面两个式子中，得：</p><script type="math/tex; mode=display">\begin{array}{l}{(1-u) y^{2}-u x^{2}-\cos \langle b, c\rangle y+2 u x y \cos \langle a, b\rangle+ 1=0} \\ {(1-w) x^{2}-w y^{2}-\cos \langle a, c\rangle x+2 w x y \cos \langle a, b\rangle+ 1=0}\end{array}</script><p>上式中几个余弦角度$\cos \langle a, b\rangle, \cos \langle b, c\rangle, \cos \langle a, c\rangle$是已知的，$u=B C^{2} / A B^{2}, w=A C^{2} / A B^{2}$也是已知的，所以未知量仅有$x,y$，解析地求解该方程组是一个复杂的过程，需要用<strong><a href="https://zh.wikipedia.org/wiki/%E5%90%B4%E6%B6%88%E5%85%83%E6%B3%95" target="_blank" rel="noopener">吴消元法</a></strong>。这样就可以求得$x,y$，然后带入$v = x^{2}+y^{2}-2 x y \cos \langle a, b\rangle$求解$v$，即可得到$OC$，进而得到$OB,OA$。该方程最多可能得到四个解，但我们可以用第4个验证点来计算最可能的解，得到$ A, B, C$ 在相机坐标系下的$3D$坐标。然后，根据$ 3D-3D $的点对，计算相机的运动 $R,t$，此处可参考文献<a href="https://igl.ethz.ch/projects/ARAP/svd_rot.pdf" target="_blank" rel="noopener">Least-Squares Rigid Motion Using SVD</a></p><h2 id="EPnP"><a href="#EPnP" class="headerlink" title="EPnP"></a>EPnP</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p><code>EPnP</code>即Efficient PnP，参考文献 <a href="https://icwww.epfl.ch/~lepetit/papers/lepetit_ijcv08.pdf" target="_blank" rel="noopener">EPnP: An Accurate O(n) Solution to the PnP Problem</a>。</p><p>问题描述如PnP，更加具体的，我们已知一组特征点，对于每个特征点$i$，我们有如下信息：</p><ul><li><p>特征点 $i$ 在世界坐标系的坐标<script type="math/tex">P_{i}^{w}=\left[\begin{array}{c}{x_{i}^{w}} \\ {y_{i}^{w}} \\ {z_{i}^{w}}\end{array}\right]</script></p></li><li><p>特征点在成像平面上的坐标<script type="math/tex">p_{i}=\left[\begin{array}{l}{u_{i}} \\ {v_{i}}\end{array}\right]</script></p></li><li>已知相机内参$K$</li></ul><p>求：世界坐标系到相机系的变换矩阵<script type="math/tex">T_{c w}=\left[\begin{array}{cc}{R_{c w}} & {t} \\ {0} & {1}\end{array}\right]</script></p><h3 id="算法假设"><a href="#算法假设" class="headerlink" title="算法假设"></a>算法假设</h3><p><code>EPnP</code>的思想是无论世界系还是相机系下的$3D$点都可以由<strong>4个控制点线性组合</strong>，记：</p><ul><li>世界系下4个控制点表示为:$\mathbf{c}_{j}^{w}, j=1, \cdots, 4$</li><li>相机系下4个控制点表示为:$\mathbf{c}_{j}^{c}, j=1, \cdots, 4$</li></ul><p>EPnP算法将参考点的坐标表示为控制点坐标的加权和：</p><script type="math/tex; mode=display">\mathbf{p}_{i}^{w}=\sum_{j=1}^{4} \alpha_{i j} \mathbf{c}_{j}^{w}, \text { with } \sum_{j=1}^{4} \alpha_{i j}=1</script><p>其中<script type="math/tex">\alpha_{i, j}, j=1, \cdots, 4</script>是加权系数，一旦虚拟控制点确定后，且满足4个控制点不共面的前提，<script type="math/tex">\alpha_{i, j}</script>是唯一的。</p><h3 id="控制点的存在性"><a href="#控制点的存在性" class="headerlink" title="控制点的存在性"></a>控制点的存在性</h3><p>现在讨论控制点的存在性，上式可以写成：</p><script type="math/tex; mode=display">\left[\begin{array}{c}{p_{i}^{w}} \\ {1}\end{array}\right]=\left[\begin{array}{cccc}{C_{1}^{w}} & {C_{2}^{w}} & {C_{3}^{w}} & {C_{4}^{w}} \\ {1} & {1} & {1} & {1}\end{array}\right] \alpha_{i} \stackrel{令}{=} C \alpha_{i}</script><p>可见只要$C$非奇异，就一定可以找到满足条件的$\alpha_{i} $，即：</p><script type="math/tex; mode=display">\left[\begin{array}{l}{\alpha_{i 1}} \\ {\alpha_{i 2}} \\ {\alpha_{i 3}} \\ {\alpha_{i 4}}\end{array}\right]=C^{-1}\left[\begin{array}{c}{\mathbf{p}_{i}^{w}} \\ {1}\end{array}\right]</script><p>接下来，我们讨论相机坐标系下，控制点和参考$3D$点之间的关系：</p><script type="math/tex; mode=display">p_{i}^{c}=R_{c w} p_{i}^{w}+t=R_{c w}\left(\sum_{j=1}^{4} \alpha_{i j} c_{i}^{w}\right)+t</script><p>由于<script type="math/tex">\sum_{j=1}^{4} \alpha_{i j}=1$，因此$t=\sum_{j=1}^{4} \alpha_{i j} t</script>,带入上式，得：</p><script type="math/tex; mode=display">p_{i}^{c}=\sum_{j=1}^{4} \alpha_{i j}\left(R_{c w} c_{i}^{w}\right)+t )=\sum_{j=1}^{4} \alpha_{i j} c_{i}^{c}</script><p>可见系数<script type="math/tex">\alpha_{i}</script>具有不变性，如果我们能够求出控制点在相机坐标系中的坐标<script type="math/tex">c_{1}^{c}, c_{2}^{c},c_{3}^{c},c_{4}^{c}</script>，那么对于任意一个3D点k，我们可以求得其在相机系下的坐标：<script type="math/tex">p_{k}^{c}=\sum_{j=1}^{4} \alpha_{k j} c_{i}^{c}</script>，这就变成了如P3P同样的问题了，即求解<code>3D-3D</code>位姿估计问题。</p><h3 id="如何选择控制点"><a href="#如何选择控制点" class="headerlink" title="如何选择控制点"></a>如何选择控制点</h3><p>记世界系下所有3D点集为<script type="math/tex">\left\{\mathbf{p}_{i}^{w}, i=1, \cdots, n\right\}</script>,第一个控制点是所有3D点的重心:</p><script type="math/tex; mode=display">\mathbf{c}_{1}^{w}=\frac{1}{n} \sum_{i=1}^{n} \mathbf{p}_{i}^{w}</script><p>对所有3D点去中心化，这些点罗列成矩阵形式：</p><script type="math/tex; mode=display">A=\left[\begin{array}{c}{\mathbf{p}_{1}^{w^{T}}-\mathbf{c}_{1}^{w^{T}}} \\ {\cdots} \\ {\mathbf{p}_{n}^{w^{T}}-\mathbf{c}_{1}^{w^{T}}}\end{array}\right]</script><p>对$A^TA$进行特征值分解（注意此时并非对A进行<code>SVD</code>分解，是为了减低时间复杂度，<code>SVD</code>分解的复杂度为$SO(3)$），其特征值为<script type="math/tex">\lambda_{c, i}, i=1,2,3</script>，对应的特征向量为<script type="math/tex">\mathbf{v}_{c, i}, i=1,2,3</script>，则剩余的3个控制点表示为如下公式：</p><script type="math/tex; mode=display">\mathbf{c}_{j}^{w}=\mathbf{c}_{1}^{w}+\lambda_{c, j-1}^{\frac{1}{2}} \mathbf{v}_{c, j-1}, j=2,3,4</script><h3 id="求解控制点在相机系下的坐标"><a href="#求解控制点在相机系下的坐标" class="headerlink" title="求解控制点在相机系下的坐标"></a>求解控制点在相机系下的坐标</h3><p>记<script type="math/tex">\left\{\mathbf{u}_{i}\right\}_{i=1, \cdots, n}</script>为相机下$3D$点<script type="math/tex">\left\{\mathbf{p}^c_{i}\right\}_{i=1, \cdots, n}</script>的图像坐标，则：</p><script type="math/tex; mode=display">\forall i, \quad w_{i}\left[\begin{array}{c}{\mathbf{u}_{i}} \\ {1}\end{array}\right]=K \mathbf{p}_{i}^{c}=K \sum_{j=1}^{4} \alpha_{i j} \mathbf{c}_{j}^{c}</script><p>其中<script type="math/tex">w_i</script>是尺度因子，将控制点<script type="math/tex">\mathbf{c}_{j}^{c}=\left[x_{j}^{c}, y_{j}^{c}, z_{j}^{c}\right]^{T}</script>带入上式，得：</p><script type="math/tex; mode=display">\forall i, \quad w_{i}\left[\begin{array}{c}{u_{i}} \\ {v_{i}} \\ {1}\end{array}\right]=\left[\begin{array}{ccc}{f_{u}} & {0} & {u_{c}} \\ {0} & {f_{v}} & {v_{c}} \\ {0} & {0} & {1}\end{array}\right] \sum_{j=1}^{4} \alpha_{i j}\left[\begin{array}{c}{x_{j}^{c}} \\ {y_{j}^{c}} \\ {z_{j}^{c}}\end{array}\right]</script><p>上式可以得到两个线性方程：</p><script type="math/tex; mode=display">\begin{array}{l}{\sum_{j=1}^{4} \alpha_{i j} f_{u} x_{j}^{c}+\alpha_{i j}\left(u_{c}-u_{i}\right) z_{j}^{c}=0} \\ {\sum_{j=1}^{4} \alpha_{i j} f_{v} y_{j}^{c}+\alpha_{i j}\left(v_{c}-v_{j}\right) z_{j}^{c}=0}\end{array}</script><p>把这N个点的约束罗列在一起，我们就可以得到如下矩阵：</p><script type="math/tex; mode=display">M\mathbf{x} = \mathbf{0}</script><p>其中<script type="math/tex">\mathbf{x}=\left[\mathbf{c}_{1}^{c \top}, \mathbf{c}_{2}^{c \top}, \mathbf{c}_{3}^{c \top}, \mathbf{c}_{4}^{c \top}\right]^{\top}</script>为<strong>12</strong>维向量，<script type="math/tex">\mathbf{M}</script>维度<script type="math/tex">2n\times 12</script>，如下形式:</p><p><img src="https://cdn.mathpix.com/snip/images/XRV8zRc_TojEnL-nnDpP-eSDrWxXmwLRJ0zOt5FmAzg.original.fullsize.png" width="75%"></p><h3 id="未完待续…"><a href="#未完待续…" class="headerlink" title="未完待续…"></a>未完待续…</h3><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://blog.csdn.net/jessecw79/article/details/82945918" target="_blank" rel="noopener">深入EPnP算法</a></li><li><a href="https://zhuanlan.zhihu.com/p/46695068" target="_blank" rel="noopener">3d-2d位姿估计之EPnP算法</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;PNP&lt;/code&gt;即“Perspective-N-Points”，是求解 3D 到 2D 点对运动的方法。它描述了当我们知道n个3D空间点以及它们在图像上的位置时，如何估计相机所在的位姿。PnP 问题有很多种求解方法，例如用三对点估计位姿的 &lt;code&gt;P3P&lt;/code&gt;（通常需要额外一个点进行验证结果），直接线性变换（&lt;code&gt;DLT&lt;/code&gt;），&lt;code&gt;EPnP&lt;/code&gt;（Efficient PnP，已知内参时用），&lt;code&gt;UPnP&lt;/code&gt;（内参未知时用） 等等）。此外，还能用非线性优化的方式，构建最小二乘问题并迭代求解，也就是万金油式的 &lt;code&gt;Bundle Adjustment&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="位姿" scheme="https://www.vincentqin.tech/tags/%E4%BD%8D%E5%A7%BF/"/>
    
      <category term="PNP" scheme="https://www.vincentqin.tech/tags/PNP/"/>
    
  </entry>
  
  <entry>
    <title>SLAM常见问题(二)：重定位Relocalisation</title>
    <link href="https://www.vincentqin.tech/posts/slam-common-issues-relocalisation/"/>
    <id>https://www.vincentqin.tech/posts/slam-common-issues-relocalisation/</id>
    <published>2019-08-08T14:51:14.000Z</published>
    <updated>2019-09-10T13:50:17.457Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>可以说整个重定位就是一个精心设计的解算当前帧位姿的模块，秉持着不抛弃不放弃的精神，ORB-SLAM的作者简直把特征匹配压榨到了极致，仿佛在说“小伙子你有很多匹配点的，不要放弃，我们优化一下位姿再找找匹配点呗”。</p><a id="more"></a><p>原理如下流程图：</p><p><img src="//www.vincentqin.tech/posts/slam-common-issues-relocalisation/relocalisation.svg" alt="重定位"></p><p>代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bool</span> Tracking::Relocalization()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Compute Bag of Words Vector</span></span><br><span class="line">    <span class="comment">// 步骤1：计算当前帧特征点的Bow映射，能够得到当前帧的词袋向量以及featureVector</span></span><br><span class="line">    <span class="comment">// 可用于SearchByBoW寻找匹配特征点</span></span><br><span class="line">    mCurrentFrame.ComputeBoW();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Relocalization is performed when tracking is lost</span></span><br><span class="line">    <span class="comment">// Track Lost: Query KeyFrame Database for keyframe candidates for relocalisation</span></span><br><span class="line">    <span class="comment">// 步骤2：找到与当前帧相似的候选关键帧，</span></span><br><span class="line">    <span class="comment">// 这里会通过查询关键帧数据库进行快速查找与当前帧相似的候选重定位帧vpCandidateKFs</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;KeyFrame*&gt; vpCandidateKFs = mpKeyFrameDB-&gt;DetectRelocalizationCandidates(&amp;mCurrentFrame);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(vpCandidateKFs.empty())</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> nKFs = vpCandidateKFs.size();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We perform first an ORB matching with each candidate</span></span><br><span class="line">    <span class="comment">// If enough matches are found we setup a PnP solver</span></span><br><span class="line">    <span class="function">ORBmatcher <span class="title">matcher</span><span class="params">(<span class="number">0.75</span>,<span class="literal">true</span>)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">vector</span>&lt;PnPsolver*&gt; vpPnPsolvers;</span><br><span class="line">    vpPnPsolvers.resize(nKFs);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;MapPoint*&gt; &gt; vvpMapPointMatches;</span><br><span class="line">    vvpMapPointMatches.resize(nKFs);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; vbDiscarded;</span><br><span class="line">    vbDiscarded.resize(nKFs);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> nCandidates=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nKFs; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        KeyFrame* pKF = vpCandidateKFs[i];</span><br><span class="line">        <span class="keyword">if</span>(pKF-&gt;isBad())</span><br><span class="line">            vbDiscarded[i] = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 步骤3：通过BoW进行匹配</span></span><br><span class="line">            <span class="comment">// 利用SearchByBoW查找当前帧与关键帧的匹配点vvpMapPointMatches</span></span><br><span class="line">            <span class="keyword">int</span> nmatches = matcher.SearchByBoW(pKF,mCurrentFrame,vvpMapPointMatches[i]);</span><br><span class="line">            <span class="comment">// 如果匹配点数小于15个点，跳过</span></span><br><span class="line">            <span class="keyword">if</span>(nmatches&lt;<span class="number">15</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                vbDiscarded[i] = <span class="literal">true</span>;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 如果匹配点数大于15个点，建立当前帧与关键帧之间的PNP求解器；</span></span><br><span class="line">            <span class="comment">// 仅仅如建立这个求解器，还未求解</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 初始化PnPsolver</span></span><br><span class="line">                PnPsolver* pSolver = <span class="keyword">new</span> PnPsolver(mCurrentFrame,vvpMapPointMatches[i]);</span><br><span class="line">                pSolver-&gt;SetRansacParameters(<span class="number">0.99</span>,<span class="number">10</span>,<span class="number">300</span>,<span class="number">4</span>,<span class="number">0.5</span>,<span class="number">5.991</span>);</span><br><span class="line">                vpPnPsolvers[i] = pSolver;</span><br><span class="line">                nCandidates++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Alternatively perform some iterations of P4P RANSAC</span></span><br><span class="line">    <span class="comment">// Until we found a camera pose supported by enough inliers</span></span><br><span class="line">    <span class="keyword">bool</span> bMatch = <span class="literal">false</span>;</span><br><span class="line">    <span class="function">ORBmatcher <span class="title">matcher2</span><span class="params">(<span class="number">0.9</span>,<span class="literal">true</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果候选关键帧数大于0且没有重定位成功</span></span><br><span class="line">    <span class="keyword">while</span>(nCandidates&gt;<span class="number">0</span> &amp;&amp; !bMatch)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nKFs; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 若当前候选关键帧与当前帧匹配数量小于15，跳过</span></span><br><span class="line">            <span class="keyword">if</span>(vbDiscarded[i])</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Perform 5 Ransac Iterations</span></span><br><span class="line">            <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; vbInliers;</span><br><span class="line">            <span class="keyword">int</span> nInliers;</span><br><span class="line">            <span class="keyword">bool</span> bNoMore;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 步骤4：通过EPnP算法估计初始位姿</span></span><br><span class="line">            PnPsolver* pSolver = vpPnPsolvers[i];</span><br><span class="line">            cv::Mat Tcw = pSolver-&gt;iterate(<span class="number">5</span>,bNoMore,vbInliers,nInliers);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// If Ransac reachs max. iterations discard keyframe</span></span><br><span class="line">            <span class="comment">// 若RANSAC失败，当前候选关键帧被提出候选帧</span></span><br><span class="line">            <span class="keyword">if</span>(bNoMore)</span><br><span class="line">            &#123;</span><br><span class="line">                vbDiscarded[i]=<span class="literal">true</span>;</span><br><span class="line">                nCandidates--;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// If a Camera Pose is computed, optimize</span></span><br><span class="line">            <span class="comment">// PNP求解出了一个比较初始的位姿，比较粗糙，需要进一步优化</span></span><br><span class="line">            <span class="keyword">if</span>(!Tcw.empty())</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 把刚刚PNP求解的位姿赋给当前帧位姿</span></span><br><span class="line">                Tcw.copyTo(mCurrentFrame.mTcw);</span><br><span class="line"></span><br><span class="line">                <span class="built_in">set</span>&lt;MapPoint*&gt; sFound;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> np = vbInliers.size();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;np; j++)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">if</span>(vbInliers[j])</span><br><span class="line">                    &#123;</span><br><span class="line">                        mCurrentFrame.mvpMapPoints[j]=vvpMapPointMatches[i][j];</span><br><span class="line">                        sFound.insert(vvpMapPointMatches[i][j]);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                        mCurrentFrame.mvpMapPoints[j]=<span class="literal">NULL</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 步骤5：通过PoseOptimization对姿态进行优化求解</span></span><br><span class="line">                <span class="keyword">int</span> nGood = Optimizer::PoseOptimization(&amp;mCurrentFrame);</span><br><span class="line">                <span class="comment">// 内点小于10，跳过</span></span><br><span class="line">                <span class="keyword">if</span>(nGood&lt;<span class="number">10</span>)</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                <span class="comment">// 刚才的PO优化会滤除一些外点</span></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> io =<span class="number">0</span>; io&lt;mCurrentFrame.N; io++)</span><br><span class="line">                    <span class="keyword">if</span>(mCurrentFrame.mvbOutlier[io])</span><br><span class="line">                        mCurrentFrame.mvpMapPoints[io]=<span class="keyword">static_cast</span>&lt;MapPoint*&gt;(<span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// If few inliers, search by projection in a coarse window and optimize again</span></span><br><span class="line">                <span class="comment">// 步骤6：如果内点较少，则通过投影的方式对之前未匹配的点进行匹配，再进行优化求解</span></span><br><span class="line">                <span class="comment">// 作者认为10&lt;=nGood&lt;50时仍有可能重定位成功，由于PO调整了位姿，</span></span><br><span class="line">                <span class="comment">// 可以通过位姿投影的方式将候选关键帧上的地图点投影在当前帧上进行搜索匹配点，</span></span><br><span class="line">                <span class="comment">// 从而增加匹配，然后再优化以得到足够多的内点</span></span><br><span class="line">                <span class="keyword">if</span>(nGood&lt;<span class="number">50</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">int</span> nadditional =matcher2.SearchByProjection(mCurrentFrame,vpCandidateKFs[i],sFound,<span class="number">10</span>,<span class="number">100</span>);</span><br><span class="line">                    <span class="comment">// 新增的点与之前PO内点之和大于50，我们考虑再进行一遍优化</span></span><br><span class="line">                    <span class="keyword">if</span>(nadditional+nGood&gt;=<span class="number">50</span>)</span><br><span class="line">                    &#123;</span><br><span class="line">                        nGood = Optimizer::PoseOptimization(&amp;mCurrentFrame);</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// If many inliers but still not enough, search by projection again in a narrower window</span></span><br><span class="line">                        <span class="comment">// the camera has been already optimized with many points</span></span><br><span class="line">                        <span class="comment">// 不够多呀，不要放弃，再来一遍</span></span><br><span class="line">                        <span class="keyword">if</span>(nGood&gt;<span class="number">30</span> &amp;&amp; nGood&lt;<span class="number">50</span>)</span><br><span class="line">                        &#123;</span><br><span class="line">                            sFound.clear();</span><br><span class="line">                            <span class="keyword">for</span>(<span class="keyword">int</span> ip =<span class="number">0</span>; ip&lt;mCurrentFrame.N; ip++)</span><br><span class="line">                                <span class="keyword">if</span>(mCurrentFrame.mvpMapPoints[ip])</span><br><span class="line">                                    sFound.insert(mCurrentFrame.mvpMapPoints[ip]);</span><br><span class="line">                            nadditional =matcher2.SearchByProjection(mCurrentFrame,vpCandidateKFs[i],sFound,<span class="number">3</span>,<span class="number">64</span>);</span><br><span class="line"></span><br><span class="line">                            <span class="comment">// Final optimization</span></span><br><span class="line">                            <span class="comment">// 最后一次优化啦~</span></span><br><span class="line">                            <span class="keyword">if</span>(nGood+nadditional&gt;=<span class="number">50</span>)</span><br><span class="line">                            &#123;</span><br><span class="line">                                nGood = Optimizer::PoseOptimization(&amp;mCurrentFrame);</span><br><span class="line"></span><br><span class="line">                                <span class="keyword">for</span>(<span class="keyword">int</span> io =<span class="number">0</span>; io&lt;mCurrentFrame.N; io++)</span><br><span class="line">                                    <span class="keyword">if</span>(mCurrentFrame.mvbOutlier[io])</span><br><span class="line">                                        mCurrentFrame.mvpMapPoints[io]=<span class="literal">NULL</span>;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// If the pose is supported by enough inliers stop ransacs and continue</span></span><br><span class="line">                <span class="comment">// 只要找到一个候选关键帧与当前帧的匹配点数大于50就重定位成功！</span></span><br><span class="line">                <span class="keyword">if</span>(nGood&gt;=<span class="number">50</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    bMatch = <span class="literal">true</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(!bMatch)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        mnLastRelocFrameId = mCurrentFrame.mnId;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;可以说整个重定位就是一个精心设计的解算当前帧位姿的模块，秉持着不抛弃不放弃的精神，ORB-SLAM的作者简直把特征匹配压榨到了极致，仿佛在说“小伙子你有很多匹配点的，不要放弃，我们优化一下位姿再找找匹配点呗”。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="ORB" scheme="https://www.vincentqin.tech/tags/ORB/"/>
    
      <category term="Relocalisation" scheme="https://www.vincentqin.tech/tags/Relocalisation/"/>
    
      <category term="重定位" scheme="https://www.vincentqin.tech/tags/%E9%87%8D%E5%AE%9A%E4%BD%8D/"/>
    
  </entry>
  
  <entry>
    <title>SLAM常见问题(一)：SearchByBoW</title>
    <link href="https://www.vincentqin.tech/posts/slam-common-issues-SearchbyBoW/"/>
    <id>https://www.vincentqin.tech/posts/slam-common-issues-SearchbyBoW/</id>
    <published>2019-08-04T15:18:14.000Z</published>
    <updated>2019-08-04T16:12:16.670Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>ORB-SLAM</strong>中使用了多种特征匹配的奇技淫巧，其中之一就是利用<strong>词袋信息</strong>进行引导匹配<code>SearchByBoW</code>：利用了<code>BOW</code>里的正向引导进行两帧之间的匹配，核心点在于位于同一个节点处的特征才有可能属于同一匹配，相较于暴力匹配匹配速度更快。</p><a id="more"></a><p>注意：每幅图像都可以通过<code>ComputeBoW</code>得到其对应的词袋向量。<code>featureVector</code>存储的是节点的索引值以及对应图像feature对应的索引向量，即<code>map&lt;node_id,vector&lt;featureID&gt;</code>。这样的话就可以根据两帧图像的<code>node_id</code>来初步确定二者共有的特征点，然后根据该<code>id</code>取出<code>vector&lt;featureID&gt;</code>，根据featureID找到图像上的特征点以及描述子，通过比较二者描述子距离来判定该特征点是否为匹配点，若距离小于某一阈值，则二者为匹配对。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief Bag of Words Representation</span></span><br><span class="line"><span class="comment"> * 计算词袋mBowVec和mFeatVec</span></span><br><span class="line"><span class="comment"> * @see CreateInitialMapMonocular() TrackReferenceKeyFrame() Relocalization()</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//同样的，关键帧也有构造函数void KeyFrame::ComputeBoW()</span></span><br><span class="line"><span class="keyword">void</span> Frame::ComputeBoW() </span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(mBowVec.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 将描述子mDescriptors转换为DBOW要求的输入格式</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;cv::Mat&gt; vCurrentDesc = Converter::toDescriptorVector(mDescriptors);</span><br><span class="line">        <span class="comment">// 转换成词袋向量mBowVec以及特征向量mFeatVec</span></span><br><span class="line">        <span class="comment">// mBowVec存储着单词及其对应的权重TF-IDF值</span></span><br><span class="line">        <span class="comment">// mFeatVec存储节点ID以及对应对应图像feature对应的索引向量</span></span><br><span class="line">        mpORBvocabulary-&gt;transform(vCurrentDesc,mBowVec,mFeatVec,<span class="number">4</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该函数在<code>Tracking</code>线程中的<code>TrackReferenceKeyFrame()</code>/<code>Relocalization()</code>进行调用（注意：<code>LoopClosing</code>线程中<code>ComputeSim3()</code>也会调用该函数，与上述二者的区别在于，ComputeSim3()中的SearchByBoW是寻找关键帧之间的匹配，而非关键帧与当前帧之间的匹配）。</p><p>下面给出<code>ORB-SLAM2</code>中用于<strong>关键帧与当前帧</strong>进行词袋引导匹配的源码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief 通过词袋，对关键帧的特征点进行跟踪</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 通过bow对pKF和F中的特征点进行快速匹配（不属于同一node的特征点直接跳过匹配） \n</span></span><br><span class="line"><span class="comment"> * 对属于同一node的特征点通过描述子距离进行匹配 \n</span></span><br><span class="line"><span class="comment"> * 根据匹配，用pKF中特征点对应的MapPoint去更新F中特征点对应的MapPoints \n</span></span><br><span class="line"><span class="comment"> * 每个特征点都对应一个MapPoint，因此pKF中每个特征点的MapPoint也就是F中对应点的MapPoint \n</span></span><br><span class="line"><span class="comment"> * 通过距离阈值、比例阈值和角度投票进行剔除误匹配</span></span><br><span class="line"><span class="comment"> * @param  pKF               KeyFrame</span></span><br><span class="line"><span class="comment"> * @param  F                 Current Frame</span></span><br><span class="line"><span class="comment"> * @param  vpMapPointMatches F中MapPoints对应的匹配，NULL表示未匹配</span></span><br><span class="line"><span class="comment"> * @return                   成功匹配的数量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> <span class="comment">//const int ORBmatcher::TH_HIGH = 100;</span></span><br><span class="line"> <span class="comment">//const int ORBmatcher::TH_LOW = 50;</span></span><br><span class="line"> <span class="comment">//const int ORBmatcher::HISTO_LENGTH = 30;</span></span><br><span class="line"><span class="keyword">int</span> ORBmatcher::SearchByBoW(KeyFrame* pKF,Frame &amp;F, <span class="built_in">vector</span>&lt;MapPoint*&gt; &amp;vpMapPointMatches)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// vpMapPointsKF：获取输入关键帧匹配到的地图点</span></span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;MapPoint*&gt; vpMapPointsKF = pKF-&gt;GetMapPointMatches();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化当前帧MapPoints对应的匹配NULL</span></span><br><span class="line">    vpMapPointMatches = <span class="built_in">vector</span>&lt;MapPoint*&gt;(F.N,<span class="keyword">static_cast</span>&lt;MapPoint*&gt;(<span class="literal">NULL</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// FeatureVector数据类型 map&lt;node_id,vector&lt;featureID&gt;，</span></span><br><span class="line">    <span class="comment">// 可以快速根据node_id找到属于该node的特征点</span></span><br><span class="line">    <span class="keyword">const</span> DBoW2::FeatureVector &amp;vFeatVecKF = pKF-&gt;mFeatVec;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> nmatches=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; rotHist[HISTO_LENGTH];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;HISTO_LENGTH;i++)</span><br><span class="line">        rotHist[i].reserve(<span class="number">500</span>);</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span> factor = HISTO_LENGTH/<span class="number">360.0f</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We perform the matching over ORB that belong to the same vocabulary node </span></span><br><span class="line">    <span class="comment">// (at a certain level)</span></span><br><span class="line">    <span class="comment">// 建立迭代器，将属于同一节点(特定层)的ORB特征进行匹配</span></span><br><span class="line">    DBoW2::FeatureVector::const_iterator KFit = vFeatVecKF.begin();</span><br><span class="line">    DBoW2::FeatureVector::const_iterator Fit = F.mFeatVec.begin();</span><br><span class="line">    DBoW2::FeatureVector::const_iterator KFend = vFeatVecKF.end();</span><br><span class="line">    DBoW2::FeatureVector::const_iterator Fend = F.mFeatVec.end();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(KFit != KFend &amp;&amp; Fit != Fend)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 步骤1：分别取出属于同一node的ORB特征点(只有属于同一node，才有可能是匹配点)</span></span><br><span class="line">        <span class="comment">// first表示node_id，只有node_id相同才表示这些特征点位于同一层</span></span><br><span class="line">        <span class="keyword">if</span>(KFit-&gt;first == Fit-&gt;first) </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// second中记录了这些特征对应图像中的ID</span></span><br><span class="line">            <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt; vIndicesKF = KFit-&gt;second;</span><br><span class="line">            <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt; vIndicesF = Fit-&gt;second;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 步骤2：遍历KF中属于该node的特征点</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">size_t</span> iKF=<span class="number">0</span>; iKF&lt;vIndicesKF.size(); iKF++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 获取关键帧上某一个特征点的ID</span></span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> realIdxKF = vIndicesKF[iKF];</span><br><span class="line">                <span class="comment">// 根据该ID得到该特征对应的MapPoint</span></span><br><span class="line">                MapPoint* pMP = vpMapPointsKF[realIdxKF]; </span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(!pMP) <span class="comment">//不存在</span></span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(pMP-&gt;isBad())<span class="comment">//被标记为坏点</span></span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 根据该ID取出KF中该特征对应的描述子</span></span><br><span class="line">                <span class="keyword">const</span> cv::Mat &amp;dKF= pKF-&gt;mDescriptors.row(realIdxKF); </span><br><span class="line"></span><br><span class="line">                <span class="keyword">int</span> bestDist1=<span class="number">256</span>; <span class="comment">// 最好的距离（最小距离）</span></span><br><span class="line">                <span class="keyword">int</span> bestIdxF =<span class="number">-1</span> ;</span><br><span class="line">                <span class="keyword">int</span> bestDist2=<span class="number">256</span>; <span class="comment">// 倒数第二好距离（倒数第二小距离）</span></span><br><span class="line"></span><br><span class="line">                <span class="comment">// 步骤3：遍历当前帧中属于该node的特征点，找到了最佳匹配点</span></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">size_t</span> iF=<span class="number">0</span>; iF&lt;vIndicesF.size(); iF++)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// 取出当前帧上位于该node上的某一个特征点的ID</span></span><br><span class="line">                    <span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> realIdxF = vIndicesF[iF];</span><br><span class="line">                    <span class="comment">// 表明这个点已经被匹配过了，不再匹配，加快速度</span></span><br><span class="line">                    <span class="keyword">if</span>(vpMapPointMatches[realIdxF])</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    <span class="comment">// 取出F中该特征对应的描述子</span></span><br><span class="line">                    <span class="keyword">const</span> cv::Mat &amp;dF = F.mDescriptors.row(realIdxF); </span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 计算描述子距离，这里是汉明距离，若非二进制描述子可选择用其他距离</span></span><br><span class="line">                    <span class="keyword">const</span> <span class="keyword">int</span> dist =  DescriptorDistance(dKF,dF); </span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 下面的操作就是分别获得最小bestDist1以及次小bestDist2的描述子距离</span></span><br><span class="line">                    <span class="comment">// dist &lt; bestDist1 &lt; bestDist2，更新bestDist1 bestDist2</span></span><br><span class="line">                    <span class="keyword">if</span>(dist&lt;bestDist1)</span><br><span class="line">                    &#123;</span><br><span class="line">                        bestDist2=bestDist1;</span><br><span class="line">                        bestDist1=dist;</span><br><span class="line">                        bestIdxF=realIdxF;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span>(dist&lt;bestDist2)<span class="comment">// bestDist1 &lt; dist &lt; bestDist2，更新bestDist2</span></span><br><span class="line">                    &#123;</span><br><span class="line">                        bestDist2=dist;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 步骤4：根据描述子距离阈值和角度投票剔除误匹配</span></span><br><span class="line">                <span class="comment">// 最小的描述子距离小于一个阈值 </span></span><br><span class="line">                <span class="keyword">if</span>(bestDist1&lt;=TH_LOW) </span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// trick!</span></span><br><span class="line">                    <span class="comment">// 最佳匹配比次佳匹配明显要好，那么最佳匹配才真正靠谱</span></span><br><span class="line">                    <span class="keyword">if</span>(<span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(bestDist1)&lt;</span><br><span class="line">                    mfNNratio*<span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(bestDist2))</span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="comment">// 步骤5：更新当前帧特征点的MapPoint</span></span><br><span class="line">                        <span class="comment">// 记录了特征点ID以及对应的特征点</span></span><br><span class="line">                        vpMapPointMatches[bestIdxF]=pMP;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 获得关键帧上的特征点位置</span></span><br><span class="line">                        <span class="keyword">const</span> cv::KeyPoint &amp;kp = pKF-&gt;mvKeysUn[realIdxKF];</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//</span></span><br><span class="line">                        <span class="keyword">if</span>(mbCheckOrientation)</span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="comment">// trick!</span></span><br><span class="line">                            <span class="comment">// angle：每个特征点在提取描述子时的旋转主方向角度，</span></span><br><span class="line">                            <span class="comment">// 如果图像旋转了，这个角度将发生改变</span></span><br><span class="line">                            <span class="comment">// 所有的特征点的角度变化应该是一致的，</span></span><br><span class="line">                            <span class="comment">// 通过直方图统计得到最准确的角度变化值</span></span><br><span class="line">                            <span class="comment">// 该特征点的角度变化值</span></span><br><span class="line">                            <span class="keyword">float</span> rot = kp.angle-F.mvKeys[bestIdxF].angle;</span><br><span class="line">                            <span class="keyword">if</span>(rot&lt;<span class="number">0.0</span>)</span><br><span class="line">                                rot+=<span class="number">360.0f</span>;</span><br><span class="line">                            <span class="keyword">int</span> bin = round(rot*factor);<span class="comment">// 将rot分配到bin组</span></span><br><span class="line">                            <span class="keyword">if</span>(bin==HISTO_LENGTH)</span><br><span class="line">                                bin=<span class="number">0</span>;</span><br><span class="line">                            assert(bin&gt;=<span class="number">0</span> &amp;&amp; bin&lt;HISTO_LENGTH);</span><br><span class="line">                            rotHist[bin].push_back(bestIdxF);</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="comment">// 匹配点+1</span></span><br><span class="line">                        nmatches++;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            KFit++;</span><br><span class="line">            Fit++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(KFit-&gt;first &lt; Fit-&gt;first)</span><br><span class="line">        &#123;</span><br><span class="line">            KFit = vFeatVecKF.lower_bound(Fit-&gt;first);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            Fit = F.mFeatVec.lower_bound(KFit-&gt;first);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据方向剔除误匹配的点，即删除那些不属于特征点角度变化最多的三个类别的匹配点</span></span><br><span class="line">    <span class="keyword">if</span>(mbCheckOrientation)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> ind1=<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">int</span> ind2=<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">int</span> ind3=<span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 计算rotHist中最大的三个的index</span></span><br><span class="line">        ComputeThreeMaxima(rotHist,HISTO_LENGTH,ind1,ind2,ind3);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;HISTO_LENGTH; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果特征点的旋转角度变化量属于这三个组，则保留</span></span><br><span class="line">            <span class="keyword">if</span>(i==ind1 || i==ind2 || i==ind3)</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 将除了ind1 ind2 ind3以外的匹配点去掉</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">size_t</span> j=<span class="number">0</span>, jend=rotHist[i].size(); j&lt;jend; j++)</span><br><span class="line">            &#123;</span><br><span class="line">                vpMapPointMatches[rotHist[i][j]]=<span class="keyword">static_cast</span>&lt;MapPoint*&gt;(<span class="literal">NULL</span>);</span><br><span class="line">                nmatches--;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nmatches;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外，<code>LoopClosing</code>线程中<code>ComputeSim3()</code>调用的<code>SearchByBoW</code>的函数声明为：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> ORBmatcher::SearchByBoW(KeyFrame *pKF1, KeyFrame *pKF2, <span class="built_in">vector</span>&lt;MapPoint *&gt; &amp;vpMatches12)</span><br></pre></td></tr></table></figure></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://blog.csdn.net/qq_24893115/article/details/52629248" target="_blank" rel="noopener">https://blog.csdn.net/qq_24893115/article/details/52629248</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;ORB-SLAM&lt;/strong&gt;中使用了多种特征匹配的奇技淫巧，其中之一就是利用&lt;strong&gt;词袋信息&lt;/strong&gt;进行引导匹配&lt;code&gt;SearchByBoW&lt;/code&gt;：利用了&lt;code&gt;BOW&lt;/code&gt;里的正向引导进行两帧之间的匹配，核心点在于位于同一个节点处的特征才有可能属于同一匹配，相较于暴力匹配匹配速度更快。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="ORB" scheme="https://www.vincentqin.tech/tags/ORB/"/>
    
      <category term="特征匹配" scheme="https://www.vincentqin.tech/tags/%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D/"/>
    
  </entry>
  
  <entry>
    <title>2019年浙大CADCG暑假SLAM培训部分课件</title>
    <link href="https://www.vincentqin.tech/posts/slam-summer-courses-CADCG-Lab/"/>
    <id>https://www.vincentqin.tech/posts/slam-summer-courses-CADCG-Lab/</id>
    <published>2019-07-20T13:52:15.000Z</published>
    <updated>2020-04-17T13:35:50.309Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>由浙江大学CAD&amp;CG国家重点实验室主办、浙江大学-商汤三维视觉联合实验室协办的“SLAM技术及应用”暑期学校于7月20日如期拉开序幕。<br>今天（2019/07/20）看到直播的时候已经是下午4点半了，只听到刘浩敏讲到末尾的一段，幸好主办方提供了讲座课件，Download下来慢慢看。</p><a id="more"></a><h2 id="2019年课件"><a href="#2019年课件" class="headerlink" title="2019年课件"></a>2019年课件</h2><ul><li><p>2019年7月20日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/Camera-model-and-Projection-Transformation.pdf" target="_blank" rel="noopener">相机模型与投影变换</a>（讲者：章国锋）</p></li><li><p>2019年7月20日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/SfM-GuofengZhang.pdf" target="_blank" rel="noopener">运动恢复结构</a>（讲者：章国锋）</p></li><li><p>2019年7月20日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/BA-haominLiu.pdf" target="_blank" rel="noopener">集束<strong>Bundle Adjustment</strong>调整</a>（讲者：刘浩敏）</p></li><li><p>2019年7月21日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/3D-Tutorial-ShuhanShen.pdf" target="_blank" rel="noopener">三维重建</a>（讲者：申抒含）</p></li><li><p>2019年7月21日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/V-SLAM-GuofengZhang.pdf" target="_blank" rel="noopener">视觉SLAM</a>（讲者：章国锋）</p></li><li><p>2019年7月21日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/RGB-D-SLAM-HanqingJiang.pdf" target="_blank" rel="noopener">RGB-D SLAM</a>（讲者：姜翰青）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/VI-SLAM.pdf" target="_blank" rel="noopener">视觉惯性SLAM</a>（讲者：黄国权）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/3D-recognition-track-XueyingQin.pdf" target="_blank" rel="noopener">三维物体的识别与跟踪</a>（讲者：秦学英）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/R2VR-VR-Fusion-Technology.pdf" target="_blank" rel="noopener">从现实到虚拟现实-虚实融合呈现技术</a>（讲者：王锐）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/Mav-DanpingZou.pdf" target="_blank" rel="noopener">面向SLAM研究的无人机快速入门与平台选择</a>（讲者：邹丹平）</p></li><li><p>2019年7月23日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/MobileVR-System-Design-Application.pdf" target="_blank" rel="noopener">移动增强现实系统的设计与应用案例解析</a>（讲者：章国锋）</p></li><li><p>2019年7月23日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/AR-Applications.pdf" target="_blank" rel="noopener">AR应用开发</a>（讲者：盛崇山）</p></li><li><p><strong><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2019-SLAM-Summer-School-slides.zip" target="_blank" rel="noopener">打包下载</a></strong></p></li></ul><h2 id="2018年课件"><a href="#2018年课件" class="headerlink" title="2018年课件"></a>2018年课件</h2><ul><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E7%9B%B8%E6%9C%BA%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%8A%95%E5%BD%B1%E5%8F%98%E6%8D%A2-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">相机模型与投影变换-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E8%BF%90%E5%8A%A8%E6%81%A2%E5%A4%8D%E7%BB%93%E6%9E%84-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">运动恢复结构-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E9%9B%86%E6%9D%9F%E8%B0%83%E6%95%B4-%E5%88%98%E6%B5%A9%E6%95%8F.pdf" target="_blank" rel="noopener">集束调整-刘浩敏</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E6%B7%B1%E5%BA%A6%E6%81%A2%E5%A4%8D%E4%B8%8E%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">深度恢复与三维重建-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E8%A7%86%E8%A7%89SLAM-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">视觉SLAM-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E8%A7%86%E8%A7%89%E6%83%AF%E5%AF%BCSLAM-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">视觉惯导SLAM-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/Visual-Inertial%20SLAM-%E6%9D%8E%E5%90%8D%E6%9D%A8.pdf" target="_blank" rel="noopener">Visual-Inertial SLAM-李名杨</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/RGB-D%20SLAM-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">RGB-D SLAM-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E5%9C%B0%E9%9D%A2%E6%97%A0%E4%BA%BA%E5%B9%B3%E5%8F%B0%E4%B8%AD%E7%9A%84SLAM%E6%8A%80%E6%9C%AF-%E5%88%98%E5%8B%87.pdf" target="_blank" rel="noopener">地面无人平台中的SLAM技术-刘勇</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E5%9F%BA%E4%BA%8E%E7%8E%AF%E5%A2%83%E7%BB%93%E6%9E%84%E5%8C%96%E7%89%B9%E6%80%A7%E7%9A%84%E8%A7%86%E8%A7%89SLAM%E6%96%B9%E6%B3%95-%E9%82%B9%E4%B8%B9%E5%B9%B3.pdf" target="_blank" rel="noopener">基于环境结构化特性的视觉SLAM方法-邹丹平</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E7%A7%BB%E5%8A%A8%E5%A2%9E%E5%BC%BA%E7%8E%B0%E5%AE%9E%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B%E8%A7%A3%E6%9E%90-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">移动增强现实系统设计与应用案例解析-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/VR-Ruiwang1.pdf" target="_blank" rel="noopener">虚实融合显示与绘制技术-王锐</a></p></li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://mp.weixin.qq.com/s/PV_xLmuE-HpnUgnJ5GyMOA" target="_blank" rel="noopener">第二届“SLAM技术及应用” 暑期学校与研讨会圆满落幕</a></li><li>章国锋主页：<a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank" rel="noopener">地址</a></li><li>CAD&amp;CG实验室主页，<a href="http://www.zjucvg.net/" target="_blank" rel="noopener">地址</a></li><li><a href="https://github.com/zju3dv" target="_blank" rel="noopener">CAD&amp;CG Github</a></li><li><a href="http://www.zjucvg.net/senseslam/" target="_blank" rel="noopener">SenseSLAM</a>,浙大-商汤三维视觉联合实验室</li><li>Shuhan Shen (申抒含)主页，<a href="http://vision.ia.ac.cn/Faculty/shshen/index.htm" target="_blank" rel="noopener">地址</a></li><li>姜翰青，<a href="https://www.linkedin.com/in/%E7%BF%B0%E9%9D%92-%E5%A7%9C-1194b411b/`" target="_blank" rel="noopener">Linkedin</a></li><li>讲座直播地址：<a href="https://www.douyu.com/7275221" target="_blank" rel="noopener">https://www.douyu.com/7275221</a></li><li>商汤泰坦公开课，<a href="https://cloud.xylink.com/live/v/2c9497116bb8b075016c082adec66ea7" target="_blank" rel="noopener">直播地址</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由浙江大学CAD&amp;amp;CG国家重点实验室主办、浙江大学-商汤三维视觉联合实验室协办的“SLAM技术及应用”暑期学校于7月20日如期拉开序幕。&lt;br&gt;今天（2019/07/20）看到直播的时候已经是下午4点半了，只听到刘浩敏讲到末尾的一段，幸好主办方提供了讲座课件，Download下来慢慢看。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>Filebrowser：一款轻量级个人网盘</title>
    <link href="https://www.vincentqin.tech/posts/build-filebrowser/"/>
    <id>https://www.vincentqin.tech/posts/build-filebrowser/</id>
    <published>2019-07-14T13:48:47.000Z</published>
    <updated>2020-03-31T15:27:36.717Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><br><br><img src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/filebrowser-banner.png" alt><br><br></p><a id="more"></a><h1 id="个人网盘-Filebrowser"><a href="#个人网盘-Filebrowser" class="headerlink" title="个人网盘 Filebrowser"></a>个人网盘 Filebrowser</h1><p>服务器仅仅用于科学上网未免有些浪费了，是时候尝试一下自建个人网盘和图床了。</p><h2 id="如何安装"><a href="#如何安装" class="headerlink" title="如何安装"></a>如何安装</h2><p><a href="https://filebrowser.xyz/installation" target="_blank" rel="noopener">官方</a>给出了一键安装大法，进入服务器输入以下命令就可以了。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://filebrowser.xyz/get.sh | bash</span><br></pre></td></tr></table></figure></p><h2 id="首次配置"><a href="#首次配置" class="headerlink" title="首次配置"></a>首次配置</h2><p>当安装好之后，你并不能立即使用它，需要修改一些配置(以下内容参考[<a href="https://www.mivm.cn/filebrowser/" target="_blank" rel="noopener">米V米</a>]的教程)。</p><ul><li>创建配置数据库：<code>filebrowser -d /etc/filebrowser.db config init</code></li><li>设置监听地址：<code>filebrowser -d /etc/filebrowser.db config set --address 0.0.0.0</code></li><li>设置监听端口：<code>filebrowser -d /etc/filebrowser.db config set --port 8088</code></li><li>设置语言环境(中文)：<code>filebrowser -d /etc/filebrowser.db config set --locale zh-cn</code></li><li>设置日志位置：<code>filebrowser -d /etc/filebrowser.db config set --log /var/log/filebrowser.log</code></li><li>添加一个用户：<code>filebrowser -d /etc/filebrowser.db users add root password --perm.admin</code>，其中的root和password分别是用户名和密码，根据自己的需求更改。</li></ul><p>有关更多配置的选项，可以参考官方文档：<a href="https://docs.filebrowser.xyz/" target="_blank" rel="noopener">https://docs.filebrowser.xyz/</a><br>配置修改好以后，就可以启动FileBrowser了，使用-d参数指定配置数据库路径。示例：<code>filebrowser -d /etc/filebrowser.db</code><br>启动成功就可以使用浏览器访问FileBrowser了，在浏览器输入 <code>服务器IP:端口</code>，示例：<code>http://192.168.1.1:8088</code></p><p>然后会看到 FileBrowser 的登陆界面：<br><img src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/filebrowser-login.png" alt></p><p>用刚刚创建的用户登陆，最后就可以放心使用啦~<br><img src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/filebrowser-demo.gif" alt></p><h2 id="后续配置"><a href="#后续配置" class="headerlink" title="后续配置"></a>后续配置</h2><p>完成以上过程之后已经可以正常访问个人网盘了，但是假如服务器重启之后就必须重新输入<code>filebrowser -d /etc/filebrowser.db</code>才能运行，为了省去这一步，我们需要进行设置服务器开机自动启动FileBrowser。<br>这里我们使用的是systemd 大法：<br>首先下载 FileBrowser 的 <code>service</code>文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl https://cdn.mivm.cn/www.mivm.cn/archives/filebrowser/filebrowser.service -o /lib/systemd/system/filebrowser.service</span><br></pre></td></tr></table></figure><p>如果你的运行命令不是<code>/usr/local/bin/filebrowser -d /etc/filebrowser.db</code>，需要对 service 文件进行修改，将文件的 ExecStart 改为你的运行命令，更改完成后需要输入<code>systemctl daemon-reload</code>。</p><p>下面祭出常用的命令：</p><ul><li>运行：<code>systemctl start filebrowser.service</code></li><li>停止运行：<code>systemctl stop filebrowser.service</code></li><li>开机启动：<code>systemctl enable filebrowser.service</code></li><li>取消开机启动：<code>systemctl disable filebrowser.service</code></li><li>查看运行状态：<code>systemctl status filebrowser.service</code></li></ul><p>这里有个<a href="https://youtu.be/sE31MBvOjxk" target="_blank" rel="noopener">视频教程</a>，需要科学上网查看。</p><p><br></p><h1 id="个人图床Chevereto"><a href="#个人图床Chevereto" class="headerlink" title="个人图床Chevereto"></a>个人图床Chevereto</h1><p>先给出安装好的样子~<br><img src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/Chevereto.png" alt></p><p>安装教程这里<a href="https://gist.github.com/biezhi/f90923b48863c7d745481ccdd678ccab" target="_blank" rel="noopener">install_chevereto.md</a>已经写得非常详细了，在此不做详细介绍。这里有个<a href="https://youtu.be/kShgzNkXRak" target="_blank" rel="noopener">视频教程</a>，我主要按照这个教程进行配置的，需要科学上网查看。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://youtu.be/sE31MBvOjxk" target="_blank" rel="noopener">把玩我的 VPS 主机 - 分分钟搭建时尚简洁的在线网盘</a></li><li><a href="https://youtu.be/kShgzNkXRak" target="_blank" rel="noopener">搭建漂亮的私人图床</a></li><li><a href="https://biezhi.me/" target="_blank" rel="noopener">王爵 nice的主页</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;img src=&quot;https://vincentqin.gitee.io/blogresource-1/build-filebrowser/filebrowser-banner.png&quot; alt&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="个人网盘" scheme="https://www.vincentqin.tech/tags/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%9B%98/"/>
    
  </entry>
  
  <entry>
    <title>SuperPoint: Self-Supervised Interest Point Detection and Description</title>
    <link href="https://www.vincentqin.tech/posts/superpoint/"/>
    <id>https://www.vincentqin.tech/posts/superpoint/</id>
    <published>2019-06-23T06:02:06.000Z</published>
    <updated>2020-03-31T15:17:35.152Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文出自近几年备受瞩目的创业公司<a href="https://www.magicleap.com/" target="_blank" rel="noopener">MagicLeap</a>，发表在CVPR 2018,一作<a href="http://www.danieldetone.com/" target="_blank" rel="noopener">Daniel DeTone</a>，<strong>[<a href="https://arxiv.org/abs/1712.07629" target="_blank" rel="noopener">paper</a>]</strong>，<strong>[<a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork/blob/master/assets/DL4VSLAM_talk.pdf" target="_blank" rel="noopener">slides</a>]</strong>，<strong>[<a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork" target="_blank" rel="noopener">code</a>]</strong>。</p><p>这篇文章设计了一种自监督网络框架，能够同时提取特征点的位置以及描述子。相比于patch-based方法，本文提出的算法能够在原始图像提取到像素级精度的特征点的位置及其描述子。<br>本文提出了一种单映性适应（<code>Homographic Adaptation</code>）的策略以增强特征点的复检率以及跨域的实用性（这里跨域指的是synthetic-to-real的能力，网络模型在虚拟数据集上训练完成，同样也可以在真实场景下表现优异的能力）。</p><a id="more"></a><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>诸多应用（诸如SLAM/SfM/相机标定/立体匹配）的首要一步就是特征点提取，这里的特征点指的是<strong>能够在不同光照&amp;不同视角下都能够稳定且可重复检测的2D图像点位置</strong>。</p><p>基于CNN的算法几乎在以图像作为输入的所有领域表现出相比于人类特征工程更加优秀的表达能力。目前已经有一些工作做类似的任务，例如人体位姿估计,目标检测以及室内布局估计等。这些算法以通常以大量的人工标注作为GT，这些精心设计的网络用来训练以得到人体上的角点，例如嘴唇的边缘点亦或人体的关节点，但是这里的问题是这里的点实际是ill-defined（我的理解是，这些点有可能是特征点，但仅仅是一个大概的位置，是特征点的子集，并没有真正的把特征点的概念定义清楚）。</p><p>本文采用了非人工监督的方法提取真实场景的特征点。本文设计了一个由特征点检测器监督的具有伪真值数据集，而非是大量的人工标记。为了得到伪真值，本文首先在大量的虚拟数据集上训练了一个全卷积网络（FCNN），这些虚拟数据集由一些基本图形组成，例如有线段、三角形、矩形和立方体等，这些基本图形具有没有争议的特征点位置，文中称这些特征点为<code>MagicPoint</code>，这个pre-trained的检测器就是<code>MagicPoint</code>检测器。这些<code>MagicPoint</code>在虚拟场景的中检测特征点的性能明显优于传统方式，但是在真实的复杂场景中表现不佳，此时作者提出了一种多尺度多变换的方法<code>Homographic Adaptation</code>。对于输入图像而言，<code>Homographic Adaptation</code>通过对图像进行多次不同的尺度/角度变换来帮助网络能够在不同视角不同尺度观测到特征点。<br>综上：<strong>SuperPoint = MagicPoint+Homographic Adaptation</strong></p><h1 id="算法优劣对比"><a href="#算法优劣对比" class="headerlink" title="算法优劣对比"></a>算法优劣对比</h1><p><img src="https://vincentqin.gitee.io/blogresource-1/superpoint/tab_1.png" alt="fig1_table1"></p><ul><li>基于图像块的算法导致特征点位置精度不够准确；</li><li>特征点与描述子分开进行训练导致运算资源的浪费，网络不够精简，实时性不足；或者仅仅训练特征点或者描述子的一种，不能用同一个网络进行联合训练；</li></ul><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_3.png" alt="fig3"></p><p>上图可见特征点检测器以及描述子网络共享一个单一的前向encoder，只是在decoder时采用了不同的结构，根据任务的不同学习不同的网络参数。这也是本框架与其他网络的不同之处：其他网络采用的是先训练好特征点检测网络，然后再去进行对特征点描述网络进行训练。<br>网络共分成以下4个主要部分，在此进行详述：</p><h2 id="1-Shared-Encoder-共享的编码网络"><a href="#1-Shared-Encoder-共享的编码网络" class="headerlink" title="1. Shared Encoder 共享的编码网络"></a>1. Shared Encoder 共享的编码网络</h2><p>从上图可以看到，整体而言，本质上有两个网络，只是前半部分共享了一部分而已。本文利用了VGG-style的encoder以用于降低图像尺寸，encoder包括卷积层，max-pooling层，以及非线性激活层。通过3个max-pooling层将图像的尺寸变成$H_c = H/8$和$H_c = H/8$，经过encoder之后，图像由$I \in \mathcal{R}^{H \times W}$变为张量$\mathcal{B} \in \mathbb{R}^{H_c \times W_c \times F}$</p><h2 id="2-Interest-Point-Decoder"><a href="#2-Interest-Point-Decoder" class="headerlink" title="2. Interest Point Decoder"></a>2. Interest Point Decoder</h2><p><img src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_10_magicPoint1.png" alt="fig_10_magicPoint1"></p><p>这里介绍的是特征点的解码端。每个像素的经过该解码器的输出是该像素是特征点的概率（probability of “point-ness”）。<br>通常而言，我们可以通过反卷积得到上采样的图像，但是这种操作会导致计算量的骤增以及会引入一种“checkerboard artifacts”。因此本文设计了一种带有“特定解码器”（这种解码器没有参数）的特征点检测头以减小模型计算量（子像素卷积）。<br>例如：输入张量的维度是$\mathbb{R}^{H_c \times W_c \times 65}$，输出维度$\mathbb{R}^{H \times W}$，即图像的尺寸。这里的65表示原图$8 \times 8$的局部区域，加上一个非特征点<code>dustbin</code>。通过在channel维度上做softmax，非特征点dustbin会被删除，同时会做一步图像的<code>reshape</code>：$\mathbb{R}^{H_c \times W_c \times 64} \Rightarrow \mathbb{R}^{H \times W}$ 。（这就是<strong><a href="https://blog.csdn.net/leviopku/article/details/84975282" target="_blank" rel="noopener">子像素卷积</a></strong>的意思，俗称像素洗牌）</p><h2 id="3-Descriptor-Decoder"><a href="#3-Descriptor-Decoder" class="headerlink" title="3. Descriptor Decoder"></a>3. Descriptor Decoder</h2><p>首先利用类似于UCN的网络得到一个半稠密的描述子（此处参考文献<a href="https://arxiv.org/abs/1606.03558" target="_blank" rel="noopener">UCN</a>），这样可以减少算法训练内存开销同时减少算法运行时间。之后通过双三次多项式插值得到其余描述，然后通过<code>L2-normalizes</code>归一化描述子得到统一的长度描述。特征维度由$\mathcal{D} \in \mathbb{R}^{H_c \times W_c \times D}$变为$\mathbb{R}^{H\times W \times D}$ 。</p><p><img src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_11_des_decoder.png" alt="fig_11_des_decoder"></p><p>由特征点得到其描述子的过程文中没有细讲，看了一下<a href="https://github.com/pytorch/pytorch/blob/f064c5aa33483061a48994608d890b968ae53fb5/aten/src/THNN/generic/SpatialGridSamplerBilinear.c" target="_blank" rel="noopener">源代码</a>就明白了。其实该过程主要用了一个函数即<code>grid_sample</code>，画了一个草图作为解释。</p><ul><li>图像尺寸归一化：首先对图像的尺寸进行归一化，(-1,-1)表示原来图像的(0,0)位置，(1,1)表示原来图像的(H-1,W-1)位置，这样一来，特征点的位置也被归一化到了相应的位置。</li><li>构建grid：将归一化后的特征点罗列起来，构成一个尺度为1*1*K*2的张量，其中K表示特征数量，2分别表示xy坐标。</li><li>特征点位置反归一化：根据输入张量的H与W对grid(1,1,0,:)（表示第一个特征点，其余特征点类似）进行反归一化，其实就是按照比例进行缩放+平移，得到反归一化特征点在张量某个slice（通道）上的位置；但是这个位置可能并非为整像素，此时要对其进行双线性插值补齐，然后其余slice按照同样的方式进行双线性插值。注：代码中实际的就是双线性插值，并非文中讲的双三次插值；</li><li>输出维度：1*C*1*K。</li></ul><p><img src="https://vincentqin.gitee.io/blogresource-1/superpoint/grid_sample.png" alt></p><h2 id="4-误差构建"><a href="#4-误差构建" class="headerlink" title="4. 误差构建"></a>4. 误差构建</h2><script type="math/tex; mode=display">\begin{array}{l}{\mathcal{L}\left(\mathcal{X}, \mathcal{X}^{\prime}, \mathcal{D}, \mathcal{D}^{\prime} ; Y, Y^{\prime}, S\right)=} \\ {\qquad \mathcal{L}_{p}(\mathcal{X}, Y)+\mathcal{L}_{p}\left(\mathcal{X}^{\prime}, Y^{\prime}\right)+\lambda \mathcal{L}_{d}\left(\mathcal{D}, \mathcal{D}^{\prime}, S\right)}\end{array}</script><p>可见损失函数由两项组成，其中一项为特征点检测loss$\mathcal{L}_{p}$ ，另外一项是描述子的loss$\mathcal{L}_{d}$。</p><p>对于检测项loss，此时采用了交叉熵损失函数:</p><script type="math/tex; mode=display">\mathcal{L}_{p}(\mathcal{X}, Y)=\frac{1}{H_{c} W_{c}} \sum_{h=1 \atop w=1}^{H_{c}, W_{c}} l_{p}\left(\mathbf{x}_{h w} ; y_{h w}\right)</script><p>其中：</p><script type="math/tex; mode=display">l_{p}\left(\mathbf{x}_{h w} ; y\right)=-\log \left(\frac{\exp \left(\mathbf{x}_{h w y}\right)}{\sum_{k=1}^{65} \exp \left(\mathbf{x}_{h w k}\right)}\right)</script><p>描述子的损失函数:</p><script type="math/tex; mode=display">\mathcal{L}_{d}\left(\mathcal{D}, \mathcal{D}^{\prime}, S\right)=\frac{1}{\left(H_{c} W_{c}\right)^{2}} \sum_{h=1 \atop w=1}^{H_{c}, W_{c}} \sum_{h^{\prime}=1 \atop w^{\prime}=1}^{H_{c}, W_{c}} l_{d}\left(\mathbf{d}_{h w}, \mathbf{d}_{h^{\prime} w^{\prime}}^{\prime} ; s_{h w h^{\prime} w^{\prime}}\right)</script><p>其中<script type="math/tex">l_{d}</script>为<code>Hinge-loss</code>（合页损失函数，用于SVM，如支持向量的软间隔，可以保证最后解的稀疏性）；</p><script type="math/tex; mode=display">l_{d}\left(\mathbf{d}, \mathbf{d}^{\prime} ; s\right)=\lambda_{d} * s * \max \left(0, m_{p}-\mathbf{d}^{T} \mathbf{d}^{\prime}\right)+(1-s) * \max \left(0, \mathbf{d}^{T} \mathbf{d}^{\prime}-m_{n}\right)</script><p>同时指示函数为<script type="math/tex">s_{h w h^{\prime} w^{\prime}}</script>,$S$表示所有正确匹配对集合:</p><script type="math/tex; mode=display">s_{h w h^{\prime} w^{\prime}}=\left\{\begin{array}{ll}{1,} & {\text { if }\left\|\widehat{\mathcal{H} \mathbf{p}_{h w}}-\mathbf{p}_{h^{\prime} w^{\prime}}\right\| \leq 8} \\ {0,} & {\text { otherwise }}\end{array}\right.</script><h1 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h1><p><img src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_2.png" alt="fig2"></p><p>本文一共设计了两个网络，一个是<code>BaseDetector</code>，用于检测角点（注意，此处提取的并不是最终输出的特征点，可以理解为候选的特征点），另一个是<code>SuperPoint</code>网络，输出特征点和描述子。</p><p>网络的训练共分为三个步骤：</p><ol><li>第一步是采用虚拟的三维物体作为数据集，训练网络去提取角点，这里得到的是<code>BaseDetector</code>即，<code>MagicPoint</code>；</li><li>使用真实场景图片，用第一步训练出来的网络<code>MagicPoint</code> +<code>Homographic Adaptation</code>提取角点，这一步称作兴趣点自标注（Interest Point Self-Labeling）</li><li>对第二步使用的图片进行几何变换得到新的图片，这样就有了已知位姿关系的图片对，把这两张图片输入SuperPoint网络，提取特征点和描述子。</li></ol><h2 id="预训练Magic-Point"><a href="#预训练Magic-Point" class="headerlink" title="预训练Magic Point"></a>预训练Magic Point</h2><p>此处参考作者之前发表的一篇论文<strong>[<a href="https://arxiv.org/abs/1707.07410" target="_blank" rel="noopener">Toward Geometric Deep SLAM</a>]</strong>，其实就是<code>MagicPoint</code>，在此不做展开介绍。<br><img src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_10_magicPoint1.png" alt="fig2"></p><p><img src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_4.png" alt="fig4"></p><h2 id="Homographic-Adaptation"><a href="#Homographic-Adaptation" class="headerlink" title="Homographic Adaptation"></a>Homographic Adaptation</h2><p>算法在虚拟数据集上表现极其优秀，但是在真实场景下表示没有达到预期，此时本文进行了<code>Homographic Adaptation</code>。<br>作者使用的数据集是<code>MS-COCO</code>，为了使网络的泛化能力更强，本文不仅使用原始了原始图片，而且对每张图片进行随机的旋转和缩放形成新的图片，新的图片也被用来进行识别。这一步其实就类似于训练里常用的数据增强。经过一系列的单映变换之后特征点的复检率以及普适性得以增强。值得注意的是，在实际训练时，这里采用了迭代使用单映变换的方式，例如使用优化后的特征点检测器重新进行单映变换进行训练，然后又可以得到更新后的检测器，如此迭代优化，这就是所谓的self-supervisd。<br><img src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_5.png" alt="fig5"></p><p><img src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_9_HA.png" alt="fig_9_HA"></p><p>最后的关键点检测器，即<script type="math/tex">\hat{F}\left(I ; f_{\theta}\right)</script>，可以表示为再所有随机单映变换/反变换的聚合：</p><script type="math/tex; mode=display">\hat{F}\left(I ; f_{\theta}\right)=\frac{1}{N_{h}} \sum_{i=1}^{N_{h}} \mathcal{H}_{i}^{-1} f_{\theta}\left(\mathcal{H}_{i}(I)\right)</script><p><img src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_6.png" alt="fig_6"></p><h2 id="构建残差，迭代优化描述子以及检测器"><a href="#构建残差，迭代优化描述子以及检测器" class="headerlink" title="构建残差，迭代优化描述子以及检测器"></a>构建残差，迭代优化描述子以及检测器</h2><p>利用上面网络得到的关键点位置以及描述子表示构建残差，利用<code>ADAM</code>进行优化。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><img src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_8.png" alt="fig_8"></p><p><img src="https://vincentqin.gitee.io/blogresource-1/superpoint/tab_3.png" alt="tab_3"></p><p><img src="https://vincentqin.gitee.io/blogresource-1/superpoint/tab_4.png" alt="tab_4"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol><li>it is possible to transfer knowledge from a synthetic dataset onto real-world images</li><li>sparse interest point detection and description can be cast as a single, efficient convolutional neural network</li><li>the resulting system works well for geometric computer vision matching tasks such as Homography Estimation</li></ol><p>未来工作:</p><ol><li>研究Homographic Adaptation能否在语义分割任务或者目标检测任务中有提升作用</li><li>兴趣点提取以及描述这两个任务是如何影响彼此的</li></ol><p>作者最后提到，他相信该网络能够解决SLAM或者SfM领域的数据关联<em>，并且</em><code>learning-based</code>前端可以使得诸如机器人或者AR等应用获得更加鲁棒。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文出自近几年备受瞩目的创业公司&lt;a href=&quot;https://www.magicleap.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MagicLeap&lt;/a&gt;，发表在CVPR 2018,一作&lt;a href=&quot;http://www.danieldetone.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Daniel DeTone&lt;/a&gt;，&lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/abs/1712.07629&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;，&lt;strong&gt;[&lt;a href=&quot;https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork/blob/master/assets/DL4VSLAM_talk.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;slides&lt;/a&gt;]&lt;/strong&gt;，&lt;strong&gt;[&lt;a href=&quot;https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;code&lt;/a&gt;]&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这篇文章设计了一种自监督网络框架，能够同时提取特征点的位置以及描述子。相比于patch-based方法，本文提出的算法能够在原始图像提取到像素级精度的特征点的位置及其描述子。&lt;br&gt;本文提出了一种单映性适应（&lt;code&gt;Homographic Adaptation&lt;/code&gt;）的策略以增强特征点的复检率以及跨域的实用性（这里跨域指的是synthetic-to-real的能力，网络模型在虚拟数据集上训练完成，同样也可以在真实场景下表现优异的能力）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="CV" scheme="https://www.vincentqin.tech/categories/CV/"/>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="Deep Learning" scheme="https://www.vincentqin.tech/tags/Deep-Learning/"/>
    
      <category term="特征提取" scheme="https://www.vincentqin.tech/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
      <category term="SuperPoint" scheme="https://www.vincentqin.tech/tags/SuperPoint/"/>
    
      <category term="MagicLeap" scheme="https://www.vincentqin.tech/tags/MagicLeap/"/>
    
      <category term="深度学习" scheme="https://www.vincentqin.tech/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Bygones</title>
    <link href="https://www.vincentqin.tech/posts/bygones/"/>
    <id>https://www.vincentqin.tech/posts/bygones/</id>
    <published>2019-05-28T16:57:39.000Z</published>
    <updated>2020-03-02T15:15:31.025Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><!-- <div id="dplayer1" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer1"),"loop":true,"video":{"url":"https://www.bilibili.com/video/av92924509/"},"danmaku":{"id":"bbe4286bf164ef6a1497f18a7b42ff944e684b821","api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> --><iframe src="//player.bilibili.com/player.html?aid=92924509&cid=158651721&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><a id="more"></a><p><strong><center>致我们逝去的青春！</center></strong></p>]]></content>
    
    <summary type="html">
    
      &lt;!-- &lt;div id=&quot;dplayer1&quot; class=&quot;dplayer hexo-tag-dplayer-mark&quot; style=&quot;margin-bottom: 20px;&quot;&gt;&lt;/div&gt;&lt;script&gt;(function(){var player = new DPlayer({&quot;container&quot;:document.getElementById(&quot;dplayer1&quot;),&quot;loop&quot;:true,&quot;video&quot;:{&quot;url&quot;:&quot;https://www.bilibili.com/video/av92924509/&quot;},&quot;danmaku&quot;:{&quot;id&quot;:&quot;bbe4286bf164ef6a1497f18a7b42ff944e684b821&quot;,&quot;api&quot;:&quot;https://api.prprpr.me/dplayer/&quot;}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()&lt;/script&gt; --&gt;
&lt;iframe src=&quot;//player.bilibili.com/player.html?aid=92924509&amp;cid=158651721&amp;page=1&quot; scrolling=&quot;no&quot; border=&quot;0&quot; frameborder=&quot;no&quot; framespacing=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt; &lt;/iframe&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Black Hole</title>
    <link href="https://www.vincentqin.tech/posts/first-black-hole/"/>
    <id>https://www.vincentqin.tech/posts/first-black-hole/</id>
    <published>2019-04-10T15:55:01.000Z</published>
    <updated>2020-03-31T14:55:18.422Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://vincentqin.gitee.io/blogresource-2/first-black-hole/big-blackhole.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Awesome CV Works</title>
    <link href="https://www.vincentqin.tech/posts/awesome-works/"/>
    <id>https://www.vincentqin.tech/posts/awesome-works/</id>
    <published>2019-03-31T12:15:41.000Z</published>
    <updated>2019-05-26T02:26:30.494Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>The post contains papers-with-code about SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc. <a href="https://github.com/Vincentqyw/Recent-Stars-2019" target="_blank" rel="noopener"><img src="https://img.shields.io/github/stars/Vincentqyw/Recent-Stars-2019.svg?logo=github&amp;label=Stars" alt="GitHub stars"></a></p><a id="more"></a><h2 id="SLAM-related"><a href="#SLAM-related" class="headerlink" title="SLAM related"></a>SLAM related</h2><ul><li><a href="https://github.com/kiran-mohan/SLAM-Algorithms-Octave" target="_blank" rel="noopener">Solutions to assignments of Robot Mapping Course WS 2013/14 by Dr. Cyrill Stachniss at University of Freiburg</a>,SLAM算法学习课后作业答案</li><li><a href="https://github.com/RonaldSun/VI-Stereo-DSO" target="_blank" rel="noopener">Direct sparse odometry combined with stereo cameras and IMU</a>,双目DSO+IMU</li><li><a href="https://github.com/HorizonAD/stereo_dso" target="_blank" rel="noopener">Direct Sparse Odometry with Stereo Cameras</a>,双目DSO</li><li><a href="https://github.com/uoip/g2opy" target="_blank" rel="noopener">Python binding of SLAM graph optimization framework g2o</a>,python版本的g2o实现</li><li><a href="https://github.com/mihaidusmanu/d2-net" target="_blank" rel="noopener">D2-Net: A Trainable CNN for Joint Description and Detection of Local Features</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1905.03561" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://dsmn.ml/publications/d2-net.html" target="_blank" rel="noopener">Project Page</a>]</strong>, 深度学习描述子</li><li><a href="https://github.com/ethz-asl/orb_slam_2_ros" target="_blank" rel="noopener">ROS interface for ORBSLAM2</a>,ROS版本的ORBSLAM2</li><li><a href="https://github.com/yan99033/CNN-SVO" target="_blank" rel="noopener">CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction</a>， <strong>[<a href="https://arxiv.org/pdf/1810.01011.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/ManiiXu/VINS-Mono-Learning" target="_blank" rel="noopener">VINS-Mono-Learning</a>，代码注释版VINS-Mono，初学者学习</li><li><a href="https://github.com/xdspacelab/openvslam" target="_blank" rel="noopener">OpenVSLAM: Versatile Visual SLAM Framework</a>,  <strong>[<a href="https://openvslam.readthedocs.io/" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/fabianschenk/RESLAM" target="_blank" rel="noopener">RESLAM: A real-time robust edge-based SLAM system</a>, <strong>[<a href="https://github.com/fabianschenk/fabianschenk.github.io/raw/master/files/schenk_icra_2019.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/rubengooj/pl-slam" target="_blank" rel="noopener">PL-SLAM: a Stereo SLAM System through the Combination of Points and Line Segments</a>, <strong>[<a href="https://arxiv.org/abs/1705.09479" target="_blank" rel="noopener">Paper</a>]</strong>，线特征SLAM</li><li><a href="https://github.com/YipuZhao/GF_PL_SLAM" target="_blank" rel="noopener">Good Line Cutting: towards Accurate Pose Tracking of Line-assisted VO/VSLAM</a>, ECCV 2018, <strong>[<a href="https://sites.google.com/site/zhaoyipu/good-feature-visual-slam" target="_blank" rel="noopener">Project Page</a>]</strong>, 改进的PL-SLAM</li><li><a href="https://github.com/leoshine/Spherical_Regression" target="_blank" rel="noopener">Spherical Regression: Learning Viewpoints, Surface Normals and 3D Rotations on n-Spheres</a>, CVPR 2019, <strong>[<a href="http://arxiv.org/abs/1904.05404" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/icsl-Jeon/traj_gen_vis" target="_blank" rel="noopener">svo_edgelet</a>, 在线轨迹生成</li><li><a href="https://github.com/TimboKZ/caltech_samaritan" target="_blank" rel="noopener">Drone SLAM project for Caltech’s ME 134 Autonomy class</a>, <strong>[<a href="https://github.com/TimboKZ/caltech_samaritan/blob/master/CS134_Final_Project_Report.pdf" target="_blank" rel="noopener">PDF</a>]</strong></li><li><a href="https://github.com/icsl-Jeon/traj_gen_vis" target="_blank" rel="noopener">Online Trajectory Generation of a MAV for Chasing a Moving Target in 3D Dense Environments</a>, <strong>[<a href="https://arxiv.org/pdf/1904.03421.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/AtsushiSakai/PythonRobotics" target="_blank" rel="noopener">PythonRobotics</a>,<strong>[<a href="https://arxiv.org/abs/1808.10703" target="_blank" rel="noopener">Paper</a>]</strong>, <a href="https://github.com/onlytailei/CppRobotics" target="_blank" rel="noopener">CppRobotics</a></li><li><a href="https://github.com/izhengfan/ba_demo_ceres" target="_blank" rel="noopener">Bundle adjustment demo using Ceres Solver</a>,  <strong>[<a href="https://fzheng.me/2018/01/23/ba-demo-ceres/" target="_blank" rel="noopener">Blog</a>]</strong>, ceres实现BA</li><li><a href="https://github.com/shichaoy/cube_slam" target="_blank" rel="noopener">CubeSLAM: Monocular 3D Object Detection and SLAM</a>, <strong>[<a href="https://arxiv.org/abs/1806.00557" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/sshaoshuai/PointRCNN" target="_blank" rel="noopener">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.04244" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/nrupatunga/GIST-global-Image-Descripor" target="_blank" rel="noopener">GIST-Global Image Descriptor</a>, GIST描述子</li><li><a href="https://github.com/ethz-asl/mav_voxblox_planning" target="_blank" rel="noopener">mav voxblox planning</a>, MAV planning tools using voxblox as the map representation.</li><li><a href="https://github.com/zziz/kalman-filter" target="_blank" rel="noopener">Python Kalman Filter</a>, 30行实现卡尔曼滤波</li><li><a href="https://github.com/arpg/vicalib" target="_blank" rel="noopener">vicalib</a>, 视觉惯导系统标定工具</li><li><a href="https://github.com/simondlevy/BreezySLAM" target="_blank" rel="noopener">BreezySLAM</a>, 基于雷达的SLAM，支持Python(&amp;Matlab, C++, and Java)</li><li><a href="https://github.com/Yvon-Shong/Probabilistic-Robotics" target="_blank" rel="noopener">Probabilistic-Robotics</a>, 《概率机器人》中文版，书和课后习题</li><li><a href="https://github.com/emmjaykay/stanford_self_driving_car_code" target="_blank" rel="noopener">Stanford Self Driving Car Code</a>, <strong>[<a href="http://robots.stanford.edu/papers/junior08.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, 斯坦福自动驾驶车代码</li><li><a href="https://github.com/ndrplz/self-driving-car" target="_blank" rel="noopener">Udacity Self-Driving Car Engineer Nanodegree projects</a></li><li><a href="https://github.com/TUMFTM/Lecture_AI_in_Automotive_Technology" target="_blank" rel="noopener">Artificial Intelligence in Automotive Technology</a>, TUM自动驾驶技术中的人工智能课程</li><li><a href="https://github.com/hlzz/DeepMatchVO" target="_blank" rel="noopener">DeepMatchVO: Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation</a>,ICRA 2019, <strong>[<a href="https://arxiv.org/abs/1902.09103" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/zdzhaoyong/GSLAM" target="_blank" rel="noopener">GSLAM: A General SLAM Framework and Benchmark</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1902.07995" target="_blank" rel="noopener">Paper</a>]</strong>, 集成了各种传感器输入的SLAM统一框架</li><li><a href="https://github.com/izhengfan/se2lam" target="_blank" rel="noopener">Visual-Odometric Localization and Mapping for Ground Vehicles Using SE(2)-XYZ Constraints</a>，ICRA 2019,基于SE(2)-XYZ约束的VO系统</li><li><a href="https://github.com/nicolov/simple_slam_loop_closure" target="_blank" rel="noopener">Simple bag-of-words loop closure for visual SLAM</a>, <strong>[<a href="https://nicolovaligi.com/bag-of-words-loop-closure-visual-slam.html" target="_blank" rel="noopener">Blog</a>]</strong>, 回环</li><li><a href="https://github.com/rmsalinas/fbow" target="_blank" rel="noopener">FBOW (Fast Bag of Words), an extremmely optimized version of the DBow2/DBow3 libraries</a>,优化版本的DBow2/DBow3</li><li><a href="https://github.com/tomas789/tonav" target="_blank" rel="noopener">Multi-State Constraint Kalman Filter (MSCKF) for Vision-aided Inertial Navigation(master’s thesis)</a></li><li><a href="https://github.com/yuzhou42/MSCKF" target="_blank" rel="noopener">MSCKF</a>, MSCKF中文注释版</li><li><a href="https://github.com/hbtang/calibcamodo" target="_blank" rel="noopener">Calibration algorithm for a camera odometry system</a>, VO系统的标定程序</li><li><a href="https://github.com/cggos/vins_mono_cg" target="_blank" rel="noopener">Modified version of VINS-Mono</a>, 注释版本VINS Mono</li><li><a href="https://github.com/zhenpeiyang/RelativePose" target="_blank" rel="noopener">Extreme Relative Pose Estimation for RGB-D Scans via Scene Completion</a>,<strong>[<a href="https://arxiv.org/abs/1901.00063" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/jessecw/EPnP_Eigen" target="_blank" rel="noopener">Implementation of EPnP algorithm with Eigen</a>,利用Eigen编写的EPnP</li><li><a href="https://github.com/jiexiong2016/GCNv2_SLAM" target="_blank" rel="noopener">Real-time SLAM system with deep features</a>, 深度学习描述子(ORB vs. GCNv2)</li><li><a href="https://github.com/Huangying-Zhan/Depth-VO-Feat" target="_blank" rel="noopener">Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction</a>, CVPR 2018, 无监督单目深度恢复以及VO</li><li><a href="https://github.com/Phylliida/orbslam-windows" target="_blank" rel="noopener">ORB-SLAM-windows</a>, Windows版本的ORB-SLAM</li><li><a href="https://github.com/danping/structvio" target="_blank" rel="noopener">StructVIO : Visual-inertial Odometry with Structural Regularity of Man-made Environments</a>,<strong>[<a href="http://drone.sjtu.edu.cn/dpzou/project/structvio.html" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/irvingzhang/KalmanFiltering" target="_blank" rel="noopener">KalmanFiltering</a>, 各种卡尔曼滤波器的demo</li><li><a href="https://github.com/ZhenghaoFei/visual_odom" target="_blank" rel="noopener">Stereo Odometry based on careful Feature selection and Tracking</a>, <strong>[<a href="https://lamor.fer.hr/images/50020776/Cvisic2017.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, C++ OpenCV实现SOFT</li><li><a href="https://github.com/dzunigan/zSLAM" target="_blank" rel="noopener">Visual SLAM with RGB-D Cameras based on Pose Graph Optimization</a></li><li><a href="https://github.com/drsrinathsridhar/GRANSAC" target="_blank" rel="noopener">Multi-threaded generic RANSAC implemetation</a>, 多线程RANSAC</li><li><a href="https://github.com/PyojinKim/OPVO" target="_blank" rel="noopener">Visual Odometry with Drift-Free Rotation Estimation Using Indoor Scene Regularities</a>, BMVC 2017, <strong>[<a href="http://pyojinkim.me/pub/Visual-Odometry-with-Drift-Free-Rotation-Estimation-Using-Indoor-Scene-Regularities/" target="_blank" rel="noopener">Project Page</a>]</strong>，利用平面正交信息进行VO</li><li><a href="https://github.com/baidu/ICE-BA" target="_blank" rel="noopener">ICE-BA</a>, CVPR 2018, <strong>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_ICE-BA_Incremental_Consistent_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/AIBluefisher/GraphSfM" target="_blank" rel="noopener">GraphSfM: Robust and Efficient Graph-based Structure from Motion</a>, <strong>[<a href="https://aibluefisher.github.io/GraphSfM/" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/cuitaixiang/LOAM_NOTED" target="_blank" rel="noopener">LOAM_NOTED</a>, loam中文注解版</li><li><a href="https://github.com/Ethan-Zhou/MWO" target="_blank" rel="noopener">Divide and Conquer: Effcient Density-Based Tracking of 3D Sensors in Manhattan Worlds</a>,ACCV 2016,<strong>[<a href="http://users.cecs.anu.edu.au/~u5535909/" target="_blank" rel="noopener">Project Page</a>]</strong>,曼哈顿世界利用深度传感器进行旋转量平移量分离优化</li><li><a href="https://github.com/jstraub/rtmf" target="_blank" rel="noopener">Real-time Manhattan World Rotation Estimation in 3D</a>,IROS 2015,实时曼哈顿世界旋转估计</li></ul><h2 id="Pose-Object-tracking"><a href="#Pose-Object-tracking" class="headerlink" title="Pose/Object tracking"></a>Pose/Object tracking</h2><ul><li><a href="https://github.com/cbsudux/Human-Pose-Estimation-101" target="_blank" rel="noopener">Basics of 2D and 3D Human Pose Estimation</a>,人体姿态估计入门</li><li><a href="https://github.com/OceanPang/Libra_R-CNN" target="_blank" rel="noopener">Libra R-CNN: Towards Balanced Learning for Object Detection</a></li><li><a href="https://github.com/HRNet/HRNet-Object-Detection" target="_blank" rel="noopener">High-resolution networks (HRNets) for object detection</a>, <strong>[<a href="https://arxiv.org/pdf/1904.04514.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/xiaolonw/TimeCycle" target="_blank" rel="noopener">Learning Correspondence from the Cycle-Consistency of Time</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1903.07593" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/zju3dv/pvnet" target="_blank" rel="noopener">PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.11788" target="_blank" rel="noopener">Paper</a>], [<a href="https://zju3dv.github.io/pvnet" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/mkocabas/EpipolarPose" target="_blank" rel="noopener">Self-Supervised Learning of 3D Human Pose using Multi-view Geometry</a>, CVPR 2018, <strong>[<a href="https://arxiv.org/abs/1903.02330" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/vita-epfl/openpifpaf" target="_blank" rel="noopener">PifPaf: Composite Fields for Human Pose Estimation</a>, <strong>[<a href="https://arxiv.org/abs/1903.06593" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" target="_blank" rel="noopener">Deep High-Resolution Representation Learning for Human Pose Estimation</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09212.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://jingdongwang2017.github.io/Projects/HRNet/PoseEstimation.html" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/YuliangXiu/PoseFlow" target="_blank" rel="noopener">PoseFlow: Efficient Online Pose Tracking)</a>, BMVC 2018, <strong>[<a href="https://arxiv.org/abs/1802.00977" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/vana77/Bottom-up-Clustering-Person-Re-identification" target="_blank" rel="noopener">A Bottom-Up Clustering Approach to Unsupervised Person Re-identification</a>，AAAI 2019, 重定位</li><li><a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener">Fast Online Object Tracking and Segmentation: A Unifying Approach</a>,CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1812.05050" target="_blank" rel="noopener">Paper</a>] [<a href="https://youtu.be/I_iOVrcpEBw" target="_blank" rel="noopener">Video</a>] [<a href="http://www.robots.ox.ac.uk/~qwang/SiamMask" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/TuSimple/simpledet" target="_blank" rel="noopener">SimpleDet - A Simple and Versatile Framework for Object Detection and Instance Recognition</a>,<strong>[<a href="https://arxiv.org/abs/1903.05831" target="_blank" rel="noopener">Paper</a>]</strong> </li></ul><h2 id="Depth-Disparity-amp-Flow-estimation"><a href="#Depth-Disparity-amp-Flow-estimation" class="headerlink" title="Depth/Disparity &amp; Flow estimation"></a>Depth/Disparity &amp; Flow estimation</h2><ul><li><a href="https://github.com/muskie82/AR-Depth-cpp" target="_blank" rel="noopener">Fast Depth Densification for Occlusion-aware Augmented Reality</a>, SIGGRAPH-Asia 2018, <strong>[<a href="https://homes.cs.washington.edu/~holynski/publications/occlusion/index.html" target="_blank" rel="noopener">Project Page</a>]</strong>,<a href="https://github.com/facebookresearch/AR-Depth" target="_blank" rel="noopener">another version</a></li><li><a href="https://github.com/CVLAB-Unibo/Learning2AdaptForStereo" target="_blank" rel="noopener">Learning To Adapt For Stereo</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1904.02957" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/JiaRenChang/PSMNet" target="_blank" rel="noopener">Pyramid Stereo Matching Network</a>,<strong>[<a href="https://arxiv.org/abs/1803.08669" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/lelimite4444/BridgeDepthFlow" target="_blank" rel="noopener">Bridging Stereo Matching and Optical Flow via Spatiotemporal Correspondence</a>, <strong>[<a href="https://arxiv.org/abs/1905.09265" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/wvangansbeke/Sparse-Depth-Completion" target="_blank" rel="noopener">Sparse Depth Completion</a>, <strong>[<a href="https://arxiv.org/pdf/1902.05356.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, RGB图像辅助雷达深度估计</li><li><a href="https://github.com/sshan-zhao/GASDA" target="_blank" rel="noopener">GASDA</a>, CVPR 2019, <strong>[<a href="https://sshan-zhao.github.io/papers/gasda.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/xy-guo/MVSNet_pytorch" target="_blank" rel="noopener">MVSNet: Depth Inference for Unstructured Multi-view Stereo</a>, <strong>[<a href="https://arxiv.org/abs/1804.02505" target="_blank" rel="noopener">Paper</a>]</strong>, 非官方实现版本的MVSNet</li><li><a href="https://github.com/HKUST-Aerial-Robotics/Stereo-RCNN" target="_blank" rel="noopener">Stereo R-CNN based 3D Object Detection for Autonomous Driving</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09738.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo" target="_blank" rel="noopener">Real-time self-adaptive deep stereo</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1810.05424" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/ialhashim/DenseDepth" target="_blank" rel="noopener">High Quality Monocular Depth Estimation via Transfer Learning</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.11941" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://ialhashim.github.io/publications/index.html" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/xy-guo/GwcNet" target="_blank" rel="noopener">Group-wise Correlation Stereo Network</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1903.04025" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/phuang17/DeepMVS" target="_blank" rel="noopener">DeepMVS: Learning Multi-View Stereopsis</a>, CVPR 2018,<strong>[<a href="https://phuang17.github.io/DeepMVS/index.html" target="_blank" rel="noopener">Project Page</a>]</strong>,多目深度估计</li><li><a href="https://github.com/sampepose/flownet2-tf" target="_blank" rel="noopener">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</a>, CVPR 2017, 深度学习光流恢复</li><li><a href="https://github.com/DLuensch/StereoVision-ADCensus" target="_blank" rel="noopener">StereoVision-ADCensus</a>,深度恢复代码集合(<strong>ADCensus, SGBM, BM</strong>)</li><li><a href="https://github.com/yangguorun/SegStereo" target="_blank" rel="noopener">SegStereo: Exploiting Semantic Information for Disparity Estimation</a>, 探究语义信息在深度估计中的作用</li><li><a href="https://github.com/kuantingchen04/Light-Field-Depth-Estimation" target="_blank" rel="noopener">Light Filed Depth Estimation using GAN</a>，利用GAN进行光场深度恢复</li><li><a href="https://github.com/daniilidis-group/EV-FlowNet" target="_blank" rel="noopener">EV-FlowNet: Self-Supervised Optical Flow for Event-based Cameras</a>,Proceedings of Robotics 2018,<strong>[<a href="https://arxiv.org/abs/1802.06898" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/vt-vl-lab/DF-Net" target="_blank" rel="noopener">DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency</a>, ECCV 2018, <strong>[<a href="https://arxiv.org/abs/1809.01649" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/yzcjtr/GeoNet" target="_blank" rel="noopener">GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose</a>, CVPR 2018, <strong>[<a href="https://arxiv.org/abs/1803.02276" target="_blank" rel="noopener">Paper</a>]</strong> </li></ul><h2 id="3D-amp-Graphic"><a href="#3D-amp-Graphic" class="headerlink" title="3D &amp; Graphic"></a>3D &amp; Graphic</h2><ul><li><a href="https://github.com/PRBonn/refusion" target="_blank" rel="noopener">ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals</a>, <strong>[<a href="https://arxiv.org/pdf/1905.02082.pdf" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/Lotayou/densebody_pytorch" target="_blank" rel="noopener">densebody_pytorch</a>, <strong>[<a href="https://arxiv.org/abs/1903.10153v3" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/svip-lab/PlanarReconstruction" target="_blank" rel="noopener">Single-Image Piece-wise Planar 3D Reconstruction via Associative Embedding</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09777.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, 单目3D重建</li><li><a href="https://github.com/sunset1995/HorizonNet" target="_blank" rel="noopener">HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch Data Augmentation</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1901.03861" target="_blank" rel="noopener">Paper</a>]</strong>, 深度学习全景转3D</li><li><a href="https://github.com/Microsoft/O-CNN" target="_blank" rel="noopener">Adaptive O-CNN: A Patch-based Deep Representation of 3D Shapes</a>,SIGGRAPH Asia 2018, <strong>[<a href="https://wang-ps.github.io/AO-CNN.html" target="_blank" rel="noopener">Project Page</a>]</strong></li></ul><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><ul><li><a href="https://live.bilibili.com/7332534?visit_id=9ytrx9lpsy80" target="_blank" rel="noopener">End-to-end Adversarial Learning for Generative Conversational Agents</a>，2017，介绍了一种端到端的基于GAN的聊天机器人</li><li><a href="https://github.com/yulunzhang/RNAN" target="_blank" rel="noopener">Residual Non-local Attention Networks for Image Restoration</a>,ICLR 2019.</li><li><a href="https://github.com/HelenMao/MSGAN" target="_blank" rel="noopener">MSGAN: Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis</a>, CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1903.05628" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/NVlabs/SPADE" target="_blank" rel="noopener">SPADE: Semantic Image Synthesis with Spatially-Adaptive Normalization</a>,CVPR 2019, <strong>[<a href="https://nvlabs.github.io/SPADE/" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/Oldpan/Faceswap-Deepfake-Pytorch" target="_blank" rel="noopener">Faceswap with Pytorch or DeepFake with Pytorch</a>, 假脸</li></ul><h2 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h2><ul><li><a href="https://github.com/RemoteML/bestofml" target="_blank" rel="noopener">The best resources around Machine Learning</a></li><li><a href="https://github.com/cydonia999/VGGFace2-pytorch" target="_blank" rel="noopener">VGGFace2: A dataset for recognising faces across pose and age</a></li><li><a href="https://github.com/SmirkCao/Lihang" target="_blank" rel="noopener">Statistical learning methods</a></li></ul><h2 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h2><ul><li><a href="https://github.com/ZhaoJ9014/face.evoLVe.PyTorch" target="_blank" rel="noopener">High-Performance Face Recognition Library on PyTorch</a>，人脸识别库</li><li><a href="https://github.com/enggen/Deep-Learning-Coursera" target="_blank" rel="noopener">Deep-Learning-Coursera</a>，深度学习教程（deeplearning.ai）</li></ul><h2 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h2><ul><li><a href="https://github.com/JuliaLang/julia" target="_blank" rel="noopener">Julia</a></li><li><a href="https://github.com/alan-turing-institute/MLJ.jl" target="_blank" rel="noopener">A Julia machine learning framework</a>，一种基于Julia的机器学习框架</li></ul><p><img src="https://github.com/alan-turing-institute/MLJ.jl/blob/master/doc/two_model_stack.png" alt></p><h2 id="Collections"><a href="#Collections" class="headerlink" title="Collections"></a>Collections</h2><ul><li><a href="https://github.com/wwxFromTju/awesome-reinforcement-learning-zh" target="_blank" rel="noopener">awesome-reinforcement-learning-zh</a>,强化学习从入门到放弃的资料</li><li><a href="https://github.com/uzh-rpg/event-based_vision_resources" target="_blank" rel="noopener">Event-based Vision Resources</a>，关于事件相机的资源</li><li><a href="https://github.com/DeepTecher/AutonomousVehiclePaper" target="_blank" rel="noopener">AutonomousVehiclePaper</a>，无人驾驶相关论文速递</li><li><a href="https://github.com/wutianyiRosun/Segmentation.X" target="_blank" rel="noopener">Segmentation.X</a>, Segmentation相关论文&amp;代码</li><li><a href="https://github.com/amusi/CVPR2019-Code" target="_blank" rel="noopener">CVPR-2019</a>, CVPR 2019 论文开源项目合集</li><li><a href="https://github.com/kanster/awesome-slam" target="_blank" rel="noopener">awesome-slam</a>, SLAM合集</li><li><a href="https://github.com/tzutalin/awesome-visual-slam" target="_blank" rel="noopener">awesome-visual-slam</a>, 视觉SLAM合集</li><li><a href="https://github.com/zziz/pwc" target="_blank" rel="noopener">Papers with code</a>, 周更论文with代码</li><li><a href="https://github.com/cbsudux/awesome-human-pose-estimation" target="_blank" rel="noopener">Awesome Human Pose Estimation</a>,<a href="https://github.com/nkalavak/awesome-object-pose" target="_blank" rel="noopener">awesome-object-pose</a>, 位姿估计合集</li><li><a href="https://github.com/mrgloom/awesome-semantic-segmentation" target="_blank" rel="noopener">Awesome Semantic Segmentation</a>, 语义分割集合</li><li><a href="https://github.com/mengyuest/iros2018-slam-papers" target="_blank" rel="noopener">IROS2018 SLAM Collections</a>, IROS 2018集合</li><li><a href="https://github.com/TerenceCYJ/VP-SLAM-SC-papers" target="_blank" rel="noopener">VP-SLAM-SC-papers</a>,Visual Positioning &amp; SLAM &amp; Spatial Cognition 论文统计与分析</li><li><a href="https://github.com/HuaizhengZhang/Awesome-System-for-Machine-Learning" target="_blank" rel="noopener">Awesome System for Machine Learning</a></li><li><a href="https://github.com/Thinkgamer/Machine-Learning-With-Python" target="_blank" rel="noopener">Machine-Learning-With-Python</a>, 《机器学习实战》python代码实现</li><li><a href="https://github.com/qqfly/how-to-learn-robotics" target="_blank" rel="noopener">How to learn robotics</a>, 开源机器人学学习指南</li><li><a href="https://github.com/kjw0612/awesome-deep-vision" target="_blank" rel="noopener">Awesome Deep Vision</a>,DL在CV领域的应用</li><li><a href="https://github.com/YapengTian/Single-Image-Super-Resolution" target="_blank" rel="noopener">Single-Image-Super-Resolution</a>, 一个有关<strong>图像超分辨</strong>的合集</li><li><a href="https://github.com/wifity/ai-report" target="_blank" rel="noopener">ai report</a>, AI相关的研究报告</li><li><a href="https://paperswithcode.com/sota" target="_blank" rel="noopener">State-of-the-art papers and code</a>,搜集了目前sota的论文以及代码</li><li><a href="https://github.com/extreme-assistant/cvpr2019" target="_blank" rel="noopener">CVPR 2019 (Papers/Codes/Project/Paper reading)</a></li><li><a href="https://github.com/openMVG/awesome_3DReconstruction_list" target="_blank" rel="noopener">A curated list of papers &amp; resources linked to 3D reconstruction from images</a>,有关三维重建的论文汇总</li><li><a href="https://github.com/nebula-beta/SLAM-Jobs" target="_blank" rel="noopener">SLAM-Jobs</a>, SLAM/SFM求职指南</li></ul><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><ul><li><a href="https://github.com/cszn/DPSR" target="_blank" rel="noopener">Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels</a>,CVPR 2019,超分辨</li><li><a href="https://github.com/lzhbrian/Cool-Fashion-Papers" target="_blank" rel="noopener">Cool Fashion Papers</a>, Cool resources about Fashion + AI.</li><li><a href="https://github.com/nbei/Deep-Flow-Guided-Video-Inpainting" target="_blank" rel="noopener">Deep Flow-Guided Video Inpainting</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1806.10447.pdf" target="_blank" rel="noopener">Paper</a>]</strong> ,图像修复</li><li><a href="https://github.com/dbolya/yolact" target="_blank" rel="noopener">YOLACT: Real-time Instance Segmentation</a></li><li><a href="https://github.com/lyl8213/Plate_Recognition-LPRnet" target="_blank" rel="noopener">LPRNet: License Plate Recognition via Deep Neural Networks</a>, <strong>[<a href="https://arxiv.org/pdf/1806.10447.pdf" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/xiaofengShi/CHINESE-OCR" target="_blank" rel="noopener">CHINESE-OCR</a>, 运用tf实现自然场景文字检测</li><li><a href="https://github.com/PerpetualSmile/BeautyCamera" target="_blank" rel="noopener">BeautyCamera</a>, 美颜相机，具有人脸检测、磨皮美白人脸、滤镜、调节图片、摄像功能</li><li><a href="https://github.com/zhengzhugithub/CV-arXiv-Daily" target="_blank" rel="noopener">CV-arXiv-Daily</a>, 分享计算机视觉每天的arXiv文章</li><li>Pluralistic-Inpainting, <a href="https://arxiv.org/abs/1903.04227" target="_blank" rel="noopener">ArXiv</a> | <a href="http://www.chuanxiaz.com/publication/pluralistic/" target="_blank" rel="noopener">Project Page</a> | <a href="http://www.chuanxiaz.com/project/pluralistic/" target="_blank" rel="noopener">Online Demo</a> | <a href="https://www.youtube.com/watch?v=9V7rNoLVmSs" target="_blank" rel="noopener">Video(demo)</a></li><li><a href="https://github.com/Jezzamonn/fourier" target="_blank" rel="noopener">An Interactive Introduction to Fourier Transforms</a>, 超棒的傅里叶变换图形化解释</li><li><a href="https://github.com/datawhalechina/pumpkin-book" target="_blank" rel="noopener">pumpkin-book</a>, 《机器学习》（西瓜书）公式推导解析</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The post contains papers-with-code about SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc. &lt;a href=&quot;https://github.com/Vincentqyw/Recent-Stars-2019&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/Vincentqyw/Recent-Stars-2019.svg?logo=github&amp;amp;label=Stars&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="disparity" scheme="https://www.vincentqin.tech/tags/disparity/"/>
    
      <category term="pose-tracking" scheme="https://www.vincentqin.tech/tags/pose-tracking/"/>
    
      <category term="object-tracking" scheme="https://www.vincentqin.tech/tags/object-tracking/"/>
    
      <category term="depth-estimation" scheme="https://www.vincentqin.tech/tags/depth-estimation/"/>
    
      <category term="flow-estimation" scheme="https://www.vincentqin.tech/tags/flow-estimation/"/>
    
      <category term="3D-graphics" scheme="https://www.vincentqin.tech/tags/3D-graphics/"/>
    
  </entry>
  
  <entry>
    <title>开启SSR模式</title>
    <link href="https://www.vincentqin.tech/posts/build-ssr-server/"/>
    <id>https://www.vincentqin.tech/posts/build-ssr-server/</id>
    <published>2019-03-31T05:56:15.000Z</published>
    <updated>2020-04-15T14:39:28.955Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>关于科学上网，食用别人调配的酸酸乳总觉味道不对，自食其力心里才会感到踏实。受<a href="https://newdee.cf/" target="_blank" rel="noopener">Newdee老贼</a>指点，鄙人成功在服务器上开启了酸酸乳服务。详细过程Newdee已经在博文“<a href="https://newdee.cf/posts/1420aa47/" target="_blank" rel="noopener">SS服务器搭建</a>”介绍地相当详细。本人记性不好，遂本文将记录几个关键步骤，以备后续不时之需。<br><strong><font color="#FF0000" face="宋体">注意：本文仅供个人学习使用，不可用于商业或者违法行为！</font></strong></p><a id="more"></a><h2 id="购买服务器"><a href="#购买服务器" class="headerlink" title="购买服务器"></a>购买服务器</h2><ul><li>购买服务器(支持alipay &amp; wechat pay)，地址: <a href="https://www.vultr.com/?ref=7996819" target="_blank" rel="noopener">https://www.vultr.com/</a></li></ul><p><img src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/vultr.png" alt></p><ul><li>经过几个步骤：1. Server Location, 2. Server Type, 3. Server Size, 4. Additional Features,5,6,7可以忽略，最后点击右下角的<strong>Deploy New</strong>即可部署。</li></ul><p><img src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/buy-vultr.png" alt></p><p>后台是这样的：</p><p><img src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/vultr-backend.png" alt></p><p>然后根据IP以及用户名利用SSH在本地进行远程连接，进行以下步骤。</p><h2 id="安装SSR"><a href="#安装SSR" class="headerlink" title="安装SSR"></a>安装SSR</h2><p>如果是单用户使用，进入服务器直接执行下述命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -N --no-check-certificate https://raw.githubusercontent.com/Vincentqyw/doubi/master/ssr.sh &amp;&amp; chmod +x ssr.sh &amp;&amp; bash ssr.sh</span><br></pre></td></tr></table></figure><p>关于加密协议以及混淆的设置参见下图：<br><img src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/account.png" alt></p><p>多用户使用的版本（可配置多个账号）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/ssrmu.sh &amp;&amp; chmod +x ssrmu.sh &amp;&amp; bash ssrmu.sh</span><br></pre></td></tr></table></figure><p>设置完毕之后，后续进行管理直接运行<code>bash ssrmu.sh</code>选择不同的功能项即可。</p><h2 id="封禁某些端口-可选"><a href="#封禁某些端口-可选" class="headerlink" title="封禁某些端口(可选)"></a>封禁某些端口(可选)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -N --no-check-certificate https://raw.githubusercontent.com/Vincentqyw/doubi/master/ban_iptables.sh &amp;&amp; chmod +x ban_iptables.sh &amp;&amp; bash ban_iptables.sh</span><br></pre></td></tr></table></figure><p>选择封禁垃圾邮件端口就行。<br><img src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/ban-mails.png" alt></p><h2 id="BBR加速-可选"><a href="#BBR加速-可选" class="headerlink" title="BBR加速(可选)"></a>BBR加速(可选)</h2><p><img src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/bbr.png" alt></p><h2 id="安装SSRR-可选"><a href="#安装SSRR-可选" class="headerlink" title="安装SSRR(可选)"></a>安装SSRR(可选)</h2><p>接下来的链接给出了SSRR的安装教程，不再赘述，<a href="https://gist.github.com/biezhi/45fac901f02f7c867e46aecd41076d70#kcp-%E5%AE%A2%E6%88%B7%E7%AB%AF" target="_blank" rel="noopener">Link</a>。</p><h2 id="建立快照"><a href="#建立快照" class="headerlink" title="建立快照"></a>建立快照</h2><p>建立系统快照就是将系统某个状态下的各种数据记录在一个文件里，下一次新建完主机后恢复快照就能够恢复成之前系统的样子。</p><p>若已有了主机，点击下图所示的<code>Snapshots</code>对该系统建立快照。</p><p><img src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/snapshots-step1.png" alt></p><p>随后就会出现下图所示的页面，在<code>Label</code>一栏输入这个快照的标签，方便区分不同的快照。</p><p><img src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/snapshots-step2.png" alt></p><p>若想对新建的系统恢复以前建立的快照，可以点击上图中的类似于<code>循环</code>的标志。</p><h2 id="不可描述"><a href="#不可描述" class="headerlink" title="不可描述"></a>不可描述</h2><ul><li><a href="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/SSR-WIN-ANDROID-IOS.7z" target="_blank" rel="noopener">不可描述</a></li><li>PC终端可自行挑选，鄙人推荐<a href="https://www.termius.com/windows" target="_blank" rel="noopener">termius</a>, <a href="https://git-scm.com/downloads" target="_blank" rel="noopener">Git Bash</a></li><li>参考链接：<a href="https://gist.github.com/biezhi/45fac901f02f7c867e46aecd41076d70" target="_blank" rel="noopener">ShadowsocksR 协议插件文档</a></li></ul><p>手机端以及电脑端输入对应的IP/端口/混淆/加密等信息进行连接即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于科学上网，食用别人调配的酸酸乳总觉味道不对，自食其力心里才会感到踏实。受&lt;a href=&quot;https://newdee.cf/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Newdee老贼&lt;/a&gt;指点，鄙人成功在服务器上开启了酸酸乳服务。详细过程Newdee已经在博文“&lt;a href=&quot;https://newdee.cf/posts/1420aa47/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SS服务器搭建&lt;/a&gt;”介绍地相当详细。本人记性不好，遂本文将记录几个关键步骤，以备后续不时之需。&lt;br&gt;&lt;strong&gt;&lt;font color=&quot;#FF0000&quot; face=&quot;宋体&quot;&gt;注意：本文仅供个人学习使用，不可用于商业或者违法行为！&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="科学上网" scheme="https://www.vincentqin.tech/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/"/>
    
  </entry>
  
  <entry>
    <title>虚实:「未麻的部屋」</title>
    <link href="https://www.vincentqin.tech/posts/recent-status/"/>
    <id>https://www.vincentqin.tech/posts/recent-status/</id>
    <published>2019-01-13T09:55:31.000Z</published>
    <updated>2020-03-31T14:51:06.334Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://vincentqin.gitee.io/blogresource-1/recent-status/Perfect-Blue-cover.png" alt></p><a id="more"></a><p><img src="https://vincentqin.gitee.io/blogresource-1/recent-status/post-2.jpg" alt></p><p>过去，到底是哪一条支线造就了现在的自己。虚实之间，到底是谁在支配？</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://vincentqin.gitee.io/blogresource-1/recent-status/Perfect-Blue-cover.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>笔记：李群与李代数求导</title>
    <link href="https://www.vincentqin.tech/posts/LieAlgebra/"/>
    <id>https://www.vincentqin.tech/posts/LieAlgebra/</id>
    <published>2018-12-04T15:40:56.000Z</published>
    <updated>2020-03-31T15:37:28.963Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note primary">            <p>最近一段时间在推$Jacobian$，会用到一些关于李代数求导的知识。参考高博《视觉slam十四讲》一书，在此总结一些常用的关于李群与李代数相关的知识点。</p>          </div><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在SLAM中位姿是未知的，而我们需要解决什么样的相机位姿最符合当前观测数据这样的问题。一种典型的方式是把它构建成一个优化问题，求解最优的$R$,$t$，使得误差最小化。<br>旋转矩阵自身是带有约束的（正交且行列式为 1）。它们作为优化变量时，会引入额外的约束，使优化变得困难。通过李群——李代数间的转换关系，我们希望把位姿估计变成无约束的优化问题，简化求解方式。群（Group）是一种集合加上一种运算的代数结构，李群是指具有连续（光滑）性质的群，李群在相机姿态估计时具有重要意义，接下来主要讨论特殊正交群$SO(n)$与特殊欧式群$SE(n)$。</p><h2 id="特殊正交群与特殊欧式群"><a href="#特殊正交群与特殊欧式群" class="headerlink" title="特殊正交群与特殊欧式群"></a>特殊正交群与特殊欧式群</h2><script type="math/tex; mode=display">SO(n) = \{ \mathbf{R} \in \mathbb{R}^{n \times n} | \mathbf{R R}^T = \mathbf{I}, det(\mathbf{R})=1 \}</script><script type="math/tex; mode=display">SE(3) = \left\{ \mathbf{T} = \left[ {\begin{array}{*{20}{c}} \mathbf{R} & \mathbf{t} \\ \mathbf{0}^T & 1 \end{array}} \right]  \in \mathbb{R}^{4 \times 4} | \mathbf{R} \in SO(3), \mathbf{t} \in \mathbb{R}^3\right\}</script><p>在李群中，我们使用矩阵来表达一个旋转和平移，这存在冗余的自由度。三维空间的旋转只有三自由度，旋转+平移有六自由度。因此，我们希望寻找一个没有冗余自由度（但是相应的存在奇异性）的表示，也就是李代数$\mathfrak{so}(3)$和$\mathfrak{se}(3)$。且无论是旋转还是变换矩阵，它们都是对加法不封闭的，但是对乘法是封闭的。</p><h2 id="李代数的引出"><a href="#李代数的引出" class="headerlink" title="李代数的引出"></a>李代数的引出</h2><p>对于任意旋转矩阵$R$，它必定满足：</p><script type="math/tex; mode=display">\mathbf{R} \mathbf{R}^T＝\mathbf{I}.</script><p>考虑它随时间发生变化，即从$\mathbf{R}$变为$\mathbf{R(t)}$，它仍然满足如下如下等式：</p><script type="math/tex; mode=display">\mathbf{R}(t) \mathbf{R}(t) ^T = \mathbf{I}</script><p>对两侧同时对时间求导数得：</p><script type="math/tex; mode=display">\mathbf{\dot{R}} (t) \mathbf{R} {(t)^T} + \mathbf{R} (t) \mathbf{\dot{R}} {(t)^T} = 0</script><p>则有：</p><script type="math/tex; mode=display">\mathbf{\dot{R}} (t) \mathbf{R} {(t)^T} = - \left(  \mathbf{\dot{R}} (t) \mathbf{R} {(t)^T} \right)^T</script><p>可见$\mathbf{\dot{R}} (t) \mathbf{R} {(t)^T}$是一个反对称矩阵，将其记作$\mathbf{A}$，于是$\mathbf{A}^T=-\mathbf{A}$,所以它主对角线元素必为，而非对角线元素则只有三个自由度。我们一定可以找到一个这样的向量$\mathbf{a}=[a_1, a_2, a_3]^T$使得：</p><script type="math/tex; mode=display">{\mathbf{a}^ \wedge } = \mathbf{A} = \left[ {\begin{array}{*{20}{c}} 0& -a_3 & a_2\\ {a_3}&0& - {a_1}\\  - {a_2}&{a_1}&0 \end{array}} \right]</script><p>其中$^{\wedge}$符号表示由向量转换为矩阵，反之我们也可以用$^{\vee}$符号定义由矩阵转换为向量的方式:</p><script type="math/tex; mode=display">{ \mathbf{A}^ \vee } = \mathbf{a}</script><p>现在，由于$\mathbf{\dot{R}} (t) \mathbf{R} {(t)^T}$是一个反对称矩阵，所以我们一定可以找到一个三维向量$\mathbf{\phi} (t) \in \mathbb{R}^3$与之对应。于是我们有：</p><script type="math/tex; mode=display">\mathbf{ \dot{R} } (t) \mathbf{R}(t)^T = \mathbf{\phi} (t) ^ {\wedge}</script><p>左右各右乘$\mathbf{R}(t)$，由于其正交性，有：</p><script type="math/tex; mode=display">\mathbf{ \dot{R} } (t)  = \mathbf{\phi} (t)^{\wedge} \mathbf{R}(t) =   \left[ {\begin{array}{*{20}{c}} 0&- {\phi _3}&{\phi _2}\\ {\phi _3}&0& - {\phi _1}\\ { - \phi _2}&{\phi _1}&0 \end{array}} \right] \mathbf{R} (t)</script><p>可以看到，每对旋转矩阵求一次导数，只需左乘一个矩阵$\mathbf{\phi} (t)^{\wedge}$即可。由于$\mathbf{\phi} (t)^{\wedge}$反映了的导数性质，故称它在的正切空间(tangent space)上。同时在$t_0$附近，设$\mathbf{\phi}$保持为常数$\mathbf{\phi}(t_0)=\mathbf{\phi}_0$，我们有：</p><script type="math/tex; mode=display">\mathbf{ \dot{R} } (t)  = \mathbf{\phi} (t_0)^{\wedge} \mathbf{R}(t)= \mathbf{\phi}_0^{\wedge} \mathbf{R}(t)</script><p>又因为初始值$\mathbf{ R } (0) = \mathbf{I} $对上式进行求解可得：</p><script type="math/tex; mode=display">\label{eq:so3ode} \mathbf{R}(t) = \exp \left( \mathbf{\phi}_0^{\wedge} t\right) .</script><p>上式描述$\mathbf{R}$在局部的导数关系。</p><h2 id="李代数-mathfrak-so-3"><a href="#李代数-mathfrak-so-3" class="headerlink" title="李代数 $\mathfrak{so}(3)$"></a>李代数 $\mathfrak{so}(3)$</h2><p>上文提及的$\mathbf{\phi}$是一种李代数，$SO(3)$对应的李代数是定义在$\mathbb{R}^3$上的向量，我们记作$\mathbf{\phi}$，它 对应与一个反对称矩阵：</p><script type="math/tex; mode=display">\label{eq:phi} \mathbf{\Phi} = \mathbf{\phi}^{\wedge} = \left[ {\begin{array}{*{20}{c}}     0&{ - \phi _3}&{\phi _2}\\     {\phi _3}&0&{ - \phi _1}\\     { - \phi _2}&{\phi _1}&0     \end{array}} \right] \in \mathbb{R}^{3 \times 3}</script><p>由于它与反对称矩阵关系很紧密，在不引起歧义的情况下，就说的元素是3维向量或者3维反对称矩阵，不加区别：</p><script type="math/tex; mode=display">\bbox[5px,border:2px solid red]{\mathfrak{so}(3) = \left\{ \Phi = \mathbf{\phi^\wedge} \in \mathbb{R}^{3 \times 3} | \mathbf{\phi} \in \mathbb{R}^3 \right\}}</script><h2 id="李代数-mathfrak-se-3"><a href="#李代数-mathfrak-se-3" class="headerlink" title="李代数 $\mathfrak{se}(3)$"></a>李代数 $\mathfrak{se}(3)$</h2><p>$SE(3)$对应的李代数为$\mathfrak{se}(3)$，$\mathfrak{se}(3)$定义在$\mathbb{R}^{6}$空间，其具体形式如下：</p><script type="math/tex; mode=display">\bbox[5px,border:2px solid red]{\mathfrak{se}(3) = \left\{ \mathbf{ \xi } = \left[ \begin{array}{l}     \mathbf{\rho} \\     \mathbf{\phi}      \end{array} \right] \in \mathbb{R}^{6}, \mathbf{\rho} \in \mathbb{R}^{3},\mathbf{\phi} \in \mathfrak{so}(3),\mathbf{\xi}^\wedge  = \left[ {\begin{array}{*{20}{c}}     \mathbf{\phi} ^ \wedge &\mathbf{\rho} \\ \mathbf{0}^T&0 \end{array}} \right] \in \mathbb{R}^{4 \times 4} \right\}}</script><p>$\mathfrak{se}(3)$是一个这样的六维向量，前三维表示平移，记作$\mathbf{\rho}$；后三维表示旋转，记作$\mathbf{\phi}$（有时候这两个参数会反过来，可也可以的）。</p><h2 id="指数映射"><a href="#指数映射" class="headerlink" title="指数映射"></a>指数映射</h2><p>$\mathfrak{so}(3)$以及$\mathfrak{se}(3)$的指数映射分别对应于$SO(3)$以及$SE(3)$，它们之间的转换关系可以由下图表示：</p><p><img src="https://vincentqin.gitee.io/blogresource-3/LieAlgebra/lieGroup.png" alt="lieGroup"></p><h2 id="李代数求导"><a href="#李代数求导" class="headerlink" title="李代数求导"></a>李代数求导</h2><h3 id="对旋转矩阵李代数求导"><a href="#对旋转矩阵李代数求导" class="headerlink" title="对旋转矩阵李代数求导"></a>对旋转矩阵李代数求导</h3><p>对$\mathbf{R}$进行一次扰动$\Delta \mathbf{R}$，假设左扰动$\Delta \mathbf{R}$对应的李代数为$ {\boldsymbol \varphi}$，对$ {\boldsymbol \varphi}$求导，得到：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial ({\boldsymbol Rp})}{\partial {\boldsymbol \varphi}}&= \lim_{\boldsymbol \varphi \to 0}\frac{ \overbrace{ \exp ({\boldsymbol \varphi}^{\land}) }^{\color{Red}{可作泰勒展开}} \exp ({\boldsymbol \phi}^{\land}) {\boldsymbol p} - \exp ({\boldsymbol \phi}^{\land}) {\boldsymbol p}}{ {\boldsymbol \varphi} }\\&\approx \lim_{\boldsymbol \varphi \to 0}\frac{({\boldsymbol I} + {\boldsymbol \varphi}^{\land}) \exp ({\boldsymbol \phi}^{\land}) {\boldsymbol p} - \exp ({\boldsymbol \phi}^{\land}) {\boldsymbol p}}{ {\boldsymbol \varphi} }  \\&= \lim_{\boldsymbol \varphi \to 0}\frac{ {\boldsymbol \varphi}^{\land} {\boldsymbol {Rp}} }{ {\boldsymbol \varphi} }  \\&= \lim_{\boldsymbol \varphi \to 0}\frac{ -({\boldsymbol {Rp}})^{\land} {\boldsymbol \varphi} }{ {\boldsymbol \varphi} } \\&= -({\boldsymbol {Rp}})^{\land}\end{aligned}</script><h3 id="变换转矩阵李代数求导"><a href="#变换转矩阵李代数求导" class="headerlink" title="变换转矩阵李代数求导"></a>变换转矩阵李代数求导</h3><p>假设空间点${\boldsymbol p}$经过一次变换${\boldsymbol T}$（对应的李代数为${\boldsymbol \xi}$）后变为 ${\boldsymbol Tp}$ 。当给${\boldsymbol T}$左乘一个扰动$\Delta {\boldsymbol T} = \exp (\delta {\boldsymbol \xi}^{\land})$，设扰动项的李代数为$\delta {\boldsymbol \xi} = [\delta {\boldsymbol \rho}, \delta {\boldsymbol \phi}]^{T}$，有：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial ({\boldsymbol {Tp}})}{\partial \delta{\boldsymbol \xi}}&= \lim_{\delta{\boldsymbol \xi} \to 0}\frac{ \overbrace{ \exp (\delta {\boldsymbol \xi}^{\land}) }^{\color{Red}{可作泰勒展开}}  \exp ({\boldsymbol \xi}^{\land}) {\boldsymbol p} - \exp ({\boldsymbol \xi}^{\land}) {\boldsymbol p}}{ \delta {\boldsymbol \xi} } \\&\approx \lim_{\delta{\boldsymbol \xi} \to 0}\frac{ ({\boldsymbol I} + \delta {\boldsymbol \xi}^{\land}) \exp ({\boldsymbol \xi}^{\land}) {\boldsymbol p} - \exp ({\boldsymbol \xi}^{\land}) {\boldsymbol p} }{ \delta {\boldsymbol \xi} } \\&= \lim_{\delta{\boldsymbol \xi} \to 0}\frac{  \delta {\boldsymbol \xi}^{\land} \exp ({\boldsymbol \xi}^{\land}) {\boldsymbol p}  }{ \delta {\boldsymbol \xi} } \\&= \lim_{\delta{\boldsymbol \xi} \to 0}\frac{\begin{bmatrix} \delta {\boldsymbol \phi}^{\land}  &   \delta {\boldsymbol \rho}     \\     {\boldsymbol 0}^{T}                 &                      1                           \\\end{bmatrix}\begin{bmatrix}   {\boldsymbol {Rp}} +  {\boldsymbol t}     \\                                     1                                \\\end{bmatrix}}{ \delta {\boldsymbol \xi} } \\&= \lim_{\delta{\boldsymbol \xi} \to 0}\frac{\begin{bmatrix}   \delta {\boldsymbol \phi}^{\land} ({\boldsymbol {Rp}} + {\boldsymbol t}) + \delta {\boldsymbol \rho}     \\                                     0                                \\\end{bmatrix}}{ \delta {\boldsymbol \xi} } \\&=\overbrace{\begin{bmatrix} {\boldsymbol I}            &   -({\boldsymbol {Rp}} + {\boldsymbol t})^{\land}    \\ {\boldsymbol 0}^{T}    &     {\boldsymbol 0}^{T}             \\\end{bmatrix}}^{\color{Red}{上式分块求导}}\\&= ({\boldsymbol {Tp}})^{\bigodot}\end{aligned}</script><p>上式中运算符号$\bigodot$的含义：将一个齐次坐标的空间点变换成一个$4 \times 6$的矩阵。</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><h3 id="SE-3-左扰"><a href="#SE-3-左扰" class="headerlink" title="$SE(3)$左扰"></a>$SE(3)$左扰</h3><script type="math/tex; mode=display">\begin{aligned}\rm{exp}\left( {\Delta {\xi}^{\land} } \right){\rm{exp}}\left( {\xi}^{\land}  \right)& \approx \left( {I + {\left[ {\Delta \xi } \right]}_ \times } \right){\rm{exp}}(\xi) \\&= \left( I_{4 \times 4} +\left[\begin{array}{*{20}{c}}{  \left[ \Delta \phi  \right]}_{\times}   &   \Delta \rho    \\0&0\end{array}\right]\right)\left[\begin{array}{*{20}{c}} R   &   t    \\0&1\end{array}\right] \\&=\left[\begin{array}{*{20}{c}}{  \left[ \Delta \phi  \right]}_{\times}+I_{3 \times 3}   &   \Delta \rho    \\0&1\end{array}\right]\left[\begin{array}{*{20}{c}} R   &   t    \\0&1\end{array}\right] \\&=\left[\begin{array}{*{20}{c}} \left( \left[ \Delta \phi  \right]_{\times}+I_{3 \times 3} \right) R  &    \left( \left[ \Delta \phi  \right]_{\times}+I_{3 \times 3} \right) t + \Delta \rho    \\0&1\end{array}\right]\end{aligned}</script><h3 id="SE-3-右扰"><a href="#SE-3-右扰" class="headerlink" title="$SE(3)$右扰"></a>$SE(3)$右扰</h3><script type="math/tex; mode=display">\begin{aligned}\rm{exp}\left( {\xi}^{\land}  \right) \rm{exp}\left( {\Delta {\xi}^{\land} } \right)& \approx {\rm{exp}}({\xi}^{\land}) \left( {I + {\left[ {\Delta \xi } \right]}_ \times } \right)\\&=\left[\begin{array}{*{20}{c}} R   &   t    \\0&1\end{array}\right]\left( I_{4 \times 4} +\left[\begin{array}{*{20}{c}}{  \left[ \Delta \phi  \right]}_{\times}   &   \Delta \rho    \\0&0\end{array}\right]\right)\\&=\left[\begin{array}{*{20}{c}} R   &   t    \\0&1\end{array}\right]\left[\begin{array}{*{20}{c}}{  \left[ \Delta \phi  \right]}_{\times}+I_{3 \times 3}   &   \Delta \rho    \\0&1\end{array}\right] \\&=\left[\begin{array}{*{20}{c}} R \left( \left[ \Delta \phi  \right]_{\times}+I_{3 \times 3} \right)  &   R \Delta \rho + t   \\0&1\end{array}\right]\end{aligned}</script>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note primary&quot;&gt;
            &lt;p&gt;最近一段时间在推$Jacobian$，会用到一些关于李代数求导的知识。参考高博《视觉slam十四讲》一书，在此总结一些常用的关于李群与李代数相关的知识点。&lt;/p&gt;
          &lt;/div&gt;
    
    </summary>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/categories/SLAM/"/>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="computer vision" scheme="https://www.vincentqin.tech/tags/computer-vision/"/>
    
      <category term="李代数" scheme="https://www.vincentqin.tech/tags/%E6%9D%8E%E4%BB%A3%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>资料：ORB SLAM2 阅读报告</title>
    <link href="https://www.vincentqin.tech/posts/orb-slam/"/>
    <id>https://www.vincentqin.tech/posts/orb-slam/</id>
    <published>2018-11-30T15:14:30.000Z</published>
    <updated>2020-03-31T15:50:48.086Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>首先，解释下SLAM的概念，借鉴高博《视觉 SLAM 十四讲》中的一句话：SLAM 是 Simultaneous Localization and Mapping 的缩写，中文译作“同时定位与地图构建”。它是指搭载特定传感器的主体，在没有环境先验信息的情况下，于运动过程中建立环境的模型，同时估计自己的运动。如果这里的传感器主要为相机，那就称为“视觉 SLAM”。</p><a id="more"></a><p>先来张图，下图就是利用相机作为传感器在环境中采集一系列的图像，经过SLAM系统建立的点云图以及相机轨迹。</p><p><img src="https://vincentqin.gitee.io/blogresource-1/orb-slam/MH01-traj.gif" alt></p><p>SLAM自1986年提出之后，一直以来是机器人领域的热点问题。这里总结一些常用开源SLAM方案。</p><style>table th:nth-of-type(1) {    width: 150px;}table th:nth-of-type(2) {    width: 200px;}</style><div class="table-container"><table><thead><tr><th>方案名称</th><th>传感器形式</th><th>地址</th></tr></thead><tbody><tr><td>MonoSLAM</td><td>单目</td><td><a href="https://github.com/hanmekim/SceneLib2" target="_blank" rel="noopener">https://github.com/hanmekim/SceneLib2</a></td></tr><tr><td>PTAM</td><td>单目</td><td><a href="http://www.robots.ox.ac.uk/~gk/PTAM/" target="_blank" rel="noopener">http://www.robots.ox.ac.uk/~gk/PTAM/</a></td></tr><tr><td>ORB-SLAM</td><td>单目</td><td><a href="http://webdiis.unizar.es/~raulmur/orbslam/" target="_blank" rel="noopener">http://webdiis.unizar.es/~raulmur/orbslam/</a></td></tr><tr><td>ORB-SLAM2</td><td>单目/双目/RGB-D</td><td><a href="https://github.com/raulmur/ORB_SLAM2" target="_blank" rel="noopener">https://github.com/raulmur/ORB_SLAM2</a></td></tr><tr><td>LSD-SLAM</td><td>单目为主</td><td><a href="http://vision.in.tum.de/research/vslam/lsdslam" target="_blank" rel="noopener">http://vision.in.tum.de/research/vslam/lsdslam</a></td></tr><tr><td>SVO</td><td>单目</td><td><a href="https://github.com/uzh-rpg/rpg_svo" target="_blank" rel="noopener">https://github.com/uzh-rpg/rpg_svo</a></td></tr><tr><td>DTAM</td><td>RGB-D</td><td><a href="https://github.com/anuranbaka/OpenDTAM" target="_blank" rel="noopener">https://github.com/anuranbaka/OpenDTAM</a></td></tr><tr><td>DVO</td><td>RGB-D</td><td><a href="https://github.com/tum-vision/dvo_slam" target="_blank" rel="noopener">https://github.com/tum-vision/dvo_slam</a></td></tr><tr><td>DSO</td><td>单目</td><td><a href="https://github.com/JakobEngel/dso" target="_blank" rel="noopener">https://github.com/JakobEngel/dso</a></td></tr><tr><td>RTAB-MAP</td><td>双目/RGB-D</td><td><a href="https://github.com/introlab/rtabmap" target="_blank" rel="noopener">https://github.com/introlab/rtabmap</a></td></tr><tr><td>RGBD-SLAM-V2</td><td>RGB-D</td><td><a href="https://github.com/felixendres/rgbdslam_v2" target="_blank" rel="noopener">https://github.com/felixendres/rgbdslam_v2</a></td></tr><tr><td>Elastic Fusion</td><td>RGB-D</td><td><a href="https://github.com/mp3guy/ElasticFusion" target="_blank" rel="noopener">https://github.com/mp3guy/ElasticFusion</a></td></tr><tr><td>Hector SLAM</td><td>激光</td><td><a href="http://wiki.ros.org/hector_slam" target="_blank" rel="noopener">http://wiki.ros.org/hector_slam</a></td></tr><tr><td>GMapping</td><td>激光</td><td><a href="http://wiki.ros.org/gmapping" target="_blank" rel="noopener">http://wiki.ros.org/gmapping</a></td></tr><tr><td>OKVIS</td><td>多目+IMU</td><td><a href="https://github.com/ethz-asl/okvis" target="_blank" rel="noopener">https://github.com/ethz-asl/okvis</a></td></tr><tr><td>ROVIO</td><td>多目+IMU</td><td><a href="https://github.com/ethz-asl/rovio" target="_blank" rel="noopener">https://github.com/ethz-asl/rovio</a></td></tr><tr><td>VINS</td><td>单目+IMU</td><td><a href="https://github.com/HKUST-Aerial-Robotics/VINS-Mono" target="_blank" rel="noopener">https://github.com/HKUST-Aerial-Robotics/VINS-Mono</a></td></tr></tbody></table></div><p>ORB-SLAM应该是SLAM最具有代表性的算法，<a href="https://arxiv.org/abs/1610.06475" target="_blank" rel="noopener">ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras；</a> <a href="https://github.com/raulmur/ORB_SLAM2" target="_blank" rel="noopener"><strong>code；</strong></a> <a href="http://webdiis.unizar.es/~raulmur/orbslam/" target="_blank" rel="noopener"><strong>主页；</strong></a></p><p>ORB-SLAM 是PTAM 的继承者们中非常有名的一位。它提出于 2015 年，是现代 SLAM 系统中做的非常完善，非常易用的系统之一（如果不是最完善和易用的话）。ORB-SLAM 代表着主流的特征点 SLAM 的一个高峰。相比于之前的工作，ORB-SLAM 具有以下几条明显的优势：</p><ul><li>支持单目、双目、RGB-D 三种模式。这使得无论我们拿到了任何一种常见的传感器，都可以先放到 ORB-SLAM 上测试一下，它具有良好的泛用性。</li><li>整个系统围绕 ORB 特征进行计算，包括视觉里程计与回环检测的 ORB 字典。它体现出 ORB 特征是现阶段计算平台的一种优秀的效率与精度之间的折衷方式。ORB不像 SIFT 或 SURF 那样费时，在 CPU 上面即可实时计算；相比 Harris 角点等简单角点特征，又具有良好的旋转和缩放不变性。并且，ORB 提供描述子，使我们在大范围运动时能够进行回环检测和重定位。</li><li>ORB 的回环检测是它的亮点。优秀的回环检测算法保证了 ORB-SLAM 有效地防止累计误差，并且在丢失之后还能迅速找回，这在许多现有的 SLAM 系统中都不够完善。为此，ORB-SLAM 在运行之前必须加载一个很大的 ORB 字典文件。</li><li>ORB-SLAM 创新式地使用了三个线程完成 SLAM：实时跟踪特征点的 Tracking 线程，局部 Bundle Adjustment 的优化线程（Co-visibility Graph，俗称小图），以及全局 Pose Graph 的回环检测与优化线程（Essential Graph 俗称大图）。其中，Tracking线程负责对每张新来的图像提取 ORB 特征点，并与最近的关键帧进行比较，计算特征点的位置并粗略估计相机位姿。小图线程求解一个Bundle Adjustment 问题，它包括局部空间内的特征点与相机位姿。这个线程负责求解更精细的相机位姿与特征点空间位置。不过，仅有前两个线程，只完成了一个比较好的视觉里程计。第三个线程，也就是大图线程，对全局的地图与关键帧进行回环检测，消除累积误差。由于全局地图中的地图点太多，所以这个线程的优化不包括地图点，而只有相机位姿组成的位姿图。继 PTAM 的双线程结构之后，ORB-SLAM 的三线程结构取得了非常好的跟踪和建图效果，能够保证轨迹与地图的全局一致性。这种三线程结构亦将被后续的研究者认同和采用。</li><li>ORB-SLAM围绕特征点进行了不少的优化。例如，在OpenCV的特征提取基础上保证了特征点的均匀分布；在优化位姿时使用了一种循环优化四遍以得到更多正确匹配的方法；比PTAM更为宽松的关键帧选取策略等等。这些细小的改进使得 ORB-SLAM 具有远超其他方案的鲁棒性：即使对于较差的场景，较差的标定内参，ORB-SLAM 都能够顺利地工作。</li></ul><p>整个ORB-SLAM系统包括三个部分组成，分别是跟踪（Tracking）、局部建图（Local Mapping）以及回环检测（Loop Closing）模块，它们分别被三个线程并行地进行处理。接下来对这个系统进行介绍。</p><p><img src="https://vincentqin.gitee.io/blogresource-1/orb-slam/orb-slam2-mainflow.png" alt></p><h1 id="理论篇"><a href="#理论篇" class="headerlink" title="理论篇"></a>理论篇</h1><h2 id="跟踪模块"><a href="#跟踪模块" class="headerlink" title="跟踪模块"></a>跟踪模块</h2><p>跟踪（Tracking）是在每帧中粗略地定位相机位姿以及决定何时插入新的关键帧。算法设计了运动模型以及跟踪参考帧模型去大致预测出相机的位姿。如相机跟踪失败（由于遮挡、大幅度运动等），就启动重定位模块对相机进行位置查找。如果已经有了初始位姿以及特征匹配，利用关键帧的Covisibility Graph恢复出局部可见图。之后，局部地图点的匹配可利用重投影实现，随后相机的位姿利用BA来优化。最后，Tracking线程决定是否插入新的关键帧。</p><ul><li>初始位姿估计：利用运动模型或者关键帧模型去预测相机位姿。如果运动模型已经跟踪到了当前帧，会利用引导匹配（Guided Search）在上一帧中寻找地图点。如果没有找到足够的匹配（如，运动模型不适用的情况），我们就在上一帧中更大的范围中寻找地图点。如不满足运动模型条件，导致运动模型失败，则采用参考关键帧模型利用参考帧模型对当前帧进行跟踪。通过以上两个模型即可对相机位姿进行初步定位。</li><li>跟踪局部地图：一旦我们已经估计了相机位姿以及我们得到一系列匹配的特征。我们可以将地图点投影到该帧上以搜索更多的匹配地图点。为了减小计算大图的超大复杂度，我们仅将其投影局部小图。局部地图包括，一系列关键帧<script type="math/tex">K_1</script>，这些关键帧与当前帧共享着相同的地图点；还有与<script type="math/tex">K_1</script>有共视关系的关键帧<script type="math/tex">K_2</script>们。局部图还有一个参考帧<script type="math/tex">K_{ref}</script>，这个关键帧与当前帧有最多的匹配点。</li><li>重定位：当运动模型以及跟踪关键帧失败时，可利用重定位来恢复得到相机位姿。应该从历史关键帧中选取和当前帧相似的图片，对当前帧进行位姿估计以及位姿优化。</li></ul><h2 id="局部建图模块"><a href="#局部建图模块" class="headerlink" title="局部建图模块"></a>局部建图模块</h2><p>局部建图（Local Mapping）的主要任务：当跟踪当前帧成功之后，需要利用局部建图更新其运动模型同时更新地图点。等待跟踪过程判断是否应该插入一个新的关键帧，并把关键帧插入到地图中，并对局部地图进行局部BA优化。这个线程能够获得更为精细的相机位姿以及点云。</p><ul><li>处理关键帧：跟踪成功之后，需要对关键帧进行处理以得到地图。具体而言：从关键帧队列中获得一帧，计算出其特征点的BoW映射向量（表示）。关键帧和其对应的地图点进行绑定，更新地图点的平均观测方向以及观测距离范围。更新关键帧之间的连接关系（共视关系），最后将关键帧插入地图中。</li><li>精选地图点：由于跟踪过程引入地图点的策略较为宽松，此时需要检查最近加入的地图点，并将一些冗余的地图点从最近地图点的列表中剔除。</li><li>创建新地图点：由于上一步已经剔除了一些冗余地图点，该模块需要通过当前关键帧及其共视关键帧利用三角化得到更多高质量的3D地图点并添加地图点的属性。</li><li>Local BA：该步骤通过局部BA优化局部地图点以及局部关键帧的位姿。</li><li>精选关键帧：剔除冗余的关键帧，这样不至于增加后期BA的压力，而且可以保证在相同的环境下，关键帧的数目不会无限制的增长，同时减小存储压力。</li></ul><h2 id="回环检测模块"><a href="#回环检测模块" class="headerlink" title="回环检测模块"></a>回环检测模块</h2><p>回环检测（Loop Closing）的主要目标是检测当前关键帧是否经过历史位置。如有经过，则利用回环检测得到的回环帧去修正整个SLAM长期跟踪过程中带来的累积误差、尺度漂移等。如果仅有前两个线程的话，仅仅完成了一个很好的视觉里程计（VO），这个线程会对全局地图以及关键帧进行回环检测，以消除上述累积误差。</p><ul><li>候选关键帧检测：当前关键帧仅有与历史关键帧足够相似才可能成为回环候选帧，该模块通过一定的筛选策略对当前关键帧进行筛选，判断其是否为闭环候选关键帧。由于在实际闭环检测过程中，回环候选帧及其共视关键帧，在一定连续的时间内都可能被观测到。该模块主要通过利用这一条件，对闭环候选关键帧进一步地筛选，通过筛选条件的候选关键帧将进行下一步的判断。</li><li>相似性变换计算：考虑到单目SLAM的尺度漂移，当前帧和回环帧之间的相对位姿应是一个相似变换，并且，二者之间应具有足够多的匹配点。该模块主要是通过循环计算当前帧和上述经过筛选后的候选关键帧之间的相似变换，直到找到一个和当前帧具有足够多匹配点的相似变换，对应的候选关键帧即为最终的回环帧。</li><li>回环修正：受累积误差的影响，时间越久，越接近当前帧的关键帧及相应的地图点，误差将越大。若寻找到的回环帧，当前帧位姿及其对应的地图点会更精确。该模块就是为了修正累积误差，利用回环帧及其共视关键帧，以及对应的地图点，来修正当前帧及其共视关键帧的位姿以及对应的地图点的世界坐标。紧接着进行地图点融合，更新共视图，然后通过本质图优化相机位姿，最后进行全局BA来修正整个SLAM的累积误差（相机位姿以及地图点）。</li></ul><p>牛吹完了，说下缺点。<br>当然，ORB-SLAM 也存在一些不足之处。首先，由于整个 SLAM 系统都采用特征点进行计算，我们必须对每张图像都计算一遍 ORB 特征，这是非常耗时的。ORB-SLAM 的三线程结构也对 CPU 带来了较重的负担，使得它只有在当前 PC 架构的 CPU 上才能实时运算，移植到嵌入式端则有一定困难。其次，ORB-SLAM 的建图为稀疏特征点，目前还没有开放存储和读取地图后重新定位的功能（虽然从实现上来讲并不困难）。根据我们在建图章节的分析，稀疏特征点地图只能满足我们对定位的需求，而无法提供导航、避障、交互等诸多功能。然而，如果我们仅用 ORB-SLAM 处理定位问题，似乎又嫌它有些过于重量级了。相比之下，另外一些方案提供了更为轻量级的定位，使我们能够在低端的处理器上运行 SLAM，或者让 CPU 有余力处理其他的事务。</p><h1 id="实践篇"><a href="#实践篇" class="headerlink" title="实践篇"></a>实践篇</h1><p>在这里下载ORB-SLAM2的源码，然后参考ORB-SLAM2项目的说明文档，安装一些必要的第三方软件：</p><ul><li>pangolin：<a href="http://eigen.tuxfamily.org/index.php?title=Main_Page" target="_blank" rel="noopener">http://eigen.tuxfamily.org/index.php?title=Main_Page</a></li><li>Eigen：<a href="http://eigen.tuxfamily.org/index.php?title=Main_Page" target="_blank" rel="noopener">http://eigen.tuxfamily.org/index.php?title=Main_Page</a></li><li>opencv 3.4.2 ：<a href="https://blog.csdn.net/haoqimao_hard/article/details/82049565" target="_blank" rel="noopener">https://blog.csdn.net/haoqimao_hard/article/details/82049565</a></li><li>ROS：<a href="http://wiki.ros.org/melodic/Installation/Ubuntu#Ubuntu_install_of_ROS_Melodic" target="_blank" rel="noopener">http://wiki.ros.org/melodic/Installation/Ubuntu#Ubuntu_install_of_ROS_Melodic</a></li></ul><p>注意，其中有坑，务必安装正确。安装好之后顺便在Euroc数据集中的MH01上测试，得到下面的轨迹地图。</p><p><img src="//www.vincentqin.tech/posts/orb-slam/MH01-traj.png" alt></p><h2 id="EuRoC数据集"><a href="#EuRoC数据集" class="headerlink" title="EuRoC数据集"></a>EuRoC数据集</h2><p>EuRoC数据集包含11个双目序列，这个序列由小型无人机在两个房间（V1/V2, Vicon Room）以及一个大工厂环境(MH, Machine Hall)中拍摄得到。相机的基线长约为11cm，以20Hz速度拍摄图片。序列被分成了三种（根据MAV的速度，光照以及场景纹理）easy , medium, difficult。<br>$ATE$表示绝对轨迹误差，是衡量相机位姿的标准之一。假设有真实位姿序列：<script type="math/tex">P_1,P_2,P_3,...,P_n</script>以及估计的位姿序列：<script type="math/tex">Q_1,Q_2,Q_3,...,Q_n</script> ，它们已经做了包括时间戳对齐等操作。实际场景中，这两个序列可能有不同的采样率、长度亦或数据可能丢失，此时需要进行数据关联和插值。首先得的在第i时刻的轨迹误差：</p><script type="math/tex; mode=display">F_i := Q_i^{-1}SP_i</script><p>其中$S$是从<script type="math/tex">P_{1:n}</script>到<script type="math/tex">Q_{1:n}</script>的最小二乘刚体变换，通过求取以上误差在所有位置时刻的均方根我们得到APE的具体形式：</p><script type="math/tex; mode=display">RMSE(F_{1:n}):=\left(\frac{1}{n}\sum_{i=1}^n||trans(F_i)||^2\right)^{\frac{1}{2}}</script><p>其中的$trans(·)$表示求取该位姿的平移分量算子。</p><p>文中大部分内容来自网络以及高博十四讲。因本人水平有限，如有错误，谢谢指出。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="http://www.slamcn.org/index.php/%E9%A6%96%E9%A1%B5" target="_blank" rel="noopener">http://www.slamcn.org/index.php/%E9%A6%96%E9%A1%B5</a></li><li><a href="https://blog.csdn.net/qinruiyan/article/details/50918504" target="_blank" rel="noopener">https://blog.csdn.net/qinruiyan/article/details/50918504</a></li><li>Sturm J, Engelhard N, Endres F, et al. A benchmark for the evaluation of RGB-D SLAM systems[C]. Ieee International Conference on Intelligent Robots and Systems. IEEE, 2012:573-580.</li><li>Horn B K P. Closed-form solution of absolute orientation using unit quaternions[J]. J.opt.soc.am.a, 1987, 5(7):1127-1135.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;首先，解释下SLAM的概念，借鉴高博《视觉 SLAM 十四讲》中的一句话：SLAM 是 Simultaneous Localization and Mapping 的缩写，中文译作“同时定位与地图构建”。它是指搭载特定传感器的主体，在没有环境先验信息的情况下，于运动过程中建立环境的模型，同时估计自己的运动。如果这里的传感器主要为相机，那就称为“视觉 SLAM”。&lt;/p&gt;
    
    </summary>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/categories/SLAM/"/>
    
    
  </entry>
  
  <entry>
    <title>资料：SLAM草稿</title>
    <link href="https://www.vincentqin.tech/posts/slam/"/>
    <id>https://www.vincentqin.tech/posts/slam/</id>
    <published>2018-11-29T16:53:00.000Z</published>
    <updated>2020-03-31T14:59:59.921Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="ECCV-2018-视觉定位综述"><a href="#ECCV-2018-视觉定位综述" class="headerlink" title="ECCV 2018 视觉定位综述"></a>ECCV 2018 视觉定位综述</h2><p>以下是今年ECCV上几位大牛介绍SLAM技术的tutorial pdfs，涉及基于特征点以及基于学习的SLAM算法介绍，并在最后探究了SLAM领域的主要问题以及未来的发展趋势。</p><ul><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/1.Intro.pdf" target="_blank" rel="noopener">Feature-based vs. Learned Approaches</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/2.Sattler-Feature-Based-3D-Localization.pdf" target="_blank" rel="noopener">Current State of Feature-based Localization</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/3.Learning-Based Localization_Upload.pdf" target="_blank" rel="noopener">Learning-based Localization</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/4.Failure_Cases.pdf" target="_blank" rel="noopener">Failure Cases of Feature-based and Learning-based Methods</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/5.Long_Term_Localization.pdf" target="_blank" rel="noopener">Long-term Localization: Towards Higher-level Scene Understanding</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/6.Learning Problems_Upload.pdf" target="_blank" rel="noopener">Open Problems of Learning-based Methods</a></li></ul><p>整理还未完备，先祭出这几本书&amp;博客，方便随时查看。</p><a id="more"></a><h2 id="教程"><a href="#教程" class="headerlink" title="教程"></a>教程</h2><ul><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/slambook14.pdf" target="_blank" rel="noopener">slambook</a></li><li><a href="https://github.com/gaoxiang12/slambook/tree/master/project/0.3" target="_blank" rel="noopener">slambook code</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/OpenCV3book.pdf" target="_blank" rel="noopener">OpenCV3.0</a></li><li><a href="https://blog.csdn.net/OnafioO/article/details/73175835" target="_blank" rel="noopener">SLAM前世今生</a></li><li><a href="https://github.com/RainerKuemmerle/g2o" target="_blank" rel="noopener">g2o Github</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/g2o-details.pdf" target="_blank" rel="noopener">g2o details</a></li><li><a href="https://blog.csdn.net/u012700322/article/details/52857244" target="_blank" rel="noopener">g2o译文</a></li><li><a href="https://www.cnblogs.com/gaoxiang12/p/5304272.html" target="_blank" rel="noopener">深入理解图优化与g2o：g2o篇</a></li><li><a href="https://github.com/RainerKuemmerle/g2o" target="_blank" rel="noopener">深入理解图优化与g2o：g2o篇 code</a></li><li><a href="https://me.csdn.net/heyijia0327" target="_blank" rel="noopener">白巧克力亦唯心的博客</a></li><li><a href="http://www.360doc.com/content/17/0718/14/44420101_672315705.shtml" target="_blank" rel="noopener">Graph slam学习</a></li></ul><h2 id="g2o漫谈"><a href="#g2o漫谈" class="headerlink" title="g2o漫谈"></a><a href="https://openslam-org.github.io/g2o.html" target="_blank" rel="noopener">g2o</a>漫谈</h2><p>g2o里面有各种各样的求解器，而它的顶点、边的类型多种多样。通过自定义顶点和边，事实上，只要一个优化问题能够表达成图，就可以用g2o去求解它。常见的，比如bundle adjustment，ICP，数据拟合等。g2o是一个C++项目，其中矩阵数据结构多来自Eigen。</p><p><img src="https://vincentqin.gitee.io/blogresource-4/slam/g2o.png" alt></p><p>先看上部分，SparseOptimizer是我们需要维护的东西，是一个Optimizable Graph，也是一个Hyper Graph。一个SparseOptimizer含有很多个顶点（继承与Base Vertex）和多条边（继承自BaseUnaryEdge，BaseBinaryEdge或BaseMultiEdge）。这些Base Vertex和Base Edge都是抽象的基类，而实际用的顶点和边，都是它们的派生类。</p><p>我们用SparseOptimizer.addVertex 和 SparseOptimizer.addEdge 向图中添加顶点和边，然后调用SpaseOptimizer.optimize来优化。</p><p>在优化前，需要指定我们用的求解器和迭代算法。从图下半部分来看，一个SparseOptimization拥有一个Optimization Algorithm,继承自Gusss-Newton，Levernberg-Marquardt，Powell’s dogleg 三者之一，同时拥有一个Solver，含有俩个部分。一个是SparseBlockMatrix，用于计算稀疏的雅克比和海塞；一个用于计算 <script type="math/tex">H\Delta x = -b</script>，需要一个线性方程的求解器。而这个求解器，可以从PCG，CSparse，Choldmod三者选一。<br>则一共三个步骤：</p><ol><li>选择一个线性方程求解器，从 PCG, CSparse, Choldmod中选</li><li>选择一个 BlockSolver</li><li>选择一个迭代策略，从GN, LM, Doglog中选</li></ol><h3 id="BlockSolver，块求解器"><a href="#BlockSolver，块求解器" class="headerlink" title="BlockSolver，块求解器"></a>BlockSolver，块求解器</h3><p>块求解器是包含线性求解器的存在，之所以是包含，是因为块求解器会构建好线性求解器所需要的矩阵块（也就是<script type="math/tex">H</script>和<script type="math/tex">b</script>，<script type="math/tex">H\Delta x = -b</script>），之后给线性求解器让它进行运算，边的jacobian也就是在这个时候发挥了自己的光和热。</p><p>这里再记录下一个比较容易混淆的问题，也就是在初始化块求解器的时候的参数问题。大部分的例程在初始化块求解器的时候都会使用如下的程序代码：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;g2o::BlockSolver_6_3::LinearSolverType&gt; linearSolver = g2o::make_unique&lt;g2o::LinearSolverCholmod&lt;g2o::BlockSolver_6_3::PoseMatrixType&gt;&gt;();</span><br></pre></td></tr></table></figure></p><p>其中的BlockSolver_6_3有两个参数，分别是6和3，在定义的时候可以看到这是一个模板的重命名（模板类的重命名只能用using）<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">int</span> p, <span class="keyword">int</span> l&gt;  </span><br><span class="line"><span class="keyword">using</span> BlockSolverPL = BlockSolver&lt; BlockSolverTraits&lt;p, l&gt; &gt;;</span><br></pre></td></tr></table></figure></p><p>其中<strong>p代表pose的维度，l表示landmark的维度</strong>，且这里都表示的是增量的维度。</p><h3 id="g2o的顶点（Vertex）"><a href="#g2o的顶点（Vertex）" class="headerlink" title="g2o的顶点（Vertex）"></a>g2o的顶点（Vertex）</h3><ul><li><code>g2o::BaseVertex&lt; D, T &gt;</code> 其中 <code>int D, typename T</code></li></ul><p>首先记录一下定义模板的两个参数D和T，两个类型分别是int和typename的类型，D表示的是维度，g2o源码里面是这个注释的:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> Dimension = D; <span class="comment">//&lt; dimension of the estimate (minimal) in the manifold space</span></span><br></pre></td></tr></table></figure><p>可以看到这个D并非是顶点（更确切的说是状态变量）的维度，而是其在流形空间（manifold）的最小表示，这里一定要区别开；之后是T，源码里面也给出了T的作用:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> T EstimateType;</span><br><span class="line">EstimateType _estimate;</span><br></pre></td></tr></table></figure><p>可以看到，这里T就是顶点（状态变量）的类型。在顶点的继承中，这两个参数是直接面向我们的，所以务必要定义妥当。</p><h3 id="g2o的边（Edge）"><a href="#g2o的边（Edge）" class="headerlink" title="g2o的边（Edge）"></a>g2o的边（Edge）</h3><ul><li><code>g2o::BaseBinaryEdge&lt; D, E, VertexXi, VertexXj &gt;</code> 其中 <code>int D, typename E</code></li></ul><p>首先还是介绍这两个参数，还是从源码上来看：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> Dimension = D;</span><br><span class="line"><span class="keyword">typedef</span> E Measurement;</span><br><span class="line"><span class="keyword">typedef</span> Eigen::Matrix&lt;<span class="keyword">number_t</span>, D, <span class="number">1</span>, Eigen::ColMajor&gt; ErrorVector;</span><br></pre></td></tr></table></figure><p>可以看到，D决定了误差的维度，从映射的角度讲，三维情况下就是2维的，二维的情况下是1维的；然后E是measurement的类型，也就是测量值是什么类型的，这里E就是什么类型的（一般都是Eigen::VectorN表示的，N是自然数）。</p><ul><li><code>typename VertexXi</code>, <code>typename VertexXj</code></li></ul><p>这两个参数就是边连接的两个顶点的类型，这里特别注意一下，这两个必须一定是顶点的类型，也就是继承自<code>BaseVertex</code>等基础类的类！不是顶点的数据类！例如必须是<code>VertexSE3Expmap</code>而不是<code>VertexSE3Expmap</code>的数据类型类<code>SE3Quat</code>。原因的话源码里面也很清楚，因为后面会用到一系列顶点的维度等等的属性，这些属性是数据类型类里面没有的。</p><ul><li><code>_jacobianOplusXi</code>，<code>_jacobianOplusXj</code></li></ul><p>在成员函数<code>linearizeOplus()</code>（线性化函数）中维护着这两个量，<code>_jacobianOplusXi</code>和<code>_jacobianOplusXj</code>就是所谓的雅可比矩阵，如果是二元边的话二者都有；若为一元边，只有<code>_jacobianOplusXi</code>。</p><p>这两个变量本质上是Eigen::Matrix类型的，具体定义在这里：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">typename</span> Eigen::Matrix&lt;<span class="keyword">number_t</span>, D, Di, D==<span class="number">1</span>?Eigen::RowMajor:Eigen::ColMajor&gt;::AlignedMapType JacobianXiOplusType;</span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">typename</span> Eigen::Matrix&lt;<span class="keyword">number_t</span>, D, Dj, D==<span class="number">1</span>?Eigen::RowMajor:Eigen::ColMajor&gt;::AlignedMapType JacobianXjOplusType;</span><br><span class="line">JacobianXiOplusType _jacobianOplusXi;</span><br><span class="line">JacobianXjOplusType _jacobianOplusXj;</span><br></pre></td></tr></table></figure></p><hr><p>to be continued…</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li><a href="https://blog.csdn.net/Hansry/article/details/78080807" target="_blank" rel="noopener">RGB-D SLAM——g2o篇（三）</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ECCV-2018-视觉定位综述&quot;&gt;&lt;a href=&quot;#ECCV-2018-视觉定位综述&quot; class=&quot;headerlink&quot; title=&quot;ECCV 2018 视觉定位综述&quot;&gt;&lt;/a&gt;ECCV 2018 视觉定位综述&lt;/h2&gt;&lt;p&gt;以下是今年ECCV上几位大牛介绍SLAM技术的tutorial pdfs，涉及基于特征点以及基于学习的SLAM算法介绍，并在最后探究了SLAM领域的主要问题以及未来的发展趋势。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://vincentqin.gitee.io/blogresource-4/slam/1.Intro.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Feature-based vs. Learned Approaches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://vincentqin.gitee.io/blogresource-4/slam/2.Sattler-Feature-Based-3D-Localization.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Current State of Feature-based Localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://vincentqin.gitee.io/blogresource-4/slam/3.Learning-Based Localization_Upload.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Learning-based Localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://vincentqin.gitee.io/blogresource-4/slam/4.Failure_Cases.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Failure Cases of Feature-based and Learning-based Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://vincentqin.gitee.io/blogresource-4/slam/5.Long_Term_Localization.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Long-term Localization: Towards Higher-level Scene Understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://vincentqin.gitee.io/blogresource-4/slam/6.Learning Problems_Upload.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Open Problems of Learning-based Methods&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;整理还未完备，先祭出这几本书&amp;amp;博客，方便随时查看。&lt;/p&gt;
    
    </summary>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/categories/SLAM/"/>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="cv" scheme="https://www.vincentqin.tech/tags/cv/"/>
    
      <category term="computer vision" scheme="https://www.vincentqin.tech/tags/computer-vision/"/>
    
      <category term="AR" scheme="https://www.vincentqin.tech/tags/AR/"/>
    
  </entry>
  
  <entry>
    <title>资料：Line Segments Detection</title>
    <link href="https://www.vincentqin.tech/posts/line-segments-detection/"/>
    <id>https://www.vincentqin.tech/posts/line-segments-detection/</id>
    <published>2018-11-18T07:11:20.000Z</published>
    <updated>2018-12-09T15:05:49.466Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>线段检测算法汇总，若无时间看正文，直接到<a href="https://github.com/Vincentqyw/LineSegmentsDetection" target="_blank" rel="noopener">链接</a>看线段检测算法代码集合。</p><a id="more"></a><h2 id="HoughLine"><a href="#HoughLine" class="headerlink" title="HoughLine"></a>HoughLine</h2><p>基于<code>hough</code>变换的线段检测算法。<code>opencv</code>提供了2个基于<code>hough</code>变换的函数：<code>cv::HoughLines()</code>以及<code>cv::HoughLinesP()</code>。其中<code>cv::HoughLines()</code>为<a href="https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#ga46b4e588934f6c8dfd509cc6e0e4545a" target="_blank" rel="noopener">标准霍夫变换</a>，此函数通常不会用到，它经常会被<a href="https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#ga8618180a5948286384e3b7ca02f6feeb" target="_blank" rel="noopener">累积概率霍夫变换</a>函数<code>cv::HoughLinesP()</code>代替。概率霍夫变换函数原型：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> cv::HoughLinesP(InputArray image,</span><br><span class="line">OutputArray lines,</span><br><span class="line"><span class="keyword">double</span> rho,</span><br><span class="line"><span class="keyword">double</span> theta,</span><br><span class="line"><span class="keyword">int</span> threshold,</span><br><span class="line"><span class="keyword">double</span> minLineLength = <span class="number">0</span>,</span><br><span class="line"><span class="keyword">double</span> maxLineGap = <span class="number">0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>注意函数返回值<code>line</code>，它是一个四维向量$(x_1,y_1,x_2,y_2)$，其中$(x_1,y_1)$以及$(x_2,y_2)$分别表示线段的端点坐标。以下给出示例代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/imgproc.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Mat src, dst, color_dst;</span><br><span class="line">    <span class="keyword">if</span>( argc != <span class="number">2</span> || !(src=imread(argv[<span class="number">1</span>], <span class="number">0</span>)).data)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="comment">// 边缘检测转换为二值边缘图</span></span><br><span class="line">    Canny( src, dst, <span class="number">50</span>, <span class="number">200</span>, <span class="number">3</span> );</span><br><span class="line">    cvtColor( dst, color_dst, COLOR_GRAY2BGR );</span><br><span class="line">    <span class="built_in">vector</span>&lt;Vec4i&gt; lines;</span><br><span class="line">    HoughLinesP( dst, lines, <span class="number">1</span>, CV_PI/<span class="number">180</span>, <span class="number">80</span>, <span class="number">30</span>, <span class="number">10</span> );</span><br><span class="line">    <span class="keyword">for</span>( <span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; lines.size(); i++ )</span><br><span class="line">    &#123;</span><br><span class="line">        line( color_dst, Point(lines[i][<span class="number">0</span>], lines[i][<span class="number">1</span>]),</span><br><span class="line">        Point( lines[i][<span class="number">2</span>], lines[i][<span class="number">3</span>]), Scalar(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>), <span class="number">3</span>, <span class="number">8</span> );</span><br><span class="line">    &#125;</span><br><span class="line">    namedWindow( <span class="string">"Source"</span>, <span class="number">1</span> );</span><br><span class="line">    imshow( <span class="string">"Source"</span>, src );</span><br><span class="line">    namedWindow( <span class="string">"Detected Lines"</span>, <span class="number">1</span> );</span><br><span class="line">    imshow( <span class="string">"Detected Lines"</span>, color_dst );</span><br><span class="line">    waitKey(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="LSD"><a href="#LSD" class="headerlink" title="LSD"></a><a href="http://www.ipol.im/pub/art/2012/gjmr-lsd/" target="_blank" rel="noopener">LSD</a></h2><ul><li>论文标题：”LSD: a Line Segment Detector”</li><li>项目主页：<a href="http://www.ipol.im/pub/art/2012/gjmr-lsd/" target="_blank" rel="noopener">http://www.ipol.im/pub/art/2012/gjmr-lsd/</a></li><li>论文地址：<a href="http://www.ipol.im/pub/art/2012/gjmr-lsd/article.pdf" target="_blank" rel="noopener">http://www.ipol.im/pub/art/2012/gjmr-lsd/article.pdf</a></li><li>代码地址：<a href="http://www.ipol.im/pub/art/2012/gjmr-lsd/lsd_1.5.zip" target="_blank" rel="noopener">http://www.ipol.im/pub/art/2012/gjmr-lsd/lsd_1.5.zip</a></li></ul><p>该方法是目前性价比（速度精度）最好的算法，现已经集成到<code>opencv</code>中<a href="https://docs.opencv.org/master/d1/dbd/classcv_1_1line__descriptor_1_1LSDDetector.html" target="_blank" rel="noopener"><code>LSDDetector</code></a>。LSD能够在线性时间内检测到亚像素精度的线段。无需调整参数，适用于各种场景。因为每张图有误检，LSD能够控制误检率。PS：论文此处不介绍了，可以参考<a href="https://blog.csdn.net/chishuideyu/article/details/78081643?locationNum=9&amp;fps=1" target="_blank" rel="noopener">这里</a>。</p><h2 id="LSWMS"><a href="#LSWMS" class="headerlink" title="LSWMS"></a>LSWMS</h2><ul><li>论文标题：”Line segment detection using weighted mean shift procedures on a 2D slice sampling strategy”</li><li>论文地址：<a href="https://www.researchgate.net/profile/Marcos_Nieto3/publication/220654859_Line_segment_detection_using_weighted_mean_shift_procedures_on_a_2D_slice_sampling_strategy/links/56a5d56a08aef91c8c16b1ac.pdf?inViewer=0&amp;origin=publication_detail&amp;pdfJsDownload=0" target="_blank" rel="noopener">https://www.researchgate.net/LSWMS.pdf</a></li><li>代码地址：<a href="https://sourceforge.net/projects/lswms/" target="_blank" rel="noopener">https://sourceforge.net/projects/lswms/</a></li></ul><h2 id="EDline（ED-Edge-Drawing）"><a href="#EDline（ED-Edge-Drawing）" class="headerlink" title="EDline（ED: Edge Drawing）"></a>EDline（ED: Edge Drawing）</h2><ul><li>论文标题：”Edge Drawing: A Combined Real-Time Edge and Segment Detector”</li><li>论文地址：<a href="https://sci-hub.tw/10.1016/j.jvcir.2012.05.004" target="_blank" rel="noopener">https://sci-hub.tw/10.1016/j.jvcir.2012.05.004</a></li><li>代码地址：<a href="https://github.com/mtamburrano/LBD_Descriptor" target="_blank" rel="noopener">https://github.com/mtamburrano/LBD_Descriptor</a></li></ul><h2 id="CannyLines"><a href="#CannyLines" class="headerlink" title="CannyLines"></a><a href="http://cvrs.whu.edu.cn/projects/cannyLines/" target="_blank" rel="noopener">CannyLines</a></h2><ul><li>论文标题：”CannyLines: A Parameter-Free Line Segment Detector”</li><li>项目主页：<a href="http://cvrs.whu.edu.cn/projects/cannyLines/" target="_blank" rel="noopener">http://cvrs.whu.edu.cn/projects/cannyLines/</a></li><li>论文地址：<a href="http://cvrs.whu.edu.cn/projects/cannyLines/papers/CannyLines-ICIP2015.pdf" target="_blank" rel="noopener">http://cvrs.whu.edu.cn/projects/cannyLines/papers/CannyLines-ICIP2015.pdf</a></li><li>代码地址：<a href="http://cvrs.whu.edu.cn/projects/cannyLines/codes/CannyLines-v3.rar" target="_blank" rel="noopener">http://cvrs.whu.edu.cn/projects/cannyLines/codes/CannyLines-v3.rar</a></li></ul><p>本文提出了一种鲁棒的线段检测算法来有效地检测来自输入图像的线段。首先，文章提出了一种无参数的Canny算子，称为Canny（PANYPF），通过自适应地设置传统Canny算子的低阈值和高阈值来鲁棒地从输入图像中提取边缘。然后，提出了有效的像素连接和分割技术，直接从边缘图中收集共线点群，用于基于最小二乘拟合方法拟合初始线段。第三，通过有效的扩展和合并，产生更长、更完整的线段。最后，利用亥姆霍兹原理（Helmholtz Principle）对所有的检测线段检测，主要考虑梯度方向和幅度信息。该算法能够在人工场景中能够获得比LSD以及EDline精度更高以及平均长度更高的线段。</p><h2 id="MCMLSD"><a href="#MCMLSD" class="headerlink" title="MCMLSD"></a>MCMLSD</h2><ul><li>论文标题：”MCMLSD: A Dynamic Programming Approach to Line Segment Detection”</li><li>论文地址：<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Almazan_MCMLSD_A_Dynamic_CVPR_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/papers/Almazan_MCMLSD_A_Dynamic_CVPR_2017_paper.pdf</a></li><li>代码地址：<a href="http://www.elderlab.yorku.ca/?smd_process_download=1&amp;download_id=8423" target="_blank" rel="noopener">http://www.elderlab.yorku.ca/?smd_process_download=1&amp;download_id=8423</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;线段检测算法汇总，若无时间看正文，直接到&lt;a href=&quot;https://github.com/Vincentqyw/LineSegmentsDetection&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;链接&lt;/a&gt;看线段检测算法代码集合。&lt;/p&gt;
    
    </summary>
    
    
      <category term="CV" scheme="https://www.vincentqin.tech/categories/CV/"/>
    
    
      <category term="cv" scheme="https://www.vincentqin.tech/tags/cv/"/>
    
      <category term="line segment" scheme="https://www.vincentqin.tech/tags/line-segment/"/>
    
      <category term="detection" scheme="https://www.vincentqin.tech/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>装机只是为了换种心情</title>
    <link href="https://www.vincentqin.tech/posts/new-pc/"/>
    <id>https://www.vincentqin.tech/posts/new-pc/</id>
    <published>2018-10-16T15:25:22.000Z</published>
    <updated>2020-03-31T14:51:17.816Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>上周配了一台中档主机，具体配置如下。<br><a id="more"></a></p><div class="table-container"><table><thead><tr><th>配件</th><th>品牌型号</th><th>价格（元）</th></tr></thead><tbody><tr><td>CPU+主板</td><td>AMD R5-2600X 六核盒装处理器搭华硕B450台式机电脑主板CPU套装</td><td>2199</td></tr><tr><td>散热器</td><td>九州风神（ DEEPCOOL） 玄冰400幻彩版CPU风冷散热器</td><td>129</td></tr><tr><td>内存</td><td>影驰 （Galaxy）GAMER DDR4 8GB 2400 频率单条台式机四代内存条</td><td>427</td></tr><tr><td>机械硬盘</td><td>WD/西部数据 WD10SPZX 蓝盘1TB</td><td>289</td></tr><tr><td>固态硬盘</td><td>Teclast/台电 240G M.2 幻影 NVME PCI-E笔记本台式机SSD固态硬盘</td><td>339</td></tr><tr><td>主机箱</td><td>AIGO/爱国者炫影2全侧透水冷电竞机箱DIY组装整机电脑游戏主机箱</td><td>219</td></tr><tr><td>电源</td><td>酷冷至尊（CoolerMaster）额定500W MWE450机箱电源(80PLUS铜牌)</td><td>269</td></tr><tr><td>显卡</td><td>影驰 (Galaxy)GTX 1060 大将 6GB大显存台式机独立电脑显卡</td><td>1539</td></tr><tr><td>显示器</td><td>翔野27英寸144hz电竞显示器2K台式液晶电脑显示屏幕</td><td>1288</td></tr><tr><td>键盘</td><td>IKBC C87c104机械键盘吃鸡游戏 cherry樱桃 青轴</td><td>387</td></tr><tr><td>无线网卡</td><td>TP-LINK TL-WN725N免驱版 迷你USB无线网卡mini</td><td>45</td></tr><tr><td>合计</td><td>-</td><td>7130</td></tr></tbody></table></div><p>就是这些无疑了，上一张全家福：<br><img src="https://vincentqin.gitee.io/blogresource-1/new-pc/family.jpg" alt></p><!--more--><p>B450-PLUS大主板</p><p><img src="https://vincentqin.gitee.io/blogresource-1/new-pc/matherboard.jpg" alt></p><p>CPU选用了性价比更高的AMD 2600X，对标于Intel的i7-6900K，详情见<a href="http://www.mydrivers.com/zhuanti/tianti/cpu/" target="_blank" rel="noopener">CPU天梯图</a>。</p><p><img src="https://vincentqin.gitee.io/blogresource-1/new-pc/amd.jpg" alt></p><p>第一次装机，主机箱和主板之间连接的开机控制线接错了一根，没能一次点亮。幸好我火眼金睛，及时纠正。说到最关心的性能，还行吧，鲁大师跑分32W+。夜深了。下面就是闪亮主机箱：</p><p><img src="https://vincentqin.gitee.io/blogresource-1/new-pc/pc-box.jpg" alt></p><p><img src="https://vincentqin.gitee.io/blogresource-1/new-pc/pc-part.jpg" alt></p><p>po一张壁纸，嘿嘿~</p><p><img src="https://vincentqin.gitee.io/blogresource-1/new-pc/desktop.png" alt></p><p>远远地望去就是这样了：</p><p><img src="https://vincentqin.gitee.io/blogresource-1/new-pc/desktop-all.jpg" alt></p><p>凑个数：</p><p><img src="https://vincentqin.gitee.io/blogresource-1/new-pc/wall-st.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上周配了一台中档主机，具体配置如下。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="装机" scheme="https://www.vincentqin.tech/tags/%E8%A3%85%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>资料：那些年我们一起调过的Bug</title>
    <link href="https://www.vincentqin.tech/posts/tips-for-fix-errors/"/>
    <id>https://www.vincentqin.tech/posts/tips-for-fix-errors/</id>
    <published>2018-08-25T19:24:49.000Z</published>
    <updated>2018-08-25T19:37:21.579Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><ul><li><a href="http://www.cnblogs.com/gaoxiang12/p/5244828.html" target="_blank" rel="noopener">图优化</a></li><li><a href="http://www.cnblogs.com/yiyezhai/p/3176725.html" target="_blank" rel="noopener">三维旋转</a></li><li><a href="https://blog.csdn.net/u013390476/article/details/50209603" target="_blank" rel="noopener">C++记录时间</a></li><li><a href="https://blog.csdn.net/u010128736/article/details/52850444" target="_blank" rel="noopener">相机内外参解释</a></li><li><a href="https://www.cnblogs.com/gaochsh/p/6901809.html" target="_blank" rel="noopener">shell echo字符串处理</a></li><li><a href="http://www.ceres-solver.org/installation.html#linux" target="_blank" rel="noopener">ubuntu 14.04 安装 ceres</a></li><li><a href="http://wiki.ros.org/cn/indigo/Installation/Ubuntu" target="_blank" rel="noopener">Ubuntu 14.04 安装 ROS indigo</a></li><li><a href="https://blog.csdn.net/u010472607/article/details/76166008" target="_blank" rel="noopener">Ubuntu 14.04 安装 Cmake 3.9.x</a></li><li><a href="https://blog.csdn.net/youngpan1101/article/details/58027049" target="_blank" rel="noopener">ubuntu 14.04 安装 Opencv 3.2.0</a></li><li><a href="https://blog.csdn.net/a874909657/article/details/79161533" target="_blank" rel="noopener">VMware Ubuntu 无法全屏解决方案</a></li><li><a href="https://blog.csdn.net/poem_qianmo/article/details/30974513" target="_blank" rel="noopener">OpenCV重映射 &amp; SURF特征点检测合辑</a></li><li><a href="https://blog.csdn.net/xiaotanyu13/article/details/8210955" target="_blank" rel="noopener">VS2010控制台程序运行一闪而过的完美解决办法</a></li><li><a href="https://blog.csdn.net/iracer/article/details/51339377" target="_blank" rel="noopener">OpenCV数据持久化: <strong>FileStorage</strong>类的数据存取操作与示例</a></li><li><a href="http://answers.opencv.org/question/121651/fata-error-lapacke_h_path-notfound-when-building-opencv-32/" target="_blank" rel="noopener">fata error: LAPACKE_H_PATH-NOTFOUND when building OpenCV 3.2</a></li><li><a href="https://blog.csdn.net/zyxlinux888/article/details/6358615" target="_blank" rel="noopener">E:Could not get lock /var/lib/apt/lists/lock - open (11: Resource temporarily unavailable)</a></li></ul><a id="more"></a><ul><li><p>Ubuntu下上Matlab运行时terminal会提示一堆错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">libGL error: unable to load driver: vmwgfx_dri.so</span><br><span class="line">libGL error: driver pointer missing</span><br><span class="line">libGL error: failed to load driver: vmwgfx</span><br><span class="line">libGL error: unable to load driver: swrast_dri.so</span><br><span class="line">libGL error: failed to load driver: swrast</span><br><span class="line">X Error of failed request:  BadValue (integer parameter out of range for operation)</span><br><span class="line">  Major opcode of failed request:  155 (GLX)</span><br><span class="line">  Minor opcode of failed request:  3 (X_GLXCreateContext)</span><br><span class="line">  Value in failed request:  0x0</span><br><span class="line">  Serial number of failed request:  31</span><br><span class="line">  Current serial number in output stream:  34</span><br></pre></td></tr></table></figure><p>  <a href="https://bbs.archlinux.org/viewtopic.php?id=154775" target="_blank" rel="noopener">解决方法</a>：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -sf /lib/libstdc++.so.6 /usr/local/MATLAB/R2015a/sys/os/glnxa64/libstdc++.so.6</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/gaoxiang12/p/5244828.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;图优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/yiyezhai/p/3176725.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;三维旋转&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/u013390476/article/details/50209603&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;C++记录时间&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/u010128736/article/details/52850444&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;相机内外参解释&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.cnblogs.com/gaochsh/p/6901809.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;shell echo字符串处理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.ceres-solver.org/installation.html#linux&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ubuntu 14.04 安装 ceres&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://wiki.ros.org/cn/indigo/Installation/Ubuntu&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ubuntu 14.04 安装 ROS indigo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/u010472607/article/details/76166008&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ubuntu 14.04 安装 Cmake 3.9.x&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/youngpan1101/article/details/58027049&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ubuntu 14.04 安装 Opencv 3.2.0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/a874909657/article/details/79161533&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;VMware Ubuntu 无法全屏解决方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/poem_qianmo/article/details/30974513&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;OpenCV重映射 &amp;amp; SURF特征点检测合辑&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/xiaotanyu13/article/details/8210955&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;VS2010控制台程序运行一闪而过的完美解决办法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/iracer/article/details/51339377&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;OpenCV数据持久化: &lt;strong&gt;FileStorage&lt;/strong&gt;类的数据存取操作与示例&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://answers.opencv.org/question/121651/fata-error-lapacke_h_path-notfound-when-building-opencv-32/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;fata error: LAPACKE_H_PATH-NOTFOUND when building OpenCV 3.2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/zyxlinux888/article/details/6358615&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;E:Could not get lock /var/lib/apt/lists/lock - open (11: Resource temporarily unavailable)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="资料" scheme="https://www.vincentqin.tech/categories/%E8%B5%84%E6%96%99/"/>
    
    
      <category term="bugs" scheme="https://www.vincentqin.tech/tags/bugs/"/>
    
  </entry>
  
</feed>
