<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RealCat</title>
  
  <subtitle>Turn on, Tune in, Drop out</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.vincentqin.tech/"/>
  <updated>2020-05-27T16:47:05.579Z</updated>
  <id>https://www.vincentqin.tech/</id>
  
  <author>
    <name>Vincent Qin</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【译】图解卡尔曼滤波</title>
    <link href="https://www.vincentqin.tech/posts/kalman-filter-in-pictures/"/>
    <id>https://www.vincentqin.tech/posts/kalman-filter-in-pictures/</id>
    <published>2020-05-24T11:41:50.000Z</published>
    <updated>2020-05-27T16:47:05.579Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>译者注</strong>：这恐怕是全网有关卡尔曼滤波最简单易懂的解释，如果你认真的读完本文，你将对卡尔曼滤波有一个更加清晰的认识，并且可以手推卡尔曼滤波。原文作者使用了漂亮的图片和颜色来阐明它的原理（读起来并不会因公式多而感到枯燥），所以请勇敢地读下去！</p><p>本人翻译水平有限，如有疑问，请阅读原文；如有错误，请在评论区指出。</p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="关于滤波"><a href="#关于滤波" class="headerlink" title="关于滤波"></a>关于滤波</h3><p>首先援引来自知乎大神的解释。</p><div class="note info">            <p>一位专业课的教授给我们上课的时候，曾谈到：filtering is weighting（滤波即加权）。滤波的作用就是给不同的信号分量不同的权重。最简单的loss pass filter， 就是直接把低频的信号给1权重，而给高频部分0权重。对于更复杂的滤波，比如维纳滤波, 则要根据信号的统计知识来设计权重。</p><p>从统计信号处理的角度，降噪可以看成滤波的一种。降噪的目的在于突出信号本身而抑制噪声影响。从这个角度，降噪就是给信号一个高的权重而给噪声一个低的权重。维纳滤波就是一个典型的降噪滤波器。</p>          </div><h3 id="关于卡尔曼滤波"><a href="#关于卡尔曼滤波" class="headerlink" title="关于卡尔曼滤波"></a>关于卡尔曼滤波</h3><p>Kalman Filter 算法，是一种递推预测滤波算法，算法中涉及到滤波，也涉及到对下一时刻数据的预测。Kalman Filter 由一系列递归数学公式描述。它提供了一种高效可计算的方法来估计过程的状态，并使估计均方误差最小。卡尔曼滤波器应用广泛且功能强大：它可以估计信号的过去和当前状态，甚至能估计将来的状态，即使并不知道模型的确切性质。</p><p>Kalman Filter 也可以被认为是一种数据融合算法（Data fusion algorithm），已有50多年的历史，是当今使用最重要和最常见的数据融合算法之一。Kalman Filter 的巨大成功归功于其小的计算需求，优雅的递归属性以及作为具有高斯误差统计的一维线性系统的最优估计器的状态<sup><a href="#fn_4" id="reffn_4">4</a></sup>。</p><p>Kalman Filter 只能减小均值为0的测量噪声带来的影响。只要噪声期望为0，那么不管方差多大，只要迭代次数足够多，那效果都很好。反之，噪声期望不为0，那么估计值就还是与实际值有偏差。</p><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>上面的描述可能把大家绕晕了，下面来点轻松的。</p><p>我们会有一个疑问：卡尔曼滤波到底是如何解决实际问题的呢？</p><p>我们以机器人为例介绍卡尔曼滤波的原理，我们的任务是要预测机器人的状态$\vec{x}$，包括位置$p$与速度$v$，即可表示为：</p><script type="math/tex; mode=display">\vec{x} = \begin{bmatrix}  p\\  v  \end{bmatrix}</script><p>某个时刻，我们不知道真实的位置与速度到底是多少，二者存在一个所有可能性的组合，大致如下图所示。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_0.png"></p><p><strong>卡尔曼滤波假设状态所有的变量都是随机的且都服从高斯分布，每个变量都有其对应的均值$\mu$以及方差$\sigma^2$（它代表了不确定性）</strong>。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_1.png"></p><p>在上图中，位置和速度是不相关（<strong>uncorrelated</strong>）的，这意味着某个变量的状态不会告诉你其他变量的状态是怎样的。即虽然我们知道现在的速度，但是无法从现在的速度推测出现在的位置。但实际上并非如此，我们知道速度和位置是有关系的（<strong>correlated</strong>），这样一来二者之间的组合关系变成了如下图所示的情况。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_3.png"></p><p>这种情况是很容易发生的，例如，如果速度很快，我们可能会走得更远，所以我们的位置会更大。如果我们走得很慢，我们就不会走得太远。</p><p>这种状态变量之间的关系很重要，因为它可以为我们提供<strong>更多信息</strong>：One measurement tells us something about what the others could be。这就是卡尔曼滤波器的目标，我们希望从不确定的测量中尽可能多地获取信息！</p><p>这种状态量的相关性可以由协方差矩阵表示。简而言之，矩阵$\Sigma_{ij}$的每个元素是第i个状态变量和第j个状态变量之间的相关度。（显然地可以知道协方差矩阵是对称的，这意味着您交换i和j都没关系）。协方差矩阵通常标记为“ $\Sigma$”，因此我们将它们的元素称为“$\Sigma_{ij}$”。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_2.png"></p><h2 id="状态预测"><a href="#状态预测" class="headerlink" title="状态预测"></a>状态预测</h2><h3 id="问题的矩阵形式表示"><a href="#问题的矩阵形式表示" class="headerlink" title="问题的矩阵形式表示"></a>问题的矩阵形式表示</h3><p>我们把状态建模成高斯分布（Gaussian blob，由于二维高斯分布长得像一个个小泡泡，之所以长这个样子，可参考链接<sup><a href="#fn_2" id="reffn_2">2</a></sup>）。我们需要求解/估计在时间$k$时刻的两个信息：1. 最优估计$\mathbf{\hat{x}_k}$以及它的协方差矩阵$\mathbf{P_k}$，我们可以写成下面矩阵形式：</p><script type="math/tex; mode=display">\begin{equation} \label{eq:statevars}  \begin{aligned}  \mathbf{\hat{x}}_k &= \begin{bmatrix}  \text{position}\\  \text{velocity}  \end{bmatrix}\\  \mathbf{P}_k &=  \begin{bmatrix}  \Sigma_{pp} & \Sigma_{pv} \\  \Sigma_{vp} & \Sigma_{vv} \\  \end{bmatrix}  \end{aligned}  \end{equation}</script><p>（当然，这里我们仅使用位置和速度，但是请记住状态可以包含任意数量的变量，并且可以表示所需的任何变量）</p><p>接下来，我们需要某种方式来查看当前状态（$k-1$时刻）并<strong>预测</strong>在时刻$k$处的状态。请记住，我们不知道哪个状态是“真实”状态，但是我们提到的<strong>预测</strong>（prediction）并不在乎这些。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_7.jpg"></p><p>我们可以用一个矩阵$\mathbf{F_k}$来表示这个预测过程：</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_8.jpg"></p><p>这个矩阵$\mathbf{F_k}$将原始估计中的每个点移动到新的预测位置。</p><p>那么问题来了，应该如何使用上述矩阵来预测下一时刻的位置和速度呢？为了阐述这个过程，我们使用了一个非常基础的运动学公式（初中物理中就学过）进行描述：</p><script type="math/tex; mode=display">\begin{split} \color{deeppink}{p_k} &= \color{royalblue}{p_{k-1}} + \Delta t &\color{royalblue}{v_{k-1}} \\ \color{deeppink}{v_k} &= &\color{royalblue}{v_{k-1}} \end{split}</script><p>写成矩阵形式：</p><script type="math/tex; mode=display">\begin{align} \color{deeppink}{\mathbf{\hat{x}}_k} &= \begin{bmatrix} 1 & \Delta t \\ 0 & 1 \end{bmatrix} \color{royalblue}{\mathbf{\hat{x}}_{k-1}} \\ &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} \label{statevars} \end{align}</script><p>现在我们有了一个<strong>预测矩阵</strong>或者叫做<strong>状态转移矩阵</strong>，该矩阵可以帮助我们计算下一个时刻的状态。但我们仍然不知道如何更新状态的协方差矩阵，其实过程也是很简单，如果我们将分布中的每个点乘以矩阵$A$，那么其协方差矩阵$\Sigma$会发生什么？</p><script type="math/tex; mode=display">\begin{equation} \begin{split} Cov(x) &= \Sigma\\ Cov(\color{firebrick}{\mathbf{A}}x) &= \color{firebrick}{\mathbf{A}} \Sigma \color{firebrick}{\mathbf{A}}^T \end{split} \label{covident} \end{equation}</script><p>将公式$\eqref{covident}$带入公式$\eqref{statevars}$，我们可以得到：</p><script type="math/tex; mode=display">\begin{equation} \begin{split} \color{deeppink}{\mathbf{\hat{x}}_k} &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} \\ \color{deeppink}{\mathbf{P}_k} &= \mathbf{F_k} \color{royalblue}{\mathbf{P}_{k-1}} \mathbf{F}_k^T \end{split} \end{equation}</script><h3 id="External-influence"><a href="#External-influence" class="headerlink" title="External influence"></a>External influence</h3><p>不过我们并没有考虑到所有的影响因素。可能有一些与状态本身无关的变化——如外界因素可能正在影响系统。</p><p>例如，我们用状态对列车的运动进行建模，如果列车长加大油门，火车就加速。同样，在我们的机器人示例中，导航系统软件可能会发出使车轮转动或停止的命令。如果我们很明确地知道这些因素，我们可以将其放在一起构成一个向量$\color{darkorange}{\vec{\mathbf{u}_k}}$，对这个量进行处理，然后将其添加到我们的预测中对状态进行更正。</p><p>假设我们知道由于油门设置或控制命令而产生的预期加速度$\color{darkorange}{a}$。根据基本运动学，我们得到下式：</p><script type="math/tex; mode=display">\begin{split} \color{deeppink}{p_k} &= \color{royalblue}{p_{k-1}} + {\Delta t} &\color{royalblue}{v_{k-1}} + &\frac{1}{2} \color{darkorange}{a} {\Delta t}^2 \\ \color{deeppink}{v_k} &= &\color{royalblue}{v_{k-1}} + & \color{darkorange}{a} {\Delta t} \end{split}</script><p>矩阵形式：</p><script type="math/tex; mode=display">\begin{equation} \begin{split} \color{deeppink}{\mathbf{\hat{x}}_k} &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} + \begin{bmatrix} \frac{\Delta t^2}{2} \\ \Delta t \end{bmatrix} \color{darkorange}{a} \\ &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} + \mathbf{B}_k \color{darkorange}{\vec{\mathbf{u}_k}} \end{split} \end{equation}</script><p>其中$\mathbf{B}_k$被称为<strong>控制矩阵</strong>，$\color{darkorange}{\vec{\mathbf{u}_k}}$被称为<strong>控制向量</strong>。（注意：对于没有外部影响的简单系统，可以忽略该控制项）。</p><p>如果我们的预测并不是100％准确模型，这会发生什么呢？</p><h3 id="External-uncertainty"><a href="#External-uncertainty" class="headerlink" title="External uncertainty"></a>External uncertainty</h3><p>如果状态仅仅依赖其自身的属性进行演进，那一切都很好。如果状态受到外部因素进行演进，我们只要知道那些外部因素是什么，那么一切仍然很好。</p><p>但在实际使用中，我们有时不知道的那些外部因素到底是如何被建模的。例如，我们要跟踪四轴飞行器，它可能会随风摇晃；如果我们跟踪的是轮式机器人，则车轮可能会打滑，或者因地面颠簸导致其减速。我们无法跟踪这些外部因素，如果发生任何这些情况，我们的预测可能会出错，因为我们并没有考虑这些因素。</p><p>通过在每个预测步骤之后添加一些新的不确定性，我们可以对与“世界”相关的不确定性进行建模（如我们无法跟踪的事物）：</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_9.jpg"></p><p>这样一来，由于新增的不确定性<strong>原始估计中的每个状态都可能迁移到多个状态</strong>。 因为我们非常喜欢用高斯分布进行建模，此时也不例外。我们可以说$\color{royalblue}{\mathbf{\hat{x}}_{k-1}}$的每个点都移动到具有协方差$\color{mediumaquamarine}{\mathbf{Q}_k}$的高斯分布内的某个位置，如下图所示：</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_10a.jpg"></p><p>这将产生一个新的高斯分布，其协方差不同（但均值相同）：</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_10b.jpg"></p><p>所以呢，我们在状态量的协方差中增加了额外的协方差$\color{mediumaquamarine}{\mathbf{Q}_k}$，所以预测阶段完整的状态转移方程为：</p><script type="math/tex; mode=display">\begin{equation}  \begin{split}  \color{deeppink}{\mathbf{\hat{x}}_k} &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} + \mathbf{B}_k \color{darkorange}{\vec{\mathbf{u}_k}} \\  \color{deeppink}{\mathbf{P}_k} &= \mathbf{F_k} \color{royalblue}{\mathbf{P}_{k-1}} \mathbf{F}_k^T + \color{mediumaquamarine}{\mathbf{Q}_k}  \end{split}  \label{kalpredictfull}  \end{equation}</script><p>换句话说：<strong><font color="deeppink">新的最佳估计</font></strong>是根据<strong><font color="royalblue">先前的最佳估计</font></strong>做出的<strong>预测</strong>，再加上对<strong><font color="darkorange">已知外部影响</font></strong>的校正。</p><p><strong><font color="deeppink">新的不确定度</font></strong>是根据<strong><font color="royalblue">先前的不确定度</font></strong>做出的<strong>预测</strong>，再加上<strong><font color="mediumaquamarine">来自环境额外的不确定度</font></strong>。</p><p>上述过程描绘了状态预测过程，那么当我们从传感器中获取一些测量数据时会发生什么呢？</p><h2 id="状态更新"><a href="#状态更新" class="headerlink" title="状态更新"></a>状态更新</h2><h3 id="利用测量进一步修正状态"><a href="#利用测量进一步修正状态" class="headerlink" title="利用测量进一步修正状态"></a>利用测量进一步修正状态</h3><p>假设我们有几个传感器，这些传感器可以向我们提供有关系统状态的信息。就目前而言，测量什么量都无关紧要，也许一个读取位置，另一个读取速度。每个传感器都告诉我们有关状态的一些间接信息（换句话说，传感器在状态下运作并产生一组测量读数）。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_12.jpg"></p><p>请注意，测量的单位可能与状态量的单位不同。我们使用矩阵$\mathbf{H}_k$对传感器的测量进行建模。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_13.jpg"></p><p>所以我们期望传感器的度数可以被建模成如下形式：</p><script type="math/tex; mode=display">\begin{equation}  \begin{aligned}  \vec{\mu}_{\text{expected}} &= \mathbf{H}_k \color{deeppink}{\mathbf{\hat{x}}_k} \\  \mathbf{\Sigma}_{\text{expected}} &= \mathbf{H}_k \color{deeppink}{\mathbf{P}_k} \mathbf{H}_k^T  \end{aligned}  \end{equation}</script><p>卡尔曼滤波器的伟大之处就在于它能够处理传感器噪声。换句话说，传感器本身的测量是不准确的，且原始估计中的每个状态都可能导致一定范围的传感器读数，而卡尔曼滤波能够在这些不确定性存在的情况下找到最优的状态。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_14.jpg"></p><p>根据传感器的读数，我们会猜测系统正处于某个特定状态。但是由于不确定性的存在，<strong>某些状态比其他状态更可能产生我们看到的读数</strong>：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_11.jpg"></p><p>我们将这种不确定性（如传感器噪声）的<strong>协方差</strong>表示为$\color{mediumaquamarine}{\mathbf{R}_k}$，读数的分布<strong>均值</strong>等于我们观察到传感器的读数，我们将其表示为$\color{yellowgreen}{\vec{\mathbf{z}_k}}$</p><p>这样一来，我们有了两个高斯分布：一个围绕通过状态转移预测的平均值，另一个围绕实际传感器读数。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_4.jpg"></p><p>因此，我们需要将基于预测状态（<strong><font color="deeppink">粉红色</font></strong>）的推测读数与基于实际观察到的传感器读数（<strong><font color="yellowgreen">绿色</font></strong>）进行融合。</p><p>那么融合后<strong>最有可能的新状态</strong>是什么？ 对于任何可能的读数$(z_1,z_2)$，我们都有两个相关的概率：（1）我们的传感器读数$\color{yellowgreen}{\vec{\mathbf{z}_k}}$是$(z_1,z_2)$的（误-）测量值的概率，以及（2）先前估计值的概率认为$(z_1,z_2)$是我们应该看到的读数。</p><p>如果我们有两个概率，并且想知道两个概率都为真的机会，则将它们相乘。因此，我们对两个高斯分布进行了相乘处理：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_6a.png"></p><p>两个概率分布相乘得到的就是上图中的重叠部分。而且重叠部分的概率分布会比我们之前的任何一个估计值/读数都精确得多，这个分布的均值就是两种估计最有可能配置（得到的状态）。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_6.png"></p><p>事实证明，<strong>两个独立的高斯分布相乘之后会得到一个新的具有其均值和协方差矩阵的高斯分布</strong>！下面开始推公式。</p><h3 id="合并两个高斯分布"><a href="#合并两个高斯分布" class="headerlink" title="合并两个高斯分布"></a>合并两个高斯分布</h3><p>首先考虑一维高斯情况：一个均值为$\mu$，方差为$\sigma^2$的高斯分布的形式为：</p><script type="math/tex; mode=display">\begin{equation} \label{gaussformula} \mathcal{N}(x, \mu,\sigma) = \frac{1}{ \sigma \sqrt{ 2\pi } } e^{ -\frac{ (x – \mu)^2 }{ 2\sigma^2 } } \end{equation}</script><p>我们想知道将两个高斯曲线相乘会发生什么。下图中的蓝色曲线表示两个高斯总体的（未归一化）交集：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_joint.png"></p><script type="math/tex; mode=display">\begin{equation} \label{gaussequiv} \mathcal{N}(x, \color{fuchsia}{\mu_0}, \color{deeppink}{\sigma_0}) \cdot \mathcal{N}(x, \color{yellowgreen}{\mu_1}, \color{mediumaquamarine}{\sigma_1}) \stackrel{?}{=} \mathcal{N}(x, \color{royalblue}{\mu’}, \color{mediumblue}{\sigma’}) \end{equation}</script><p>将公式$\eqref{gaussformula}$代入公式$\eqref{gaussequiv}$，我们可以得到新的高斯分布的均值和方差如下所示：</p><script type="math/tex; mode=display">\begin{equation} \label{fusionformula} \begin{aligned} \color{royalblue}{\mu’} &= \mu_0 + \frac{\sigma_0^2 (\mu_1 – \mu_0)} {\sigma_0^2 + \sigma_1^2}\\ \color{mediumblue}{\sigma’}^2 &= \sigma_0^2 – \frac{\sigma_0^4} {\sigma_0^2 + \sigma_1^2} \end{aligned} \end{equation}</script><p>我们将其中的一小部分重写为$\color{purple}{\mathbf{k}}$：</p><script type="math/tex; mode=display">\begin{equation} \label{gainformula} \color{purple}{\mathbf{k}} = \frac{\sigma_0^2}{\sigma_0^2 + \sigma_1^2} \end{equation}</script><script type="math/tex; mode=display">\begin{equation} \begin{split} \color{royalblue}{\mu’} &= \mu_0 + &\color{purple}{\mathbf{k}} (\mu_1 – \mu_0)\\ \color{mediumblue}{\sigma’}^2 &= \sigma_0^2 – &\color{purple}{\mathbf{k}} \sigma_0^2 \end{split} \label{update} \end{equation}</script><p>这样一来，公式的形式就简单多了！我们顺势将公式$\eqref{gainformula}$和$\eqref{update}$的矩阵形式写在下面：</p><script type="math/tex; mode=display">\begin{equation} \label{matrixgain} \color{purple}{\mathbf{K}} = \Sigma_0 (\Sigma_0 + \Sigma_1)^{-1} \end{equation}</script><script type="math/tex; mode=display">\begin{equation} \begin{split} \color{royalblue}{\vec{\mu}’} &= \vec{\mu_0} + &\color{purple}{\mathbf{K}} (\vec{\mu_1} – \vec{\mu_0})\\ \color{mediumblue}{\Sigma’} &= \Sigma_0 – &\color{purple}{\mathbf{K}} \Sigma_0 \end{split} \label{matrixupdate} \end{equation}</script><p>其中$\Sigma$表示新高斯分布的协方差矩阵，$\vec{\mu}$是每个维度的均值，$\color{purple}{\mathbf{K}}$就是大名鼎鼎的“<strong>卡尔曼增益</strong>”（<strong>Kalman gain</strong>）。</p><h3 id="公式汇总"><a href="#公式汇总" class="headerlink" title="公式汇总"></a>公式汇总</h3><p>我们有两个高斯分布，一个是我们预测的观测$(\color{fuchsia}{\mu_0}, \color{deeppink}{\Sigma_0}) = (\color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k}, \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T})$，另外一个是实际的观测(传感器读数)$(\color{yellowgreen}{\mu_1}, \color{mediumaquamarine}{\Sigma_1}) = (\color{yellowgreen}{\vec{\mathbf{z}_k}}, \color{mediumaquamarine}{\mathbf{R}_k})$，我们将这两个高斯分布带入公式$\eqref{matrixupdate}$中就可以得到二者的重叠区域：</p><script type="math/tex; mode=display">\begin{equation} \begin{aligned} \mathbf{H}_k \color{royalblue}{\mathbf{\hat{x}}_k’} &= \color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k} & + & \color{purple}{\mathbf{K}} ( \color{yellowgreen}{\vec{\mathbf{z}_k}} – \color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k} ) \\ \mathbf{H}_k \color{royalblue}{\mathbf{P}_k’} \mathbf{H}_k^T &= \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} & – & \color{purple}{\mathbf{K}} \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} \end{aligned} \label {kalunsimplified} \end{equation}</script><p>从公式$\eqref{matrixgain}$我们可以知道，卡尔曼增益是：</p><script type="math/tex; mode=display">\begin{equation} \label{eq:kalgainunsimplified} \color{purple}{\mathbf{K}} = \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} ( \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} + \color{mediumaquamarine}{\mathbf{R}_k})^{-1} \end{equation}</script><p>然后我们将公式$\eqref{kalunsimplified}$与公式$\eqref{eq:kalgainunsimplified}$中的$\mathbf{H}_k$去除，同时将$\color{royalblue}{\mathbf{P}_k’}$后面的$\mathbf{H}_k^T$去除，我们可以得到最终的化简形式的更新方程：</p><script type="math/tex; mode=display">\begin{equation} \begin{split} \color{royalblue}{\mathbf{\hat{x}}_k’} &= \color{fuchsia}{\mathbf{\hat{x}}_k} & + & \color{purple}{\mathbf{K}’} ( \color{yellowgreen}{\vec{\mathbf{z}_k}} – \color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k} ) \\ \color{royalblue}{\mathbf{P}_k’} &= \color{deeppink}{\mathbf{P}_k} & – & \color{purple}{\mathbf{K}’} \color{deeppink}{\mathbf{H}_k \mathbf{P}_k} \end{split} \label{kalupdatefull} \end{equation}</script><script type="math/tex; mode=display">\begin{equation} \color{purple}{\mathbf{K}’} = \color{deeppink}{\mathbf{P}_k \mathbf{H}_k^T} ( \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} + \color{mediumaquamarine}{\mathbf{R}_k})^{-1} \label{kalgainfull} \end{equation}</script><h2 id="图说"><a href="#图说" class="headerlink" title="图说"></a>图说</h2><p>大功告成，$\color{royalblue}{\mathbf{\hat{x}}_k’}$就是更新后的最优状态！接下来我们可以继续进行预测，然后更新，重复上述过程！下图给出卡尔曼滤波信息流。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/kalflow.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在上述所有数学公式中，你需要实现的只是公式$\eqref{kalpredictfull}, \eqref{kalupdatefull}$和$\eqref{kalgainfull}$。（或者，如果你忘记了这些，可以从等式$\eqref{covident}$和$\eqref{matrixupdate}$重新推导所有内容。）</p><p>这将使你能够准确地对任何线性系统建模。对于非线性系统，我们使用<strong>扩展卡尔曼滤波器</strong>，该滤波器通过简单地线性化预测和测量值的均值进行工作。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><blockquote id="fn_1"><sup>1</sup>. <a href="http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/#mathybits" target="_blank" rel="noopener">How a Kalman filter works, in pictures, 图解卡尔曼滤波是如何工作的</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. <a href="https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/" target="_blank" rel="noopener">A geometric interpretation of the covariance matrix, 协方差矩阵的几何解释</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. <a href="https://sikasjc.github.io/2018/05/08/kalman_filter" target="_blank" rel="noopener">Kalman Filter 卡尔曼滤波</a><a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. R. Faragher, “Understanding the Basis of the Kalman Filter Via a Simple and Intuitive Derivation [Lecture Notes]”, IEEE Signal Processing Magazine, vol. 29, no. 5, pp. 128–132, Sep. 2012.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. G. Welch and G. Bishop, “<a href="http://www.cs.unc.edu/~welch/media/pdf/kalman_intro.pdf" target="_blank" rel="noopener">An Introduction to the Kalman Filter</a>”, p. 16, 2006.<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. Fitzgerald, Robert J. “Divergence of the Kalman filter”, Automatic Control IEEE Transactions on 16.6(1971):736-747.<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;译者注&lt;/strong&gt;：这恐怕是全网有关卡尔曼滤波最简单易懂的解释，如果你认真的读完本文，你将对卡尔曼滤波有一个更加清晰的认识，并且可以手推卡尔曼滤波。原文作者使用了漂亮的图片和颜色来阐明它的原理（读起来并不会因公式多而感到枯燥），所以请勇敢地读下去！&lt;/p&gt;
&lt;p&gt;本人翻译水平有限，如有疑问，请阅读原文；如有错误，请在评论区指出。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>国内加速访问arxiv</title>
    <link href="https://www.vincentqin.tech/posts/redirect-arxiv/"/>
    <id>https://www.vincentqin.tech/posts/redirect-arxiv/</id>
    <published>2020-05-20T14:44:56.000Z</published>
    <updated>2020-05-22T16:01:08.675Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>国内访问论文预发布平台<code>arxiv</code>巨慢无比，让人闹心！网上找了一个很好用的方法，按照这个方法配置之后<code>arxiv</code>就秒开了。原理就是将<code>arxiv</code>重定向到<code>xxx.itp.ac.cn</code>（中科院理论物理研究所镜像）。<br>如果此时你找到了一篇文章，地址是<code>arxiv.org/abs/1911.11763</code>，只需要把<code>arxiv.org</code>换成<code>xxx.itp.ac.cn</code>即可。但每次都手动配置就很麻烦，为了贯彻将懒惰进行到底的精神，我们需要将上述过程自动化。配置如下：</p><a id="more"></a><ol><li><p>安装<a href="https://chrome.google.com/webstore/detail/tampermonkey/dhdgffkkebhmkfjojejmpbldmpobfkfo?hl=en" target="_blank" rel="noopener">Tampermonkey</a>油猴插件，自行google。</p></li><li><p>添加如下脚本</p></li></ol><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// ==UserScript==</span></span><br><span class="line"><span class="comment">// @name        redirect_arxiv</span></span><br><span class="line"><span class="comment">// @namespace   redirect_arxiv</span></span><br><span class="line"><span class="comment">// @include     *</span></span><br><span class="line"><span class="comment">// @include     https://*github.io*</span></span><br><span class="line"><span class="comment">// @include     https://*arxiv.org/*</span></span><br><span class="line"><span class="comment">// @include     https://*google.c*</span></span><br><span class="line"><span class="comment">// @include     https://*semanticscholar.org/*</span></span><br><span class="line"><span class="comment">// @include     https://*github.com*</span></span><br><span class="line"><span class="comment">// @include     https://*zhihu.com*</span></span><br><span class="line"><span class="comment">// @include     https://*brainpp.cn*</span></span><br><span class="line"><span class="comment">// @include     https://*outlook.cn*</span></span><br><span class="line"><span class="comment">// @version     1.0</span></span><br><span class="line"><span class="comment">// @grant       none</span></span><br><span class="line"><span class="comment">// ==/UserScript==</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 重定向 arxiv.org 到 xxx.itp.ac.cn（中科院理论物理研究所镜像）</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">findFatherNode</span>(<span class="params">node, nodeName=<span class="string">'A'</span>, maxDeep=<span class="number">1000</span></span>)</span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; maxDeep; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (! node)&#123;<span class="keyword">return</span> node&#125;</span><br><span class="line">        <span class="keyword">if</span> (node.nodeName == nodeName)&#123;</span><br><span class="line">            <span class="keyword">return</span> node</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            node = node.parentElement</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">document</span>.body.addEventListener(<span class="string">'mousedown'</span>, <span class="function"><span class="keyword">function</span>(<span class="params">e</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">var</span> targ = e.target || e.srcElement;</span><br><span class="line">    <span class="keyword">var</span> aTag = findFatherNode(targ, <span class="string">'A'</span>, <span class="number">10</span>);</span><br><span class="line">    <span class="keyword">if</span> (!aTag || !(aTag.href))&#123;<span class="keyword">return</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> headN = <span class="number">17</span>;</span><br><span class="line">    <span class="keyword">var</span> hrefHead = aTag.href.slice(<span class="number">0</span>, headN);</span><br><span class="line">    <span class="keyword">var</span> hrefTail = aTag.href.slice(headN);</span><br><span class="line">    <span class="keyword">if</span> ( (hrefHead.indexOf(<span class="string">'arxiv.org'</span>)==<span class="number">-1</span>))&#123;<span class="keyword">return</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ( hrefHead.match(<span class="regexp">/https?:\/\/arxiv\.org/</span>) ) &#123;</span><br><span class="line">        hrefHead = hrefHead.replace(<span class="regexp">/https?:\/\/arxiv\.org/</span>, <span class="string">'http://xxx.itp.ac.cn'</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    aTag.href = hrefHead + hrefTail</span><br><span class="line">    <span class="comment">// console.log(targ, targ.href);</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><ul><li><a href="https://www.zhihu.com/question/58912862/answer/695125360" target="_blank" rel="noopener">参考来自小磊知乎的回答</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;国内访问论文预发布平台&lt;code&gt;arxiv&lt;/code&gt;巨慢无比，让人闹心！网上找了一个很好用的方法，按照这个方法配置之后&lt;code&gt;arxiv&lt;/code&gt;就秒开了。原理就是将&lt;code&gt;arxiv&lt;/code&gt;重定向到&lt;code&gt;xxx.itp.ac.cn&lt;/code&gt;（中科院理论物理研究所镜像）。&lt;br&gt;如果此时你找到了一篇文章，地址是&lt;code&gt;arxiv.org/abs/1911.11763&lt;/code&gt;，只需要把&lt;code&gt;arxiv.org&lt;/code&gt;换成&lt;code&gt;xxx.itp.ac.cn&lt;/code&gt;即可。但每次都手动配置就很麻烦，为了贯彻将懒惰进行到底的精神，我们需要将上述过程自动化。配置如下：&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="arxiv" scheme="https://www.vincentqin.tech/tags/arxiv/"/>
    
      <category term="redirect" scheme="https://www.vincentqin.tech/tags/redirect/"/>
    
  </entry>
  
  <entry>
    <title>CVPR2020图像匹配挑战赛，新数据集+新评测方法，SOTA正瑟瑟发抖！</title>
    <link href="https://www.vincentqin.tech/posts/2020-image-matching-cvpr/"/>
    <id>https://www.vincentqin.tech/posts/2020-image-matching-cvpr/</id>
    <published>2020-05-17T04:23:09.000Z</published>
    <updated>2020-05-24T15:00:05.764Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>从一系列的图像中恢复物体的3D结构是计算机视觉研究中一个热门课题，这使得我们可以相隔万里从google map中看到复活节岛的风景。这得益于图像来自于可控的条件，使得最终的重建效果一致性且质量都很高，但是这却限制了采集设备以及视角。畅想一下，假如我们不使用专业设备，而是利用sfm技术根据互联网上大量的图片重建出这个复杂世界。</p><!-- ![A 3D reconstruction generated from over 3000 images, including those from the previous figure](https://1.bp.blogspot.com/-loSqCB3NnM0/XoTiOGP9SYI/AAAAAAAAFlE/rs8iCTq63FYapA7HbljF8iWa7fyHvh3UgCLcBGAsYHQ/s400/image3.gif) --><!-- ![](https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/image_sfm.gif) --><p><img alt data-src="https://vincentqin.gitee.io/posts/2020-image-matching-cvpr/image_sfm.gif"></p><p>为了加快这个领域的研究，更好地利用图像数据有效信息，谷歌联合 <a href="https://www.uvic.ca/" target="_blank" rel="noopener">UVIC</a>, <a href="https://www.cvut.cz/en" target="_blank" rel="noopener">CTU</a>以及EPFL发表了这篇文章 “<a href="https://arxiv.org/abs/2003.01587" target="_blank" rel="noopener">Image Matching across Wide Baselines: From Paper to Practice</a>”，[<strong><a href="http://xxx.itp.ac.cn/pdf/2003.01587v2" target="_blank" rel="noopener">PDF</a></strong>]，旨在公布一种新的衡量用于3D重建方法的标准模块+数据集，这里主要是指2D图像间的匹配。这个评价模块可以很方便地集成并评估现有流行的特征匹配算法，包括传统方法或者基于机器学习的方法。</p><p>谷歌公布2020图像匹配挑战的数据集：<a href="https://image-matching-workshop.github.io/" target="_blank" rel="noopener">官网</a>，<a href="http://ai.googleblog.com/2020/04/announcing-2020-image-matching.html" target="_blank" rel="noopener">博客</a>，文末有排行榜。</p><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>图像特征匹配是计算机视觉的基础+核心问题之一，包括image retrieval <sup><a href="#fn_48" id="reffn_48">48</a></sup> <sup><a href="#fn_7" id="reffn_7">7</a></sup> <sup><a href="#fn_69" id="reffn_69">69</a></sup> <sup><a href="#fn_91" id="reffn_91">91</a></sup> <sup><a href="#fn_63" id="reffn_63">63</a></sup>, 3D reconstruction<sup><a href="#fn_3" id="reffn_3">3</a></sup> <sup><a href="#fn_43" id="reffn_43">43</a></sup> <sup><a href="#fn_79" id="reffn_79">79</a></sup> <sup><a href="#fn_106" id="reffn_106">106</a></sup>，re-localization <sup><a href="#fn_74" id="reffn_74">74</a></sup> <sup><a href="#fn_75" id="reffn_75">75</a></sup> <sup><a href="#fn_51" id="reffn_51">51</a></sup>以及 SLAM <sup><a href="#fn_61" id="reffn_61">61</a></sup> <sup><a href="#fn_30" id="reffn_30">30</a></sup> <sup><a href="#fn_31" id="reffn_31">31</a></sup>等在内的诸多研究领域都会用到特征匹配。这个问题已经研究了几十年，但仍未被很好地解决。特征匹配面临的问题很多，主要包括以下挑战：视角，尺度，旋转，光照，遮挡以及相机渲染等。</p><p>近些年来，研究者开始将视线转移到端到端的学习方法（图像-&gt;位姿），但是这些方法甚至没有达到传统的方法（图像-&gt;匹配-&gt;BA优化）的性能。我们可以看到，传统的方法将3D重建问题拆分成为2个子问题：特征匹配与位姿解算。解决每个子问题的新方法，诸如特征匹配/位姿解算，都使用了“临时指标”，但是单独地评价单个子问题的性能不足以说明整体性能。例如，一些研究仅在某个数据集上展现了相较于手工特征SIFT的优势，但是这些算法是否能够在真实应用中仍然展现出优势呢？我们通过后续实验说明传统算法经过调整之后也可匹敌现有的标称“sota”的算法（着实打脸）。</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig1.png"></p><p>是时候换一种方式进行评价了，本文不去过多关注在临时指标上的表现，而关注在下游任务上的表现。本文贡献：</p><ol><li>30k图像+深度图+真实位姿（posed image）</li><li>模块化流水线处理流程，结合了数十种经典的和最新的特征提取和匹配以及姿态估计方法，以及多种启发式方法，可以分别交换和调整</li><li>两个下游任务，双目/多视角重建</li><li>全面研究了手工特征以及学习特征数十种方法和技术，以及它们的结合以及超参数选择的过程</li></ol><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="局部特征"><a href="#局部特征" class="headerlink" title="局部特征"></a>局部特征</h3><p>在引入SIFT特征之后，局部特征变成了主流。它的处理流程主要分为几个步骤：特征提取，旋转估计，描述子提取。除了SIFT，手工特征还有SURF <sup><a href="#fn_15" id="reffn_15">15</a></sup>, ORB <sup><a href="#fn_73" id="reffn_73">73</a></sup>, 以及 AKAZE <sup><a href="#fn_4" id="reffn_4">4</a></sup>等。</p><p>现代描述子通常在SIFT关键点（即DoG）的预裁剪图像块上训练深度网络，其中包括：Deepdesc <sup><a href="#fn_82" id="reffn_82">82</a></sup>, TFeat <sup><a href="#fn_11" id="reffn_11">11</a></sup>, L2-Net <sup><a href="#fn_89" id="reffn_89">89</a></sup>, Hardnet <sup><a href="#fn_57" id="reffn_57">57</a></sup>, SOSNet [90]以及 LogPolarDesc <sup><a href="#fn_34" id="reffn_34">34</a></sup>（它们中绝大多数都是在同一个数据集上进行的训练）。</p><p>最近有一些工作利用了其它线索，诸如几何或全局上下文信息进行训练，其中包括GeoDesc [50] and ContextDesc <sup><a href="#fn_49" id="reffn_49">49</a></sup>。</p><p>另外还有一些方法将特征点以及描述子进行单独训练，例如TILDE <sup><a href="#fn_95" id="reffn_95">95</a></sup>, TCDet <sup><a href="#fn_103" id="reffn_103">103</a></sup>, QuadNet <sup><a href="#fn_78" id="reffn_78">78</a></sup>, and Key.Net <sup><a href="#fn_13" id="reffn_13">13</a></sup>。当前还有一些算法将二者联合起来训练，例如LIFT <sup><a href="#fn_99" id="reffn_99">99</a></sup>,DELF <sup><a href="#fn_63" id="reffn_63">63</a></sup>, SuperPoint <sup><a href="#fn_31" id="reffn_31">31</a></sup>, LF-Net <sup><a href="#fn_64" id="reffn_64">64</a></sup>, D2-Net <sup><a href="#fn_33" id="reffn_33">33</a></sup>,R2D2 <sup><a href="#fn_72" id="reffn_72">72</a></sup>。</p><h3 id="鲁棒匹配"><a href="#鲁棒匹配" class="headerlink" title="鲁棒匹配"></a>鲁棒匹配</h3><p>大基线的双目匹配的外点内点率可低至10%，甚至更低。要做匹配的话需要从中选择出能够解算出位姿的算法。常用的方式包括基于随机一致采样RANSAC的5-<sup><a href="#fn_62" id="reffn_62">62</a></sup>，7-<sup><a href="#fn_41" id="reffn_41">41</a></sup>，8-point<sup><a href="#fn_39" id="reffn_39">39</a></sup>算法。它的改进算法包括local optimization <sup><a href="#fn_24" id="reffn_24">24</a></sup>, MLESAC <sup><a href="#fn_92" id="reffn_92">92</a></sup>, PROSAC <sup><a href="#fn_23" id="reffn_23">23</a></sup>, DEGENSAC <sup><a href="#fn_26" id="reffn_26">26</a></sup>, GC-RANSAC <sup><a href="#fn_12" id="reffn_12">12</a></sup>,  MAGSAC <sup><a href="#fn_29" id="reffn_29">29</a></sup>，CNe (Context Networks) <sup><a href="#fn_100" id="reffn_100">100</a></sup>+RANSAC，同样还有<sup><a href="#fn_70" id="reffn_70">70</a></sup> <sup><a href="#fn_104" id="reffn_104">104</a></sup> <sup><a href="#fn_85" id="reffn_85">85</a></sup> <sup><a href="#fn_102" id="reffn_102">102</a></sup>。作者最后加了一句“Despite their promise, it remains unclear how well they perform in real settings”（质疑中，哈哈）。</p><h3 id="运动恢复结构（SfM）"><a href="#运动恢复结构（SfM）" class="headerlink" title="运动恢复结构（SfM）"></a>运动恢复结构（SfM）</h3><p>方法 <sup><a href="#fn_3" id="reffn_3">3</a></sup> <sup><a href="#fn_43" id="reffn_43">43</a></sup> <sup><a href="#fn_27" id="reffn_27">27</a></sup> <sup><a href="#fn_37" id="reffn_37">37</a></sup> <sup><a href="#fn_106" id="reffn_106">106</a></sup>，最流行的包括VisualSFM <sup><a href="#fn_98" id="reffn_98">98</a></sup>以及COLMAP <sup><a href="#fn_79" id="reffn_79">79</a></sup>（作为真值）。</p><h3 id="数据集和标准"><a href="#数据集和标准" class="headerlink" title="数据集和标准"></a>数据集和标准</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig2.jpg"></p><p>以前的特征匹配数据集如下：</p><ul><li>Oxford dataset <sup><a href="#fn_54" id="reffn_54">54</a></sup>, 48张图像+真值单应矩阵</li><li>HPatches <sup><a href="#fn_9" id="reffn_9">9</a></sup>, 696张光照以及视角变化，无遮挡平面图像</li><li>DTU <sup><a href="#fn_1" id="reffn_1">1</a></sup>, Edge Foci <sup><a href="#fn_107" id="reffn_107">107</a></sup>, Webcam <sup><a href="#fn_95" id="reffn_95">95</a></sup>, AMOS <sup><a href="#fn_67" id="reffn_67">67</a></sup>, 以及 Strecha’s <sup><a href="#fn_83" id="reffn_83">83</a></sup></li></ul><p>上述数据集都有其限制：窄基线，真值噪声大，图像数量少。基于学习的描述子通常在<sup><a href="#fn_21" id="reffn_21">21</a></sup>上进行训练，它们之所以比SIFT好的原因可能在于过拟合了（作者看到会不会脸红）。<br>另外，用于导航/重定位以及slam的数据集包括Kitti <sup><a href="#fn_38" id="reffn_38">38</a></sup>, Aachen <sup><a href="#fn_76" id="reffn_76">76</a></sup>, Robotcar <sup><a href="#fn_52" id="reffn_52">52</a></sup>以及CMU seasons <sup><a href="#fn_75" id="reffn_75">75</a></sup> <sup><a href="#fn_8" id="reffn_8">8</a></sup>，但并不包含Phototourism数据中的多种变换。</p><h2 id="Phototourism-数据集"><a href="#Phototourism-数据集" class="headerlink" title="Phototourism 数据集"></a>Phototourism 数据集</h2><p>上述数据集这么“烂”，于是作者搞出了他们心目中最好的公开数据集——Phototourism 数据集。作者从<sup><a href="#fn_43" id="reffn_43">43</a></sup> <sup><a href="#fn_88" id="reffn_88">88</a></sup>中选择的25个受欢迎的地标集合（共30k）为基础，每个地标都有成百上千的图像。论文中，作者从中选择出11个场景，其中9个测试集和2个验证集做实验。将它们缩减为最大尺寸为1024像素，并使用COLMAP <sup><a href="#fn_79" id="reffn_79">79</a></sup>对其进行求解位姿以及点云和深度，通过建立好的模型去除遮挡物。</p><p>具体地，如下2个表格所示：</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab1.png"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab2.png"></p><h2 id="处理流程图Pipeline"><a href="#处理流程图Pipeline" class="headerlink" title="处理流程图Pipeline"></a>处理流程图Pipeline</h2><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig7.png"></p><p>流程如上图，蓝色框就是要进行的几个处理，分别介绍一下。</p><h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>作者选择了3大类特征：</p><ol><li>完全手工特征:<br>SIFT <sup><a href="#fn_48" id="reffn_48">48</a></sup> (以及RootSIFT <sup><a href="#fn_6" id="reffn_6">6</a></sup>), SURF <sup><a href="#fn_15" id="reffn_15">15</a></sup>, ORB <sup><a href="#fn_73" id="reffn_73">73</a></sup>, AKAZE <sup><a href="#fn_4" id="reffn_4">4</a></sup>，FREAK <sup><a href="#fn_107" id="reffn_107">107</a></sup>描述子+BRISK <sup><a href="#fn_108" id="reffn_108">108</a></sup>特征点，使用OpenCV的实现，除了ORB特征，降低特征提取阈值以多提取一些特征；<br>除此之外，也考虑VLFeat<sup><a href="#fn_94" id="reffn_94">94</a></sup>中DoG的一些变种：(VL-)DoG, Hessian <sup><a href="#fn_16" id="reffn_16">16</a></sup>, Hessian-Laplace <sup><a href="#fn_55" id="reffn_55">55</a></sup>, Harris-Laplace <sup><a href="#fn_55" id="reffn_55">55</a></sup>, MSER <sup><a href="#fn_53" id="reffn_53">53</a></sup>; 以及它们的仿射变种: DoG-Affine, Hessian-Affine <sup><a href="#fn_55" id="reffn_55">55</a></sup> <sup><a href="#fn_14" id="reffn_14">14</a></sup>, DoG-AffNet <sup><a href="#fn_59" id="reffn_59">59</a></sup>, Hessian-AffNet <sup><a href="#fn_59" id="reffn_59">59</a></sup></li><li>描述子从DoG特征学习得到的特征：<br>L2-Net <sup><a href="#fn_89" id="reffn_89">89</a></sup>, Hardnet <sup><a href="#fn_57" id="reffn_57">57</a></sup>,Geodesc <sup><a href="#fn_50" id="reffn_50">50</a></sup>, SOSNet <sup><a href="#fn_90" id="reffn_90">90</a></sup>, ContextDesc <sup><a href="#fn_49" id="reffn_49">49</a></sup>, LogPolarDesc <sup><a href="#fn_34" id="reffn_34">34</a></sup></li><li>端到端学习来的特征：<br>Superpoint <sup><a href="#fn_31" id="reffn_31">31</a></sup>, LF-Net <sup><a href="#fn_64" id="reffn_64">64</a></sup>, and D2-Net <sup><a href="#fn_33" id="reffn_33">33</a></sup>以及它们的多尺度变种：single- (SS) 以及 multi-scale (MS)</li></ol><h3 id="特征匹配"><a href="#特征匹配" class="headerlink" title="特征匹配"></a>特征匹配</h3><p>此处用的是最近邻。</p><h3 id="外点滤除"><a href="#外点滤除" class="headerlink" title="外点滤除"></a>外点滤除</h3><p>Context Networks <sup><a href="#fn_100" id="reffn_100">100</a></sup>+RANSAC<sup><a href="#fn_100" id="reffn_100">100</a></sup> <sup><a href="#fn_85" id="reffn_85">85</a></sup>，简称CNe，效果如下：</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig3.png"></p><h3 id="Stereo-task"><a href="#Stereo-task" class="headerlink" title="Stereo task"></a>Stereo task</h3><p>给定图像$\mathbf{I}_i$以及$\mathbf{I}_j$，解算基础矩阵 $\mathbf{F}_{i,j}$，除了现有的OpenCV<sup><a href="#fn_19" id="reffn_19">19</a></sup>以及sklearn<sup><a href="#fn_65" id="reffn_65">65</a></sup>中实现的RANSAC <sup><a href="#fn_36" id="reffn_36">36</a></sup> <sup><a href="#fn_25" id="reffn_25">25</a></sup>，作者也用到了DEGENSAC <sup><a href="#fn_26" id="reffn_26">26</a></sup>, GC-RANSAC <sup><a href="#fn_12" id="reffn_12">12</a></sup> and MAGSAC <sup><a href="#fn_29" id="reffn_29">29</a></sup>。最后通过OpenCV的<code>recoverPose</code>函数解算位姿。</p><h3 id="Multi-view-task"><a href="#Multi-view-task" class="headerlink" title="Multi-view task"></a>Multi-view task</h3><p>由于是评价<strong>特征的好坏</strong>而不是SfM算法，作者从几个大场景中<strong>随机选择</strong>出图片构成几个小的数据集，称为”bags”。其中包含3/5张图像的各有100bags，10张图像的各有50bags，25张图像的各有25bags，总共275个bags。将外点滤除后的结果送入COLMAP <sup><a href="#fn_79" id="reffn_79">79</a></sup>作为输入进行SfM重建。</p><h3 id="误差指标"><a href="#误差指标" class="headerlink" title="误差指标"></a>误差指标</h3><ol><li>mAA(mean Average Accuracy): Stereo task/Multi-view task</li><li>ATE(Absolute Trajectory Error): Multi-view task</li></ol><h2 id="实验开始——配置细节很重要"><a href="#实验开始——配置细节很重要" class="headerlink" title="实验开始——配置细节很重要"></a>实验开始——配置细节很重要</h2><p>首先比较了RANSAC在不同参数配置（置信度，极线对齐误差阈值以及最大迭代次数）下的表现：<br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig8.png"><br>总体来说，MAGSAC表现最好，DEGENSAC表现次之。另外，作者提到“default settings can be woefully inadequate. For example, OpenCV sets τ = 0.99 and η = 3 pixels, which results in a mAP at 10o of 0.5292 on the validation set – a performance drop of 23.9% relative.” 所以在日常使用OpenCV的RANSAC函数时需要自己调整下参数。</p><p>作者认为RANSAC的内点阈值对于每种局部特征也是不同的，作者做了如下实验。<br><img alt="Figure 5. RANSAC – Inlier threshold $\eta$" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig9.png"><br>上图可以直观看到从DOG学习的特征都聚集在了一起，其它特征比较分散，这也是太难选择了，于是作者使用了其他论文作者推荐的配置参数或者一些合理的参数作为内点阈值。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>作者列出了很多结果以及结论，我们仅去关注几个感兴趣的。</p><h3 id="8k特征"><a href="#8k特征" class="headerlink" title="8k特征"></a>8k特征</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab5.png"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab6.png"></p><p>大家期待已久的真的sota到底是谁呢？作者在以上特征的超参调整到最优后进行了实验，测试结果如下：</p><ol><li>mAA指标上DoG特征点占据了Top的位置，其中SOSNet排名#1，紧随其后的是HardNet。</li><li>‘HardNetAmos+’ <sup><a href="#fn_56" id="reffn_56">56</a></sup>,它在更多的数据(Brown <sup><a href="#fn_20" id="reffn_20">20</a></sup>, HPatches <sup><a href="#fn_9" id="reffn_9">9</a></sup>, AMOS <sup><a href="#fn_67" id="reffn_67">67</a></sup>)上进行了训练，但是效果却比不上在Brown的‘Liberty’上训练模型的效果。</li><li>multi-view任务中，DoG+HardNet表现属于top水平，略优于ContextDesc, SOSNet，LogpolarDesc；</li><li>R2D2是表现最好的端到端方法，同样在multi-view任务中表现较好（#6），但是在stereo任务中不如SIFT；</li><li>D2-net表现并不太好，可能由于图像下采样造成了较差的定位误差；</li><li>适当调整参数后的SIFT尤其是RootSIFT能够在stereo任务中排名#9，multi-view任务中排名#9，与所谓sota相差13.1%以及4.9%.（真为咱传统特征争气！）</li></ol><h3 id="2k特征"><a href="#2k特征" class="headerlink" title="2k特征"></a>2k特征</h3><p>这样做的理由是能够与LF-Net与Superpoint进行比较，结果如下图：</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab7.png"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab8.png"></p><p>结论：</p><ol><li>Key.Net+HardNet获得最好的表现，第二名是LogPolarDesc；</li><li>R2D2在stereo任务中排名#2，multi-view任务中排名#7</li></ol><h3 id="8k-vs-2k"><a href="#8k-vs-2k" class="headerlink" title="8k vs. 2k"></a>8k vs. 2k</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig16.png"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig17.png"></p><p>结论：</p><ol><li><strong>基于DoG的方法容易受益于多个特征，而学习的方法收益于重新训练</strong>（该结论来自于Key.Net+Hardnet的组合，作者进行了重新训练，表现优异）</li><li>整体来说基于学习的特征KeyNet, SuperPoint, R2D2, LF-Net在multi-view任务配置下比stereo任务配置下表现更好；(作者的假设是它们的鲁棒性好，但定位精度低)</li></ol><h3 id="光照变化"><a href="#光照变化" class="headerlink" title="光照变化"></a>光照变化</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig26.png"></p><p>作者用了直方图均衡化（CLAHE<sup><a href="#fn_66" id="reffn_66">66</a></sup>）去调整图像光度，结果如上图，可以看到几乎所有的基于学习的方法的测试效果都下降了，这可能由于没有专门地在这种场景中进行训练。而SIFT也没有得到明显提升，可能在于SIFT描述子是在某些假设条件下最佳表现。</p><h3 id="新指标-vs-传统指标"><a href="#新指标-vs-传统指标" class="headerlink" title="新指标 vs. 传统指标"></a>新指标 vs. 传统指标</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig18.png"></p><p>这里要说明的是传统的评价方式与本文提出方式的关系。</p><ol><li>matching score的选择还是比较明智的，它似乎与mAA相关，但也很难保证高的匹配得分就一定有助于提升mAA，例如RootSIFT vs ContextDesc；</li><li>repeatability则比较难去诠释它对最后位姿解算的效果。AKAZE的repeatability最好但是matching score和pose mAA都非常差，作者的原话(arxiv版本1)就是<strong>descriptor may hurt its performance</strong>。</li><li>Key.Net获得最好的repeatability，但是在mAA指标上弱于DoG的方法，即使使用了相同的描述子HardNet;</li></ol><p><strong>注意</strong>，以上结果都是论文发布在arxiv平台时给出的结果，最新结果参考这个官网<a href="https://vision.uvic.ca/image-matching-challenge/leaderboard/" target="_blank" rel="noopener">排行榜</a>。</p><p>由于目前正在使用superpoint特征（SuperPoint (2k features, NMS=4), DEGENSAC），所以比较关注它的表现。感觉在2k特征阵营，它的表现并不好（屈居#35,目前共52个算法），然而SuperPoint + SuperGlue + DEGENSAC以及SuperPoint+GIFT+Graph Motion Coherence Network+DEGENSAC分别位列#1以及#2，这也是结果很让人欣慰！</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/leadboard_superglue.png"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/leadboard_superpoint.png"></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><blockquote id="fn_1"><sup>1</sup>. H. Aanaes, A. L. Dahl, and K. Steenstrup Pedersen. Interesting Interest Points. IJCV, 97:18–35, 2012. 2<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. H. Aanaes and F. Kahl. Estimation of Deformable Structure and Motion. In Vision and Modelling of Dynamic Scenes Workshop, 2002. 6<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. S. Agarwal, N. Snavely, I. Simon, S.M. Seitz, and R. Szeliski. Building Rome in One Day. In ICCV, 2009. 1, 2<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. P. F. Alcantarilla, J. Nuevo, and A. Bartoli. Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces. In BMVC, 2013. 2, 3<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. Anonymous. DeepSFM: Structure From Motion Via Deep Bundle Adjustment. In Submission to ICLR, 2020. 2<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. Relja Arandjelovic. Three things everyone should know to improve object retrieval. In CVPR, 2012. 3<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote><blockquote id="fn_7"><sup>7</sup>. Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. NetVLAD: CNN Architecture for Weakly Supervised Place Recognition. In CVPR, 2016. 1<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a></blockquote><blockquote id="fn_8"><sup>8</sup>. Hernan Badino, Daniel Huber, and Takeo Kanade. The CMU Visual Localization Data Set. <a href="http://3dvis" target="_blank" rel="noopener">http://3dvis</a>. ri.cmu.edu/data-sets/localization, 2011. 2<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a></blockquote><blockquote id="fn_9"><sup>9</sup>. V. Balntas, K. Lenc, A. Vedaldi, and K. Mikolajczyk. HPatches: A Benchmark and Evaluation of Handcrafted and Learned Local Descriptors. In CVPR, 2017. 2, 7<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a></blockquote><blockquote id="fn_10"><sup>10</sup>. Vassileios Balntas, Shuda Li, and Victor Prisacariu. RelocNet: Continuous Metric Learning Relocalisation using Neural Nets. In The European Conference on Computer Vision (ECCV), September 2018. 1<a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a></blockquote><blockquote id="fn_11"><sup>11</sup>. V. Balntas, E. Riba, D. Ponsa, and K. Mikolajczyk. Learning Local Feature Descriptors with Triplets and Shallow Convolutional Neural Networks. In BMVC, 2016. 2<a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a></blockquote><blockquote id="fn_12"><sup>12</sup>. Daniel Barath and Ji Matas. Graph-cut ransac. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 2, 4<a href="#reffn_12" title="Jump back to footnote [12] in the text."> &#8617;</a></blockquote><blockquote id="fn_13"><sup>13</sup>. Axel Barroso-Laguna, Edgar Riba, Daniel Ponsa, and Krystian Mikolajczyk. Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters. In Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision, 2019. 2, 3<a href="#reffn_13" title="Jump back to footnote [13] in the text."> &#8617;</a></blockquote><blockquote id="fn_14"><sup>14</sup>. A. Baumberg. Reliable Feature Matching Across Widely Separated Views. In CVPR, pages 774–781, 2000. 3, 6<a href="#reffn_14" title="Jump back to footnote [14] in the text."> &#8617;</a></blockquote><blockquote id="fn_15"><sup>15</sup>. H. Bay, T. Tuytelaars, and L. Van Gool. SURF: Speeded Up Robust Features. In ECCV, 2006. 2, 3<a href="#reffn_15" title="Jump back to footnote [15] in the text."> &#8617;</a></blockquote><blockquote id="fn_16"><sup>16</sup>. P. R. Beaudet. Rotationally invariant image operators. In Proceedings of the 4th International Joint Conference on Pattern Recognition, pages 579–583, Kyoto, Japan, Nov. 1978. 3, 6<a href="#reffn_16" title="Jump back to footnote [16] in the text."> &#8617;</a></blockquote><blockquote id="fn_17"><sup>17</sup>. Jia-Wang Bian, Yu-Huan Wu, Ji Zhao, Yun Liu, Le Zhang, Ming-Ming Cheng, and Ian Reid. An Evaluation of Feature Matchers for Fundamental Matrix Estimation. In BMVC, 2019. 2<a href="#reffn_17" title="Jump back to footnote [17] in the text."> &#8617;</a></blockquote><blockquote id="fn_18"><sup>18</sup>. Eric Brachmann and Carsten Rother. Neural- Guided RANSAC: Learning Where to Sample Model Hypotheses. In ICCV, 2019. 2<a href="#reffn_18" title="Jump back to footnote [18] in the text."> &#8617;</a></blockquote><blockquote id="fn_19"><sup>19</sup>. G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000. 4<a href="#reffn_19" title="Jump back to footnote [19] in the text."> &#8617;</a></blockquote><blockquote id="fn_20"><sup>20</sup>. M. Brown, G. Hua, and S. Winder. Discriminative Learning of Local Image Descriptors. PAMI, 2011. 1, 2, 7<a href="#reffn_20" title="Jump back to footnote [20] in the text."> &#8617;</a></blockquote><blockquote id="fn_21"><sup>21</sup>. M. Brown and D. Lowe. Automatic Panoramic Image Stitching Using Invariant Features. IJCV, 74:59–73, 2007. 2<a href="#reffn_21" title="Jump back to footnote [21] in the text."> &#8617;</a></blockquote><blockquote id="fn_22"><sup>22</sup>. Mai Bui, Christoph Baur, Nassir Navab, Slobodan Ilic, and Shadi Albarqouni. Adversarial Networks for Camera Pose Regression and Reﬁnement. In The IEEE International Conference on Computer Vision (ICCV) Workshops, Oct 2019. 1<a href="#reffn_22" title="Jump back to footnote [22] in the text."> &#8617;</a></blockquote><blockquote id="fn_23"><sup>23</sup>. Ondˇrej Chum and Jiˇr´ı Matas. Matching with PROSAC Progressive Sample Consensus. In CVPR, pages 220–226, June 2005. 2<a href="#reffn_23" title="Jump back to footnote [23] in the text."> &#8617;</a></blockquote><blockquote id="fn_24"><sup>24</sup>. Ondˇrej Chum, Jiˇr´ı Matas, and Josef Kittler. Locally Optimized RANSAC. In PR, 2003. 2<a href="#reffn_24" title="Jump back to footnote [24] in the text."> &#8617;</a></blockquote><blockquote id="fn_25"><sup>25</sup>. Ondˇrej Chum, Jiˇr´ı Matas, and Josef Kittler. Locally optimized ransac. In Pattern Recognition, 2003. 4<a href="#reffn_25" title="Jump back to footnote [25] in the text."> &#8617;</a></blockquote><blockquote id="fn_26"><sup>26</sup>. Ondrej Chum, Tomas Werner, and Jiri Matas. Two-View Geometry Estimation Unaffected by a Dominant Plane. In CVPR, 2005. 2, 4<a href="#reffn_26" title="Jump back to footnote [26] in the text."> &#8617;</a></blockquote><blockquote id="fn_27"><sup>27</sup>. Hainan Cui, Xiang Gao, Shuhan Shen, and Zhanyi Hu. Hsfm: Hybrid structure-from-motion. In CVPR, July 2017. 2<a href="#reffn_27" title="Jump back to footnote [27] in the text."> &#8617;</a></blockquote><blockquote id="fn_28"><sup>28</sup>. Zheng Dang, Kwang Moo Yi, Yinlin Hu, Fei Wang, Pascal Fua, and Mathieu Salzmann. Eigendecomposition-Free Training of Deep Networks with Zero Eigenvalue-Based Losses. In ECCV, 2018. 4<a href="#reffn_28" title="Jump back to footnote [28] in the text."> &#8617;</a></blockquote><blockquote id="fn_29"><sup>29</sup>. Jana Noskova Daniel Barath, Jiri Matas. MAGSAC: marginalizing sample consensus. In CVPR, 2019. 1, 2, 4<a href="#reffn_29" title="Jump back to footnote [29] in the text."> &#8617;</a></blockquote><blockquote id="fn_30"><sup>30</sup>. D. Detone, T. Malisiewicz, and A. Rabinovich. Toward Geometric Deep SLAM. arXiv preprint arXiv:1707.07410, 2017. 1<a href="#reffn_30" title="Jump back to footnote [30] in the text."> &#8617;</a></blockquote><blockquote id="fn_31"><sup>31</sup>. D. Detone, T. Malisiewicz, and A. Rabinovich. Superpoint: Self-Supervised Interest Point Detection and Description. CVPR Workshop on Deep Learning for Visual SLAM, 2018. 1, 2, 3, 8<a href="#reffn_31" title="Jump back to footnote [31] in the text."> &#8617;</a></blockquote><blockquote id="fn_32"><sup>32</sup>. J. Dong and S. Soatto. Domain-Size Pooling in Local Descriptors: DSP-SIFT. In CVPR, 2015. 6<a href="#reffn_32" title="Jump back to footnote [32] in the text."> &#8617;</a></blockquote><blockquote id="fn_33"><sup>33</sup>. M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, and T. Sattler. D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. In CVPR, 2019. 1, 2, 3, 8<a href="#reffn_33" title="Jump back to footnote [33] in the text."> &#8617;</a></blockquote><blockquote id="fn_34"><sup>34</sup>. Patrick Ebel, Anastasiia Mishchuk, Kwang Moo Yi, Pascal Fua, and Eduard Trulls. Beyond Cartesian Representations for Local Descriptors. In ICCV, 2019. 2, 3, 6<a href="#reffn_34" title="Jump back to footnote [34] in the text."> &#8617;</a></blockquote><blockquote id="fn_35"><sup>35</sup>. Vassileios Balntas et.al. SILDa: A Multi-Task Dataset for Evaluating Visual Localization. <a href="https://github" target="_blank" rel="noopener">https://github</a>. com/scape-research/silda, 2018. 2<a href="#reffn_35" title="Jump back to footnote [35] in the text."> &#8617;</a></blockquote><blockquote id="fn_36"><sup>36</sup>. M.A Fischler and R.C. Bolles. Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography. Communications ACM, 24(6):381–395, 1981. 1, 2, 4<a href="#reffn_36" title="Jump back to footnote [36] in the text."> &#8617;</a></blockquote><blockquote id="fn_37"><sup>37</sup>. P. Gay, V. Bansal, C. Rubino, and A. D. Bue. Probabilistic Structure from Motion with Objects (PSfMO). In ICCV, 2017. 2<a href="#reffn_37" title="Jump back to footnote [37] in the text."> &#8617;</a></blockquote><blockquote id="fn_38"><sup>38</sup>. Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite. In CVPR, 2012. 2<a href="#reffn_38" title="Jump back to footnote [38] in the text."> &#8617;</a></blockquote><blockquote id="fn_39"><sup>39</sup>. R.I. Hartley. In Defense of the Eight-Point Algorithm. PAMI, 19(6):580–593, June 1997. 2<a href="#reffn_39" title="Jump back to footnote [39] in the text."> &#8617;</a></blockquote><blockquote id="fn_40"><sup>40</sup>. R. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, 2000. 1<a href="#reffn_40" title="Jump back to footnote [40] in the text."> &#8617;</a></blockquote><blockquote id="fn_41"><sup>41</sup>. R. I. Hartley. Projective reconstruction and invariants from multiple images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(10):1036–1041, Oct 1994. 1, 2<a href="#reffn_41" title="Jump back to footnote [41] in the text."> &#8617;</a></blockquote><blockquote id="fn_42"><sup>42</sup>. K. He, Y. Lu, and S. Sclaroff. Local Descriptors Optimized for Average Precision. In CVPR, 2018. 1<a href="#reffn_42" title="Jump back to footnote [42] in the text."> &#8617;</a></blockquote><blockquote id="fn_43"><sup>43</sup>. J. Heinly, J.L. Schoenberger, E. Dunn, and J-M. Frahm. Reconstructing the World in Six Days. In CVPR, 2015. 1, 2, 3<a href="#reffn_43" title="Jump back to footnote [43] in the text."> &#8617;</a></blockquote><blockquote id="fn_44"><sup>44</sup>. Karel Lenc and Varun Gulshan and Andrea Vedaldi. VLBenchmarks. <a href="http://www.vlfeat.org/" target="_blank" rel="noopener">http://www.vlfeat.org/</a> benchmarks/, 2011. 2<a href="#reffn_44" title="Jump back to footnote [44] in the text."> &#8617;</a></blockquote><blockquote id="fn_45"><sup>45</sup>. A. Kendall, M. Grimes, and R. Cipolla. Posenet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization. In ICCV, pages 2938–2946, 2015. 1<a href="#reffn_45" title="Jump back to footnote [45] in the text."> &#8617;</a></blockquote><blockquote id="fn_46"><sup>46</sup>. J. Krishna Murthy, Ganesh Iyer, and Liam Paull. gradSLAM: Dense SLAM meets Automatic Differentiation. arXiv, 2019. 2<a href="#reffn_46" title="Jump back to footnote [46] in the text."> &#8617;</a></blockquote><blockquote id="fn_47"><sup>47</sup>. Zhengqi Li and Noah Snavely. MegaDepth: Learning Single-View Depth Prediction from Internet Photos. In CVPR, 2018. 2<a href="#reffn_47" title="Jump back to footnote [47] in the text."> &#8617;</a></blockquote><blockquote id="fn_48"><sup>48</sup>. David G. Lowe. Distinctive Image Features from ScaleInvariant Keypoints. IJCV, 20(2):91–110, November 2004. 1, 2, 3, 4, 6, 8, 15<a href="#reffn_48" title="Jump back to footnote [48] in the text."> &#8617;</a></blockquote><blockquote id="fn_49"><sup>49</sup>. Zixin Luo, Tianwei Shen, Lei Zhou, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan. ContextDesc: Local Descriptor Augmentation with Cross-Modality Context. In CVPR, 2019. 2, 3<a href="#reffn_49" title="Jump back to footnote [49] in the text."> &#8617;</a></blockquote><blockquote id="fn_50"><sup>50</sup>. Z. Luo, T. Shen, L. Zhou, S. Zhu, R. Zhang, Y. Yao, T. Fang, and L. Quan. Geodesc: Learning Local Descriptors by Integrating Geometry Constraints. In ECCV, 2018. 2, 3<a href="#reffn_50" title="Jump back to footnote [50] in the text."> &#8617;</a></blockquote><blockquote id="fn_51"><sup>51</sup>. Simon Lynen, Bernhard Zeisl, Dror Aiger, Michael Bosse, Joel Hesch, Marc Pollefeys, Roland Siegwart, and Torsten Sattler. Large-scale, real-time visual-inertial localization revisited. arXiv Preprint, 2019. 1<a href="#reffn_51" title="Jump back to footnote [51] in the text."> &#8617;</a></blockquote><blockquote id="fn_52"><sup>52</sup>. Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year, 1000 km: The Oxford RobotCar dataset. IJRR, 36(1):3–15, 2017. 2<a href="#reffn_52" title="Jump back to footnote [52] in the text."> &#8617;</a></blockquote><blockquote id="fn_53"><sup>53</sup>. J. Matas, O. Chum, M. Urban, and T. Pajdla. Robust WideBaseline Stereo from Maximally Stable Extremal Regions. IVC, 22(10):761–767, 2004. 3, 6<a href="#reffn_53" title="Jump back to footnote [53] in the text."> &#8617;</a></blockquote><blockquote id="fn_54"><sup>54</sup>. K. Mikolajczyk and C. Schmid. A Performance Evaluation of Local Descriptors. PAMI, 27(10):1615–1630, 2004. 2<a href="#reffn_54" title="Jump back to footnote [54] in the text."> &#8617;</a></blockquote><blockquote id="fn_55"><sup>55</sup>. K. Mikolajczyk, C. Schmid, and A. Zisserman. Human Detection Based on a Probabilistic Assembly of Robust Part Detectors. In ECCV, pages 69–82, 2004. 3, 6<a href="#reffn_55" title="Jump back to footnote [55] in the text."> &#8617;</a></blockquote><blockquote id="fn_56"><sup>56</sup>. Jiri Matas Milan Pultar, Dmytro Mishkin. Leveraging Outdoor Webcams for Local Descriptor Learning. In Proceedings of CVWW 2019, 2019. 7<a href="#reffn_56" title="Jump back to footnote [56] in the text."> &#8617;</a></blockquote><blockquote id="fn_57"><sup>57</sup>. A. Mishchuk, D. Mishkin, F. Radenovic, and J. Matas. Working Hard to Know Your Neighbor’s Margins: Local Descriptor Learning Loss. In NeurIPS, 2017. 2, 3, 6<a href="#reffn_57" title="Jump back to footnote [57] in the text."> &#8617;</a></blockquote><blockquote id="fn_58"><sup>58</sup>. Dmytro Mishkin, Jiri Matas, and Michal Perdoch. MODS: PAMI, 19(6):580–593, June 1997. 2 Fast and robust method for two-view matching. CVIU, 2015. 6, 15<a href="#reffn_58" title="Jump back to footnote [58] in the text."> &#8617;</a></blockquote><blockquote id="fn_59"><sup>59</sup>. D. Mishkin, F. Radenovic, and J. Matas. Repeatability is Not Enough: Learning Affine Regions via Discriminability. In ECCV, 2018. 3, 6<a href="#reffn_59" title="Jump back to footnote [59] in the text."> &#8617;</a></blockquote><blockquote id="fn_60"><sup>60</sup>. Arun Mukundan, Giorgos Tolias, and Ondrej Chum. Explicit Spatial Encoding for Deep Local Descriptors. In CVPR, 2019. 1<a href="#reffn_60" title="Jump back to footnote [60] in the text."> &#8617;</a></blockquote><blockquote id="fn_61"><sup>61</sup>. R. Mur-Artal, J. Montiel, and J. Tardos. Orb-Slam: A Versatile and Accurate Monocular Slam System. IEEE Transactions on Robotics, 31(5):1147–1163, 2015. 1<a href="#reffn_61" title="Jump back to footnote [61] in the text."> &#8617;</a></blockquote><blockquote id="fn_62"><sup>62</sup>. D. Nister. An Efficient Solution to the Five-Point Relative Pose Problem. In CVPR, June 2003. 2<a href="#reffn_62" title="Jump back to footnote [62] in the text."> &#8617;</a></blockquote><blockquote id="fn_63"><sup>63</sup>. Hyeonwoo Noh, Andre Araujo, Jack Sim, and Tobias Weyanda nd Bohyung Han. Large-Scale Image Retrieval with Attentive Deep Local Features. In ICCV, 2017. 1, 2<a href="#reffn_63" title="Jump back to footnote [63] in the text."> &#8617;</a></blockquote><blockquote id="fn_64"><sup>64</sup>. Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi. LF-Net: Learning Local Features from Images. In NeurIPS, 2018. 2, 3<a href="#reffn_64" title="Jump back to footnote [64] in the text."> &#8617;</a></blockquote><blockquote id="fn_65"><sup>65</sup>. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. 4<a href="#reffn_65" title="Jump back to footnote [65] in the text."> &#8617;</a></blockquote><blockquote id="fn_66"><sup>66</sup>. Stephen M. Pizer, E. Philip Amburn, John D. Austin, Robert Cromartie, Ari Geselowitz, Trey Greer, Bart ter Haar Romeny, John B. Zimmerman, and Karel Zuiderveld. Adaptive histogram equalization and its variations. Computer vision, graphics, and image processing, 1987. 15<a href="#reffn_66" title="Jump back to footnote [66] in the text."> &#8617;</a></blockquote><blockquote id="fn_67"><sup>67</sup>. M. Pultar, D. Mishkin, and J. Matas. Leveraging Outdoor Webcams for Local Descriptor Learning. In Computer Vision Winter Workshop, 2019. 2, 7<a href="#reffn_67" title="Jump back to footnote [67] in the text."> &#8617;</a></blockquote><blockquote id="fn_68"><sup>68</sup>. C.R. Qi, H. Su, K. Mo, and L.J. Guibas. Pointnet: Deep Learning on Point Sets for 3D Classiﬁcation and Segmentation. In CVPR, 2017. 4<a href="#reffn_68" title="Jump back to footnote [68] in the text."> &#8617;</a></blockquote><blockquote id="fn_69"><sup>69</sup>. Filip Radenovic, Georgios Tolias, and Ondra Chum. CNN image retrieval learns from BoW: Unsupervised ﬁne-tuning with hard examples. In ECCV, 2016. 1<a href="#reffn_69" title="Jump back to footnote [69] in the text."> &#8617;</a></blockquote><blockquote id="fn_70"><sup>70</sup>. R. Ranftl and V. Koltun. Deep Fundamental Matrix Estimation. In ECCV, 2018. 2, 4<a href="#reffn_70" title="Jump back to footnote [70] in the text."> &#8617;</a></blockquote><blockquote id="fn_71"><sup>71</sup>. J. Revaud, P. Weinzaepfel, C. De Souza, N. Pion, G. Csurka, Y. Cabon, and M. Humenberger. R2D2: Repeatable and Reliable Detector and Descriptor. In arXiv Preprint, 2019. 8<a href="#reffn_71" title="Jump back to footnote [71] in the text."> &#8617;</a></blockquote><blockquote id="fn_72"><sup>72</sup>. J´erˆome Revaud, Philippe Weinzaepfel, C´esar Roberto de Souza, Noe Pion, Gabriela Csurka, Yohann Cabon, and Martin Humenberger. R2D2: Repeatable and Reliable Detector and Descriptor. In NeurIPS, 2019. 2<a href="#reffn_72" title="Jump back to footnote [72] in the text."> &#8617;</a></blockquote><blockquote id="fn_73"><sup>73</sup>. E. Rublee, V. Rabaud, K. Konolidge, and G. Bradski. ORB: An Efﬁcient Alternative to SIFT or SURF. In ICCV, 2011. 2, 3, 6<a href="#reffn_73" title="Jump back to footnote [73] in the text."> &#8617;</a></blockquote><blockquote id="fn_74"><sup>74</sup>. Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Improving Image-Based Localization by Active Correspondence Search. In ECCV, 2012. 1<a href="#reffn_74" title="Jump back to footnote [74] in the text."> &#8617;</a></blockquote><blockquote id="fn_75"><sup>75</sup>. T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg, D. Safari, M. Okutomi, M. Pollefeys, J. Sivic, F. Kahl, and T. Pajdla. Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions. In CVPR, 2018. 1, 2<a href="#reffn_75" title="Jump back to footnote [75] in the text."> &#8617;</a></blockquote><blockquote id="fn_76"><sup>76</sup>. Torsten Sattler, Tobias Weyand, Bastian Leibe, and Leif Kobbelt. Image Retrieval for Image-Based Localization Revisited. In BMVC, 2012. 2<a href="#reffn_76" title="Jump back to footnote [76] in the text."> &#8617;</a></blockquote><blockquote id="fn_77"><sup>77</sup>. Torsten Sattler, Qunjie Zhou, Marc Pollefeys, and Laura Leal-Taixe. Understanding the Limitations of CNN-based Absolute Camera Pose Regression. In CVPR, 2019. 1<a href="#reffn_77" title="Jump back to footnote [77] in the text."> &#8617;</a></blockquote><blockquote id="fn_78"><sup>78</sup>. N. Savinov, A. Seki, L. Ladicky, T. Sattler, and M. Pollefeys. Quad-Networks: Unsupervised Learning to Rank for Interest Point Detection. CVPR, 2017. 2<a href="#reffn_78" title="Jump back to footnote [78] in the text."> &#8617;</a></blockquote><blockquote id="fn_79"><sup>79</sup>. J.L. Sch¨onberger and J.M. Frahm. Structure-From-Motion Revisited. In CVPR, 2016. 1, 2, 3, 4, 6<a href="#reffn_79" title="Jump back to footnote [79] in the text."> &#8617;</a></blockquote><blockquote id="fn_80"><sup>80</sup>. J.L. Sch¨onberger, H. Hardmeier, T. Sattler, and M. Pollefeys. Comparative Evaluation of Hand-Crafted and Learned Local Features. In CVPR, 2017. 2<a href="#reffn_80" title="Jump back to footnote [80] in the text."> &#8617;</a></blockquote><blockquote id="fn_81"><sup>81</sup>. Yunxiao Shi, Jing Zhu, Yi Fang, Kuochin Lien, and Junli Gu. Self-Supervised Learning of Depth and Ego-motion with Differentiable Bundle Adjustment. arXiv Preprint, 2019. 2<a href="#reffn_81" title="Jump back to footnote [81] in the text."> &#8617;</a></blockquote><blockquote id="fn_82"><sup>82</sup>. E. Simo-serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. Moreno-Noguer. Discriminative Learning of Deep Convolutional Feature Point Descriptors. In ICCV, 2015. 2<a href="#reffn_82" title="Jump back to footnote [82] in the text."> &#8617;</a></blockquote><blockquote id="fn_83"><sup>83</sup>. C. Strecha, W.V. Hansen, L. Van Gool, P. Fua, and U. Thoennessen. On Benchmarking Camera Calibration and Multi-View Stereo for High Resolution Imagery. In CVPR, 2008. 2<a href="#reffn_83" title="Jump back to footnote [83] in the text."> &#8617;</a></blockquote><blockquote id="fn_84"><sup>84</sup>. J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. A Benchmark for the Evaluation of RGB-D SLAM Systems. In IROS, 2012. 4<a href="#reffn_84" title="Jump back to footnote [84] in the text."> &#8617;</a></blockquote><blockquote id="fn_85"><sup>85</sup>. Weiwei Sun, Wei Jiang, Eduard Trulls, Andrea Tagliasacchi, and Kwang Moo Yi. Attentive Context Normalization for Robust Permutation-Equivariant Learning. In arXiv Preprint, 2019. 2, 4, 8<a href="#reffn_85" title="Jump back to footnote [85] in the text."> &#8617;</a></blockquote><blockquote id="fn_86"><sup>86</sup>. Chengzhou Tang and Ping Tan. Ba-Net: Dense Bundle Adjustment Network. In ICLR, 2019. 2<a href="#reffn_86" title="Jump back to footnote [86] in the text."> &#8617;</a></blockquote><blockquote id="fn_87"><sup>87</sup>. Keisuke Tateno, Federico Tombari, Iro Laina, and Nassir Navab. Cnn-slam: Real-time dense monocular slam with learned depth prediction. In CVPR, July 2017. 2<a href="#reffn_87" title="Jump back to footnote [87] in the text."> &#8617;</a></blockquote><blockquote id="fn_88"><sup>88</sup>. B. Thomee, D.A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L. Li. YFCC100M: the New Data in Multimedia Research. In CACM, 2016. 3<a href="#reffn_88" title="Jump back to footnote [88] in the text."> &#8617;</a></blockquote><blockquote id="fn_89"><sup>89</sup>. Y. Tian, B. Fan, and F. Wu. L2-Net: Deep Learning of Discriminative Patch Descriptor in Euclidean Space. In CVPR, 2017. 2, 3<a href="#reffn_89" title="Jump back to footnote [89] in the text."> &#8617;</a></blockquote><blockquote id="fn_90"><sup>90</sup>. Yurun Tian, Xin Yu, Bin Fan, Fuchao Wu, Huub Heijnen, and Vassileios Balntas. SOSNet: Second Order Similarity Regularization for Local Descriptor Learning. In CVPR, 2019. 1, 2, 3<a href="#reffn_90" title="Jump back to footnote [90] in the text."> &#8617;</a></blockquote><blockquote id="fn_91"><sup>91</sup>. Giorgos Tolias, Yannis Avrithis, and Herv´e J´egou. Image Search with Selective Match Kernels: Aggregation Across Single and Multiple Images. IJCV, 116(3):247–261, Feb 2016. 1<a href="#reffn_91" title="Jump back to footnote [91] in the text."> &#8617;</a></blockquote><blockquote id="fn_92"><sup>92</sup>. P.H.S. Torr and A. Zisserman. MLESAC: A New Robust Estimator with Application to Estimating Image Geometry. CVIU, 78:138–156, 2000. 2<a href="#reffn_92" title="Jump back to footnote [92] in the text."> &#8617;</a></blockquote><blockquote id="fn_93"><sup>93</sup>. B. Triggs, P. Mclauchlan, R. Hartley, and A. Fitzgibbon. Bundle Adjustment – A Modern Synthesis. In Vision Algorithms: Theory and Practice, pages 298–372, 2000. 1<a href="#reffn_93" title="Jump back to footnote [93] in the text."> &#8617;</a></blockquote><blockquote id="fn_94"><sup>94</sup>. Andrea Vedaldi and Brian Fulkerson. Vlfeat: An open and portable library of computer vision algorithms. In Proceedings of the 18th ACM International Conference on Multimedia, MM ’10, pages 1469–1472, 2010. 3<a href="#reffn_94" title="Jump back to footnote [94] in the text."> &#8617;</a></blockquote><blockquote id="fn_95"><sup>95</sup>. Y. Verdie, K. M. Yi, P. Fua, and V. Lepetit. TILDE: A Temporally Invariant Learned DEtector. In CVPR, 2015. 2<a href="#reffn_95" title="Jump back to footnote [95] in the text."> &#8617;</a></blockquote><blockquote id="fn_96"><sup>96</sup>. S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, and K. Fragkiadaki. Sfm-Net: Learning of Structure and Motion from Video. arXiv Preprint, 2017. 2<a href="#reffn_96" title="Jump back to footnote [96] in the text."> &#8617;</a></blockquote><blockquote id="fn_97"><sup>97</sup>. X. Wei, Y. Zhang, Y. Gong, and N. Zheng. Kernelized Subspace Pooling for Deep Local Descriptors. In CVPR, 2018. 1<a href="#reffn_97" title="Jump back to footnote [97] in the text."> &#8617;</a></blockquote><blockquote id="fn_98"><sup>98</sup>. Changchang Wu. Towards Linear-Time Incremental Structure from Motion. In 3DV, 2013. 2, 6<a href="#reffn_98" title="Jump back to footnote [98] in the text."> &#8617;</a></blockquote><blockquote id="fn_99"><sup>99</sup>. Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal Fua. LIFT: Learned Invariant Feature Transform. In ECCV, 2016. 2<a href="#reffn_99" title="Jump back to footnote [99] in the text."> &#8617;</a></blockquote><blockquote id="fn_100"><sup>100</sup>. K. M. Yi, E. Trulls, Y. Ono, V. Lepetit, M. Salzmann, and P. Fua. Learning to Find Good Correspondences. In CVPR, 2018. 2, 3, 4, 7, 13, 17<a href="#reffn_100" title="Jump back to footnote [100] in the text."> &#8617;</a></blockquote><blockquote id="fn_101"><sup>101</sup>. S. Zagoruyko and N. Komodakis. Learning to Compare Image Patches via Convolutional Neural Networks. In CVPR, 2015. 6<a href="#reffn_101" title="Jump back to footnote [101] in the text."> &#8617;</a></blockquote><blockquote id="fn_102"><sup>102</sup>. Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou, Tianwei Shen, Yurong Chen, Long Quan, and Hongen Liao. Learning Two-View Correspondences and Geometry Using Order-Aware Network. ICCV, 2019. 2, 3, 4<a href="#reffn_102" title="Jump back to footnote [102] in the text."> &#8617;</a></blockquote><blockquote id="fn_103"><sup>103</sup>. Xu Zhang, Felix X. Yu, Svebor Karaman, and Shih-Fu Chang. Learning Discriminative and Transformation Covariant Local Feature Detectors. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. 2<a href="#reffn_103" title="Jump back to footnote [103] in the text."> &#8617;</a></blockquote><blockquote id="fn_104"><sup>104</sup>. Chen Zhao, Zhiguo Cao, Chi Li, Xin Li, and Jiaqi Yang. NM-Net: Mining Reliable Neighbors for Robust Feature Correspondences. In CVPR, 2019. 2, 4<a href="#reffn_104" title="Jump back to footnote [104] in the text."> &#8617;</a></blockquote><blockquote id="fn_105"><sup>105</sup>. Qunjie Zhou, Torsten Sattler, Marc Pollefeys, and Laura Leal-Taixe. To learn or not to learn: Visual localization from essential matrices. arXiv Preprint, 2019. 1<a href="#reffn_105" title="Jump back to footnote [105] in the text."> &#8617;</a></blockquote><blockquote id="fn_106"><sup>106</sup>. Siyu Zhu, Runze Zhang, Lei Zhou, Tianwei Shen, Tian Fang, Ping Tan, and Long Quan. Very Large-Scale Global SfM by Distributed Motion Averaging. In CVPR, June 2018. 1, 2<a href="#reffn_106" title="Jump back to footnote [106] in the text."> &#8617;</a></blockquote><blockquote id="fn_107"><sup>107</sup>. C.L. Zitnick and K. Ramnath. Edge Foci Interest Points. In ICCV, 2011. 2<a href="#reffn_107" title="Jump back to footnote [107] in the text."> &#8617;</a></blockquote><blockquote id="fn_108"><sup>108</sup>. A. Alahi, R. Ortiz, and P. Vandergheynst. FREAK: Fast Retina Keypoint. In CVPR, 2012. 7, 11<a href="#reffn_108" title="Jump back to footnote [108] in the text."> &#8617;</a></blockquote><blockquote id="fn_109"><sup>109</sup>. S. Leutenegger, M. Chli, and R. Y. Siegwart. Brisk: Binary robust invariant scalable keypoints. In ICCV, pages 2548–2555, 2011.7<a href="#reffn_109" title="Jump back to footnote [109] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从一系列的图像中恢复物体的3D结构是计算机视觉研究中一个热门课题，这使得我们可以相隔万里从google map中看到复活节岛的风景。这得益于图像来自于可控的条件，使得最终的重建效果一致性且质量都很高，但是这却限制了采集设备以及视角。畅想一下，假如我们不使用专业设备，而是利用sfm技术根据互联网上大量的图片重建出这个复杂世界。&lt;/p&gt;
&lt;!-- ![A 3D reconstruction generated from over 3000 images, including those from the previous figure](https://1.bp.blogspot.com/-loSqCB3NnM0/XoTiOGP9SYI/AAAAAAAAFlE/rs8iCTq63FYapA7HbljF8iWa7fyHvh3UgCLcBGAsYHQ/s400/image3.gif) --&gt;
&lt;!-- ![](https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/image_sfm.gif) --&gt;
&lt;p&gt;&lt;img src=&quot;https://vincentqin.gitee.io/posts/2020-image-matching-cvpr/image_sfm.gif&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;为了加快这个领域的研究，更好地利用图像数据有效信息，谷歌联合 &lt;a href=&quot;https://www.uvic.ca/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;UVIC&lt;/a&gt;, &lt;a href=&quot;https://www.cvut.cz/en&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CTU&lt;/a&gt;以及EPFL发表了这篇文章 “&lt;a href=&quot;https://arxiv.org/abs/2003.01587&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Image Matching across Wide Baselines: From Paper to Practice&lt;/a&gt;”，[&lt;strong&gt;&lt;a href=&quot;http://xxx.itp.ac.cn/pdf/2003.01587v2&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PDF&lt;/a&gt;&lt;/strong&gt;]，旨在公布一种新的衡量用于3D重建方法的标准模块+数据集，这里主要是指2D图像间的匹配。这个评价模块可以很方便地集成并评估现有流行的特征匹配算法，包括传统方法或者基于机器学习的方法。&lt;/p&gt;
&lt;p&gt;谷歌公布2020图像匹配挑战的数据集：&lt;a href=&quot;https://image-matching-workshop.github.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官网&lt;/a&gt;，&lt;a href=&quot;http://ai.googleblog.com/2020/04/announcing-2020-image-matching.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;博客&lt;/a&gt;，文末有排行榜。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="ORB" scheme="https://www.vincentqin.tech/tags/ORB/"/>
    
      <category term="特征匹配" scheme="https://www.vincentqin.tech/tags/%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D/"/>
    
      <category term="SuperPoint" scheme="https://www.vincentqin.tech/tags/SuperPoint/"/>
    
      <category term="SIFT" scheme="https://www.vincentqin.tech/tags/SIFT/"/>
    
  </entry>
  
  <entry>
    <title>SuperGlue:Learning Feature Matching with Graph Neural Networks</title>
    <link href="https://www.vincentqin.tech/posts/superglue/"/>
    <id>https://www.vincentqin.tech/posts/superglue/</id>
    <published>2020-04-17T17:21:56.000Z</published>
    <updated>2020-05-18T16:31:35.433Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>ETHZ ASL与Magicleap联名之作，CVPR 2020 Oral（论文见文末），一作是来自ETHZ的实习生，二作是当年CVPR2018 SuperPoint的作者Daniel DeTone。<br><!-- ![](/posts/superglue/freiburg_matches.gif) --></p><p><img alt data-src="https://vincentqin.gitee.io/posts/superglue/freiburg_matches.gif"></p><a id="more"></a><p>注：</p><ol><li>SuperPoint参见另外一篇文章<a href="https://www.vincentqin.tech/posts/superpoint/">《SuperPoint: Self-Supervised Interest Point Detection and Description》</a>，<a href="https://vincentqin.gitee.io/posts/superpoint/" target="_blank" rel="noopener">备用链接</a>。</li><li>后文中反复提到的self-attention/cross-attention，我暂时翻译成自我注意力/交叉注意力。</li><li>本人知识水平有限，如有错误请在评论区指出。当然，没有问题也可刷刷评论。</li></ol><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文提出了一种能够同时进行特征匹配以及滤除外点的网络。其中特征匹配是通过求解可微分最优化转移问题（ optimal transport problem）来解决，损失函数由GNN来构建；本文基于注意力机制提出了一种灵活的内容聚合机制，这使得SuperGlue能够同时感知潜在的3D场景以及进行特征匹配。该算法与传统的，手工设计的特征相比，能够在室内外环境中位姿估计任务中取得最好的结果，该网络能够在GPU上达到实时，预期能够集成到sfm以及slam算法中。</p><p><img alt="superglue_front" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_front.png"></p><p>SuperGlue是一种特征匹配网络，它的输入是2张图像中特征点以及描述子（手工特征或者深度学习特征均可），输出是图像特征之间的匹配关系。</p><p>作者认为学习特征匹配可以被视为找到两簇点的局部分配关系。作者受到了Transformer的启发，同时将self-和cross-attention利用特征点位置以及其视觉外观进行匹配。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="局部特征匹配"><a href="#局部特征匹配" class="headerlink" title="局部特征匹配"></a>局部特征匹配</h3><p>传统的特征可分5步走：1)提取特征点；2)计算描述子；3)最近邻匹配；4)滤除外点；5)求解几何约束；其中滤除外点一步包括点方法有：计算最优次优比，RANSAC，交叉验证以及neighborhood consensus。</p><p>最近的一些工作主要集中在设计特异性更好的稀疏特征上，而它们的匹配算法仍然依赖于NN等策略：在做匹配时并没有考虑特征的结构相似性以及外观相似性。</p><h3 id="图匹配"><a href="#图匹配" class="headerlink" title="图匹配"></a>图匹配</h3><p>这类方法将特征的匹配问题描述成“quadratic assignment problems”，这是一个NP-hard问题，求解这类问题需要复杂不切实际的算子。后来的研究者将这个问题化简成“linear assignment problems”，但仅仅用了一个浅层模型，相比之下SuperGlue利用深度神经网络构建了一种合适的代价进行求解。此处需要说明的是图匹配问题可以认为是一种“<em>optimal transport</em>”问题，<strong>它是一种有效但简单的近似解的广义线性分配，即Sinkhorn算法</strong>。</p><h3 id="深度点云匹配"><a href="#深度点云匹配" class="headerlink" title="深度点云匹配"></a>深度点云匹配</h3><p>点云匹配的目的是通过在元素之间聚集信息来设计置换等价或不变函数。一些算法同等的对待这些元素，还有一些算法主要关注于元素的局部坐标或者特征空间。注意力机制可以通过关注特定的元素和属性来实现全局以及依赖于数据的局部聚合，因而更加全面和灵活。SuperGlue借鉴了这种注意力机制。</p><h2 id="框架以及原理"><a href="#框架以及原理" class="headerlink" title="框架以及原理"></a>框架以及原理</h2><p>特征匹配必须满足的硬性要求是：i)至多有1个匹配点；ii)有些点由于遮挡等原因并没有匹配点。一个成熟的特征匹配模型应该做到：既能够找到特征之间的正确匹配，又可以鉴别错误匹配。</p><p><img alt="superglue_arch" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_arch.png"></p><p>整个框架由两个主要模块组成：注意力GNN以及最优匹配层。其中注意力GNN将特征点以及描述子编码成为一个向量（该向量可以理解为特征匹配向量），随后利用自我注意力以及交叉注意力来回增强（重复$L$次）这个向量$\mathbf{f}$的特征匹配性能；随后进入最优匹配层，通过计算特征匹配向量的内积得到匹配度得分矩阵，然后通过Sinkhorn算法（迭代$T$次）解算出最优特征分配矩阵。 </p><h3 id="公式化"><a href="#公式化" class="headerlink" title="公式化"></a>公式化</h3><p>该部分对特征匹配问题建模。给定两张图片$A,B$，每张图片上都有特征点位置$\mathbf{p}$以及对应的描述子$\mathbf{d}$，所以我们经常用$(\mathbf{p},\mathbf{d})$来表示图像特征。第$i$个特征可以表示为$\mathbf{p}_i:=(x,y,c)$，其中$c$表示特征点提取置信度，$(x,y)$表示特征坐标；描述子可以表示为$\mathbf{d}_i \in \mathbb{R}^{D}$，其中$D$表示特征维度，这里的特征可以是CNN特征，如SuperPoint，或者是传统特征SIFT。假设图像$A,B$分别有$M,N$个特征，可以表示为$\mathcal{A}:=\{1, \ldots, M\}$以及$\mathcal{B}:=\{1, \ldots, N\}$。</p><p><strong>部分分配矩阵</strong>：约束i）和ii）意味着对应关系来自两组关键点之间的部分分配。我们给出一个软分配矩阵$\mathbf{P} \in[0,1]^{M \times N}$，根据上述约束，我们有如下关系：</p><script type="math/tex; mode=display">\mathbf{P} \mathbf{1}_{N} \leq \mathbf{1}_{M} \quad \text { and } \quad \mathbf{P}^{\top} \mathbf{1}_{M} \leq \mathbf{1}_{N}</script><p>那我们设计网络的目标就是解算这个分配矩阵$\mathbf{P}$。</p><h3 id="注意力GNN"><a href="#注意力GNN" class="headerlink" title="注意力GNN"></a>注意力GNN</h3><p>这里有个有意思的说法：特征点的位置以及视觉外观能够提高其特异性。另外一个具有启发性的观点是人类在寻找匹配点过程是具有参考价值的。想一下人类是怎样进行特征匹配的，人类通过来回浏览两个图像试探性筛选匹配关键点，并进行来回检查（如果不是匹配的特征，观察一下周围有没有匹配的更好的点，直到找到匹配点/或没有匹配）。上述过程人们通过主动寻找上下文来增加特征点特异性，这样可以排除一些具有奇异性的匹配。本文的核心就是利用基于注意力机制的GNN实现上述过程，即模拟了人类进行特征匹配。</p><h4 id="特征点Encode"><a href="#特征点Encode" class="headerlink" title="特征点Encode"></a>特征点Encode</h4><p>首先根据上述说法，特征点位置+描述会获得更强的特征匹配特异性，所以这里将特征点的位置以及描述子合并成每个特征点$i$的初始表示$^{(0)} \mathbf{x}_{i}$，</p><script type="math/tex; mode=display">^{(0)} \mathbf{x}_{i}=\mathbf{d}_{i}+\mathbf{M L P}_{\mathrm{enc}}\left(\mathbf{p}_{i}\right)</script><p>其中MLP表示多层感知机（Multilayer Perceptron ，MLP）此处用于对低维特征升维，上式实际上是将视觉外观以及特征点位置进行了耦合，正因如此，这使得该Encode形式使得后续的注意力机制能够充分考虑到特征的外观以及位置相似度。</p><h4 id="多层GNN"><a href="#多层GNN" class="headerlink" title="多层GNN"></a>多层GNN</h4><p>考虑一个单一的完全图，它的节点是图像中每个特征点，这个图包括两种不同的无向边：一种是“Intra-image edges”（self edge）$\mathcal{E}_{\text {self }}$，它连接了来自图像内部特征点；另外一种是“Inter-image edges”（cross edge）$\mathcal{E}_{\text {cross }}$，它连接本图特征点$i$与另外一张图所有特征点（构成了该边）。</p><p>令$^{(\ell)} \mathbf{x}_{i}^{A}$表示为图像$A$上第$i$个元素在第$\ell$层的中间表达形式。信息（message）$\mathbf{m}_{\mathcal{E} \rightarrow i}$是聚合了所有特征点$\{j:(i, j) \in \mathcal{E}\}$之后点结果（它的具体形式后面的Attentional Aggregation会介绍，一句话来说就是将自我注意力以及交叉注意力进行聚合），其中$\mathcal{E} \in \{\mathcal{E}_{\text {self }},\mathcal{E}_{\text {self }}\}$，所以图像$A$中所有特征$i$传递更新的残差信息（residual message？）是：</p><script type="math/tex; mode=display">^{(\ell+1)} \mathbf{x}_{i}^{A}=^{(\ell)} \mathbf{x}_{i}^{A}+\operatorname{MLP}\left(\left[^{(\ell)} \mathbf{x}_{i}^{A} \| \mathbf{m}_{\mathcal{E} \rightarrow i}\right]\right)</script><p>其中$[\cdot | \cdot]$表示串联操作。同样的，图像$B$上所有特征有类似的更新形式。可以看到self 以及cross edges绑在一起并交替进行更新，先self后cross，作者提到共有固定数量的$L$层。</p><p>需要说明的是，这里的self-/cross-attention实际上就是模拟了人类来回浏览匹配的过程，其中self-attention是为了使得特征更加具有匹配特异性，而cross-attention是为了用这些具有特异性的点做图像间特征的相似度比较。</p><h4 id="Attentional-Aggregation"><a href="#Attentional-Aggregation" class="headerlink" title="Attentional Aggregation"></a>Attentional Aggregation</h4><p>文章的亮点之一就是将注意力机制用于特征匹配，这到底是如何实现的呢？作者提到，注意力机制将self以及cross信息聚合得到$\mathbf{m}_{\mathcal{E} \rightarrow i}$。其中self edge利用了self-attention[58]，cross edge利用了cross-attention。类似于数据库检索，我们想要查询$\mathbf{q}_i$基于元素的属性即键$\mathbf{k}_i$，检索到了某些元素的值$\mathbf{v}_j$。</p><script type="math/tex; mode=display">\mathbf{m}_{\mathcal{E} \rightarrow i}=\sum_{j:(i, j) \in \mathcal{E}} \alpha_{i j} \mathbf{v}_{j}</script><p>其中注意力权重${\alpha}_{ij}$是查询与检索到对象键值相似度的$\operatorname{Softmax}$即，$\alpha_{i j}=\operatorname{Softmax}_{j}\left(\mathbf{q}_{i}^{\top} \mathbf{k}_{j}\right)$。</p><p>这里需要解释一下键（key），query以及值（value）。令待查询点特征点$i$位于查询图像$Q$上，所有的源特征点位于图像$S$上，其中$(Q, S) \in\{A, B\}^{2}$，于是我们可以将key，query以及value写成下述形式：</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{q}_{i} &=\mathbf{W}_{1}^{(\ell)} \mathbf{x}_{i}^{Q}+\mathbf{b}_{1} \\\left[\begin{array}{l}\mathbf{k}_{j} \\ \mathbf{v}_{j}\end{array}\right] &=\left[\begin{array}{l}\mathbf{W}_{2} \\ \mathbf{W}_{3}\end{array}\right](\ell) \mathbf{x}_{i}^{S}+\left[\begin{array}{l}\mathbf{b}_{2} \\ \mathbf{b}_{3}\end{array}\right] \end{aligned}</script><p>每一层$\ell$都有其对应的一套投影参数，这些参数被所有的特征点共享。理解一下：此处的$\mathbf{q}_i$对应于待查询图像上某个特征点$i$的一种表示（self-attention映射），$\mathbf{k}_j$以及$\mathbf{v}_j$都是来自于召回的图像特征点$j$的一种表示（映射）；$\alpha_{i j}$表示这两个特征相似度，它是由$\mathbf{q}_i$以及$\mathbf{k}_j$计算得到（在这里体现了cross-attention的思想？），越大就表示这两个特征越相似，然后利用该相似度对$\mathbf{v}_j$加权求和得到$\mathbf{m}_{\mathcal{E} \rightarrow i}$，这就是所谓的<strong>特征聚合</strong>。</p><p>上面提到的这些概念有些难以理解，作者特意对上述过程进行了可视化，self-attention就是一张图像内部的边相连进行聚合，它能够更加关注具有特异性的所有点，且并不仅局限于其邻域位置特征（心心相依，何惧千里，逃…）；cross-attention做的就是匹配那些外观相似的两张图像见的特征。</p><p><img alt="superglue_fig_4" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_4.png"></p><p><img alt="superglue_fig_7" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_7.jpg"></p><p>下图展示了每层self-attention以及across-attention中权重${\alpha_{i j}}$的结果。按照匹配从难到易，文中画出了3个不同的特征点作为演示，绿色特征点（容易），蓝色特征点（中等）以及红色特征点（困难）。对于self-attention，初始时它（某个特征）关联了图像上所有的点（首行），然后逐渐地关注在与该特征相邻近的特征点（尾行）。同样地，cross-attention主要关注去匹配可能的特征点，随着层的增加，它逐渐减少匹配点集直到收敛。绿色特征点在第9层就已经趋近收敛，而红色特征直到最后才能趋紧收敛（匹配）。可以看到无论是self还是cross，它们关注的区域都会随着网络层深度的增加而逐渐缩小。</p><p><img alt="superglue_fig_15" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_15_1.jpg"></p><p>经过了$L$次self/cross-attention后就可以得到注意力GNN的输出，对于图像$A$我们有：</p><script type="math/tex; mode=display">\mathbf{f}_{i}^{A}=\mathbf{W} \cdot^{(L)} \mathbf{x}_{i}^{A}+\mathbf{b}, \quad \forall i \in \mathcal{A}</script><p>我们可以把$\mathbf{f}_{i}^{A}$理解为<strong>匹配描述子</strong>（类比特征描述子），专门为特征匹配服务，对于图像$B$具有类似的形式。</p><h3 id="匹配层（Optimal-matching-layer）"><a href="#匹配层（Optimal-matching-layer）" class="headerlink" title="匹配层（Optimal matching layer）"></a>匹配层（Optimal matching layer）</h3><p>接下来的任务就是去构建软分配矩阵$\mathbf{P}$。对于一般的图匹配流程，这个分配矩阵可以通过计算一个得分矩阵$\mathbf{S} \in \mathbb{R}^{M \times N}$（用来表示一些潜在的匹配）来实现。具体而言，通过最大化总体得分$\sum_{i, j} \mathbf{S}_{i, j} \mathbf{P}_{i, j}$即可得到这个分配矩阵$\mathbf{P}$，其中要注意的是$\mathbf{P}$是有约束的。</p><h4 id="匹配得分预测"><a href="#匹配得分预测" class="headerlink" title="匹配得分预测"></a>匹配得分预测</h4><p>去计算$M\times N$个潜在匹配得分是不可取的，于是作者就用GNN聚合得到的$\mathbf{f}_{i}^{A}$以及$\mathbf{f}_{i}^{B}$计算内积得到得分：</p><script type="math/tex; mode=display">\mathbf{S}_{i, j}=<\mathbf{f}_{i}^{A}, \mathbf{f}_{j}^{B}>, \forall(i, j) \in \mathcal{A} \times \mathcal{B}</script><h4 id="遮挡以及可见性"><a href="#遮挡以及可见性" class="headerlink" title="遮挡以及可见性"></a>遮挡以及可见性</h4><p>类似于SuperPoint在提取特征点时增加了一层dustbin通道，专门为了应对图像中没有特征点情况。本文借鉴了该思想，在得分矩阵$\mathbf{S}$的最后一列/行设置为dustbins可以得到$\overline{\mathbf{S}}$，这样做的作用在于可以滤出错误的匹配点。</p><script type="math/tex; mode=display">\overline{\mathbf{S}}_{i, N+1}=\overline{\mathbf{S}}_{M+1, j}=\overline{\mathbf{S}}_{M+1, N+1}=z \in \mathbb{R}</script><p>图像$A$上的特征点被分配到图像$B$上某个特征匹配或者被分配到dustbin，这就意味着每个dustbin有$N,M$个匹配，因此软分配矩阵有如下约束：</p><script type="math/tex; mode=display">\overline{\mathbf{P}} \mathbf{1}_{N+1}=\mathbf{a} \quad\text {  and } \quad \overline{\mathbf{P}}^{\top} \mathbf{1}_{M+1}=\mathbf{b}</script><p>其中$\mathbf{a}=\left[\begin{array}{ll}\mathbf{1}_{M}^{\top} &amp; N\end{array}\right]^{\top}$，$\mathbf{b}=\left[\begin{array}{ll}\mathbf{1}_{N}^{\top} &amp; M\end{array}\right]^{\top}$。（<font color="red">此处不太理解，最后一维为何为N？</font>）</p><h4 id="Sinkhorn-Algorithm"><a href="#Sinkhorn-Algorithm" class="headerlink" title="Sinkhorn Algorithm"></a>Sinkhorn Algorithm</h4><p>求解最大化总体得分可由“Sinkhorn Algorithm”[52,12]进行求解，此处并不作为重点讲解。</p><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>GNN网络以及最优匹配层都是可微的，这使得反向传播训练成为可能。网络训练使用了一种监督学习的方式，即有了匹配的真值$\mathcal{M}=\{(i, j)\} \subset \mathcal{A} \times \mathcal{B}$（如，由真值相对位姿变换得到的匹配关系），当然也可以获得一些没有匹配的特征点$\mathcal{I} \subseteq \mathcal{A}$以及$ \mathcal{J} \subseteq \mathcal{B}$。当给定真值标签，就可以去最小化分配矩阵$\overline{\mathbf{P}}$ 负对数似然函数：</p><script type="math/tex; mode=display">\begin{aligned} \operatorname{Loss}=&-\sum_{(i, j) \in \mathcal{M}} \log \overline{\mathbf{P}}_{i, j} \\ &-\sum_{i \in \mathcal{I}} \log \overline{\mathbf{P}}_{i, N+1}-\sum_{j \in \mathcal{J}} \log \overline{\mathbf{P}}_{M+1, j} \end{aligned}</script><p>这个监督学习的目标是同时最大化精度以及匹配的召回率，接下来的训练过程略过，直接开始实验阶段的介绍。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>特征匹配的目的是为了解算出两帧之间的相对位姿，所以实验对比的一个指标就是<strong>单应矩阵</strong>估计，另外还有室内外的位姿估计。只能说SuperGlue的效果太好了，直接放结果吧（本来论文7页就写完了，作者放了10页附录大招）。</p><h3 id="单应矩阵估计"><a href="#单应矩阵估计" class="headerlink" title="单应矩阵估计"></a>单应矩阵估计</h3><p>能够获得非常高的匹配召回率（98.3%）同时获得超高的精度，比传统的暴力匹配都好了一大截。</p><p><img alt="superglue_tb_1" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_tb_1.png"></p><h3 id="室内外位姿估计"><a href="#室内外位姿估计" class="headerlink" title="室内外位姿估计"></a>室内外位姿估计</h3><p>下表看来，大基线室内位姿估计也是相当棒，完胜传统算法。</p><p><img alt="superglue_tb_2" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_tb_2.png"></p><p><img alt="superglue_tb_3" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_tb_3.png"></p><h3 id="网络耗时"><a href="#网络耗时" class="headerlink" title="网络耗时"></a>网络耗时</h3><p>接下来放出大家比较关心的网络耗时，下图是在NVIDIA GeForce GTX 1080 GPU跑了500次的结果，512个点69ms（14.5fps），1024个点87ms（11.5fps）。</p><p><img alt="superglue_tb_3" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_11.png"></p><h3 id="更多匹配结果"><a href="#更多匹配结果" class="headerlink" title="更多匹配结果"></a>更多匹配结果</h3><p>第一列是SuperPoint+暴力匹配结果，第二列是SuperPoint+OAnet（ICCV 2019）结果，第三列是SuperPoint+SuperGlue结果。能看到SuperGlue惊人的特征匹配能力，尤其是在大视角变化时优势明显（红线表示错误匹配，绿线表示正确匹配）。</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_res_3.jpg"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_res_1.jpg"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_res_2.jpg"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文展示了基于注意力的图神经网络对局部特征匹配的强大功能。 SuperGlue的框架使用两种注意力：（i）自我注意力，可以增强局部描述符的接受力；以及（ii）交叉注意力，可以实现跨图像交流，并受到人类来回观察方式的启发进行匹配图像。文中方法通过解决<strong>最优运输问题</strong>，优雅地处理了特征分配问题以及遮挡点。实验表明，SuperGlue与现有方法相比有了显着改进，可以在极宽的基线室内和室外图像对上进行高精度的相对姿势估计。此外，SuperGlue可以实时运行，并且可以同时使用经典和深度学习特征。</p><p>总而言之，论文提出的可学习的中后端（middle-end）算法以功能强大的神经网络模型替代了手工启发式技术，该模型同时在单个统一体系结构中执行上下文聚合，匹配和过滤外点。作者最后提到：若与深度学习前端结合使用，SuperGlue是迈向端到端深度学习SLAM的重要里程碑。（when combined with a deep front-end, SuperGlue is a major milestone towards end-to-end deep SLAM）</p><p>这真是鼓舞SLAM研究人员的士气！</p><h2 id="附件"><a href="#附件" class="headerlink" title="附件"></a>附件</h2><ul><li><a href="https://github.com/magicleap/SuperGluePretrainedNetwork" target="_blank" rel="noopener">SuperGlue Github地址</a></li><li><a href="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/SuperGlue.pdf" target="_blank" rel="noopener">SuperGlue Paper</a></li><li>*<a href="https://image-matching-workshop.github.io/" target="_blank" rel="noopener">Image Matching: Local Features &amp; Beyond CVPR 2020 Workshop</a></li></ul><hr><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_01.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_02.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_03.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_04.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_05.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_06.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_07.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_08.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_09.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_10.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_11.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_12.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_13.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_14.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_15.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_16.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_17.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ETHZ ASL与Magicleap联名之作，CVPR 2020 Oral（论文见文末），一作是来自ETHZ的实习生，二作是当年CVPR2018 SuperPoint的作者Daniel DeTone。&lt;br&gt;&lt;!-- ![](/posts/superglue/freiburg_matches.gif) --&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://vincentqin.gitee.io/posts/superglue/freiburg_matches.gif&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="CV" scheme="https://www.vincentqin.tech/categories/CV/"/>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="Deep Learning" scheme="https://www.vincentqin.tech/tags/Deep-Learning/"/>
    
      <category term="特征提取" scheme="https://www.vincentqin.tech/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
      <category term="MagicLeap" scheme="https://www.vincentqin.tech/tags/MagicLeap/"/>
    
      <category term="深度学习" scheme="https://www.vincentqin.tech/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SuperGlue" scheme="https://www.vincentqin.tech/tags/SuperGlue/"/>
    
  </entry>
  
  <entry>
    <title>SLAM常见问题(五)：Singular Value Decomposition（SVD）分解</title>
    <link href="https://www.vincentqin.tech/posts/slam-common-issues-SVD/"/>
    <id>https://www.vincentqin.tech/posts/slam-common-issues-SVD/</id>
    <published>2019-08-18T11:12:28.000Z</published>
    <updated>2020-03-31T14:57:52.667Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>SVD分解就是一种矩阵拆解术，它能够把<strong>任意</strong>矩阵$A \in \mathbb{R}^{m \times n}$拆解成3个矩阵的乘积形式，即：</p><script type="math/tex; mode=display">A = U \Sigma V^T</script><p>其中，$U \in \mathbb{R}^{m \times m}$，$V \in \mathbb{R}^{n \times n}$都是正交矩阵，即列向量是正交的单位向量，$\Sigma \in \mathbb{R}^{m \times n}$的对角阵（奇异值）。搬运了来自MIT OpenCourseWare的在线课程并放在了B站，讲解得很清晰。</p><a id="more"></a><!-- <div id="dplayer2" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer2"),"loop":true,"video":{"url":"http://45.76.197.98:888/api/public/dl/0hkAga3Z/Singular-Value-Decomposition.mp4"},"danmaku":{"id":"bbe4286bf164ef6w1497f18a7b42ff944e6r4b821","api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> --><iframe src="//player.bilibili.com/player.html?aid=93275447&cid=159253510&page=15" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>刚才说了矩阵$U, \Sigma, V$的形式，视频中还提到了这三个矩阵的物理意义，即SVD分解可以理解为：任意矩阵都可以分解为<strong>(rotation)*(Stretch)*(rotation)</strong>的形式。接下来说明一下这三个矩阵是如何来的。</p><h2 id="计算-A-TA"><a href="#计算-A-TA" class="headerlink" title="计算 $A^TA$"></a>计算 $A^TA$</h2><script type="math/tex; mode=display">A^TA = (U \Sigma V^T)^TU \Sigma V^T = V{\Sigma}^TU^TU \Sigma V^T = V{\Sigma}^T \Sigma V^T</script><p>可见，$V$正是矩阵$A^TA$的特征向量，而${\Sigma}^T \Sigma $为矩阵$A^TA$的特征值。</p><h2 id="计算-AA-T"><a href="#计算-AA-T" class="headerlink" title="计算 $AA^T$"></a>计算 $AA^T$</h2><script type="math/tex; mode=display">AA^T = U \Sigma V^T(U \Sigma V^T)^T = U \Sigma V^TV{\Sigma}^TU^TU \Sigma V^T = U{\Sigma}^T \Sigma U^T</script><p>可见，$U$正是矩阵$AA^T$的特征向量，而${\Sigma}^T \Sigma $为矩阵$A^TA$的特征值。</p><p>所以$U, \Sigma, V$都可以通过上述方式来计算。</p><h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><script type="math/tex; mode=display">\Sigma = \left[    \begin{array}    {cccc|cccc}    {\sigma_{1}} & {0} & {\dots} & {0} & {0} & {0} & {\dots} & {0} \\     {0} & {\sigma_{2}} & {\dots} & {0} & {0} & {0} & {\dots} & {0} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {0} & {0} & {\dots} & {0} \\    {0} & {0} & {} & {\sigma_{k}} & {0} & {0} & {\dots} & {0} \\    \hline     {0} & {0} & {\dots} & {0} & {0} & {0} & {\dots} & {0} \\    {\vdots} & {\vdots} & {} & {\vdots} & {\vdots} & {\vdots} & {} & {\vdots} \\    {0} & {0} & {\dots} & {0} & {0} & {0} & {\dots} & {0}    \end{array}\right]_{m \times n}\Rightarrow\left[    \begin{array}    {cccc}    {\sigma_{1}} & {0} & {\dots} & {0} \\     {0} & {\sigma_{2}} & {\dots} & {0}  \\     {\vdots} & {\vdots} & {\ddots} & {\vdots}  \\    {0} & {0} & {} & {\sigma_{k}}    \end{array}\right]_{k \times k}</script><p>其中${\sigma}_1 \geq {\sigma}_2 \geq … {\sigma}_k &gt; 0 $，将$\Sigma$中主对角线为0的部分删去，同样的$U,V$对应的部分删去，SVD分解就变成了下图的形式。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-SVD/svd.png"></p><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><h3 id="数字例子"><a href="#数字例子" class="headerlink" title="数字例子"></a>数字例子</h3><p>有矩阵A，对其进行SVD分解，已知：</p><script type="math/tex; mode=display">A = \left[\begin{matrix}​    1 & 4 & 3 & 5 & 6  \cr ​    2 & 3 & {4} & 5 & 0  \cr ​    7 & 4 & 0 & 9 & 1  \cr  \end{matrix}\right]</script><p>计算$A^TA$以及$AA^T$：</p><script type="math/tex; mode=display">A^TA = \left[\begin{matrix}​    {54} & {38} & {11} & {78} & {13}  \cr ​    {38} & {41} & {24} & {71} & {28}  \cr ​    {11} & {24} & {25} & {35} & {18}  \cr ​    {78} & {71} & {35} & {131} & {39}  \cr ​    {13} & {28} & {18} & {39} & {37} \end{matrix}\right]\\A^TA = \left[\begin{matrix}​    {87} & {51} & {74}   \cr ​    {51} & {54} & {71}   \cr ​    {74} & {71} & {147}   \cr \end{matrix}\right]</script><p>对以上两式做特征值分解得到：</p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">V <span class="built_in">=</span></span><br><span class="line">    <span class="number">0.4269</span>    <span class="number">0.5222</span>    <span class="number">0.1760</span>   -<span class="number">0.5292</span>   -<span class="number">0.4839</span></span><br><span class="line">    <span class="number">0.4087</span>   -<span class="number">0.1757</span>   -<span class="number">0.0655</span>   -<span class="number">0.5258</span>    <span class="number">0.7221</span></span><br><span class="line">    <span class="number">0.2100</span>   -<span class="number">0.4474</span>   -<span class="number">0.7536</span>   -<span class="number">0.1512</span>   -<span class="number">0.4062</span></span><br><span class="line">    <span class="number">0.7389</span>    <span class="number">0.1520</span>   -<span class="number">0.0603</span>    <span class="number">0.6481</span>    <span class="number">0.0853</span></span><br><span class="line">    <span class="number">0.2464</span>   -<span class="number">0.6879</span>    <span class="number">0.6271</span>   -<span class="number">0.0258</span>   -<span class="number">0.2687</span></span><br><span class="line"></span><br><span class="line">U <span class="built_in">=</span></span><br><span class="line">    <span class="number">0.5095</span>    <span class="number">0.7999</span>    <span class="number">0.3171</span></span><br><span class="line">    <span class="number">0.4285</span>    <span class="number">0.0838</span>   -<span class="number">0.8997</span></span><br><span class="line">    <span class="number">0.7462</span>   -<span class="number">0.5942</span>    <span class="number">0.3001</span></span><br></pre></td></tr></table></figure><p>奇异值$\Sigma ^T \Sigma = \text{Diag}(238.2878, 37.3715, 12.3407) \Rightarrow \Sigma = \text{Diag}(15.4366, 6.1132, 3.5129)$</p><p>这与直接调用<code>svd(A)</code>结果是一致的（可能差个正负号）。</p><h3 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h3><p>祭上亲爱的Battle Angel Alita。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-SVD/alita_origin.jpg"></p><p>原始图像尺寸$1440\times 2560 $，我们可以对该图像做SVD分解，然后仅保留奇异值的前10，50，100重构图像，比较重构图像与原始图像的质量差异。可见仅仅保留其前10个奇异值时，图像质量遭到了极大破坏（此时仅保留原始图像信息的58.864%），随着奇异值数量的增多，图像质量也会逐渐提升，可以看到当奇异值个数为100时，基本上已经看不出与原图的差异（此时仅保留原始图像信息的87.37%）。由此，我们实现了图像压缩。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-SVD/alita_svd1.jpg"></p><p>下图是保留的奇异值数量与图像质量的关系图，保留的奇异值越多，图像质量越高，图像压缩效果越不明显；反之，奇异值越少，图像质量越差，图像压缩效果越明显。这只是一种非常简单的图像压缩算法，仅作原理验证使用，在实际中用到的概率不是很大。</p><p><img alt data-src="//www.vincentqin.tech/posts/slam-common-issues-SVD/alita_svd_quality.svg"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% simple test of using SVD decomposistion</span></span><br><span class="line">clear <span class="built_in">all</span>;</span><br><span class="line">close <span class="built_in">all</span>;</span><br><span class="line">clc;</span><br><span class="line"></span><br><span class="line"><span class="comment">% A = [1 4 3 5 6;</span></span><br><span class="line"><span class="comment">%          2 3 4 5 0;</span></span><br><span class="line"><span class="comment">%          7 4 0 9 1]</span></span><br><span class="line"><span class="comment">% A'*A;</span></span><br><span class="line"><span class="comment">% A*A';</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% [V,Dv] = eig(A'*A);</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% lambda = wrev(diag(Dv));</span></span><br><span class="line"><span class="comment">% V = fliplr(V)</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% [U,Du] = eig(A*A');</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% lambda = wrev(diag(Du));</span></span><br><span class="line"><span class="comment">% U = fliplr(U)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% LOADING IMAGE</span></span><br><span class="line">img <span class="built_in">=</span> imread(<span class="string">'alita_origin.png'</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ENERGE <span class="built_in">=</span> <span class="number">0</span>;</span><br><span class="line">for i <span class="built_in">=</span> <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">    [U(:,:,i) D(:,:,i) V(:,:,i)] <span class="built_in">=</span> svd(double(img(:,:,i)))  ;</span><br><span class="line">    ENERGE <span class="built_in">=</span> ENERGE +sum(diag(D(:,:,i)));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"><span class="comment">%% 10</span></span><br><span class="line">DIM <span class="built_in">=</span> <span class="number">10</span>;</span><br><span class="line">ENERGE10 <span class="built_in">=</span> <span class="number">0</span>;</span><br><span class="line">for i <span class="built_in">=</span> <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">    img_recons10(:,:,i) <span class="built_in">=</span> U(:,<span class="number">1</span>:DIM,i)*D(<span class="number">1</span>:DIM,<span class="number">1</span>:DIM,i)*V(:,<span class="number">1</span>:DIM,i)<span class="string">';</span></span><br><span class="line"><span class="string">     ENERGE10 = ENERGE10 +sum(diag(D(1:DIM,1:DIM,i)));</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">% figure;</span></span><br><span class="line"><span class="string">% imshow(mat2gray(img_recons10))</span></span><br><span class="line"><span class="string">% imwrite(mat2gray(img_recons10),'</span>alita_10.png<span class="string">');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">%% 50</span></span><br><span class="line"><span class="string">DIM = 50;</span></span><br><span class="line"><span class="string">ENERGE50 = 0;</span></span><br><span class="line"><span class="string">for i = 1:3</span></span><br><span class="line"><span class="string">    img_recons50(:,:,i) = U(:,1:DIM,i)*D(1:DIM,1:DIM,i)*V(:,1:DIM,i)'</span>;</span><br><span class="line">     ENERGE50 <span class="built_in">=</span> ENERGE50 +sum(diag(D(<span class="number">1</span>:DIM,<span class="number">1</span>:DIM,i)));</span><br><span class="line">end</span><br><span class="line"><span class="comment">% figure;</span></span><br><span class="line"><span class="comment">% imshow(mat2gray(img_recons50))</span></span><br><span class="line"><span class="comment">% imwrite(mat2gray(img_recons50),'alita_50.png');</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 100</span></span><br><span class="line">DIM <span class="built_in">=</span> <span class="number">100</span>;</span><br><span class="line">ENERGE100 <span class="built_in">=</span> <span class="number">0</span>;</span><br><span class="line">for i <span class="built_in">=</span> <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">    img_recons100(:,:,i) <span class="built_in">=</span> U(:,<span class="number">1</span>:DIM,i)*D(<span class="number">1</span>:DIM,<span class="number">1</span>:DIM,i)*V(:,<span class="number">1</span>:DIM,i)<span class="string">';</span></span><br><span class="line"><span class="string">    ENERGE100 = ENERGE100 +sum(diag(D(1:DIM,1:DIM,i)));</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">% figure;</span></span><br><span class="line"><span class="string">% imshow(mat2gray(img_recons100))</span></span><br><span class="line"><span class="string">% imwrite(mat2gray(img_recons100),'</span>alita_100.png<span class="string">');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">figure;</span></span><br><span class="line"><span class="string">set(gcf,'</span>pos<span class="string">',[ 986 414 1274 826])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">FONTSIZE = 15;</span></span><br><span class="line"><span class="string">h(1) = subplot(221);imshow(mat2gray(img)); </span></span><br><span class="line"><span class="string">xlabel('</span>origin Alita<span class="string">');set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">h(2) = subplot(222);imshow(mat2gray(img_recons10));</span></span><br><span class="line"><span class="string">xlabel(['</span>Using <span class="number">10</span> singular values: <span class="string">' num2str(ENERGE10/ENERGE)]);set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">h(3) = subplot(223);imshow(mat2gray(img_recons50));</span></span><br><span class="line"><span class="string">xlabel(['</span>Using <span class="number">50</span> singular values: <span class="string">' num2str(ENERGE50/ENERGE)]);set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">h(4) = subplot(224);imshow(mat2gray(img_recons100));</span></span><br><span class="line"><span class="string">xlabel(['</span>Using <span class="number">100</span> singular values: <span class="string">' num2str(ENERGE100/ENERGE)]);set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string">set(gcf,'</span>color<span class="string">',[1 1 1])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">%% SHOW ENERGY</span></span><br><span class="line"><span class="string">ENERGY_tmp = zeros(size(img,1),1);</span></span><br><span class="line"><span class="string">for DIM_ = 1:size(img,1)</span></span><br><span class="line"><span class="string">   for i = 1:3</span></span><br><span class="line"><span class="string">     ENERGY_tmp(DIM_,1) = ENERGY_tmp(DIM_,1) +sum(diag(D(1:DIM_,1:DIM_,i)));</span></span><br><span class="line"><span class="string">   end</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">figure;</span></span><br><span class="line"><span class="string">FONTSIZE = 30;</span></span><br><span class="line"><span class="string">ratio = ENERGY_tmp/ENERGE;</span></span><br><span class="line"><span class="string">X =   1:size(img,1);</span></span><br><span class="line"><span class="string">plot(X,ratio,'</span>linewidth<span class="string">',5,'</span>color<span class="string">','</span>r<span class="string">');</span></span><br><span class="line"><span class="string">set(gcf,'</span>color<span class="string">',[1 1 1])</span></span><br><span class="line"><span class="string">xlabel('</span>Number of Singular values<span class="string">');</span></span><br><span class="line"><span class="string">ylabel('</span>Image Quality<span class="string">');</span></span><br><span class="line"><span class="string">set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string">set(gcf,'</span>pos<span class="string">',[ 986 414 1274 826])</span></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="http://www-users.math.umn.edu/~lerman/math5467/svd.pdf" target="_blank" rel="noopener">A Singularly Valuable Decomposition The SVD of a Matrix</a></li><li>李宏毅关于SVD的介绍，<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/LA_2018/Lecture/SVD.pdf" target="_blank" rel="noopener">PPT</a>,<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_LA18.html" target="_blank" rel="noopener">课程列表</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SVD分解就是一种矩阵拆解术，它能够把&lt;strong&gt;任意&lt;/strong&gt;矩阵$A \in \mathbb{R}^{m \times n}$拆解成3个矩阵的乘积形式，即：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
A = U \Sigma V^T&lt;/script&gt;&lt;p&gt;其中，$U \in \mathbb{R}^{m \times m}$，$V \in \mathbb{R}^{n \times n}$都是正交矩阵，即列向量是正交的单位向量，$\Sigma \in \mathbb{R}^{m \times n}$的对角阵（奇异值）。搬运了来自MIT OpenCourseWare的在线课程并放在了B站，讲解得很清晰。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="SVD" scheme="https://www.vincentqin.tech/tags/SVD/"/>
    
      <category term="位姿" scheme="https://www.vincentqin.tech/tags/%E4%BD%8D%E5%A7%BF/"/>
    
  </entry>
  
  <entry>
    <title>SLAM常见问题(四)：求解ICP，利用SVD分解得到旋转矩阵</title>
    <link href="https://www.vincentqin.tech/posts/slam-common-issues-ICP/"/>
    <id>https://www.vincentqin.tech/posts/slam-common-issues-ICP/</id>
    <published>2019-08-18T03:43:04.000Z</published>
    <updated>2020-03-31T15:05:16.626Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>今天讲一篇关于利用<code>SVD</code>方法求解<code>ICP</code>问题的文献<a href="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-ICP/svd_rot.pdf" target="_blank" rel="noopener">《Least-Squares Rigid Motion Using SVD》</a>，这篇文章非常精彩地推导出将$3D$点对齐问题的解析解，同时总结了求解该问题的统一范式。</p><a id="more"></a><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>已知<script type="math/tex">{\mathcal{P}=\left\{\mathbf{p}_{1}, \mathbf{p}_{2}, \ldots, \mathbf{p}_{n}\right\}}</script>以及<script type="math/tex">{\mathcal{Q}=\left\{\mathbf{q}_{1}, \mathbf{q}_{2}, \ldots, \mathbf{q}_{n}\right\}}</script>是空间中（文中说的更加普适，<script type="math/tex">\mathbf{p}_i , \mathbf{q}_i \in \mathbb{R}^{d}</script>，可以表示$d$维空间）的匹配点集，我们试图找到这样的旋转矩阵$R$和平移向量$\mathbf{t}$最小化如下对齐误差（即<code>ICP</code>问题的形式）：</p><script type="math/tex; mode=display">(R, \mathbf{t})=\underset{R \in S O(d), \mathbf{t} \in \mathbb{R}^{d}}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left\|\left(R \mathbf{p}_{i}+\mathbf{t}\right)-\mathbf{q}_{i}\right\|^{2} \tag{1}</script><p>接下来文章分别推导了平移向量$\mathbf{t}$以及旋转矩阵$R$的解析解。</p><h2 id="计算平移量"><a href="#计算平移量" class="headerlink" title="计算平移量"></a>计算平移量</h2><p>此时假定旋转矩阵$R$是固定的，令<script type="math/tex">F(\mathbf{t}) = \sum_{i=1}^{n} w_{i}\left\|\left(R \mathbf{p}_{i}+\mathbf{t}\right)-\mathbf{q}_{i}\right\|^{2}</script>，我们可以通过$F$对$\mathbf{t}$求导的方式得到平移量的最优解，如下：</p><script type="math/tex; mode=display">\begin{aligned} 0 &=\frac{\partial F}{\partial \mathbf{t}}=\sum_{i=1}^{n} 2 w_{i}\left(R \mathbf{p}_{i}+\mathbf{t}-\mathbf{q}_{i}\right)=\\ &=2 \mathbf{t}\left(\sum_{i=1}^{n} w_{i}\right)+2 R\left(\sum_{i=1}^{n} w_{i} \mathbf{p}_{i}\right)-2 \sum_{i=1}^{n} w_{i} \mathbf{q}_{i}  \end{aligned} \tag{2}</script><p>令：</p><script type="math/tex; mode=display">\overline{\mathbf{p}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{p}_{i}}{\sum_{i=1}^{n} w_{i}},  \overline{\mathbf{q}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{q}_{i}}{\sum_{i=1}^{n} w_{i}} \tag{3}</script><p>于是我们得到$\mathbf{t}$的解：</p><script type="math/tex; mode=display">\mathbf{t} = \overline{\mathbf{q}} - R\overline{\mathbf{p}} \tag{4}</script><p>从上式看出最优的平移量$\mathbf{t}$将$\mathcal{P}$点集的加权中心映射到了$\mathcal{Q}$点集的中心。接下来将上式带入优化方程，得：</p><script type="math/tex; mode=display">\begin{aligned}\sum_{i=1}^{n} w_{i}\left\|\left(R \mathbf{p}_{i}+\mathbf{t}\right)-\mathbf{q}_{i}\right\|^{2} &= \sum_{i=1}^{n} w_{i}\left\| R \mathbf{p}_{i}+ \overline{\mathbf{q}} - R\overline{\mathbf{p}} -\mathbf{q}_{i}\right\|^{2}  \\ &= \sum_{i=1}^{n} w_{i}\left\|R (\mathbf{p}_{i} -\overline{\mathbf{p}}) - (\mathbf{q}_{i} - \overline{\mathbf{q}} ) \right\|^{2}\end{aligned} \tag{5}</script><p>由此我们将原问题转换成了无平移量的优化问题，令：</p><script type="math/tex; mode=display">\mathbf{x}_i := \mathbf{p}_{i} -\overline{\mathbf{p}}，\mathbf{y}_i := \mathbf{q}_{i} -\overline{\mathbf{q}}，\tag{6}</script><p>我们把问题简写成如下形式：</p><script type="math/tex; mode=display">R = \underset{R \in S O(d)}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2} \tag{7}</script><h2 id="计算旋转量"><a href="#计算旋转量" class="headerlink" title="计算旋转量"></a>计算旋转量</h2><p>简化上式：</p><script type="math/tex; mode=display">\begin{aligned}\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2} &= \left( R\mathbf{x}_i - \mathbf{y}_i\right)^T\left( R\mathbf{x}_i - \mathbf{y}_i\right)  = \left( \mathbf{x}_i^TR^T - \mathbf{y}_i^T\right)\left( R\mathbf{x}_i - \mathbf{y}_i\right)  \\&= \mathbf{x}_i^TR^TR\mathbf{x}_i - \mathbf{x}_i^TR^T\mathbf{y}_i - \mathbf{y}_i^TR\mathbf{x}_i +\mathbf{y}_i^T\mathbf{y}_i \end{aligned}\tag{8}</script><p>又因为旋转矩阵的正交性：$R^TR=I$；另外$ \mathbf{x}_i^TR^T\mathbf{y}_i$是标量：$\mathbf{x}_i$维度为$1 \times d$，$R^T$维度为$d \times d$，$\mathbf{y}_i$维度为$d \times 1$。于是有下式：</p><script type="math/tex; mode=display"> \mathbf{x}_i^TR^T\mathbf{y}_i = (\mathbf{x}_i^TR^T\mathbf{y}_i)^T = \mathbf{y}_i^TR\mathbf{x}_i \tag{9}</script><p>得：</p><script type="math/tex; mode=display">\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2} = \mathbf{x}_i^T\mathbf{x}_i -  2\mathbf{y}_i^TR\mathbf{x}_i +\mathbf{y}_i^T\mathbf{y}_i  \tag{10}</script><p>将整理好的上式带入简化后的$R$优化问题，得：</p><script type="math/tex; mode=display">\begin{aligned} & \underset{R \in S O(d)}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2}=\underset{R \in S O(d)}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left(\mathbf{x}_{i}^{\top} \mathbf{x}_{i}-2 \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}+\mathbf{y}_{i}^{\top} \mathbf{y}_{i}\right)=\\=& \underset{R \in S O(d)}{\operatorname{argmin}}\left(\sum_{i=1}^{n} w_{i} \mathbf{x}_{i}^{\top} \mathbf{x}_{i}-2 \sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}+\sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} \mathbf{y}_{i}\right)=\\=& \operatorname{argmin}_{R \in S O(d)}\left(-2 \sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}\right) \end{aligned}\tag{11}</script><p>接下来将要利用到如下关于迹的技巧:</p><script type="math/tex; mode=display">\begin{aligned}\left[\begin{array}{cccc}{w_1} \\ {} & {w_1} & {} &{} \\ {} & {} & {\ddots} & {}\\ {} & {} & {} & {w_n}\end{array}\right]\left[\begin{array}{ccc}{—}& {\mathbf{y}_1^T}&{—}   \\ {—}& {\mathbf{y}_2^T}&{—}   \\ {—}  & {\vdots} & {—}\\ {—}& {\mathbf{y}_n^T}&{—}\end{array}\right]\left[\begin{array}{ccc}{}& {} &{}   \\ {}& {R} &{}   \\ {}& {} &{}   \\ \end{array}\right]\left[\begin{array}{cccc}{|}& {|} &{|} &{|}  \\ {\mathbf{x}_1}& {\mathbf{x}_2} &{\dots} &{\mathbf{x}_n}  \\ {|}& {|} &{|} &{|}  \\ \end{array}\right]  \\=\left[\begin{array}{ccc}{—}& {w_1\mathbf{y}_1^T}&{—}   \\ {—}& {w_2\mathbf{y}_2^T}&{—}   \\ {—}  & {\vdots} & {—}\\ {—}& {w_n\mathbf{y}_n^T}&{—}\end{array}\right]\left[\begin{array}{cccc}{|}& {|} &{|} &{|}  \\ {R\mathbf{x}_1}& {R\mathbf{x}_2} &{\dots} &{R\mathbf{x}_n}  \\ {|}& {|} &{|} &{|}  \\ \end{array}\right] \\=\left[\begin{array}{cccc}{w_1\mathbf{y}_1^TR\mathbf{x}_1}& {} &{} &{*}  \\ {}& {w_2\mathbf{y}_2^TR\mathbf{x}_2} &{} &{}  \\ {}& {} &{\ddots} &{}  \\ {*}& {} &{} &{w_n\mathbf{y}_n^TR\mathbf{x}_n} \\ \end{array}\right]\end{aligned}</script><p>上式就是对<script type="math/tex">\sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i} =  \operatorname{tr}\left( WY^TRX\right)</script>的完美解释。</p><p>利用上式，式$(11)$可以整理得：</p><script type="math/tex; mode=display">\begin{aligned}\underset{R \in S O(d)}{\operatorname{argmin}}\left(-2 \sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}\right) &= \underset{R \in S O(d)}{\operatorname{argmax}}\left(\sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}\right) \\&= \underset{R \in S O(d)}{\operatorname{argmax}} \operatorname{tr}\left( WY^TRX\right)\end{aligned}\tag{12}</script><p>这里说明一下维度：$W = diag(w_1,w_2,…,w_n)$维度为$n \times n$，$Y^T$维度为$n \times d$，$R$维度为$d \times d$，$X$维度为$d \times n$。</p><p>接下来回顾一下迹的性质：$\operatorname{tr}(AB) = \operatorname{tr}(BA)$，因此有下式：</p><script type="math/tex; mode=display">\operatorname{tr}\left( WY^TRX\right) = \operatorname{tr}\left( (WY^T)(RX)\right) =\operatorname{tr}\left( RXWY^T\right) \tag{13}</script><p>令$d\times d$的“covariance”矩阵$S = XWY^T$，求$S$的<code>SVD</code>分解：</p><script type="math/tex; mode=display">S= U\Sigma V^T.\tag{14}</script><p>于是式$(13)$变为：</p><script type="math/tex; mode=display">\operatorname{tr}\left( WY^TRX\right) =\operatorname{tr}\left( RS\right) =\operatorname{tr}\left( RU\Sigma V^T\right)=\operatorname{tr}\left( \Sigma V^TRU\right) \tag{15}</script><p>由于$V,T,R$均为正交矩阵，因此$M = V^TRU$也是正交阵，也就是说$M$的列向量$\mathbf{m}_j$是互相正交的单位向量，即$\mathbf{m}_j^T\mathbf{m}_j=1$，于是：</p><script type="math/tex; mode=display">1=\mathbf{m}_{j}^{\top} \mathbf{m}_{j}=\sum_{i=1}^{d} m_{i j}^{2} \Rightarrow m_{i j}^{2} \leq 1 \Rightarrow\left|m_{i j}\right| \leq 1 \tag{16}</script><p>由于<code>SVD</code>分解的性质可知$\sigma$的元素均为非负数：${\sigma}_1,{\sigma}_2,{\sigma}_d \geq 0$，于是式$(18)$变为如下形式：</p><script type="math/tex; mode=display">\operatorname{tr}(\Sigma M)=\left(\begin{array}{ccccc}{\sigma_{1}} & {} & {} & {} & {} \\ {} & {\sigma_{2}} & {} & {} & {} \\ {} & {} & {\ddots} & {} & {} \\ {} & {} & {} & {} & {\sigma_{d}}\end{array}\right)\left(\begin{array}{cccc}{m_{11}} & {m_{12}} & {\dots} & {m_{1 d}} \\ {m_{21}} & {m_{22}} & {\dots} & {m_{2 d}} \\ {\vdots} & {\vdots} & {\vdots} & {\vdots} \\ {m_{d 1}} & {m_{d 2}} & {\dots} & {m_{d d}}\end{array}\right)=\sum_{i=1}^{d} \sigma_{i} m_{i i} \leq \sum_{i=1}^{d} \sigma_{i} \tag{17}</script><p>可见，当迹最大时$m_{ii} = 1 $，又由于$M$是正交阵，这使得$M$为单位阵！</p><script type="math/tex; mode=display">I = M = V^TRU \Rightarrow R = VU^T \tag{18}</script><p>看到没，R的解析解竟然如此简单，并且与<code>SVD</code>分解产生了联系，让人感觉到了数学的美妙。不过到这里还没完，后面作者进行了一步方向矫正，大意是这样的：利用公式$(18)$得到的矩阵并不一定是一个旋转矩阵，也可能为<code>反射矩阵</code>，此时可以通过验证$VU^T$的行列式来判断到底是旋转（行列式 = 1）还是反射（行列式 = -1）。但我们要求的是旋转矩阵，这时需要对公式$(18)$进行一步处理。</p><p>假设$\operatorname{det}(VU^T) = -1$，则限制$R$为旋转就意味着$M = V^TRU $为<code>反射矩阵</code>， 于是我们试图找到一个<code>反射矩阵</code>$M$最大化下式：</p><script type="math/tex; mode=display">\operatorname{tr}(\Sigma M) = {\sigma}_1 m_{11} + {\sigma}_2 m_{22} +...+ {\sigma}_d m_{dd} := f(m_{11},m_{11},...,m_{dd})  \tag{19}</script><p>即$f$是以<script type="math/tex">m_{11},m_{11},...,m_{dd}</script>为变量的线性函数，由于<script type="math/tex">m_{ii} \in \left[ -1,1\right]</script>，其极大值肯定在其定义域的边界处。于是当<script type="math/tex">{\forall} i, m_{ii} = 1</script>时，$f$取得极大值，但是此时的$R$为<code>反射矩阵</code>，所以并不能这样取值。然后我们看第二个极大值点$(1,1,…,-1)$，有：</p><script type="math/tex; mode=display">f = \operatorname{tr}(\Sigma M) = {\sigma}_1 + {\sigma}_2+...+ {\sigma}_{d-1} -  {\sigma}_d \tag{20}</script><p>这个值大于任何其它的自变量取值$(\pm 1,\pm 1,…,\pm 1)$的组合（除了$( 1, 1,…, 1)$），因为奇异值是经过排序的，${\sigma}_d$是最小的一个奇异值。</p><p>综上，为了将解转换为旋转矩阵要进行如下处理：</p><script type="math/tex; mode=display">R=V\left(\begin{array}{cccc}{1} \\ {} & {\ddots} & {} &{} \\ {} & {} & 1 & {}\\ {} & {} & {} & {\operatorname{det}\left(V U^{\top}\right)}\end{array}\right) U^{\top} \tag{21}</script><h2 id="可以总结的套路"><a href="#可以总结的套路" class="headerlink" title="可以总结的套路"></a>可以总结的套路</h2><p>为了得到<code>ICP</code>问题的最优解，我们可以采取如下套路：</p><p><strong>step1</strong>. 计算两组匹配点的加权中心：</p><script type="math/tex; mode=display">\overline{\mathbf{p}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{p}_{i}}{\sum_{i=1}^{n} w_{i}},  \overline{\mathbf{q}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{q}_{i}}{\sum_{i=1}^{n} w_{i}}</script><p><strong>step2</strong>. 得到去中心化的点集：</p><script type="math/tex; mode=display">\mathbf{x}_i := \mathbf{p}_{i} -\overline{\mathbf{p}}，\mathbf{y}_i := \mathbf{q}_{i} -\overline{\mathbf{q}}, i = 1,2...n</script><p><strong>step3</strong>. 计算$d \times d$的covariance矩阵：</p><script type="math/tex; mode=display">S = XWY^T</script><p>其中，$X,Y$为$d \times n$的矩阵，$\mathbf{x}_i,\mathbf{y}_i$分别是它们的列元素，另外$W = diag(w_1,w_2,…,w_n)$。</p><p><strong>step4</strong>. 对$S$进行<code>SVD</code>分解$S = U\Sigma V^T$，得到旋转矩阵：</p><script type="math/tex; mode=display">R=V\left(\begin{array}{cccc}{1} \\ {} & {\ddots} & {} &{} \\ {} & {} & 1 & {}\\ {} & {} & {} & {\operatorname{det}\left(V U^{\top}\right)}\end{array}\right) U^{\top}</script><p><strong>step5</strong>. 计算平移量：</p><script type="math/tex; mode=display">\mathbf{t} = \overline{\mathbf{q}} - R\overline{\mathbf{p}}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天讲一篇关于利用&lt;code&gt;SVD&lt;/code&gt;方法求解&lt;code&gt;ICP&lt;/code&gt;问题的文献&lt;a href=&quot;https://vincentqin.gitee.io/blogresource-3/slam-common-issues-ICP/svd_rot.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《Least-Squares Rigid Motion Using SVD》&lt;/a&gt;，这篇文章非常精彩地推导出将$3D$点对齐问题的解析解，同时总结了求解该问题的统一范式。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="位姿" scheme="https://www.vincentqin.tech/tags/%E4%BD%8D%E5%A7%BF/"/>
    
      <category term="ICP" scheme="https://www.vincentqin.tech/tags/ICP/"/>
    
  </entry>
  
  <entry>
    <title>SLAM常见问题(三)：PNP</title>
    <link href="https://www.vincentqin.tech/posts/slam-common-issues-PNP/"/>
    <id>https://www.vincentqin.tech/posts/slam-common-issues-PNP/</id>
    <published>2019-08-11T13:29:24.000Z</published>
    <updated>2020-03-31T15:31:47.349Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><code>PNP</code>即“Perspective-N-Points”，是求解 3D 到 2D 点对运动的方法。它描述了当我们知道n个3D空间点以及它们在图像上的位置时，如何估计相机所在的位姿。PnP 问题有很多种求解方法，例如用三对点估计位姿的 <code>P3P</code>（通常需要额外一个点进行验证结果），直接线性变换（<code>DLT</code>），<code>EPnP</code>（Efficient PnP，已知内参时用），<code>UPnP</code>（内参未知时用） 等等）。此外，还能用非线性优化的方式，构建最小二乘问题并迭代求解，也就是万金油式的 <code>Bundle Adjustment</code>。</p><a id="more"></a><h2 id="P3P"><a href="#P3P" class="headerlink" title="P3P"></a>P3P</h2><p>已知：$3D-2D$匹配点，$3D$点的<strong>世界坐标</strong>记为$A, B, C$，图像上的2D点记为$a, b, c$。</p><p>未知：<strong>相机系下3D点的坐标是未知的</strong>，即$OA,OB,OC$，一旦$ 3D$ 点在相机坐标系下的坐标能够算出，我们就得到了$3D-3D$的对应点，把<code>PnP</code>问题转换为了<code>ICP</code>问题。</p><p>我们的目标就是通过<strong>纯几何的方法</strong>求出上述未知量，过程如下。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-PNP/p3p.png"></p><p>由于余弦定理可知：</p><script type="math/tex; mode=display">\begin{array}{l}{O A^{2}+O B^{2}-2 O A \cdot O B \cdot \cos \langle a, b\rangle= A B^{2}} \\ {O B^{2}+O C^{2}-2 O B \cdot O C \cdot \cos \langle b, c\rangle= B C^{2}} \\ {O A^{2}+O C^{2}-2 O A \cdot O C \cdot \cos \langle a, c\rangle= A C^{2}}\end{array}</script><p>对上面三式全体除以$OC^{2}$，记$x=O A / O C, y=O B / O C$，得：</p><script type="math/tex; mode=display">\begin{array}{l}{x^{2}+y^{2}-2 x y \cos \langle a, b\rangle= A B^{2} / O C^{2}} \\ {y^{2}+1^{2}-2 y \cos \langle b, c\rangle= B C^{2} / O C^{2}} \\ {x^{2}+1^{2}-2 x \cos \langle a, c\rangle= A C^{2} / O C^{2}}\end{array}</script><p>记$v=A B^{2} / O C^{2}, u v=B C^{2} / O C^{2}, w v=A C^{2} / O C^{2}$，得：</p><script type="math/tex; mode=display">\begin{array}{l}{x^{2}+y^{2}-2 x y \cos \langle a, b\rangle- v=0} \\ {y^{2}+1^{2}-2 y \cos \langle b, c\rangle- u v=0} \\ {x^{2}+1^{2}-2 x \cos \langle a, c\rangle- w v=0}\end{array}</script><p>将第一个式子中$v = x^{2}+y^{2}-2 x y \cos \langle a, b\rangle$带入后面两个式子中，得：</p><script type="math/tex; mode=display">\begin{array}{l}{(1-u) y^{2}-u x^{2}-\cos \langle b, c\rangle y+2 u x y \cos \langle a, b\rangle+ 1=0} \\ {(1-w) x^{2}-w y^{2}-\cos \langle a, c\rangle x+2 w x y \cos \langle a, b\rangle+ 1=0}\end{array}</script><p>上式中几个余弦角度$\cos \langle a, b\rangle, \cos \langle b, c\rangle, \cos \langle a, c\rangle$是已知的，$u=B C^{2} / A B^{2}, w=A C^{2} / A B^{2}$也是已知的，所以未知量仅有$x,y$，解析地求解该方程组是一个复杂的过程，需要用<strong><a href="https://zh.wikipedia.org/wiki/%E5%90%B4%E6%B6%88%E5%85%83%E6%B3%95" target="_blank" rel="noopener">吴消元法</a></strong>。这样就可以求得$x,y$，然后带入$v = x^{2}+y^{2}-2 x y \cos \langle a, b\rangle$求解$v$，即可得到$OC$，进而得到$OB,OA$。该方程最多可能得到四个解，但我们可以用第4个验证点来计算最可能的解，得到$ A, B, C$ 在相机坐标系下的$3D$坐标。然后，根据$ 3D-3D $的点对，计算相机的运动 $R,t$，此处可参考文献<a href="https://igl.ethz.ch/projects/ARAP/svd_rot.pdf" target="_blank" rel="noopener">Least-Squares Rigid Motion Using SVD</a></p><h2 id="EPnP"><a href="#EPnP" class="headerlink" title="EPnP"></a>EPnP</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p><code>EPnP</code>即Efficient PnP，参考文献 <a href="https://icwww.epfl.ch/~lepetit/papers/lepetit_ijcv08.pdf" target="_blank" rel="noopener">EPnP: An Accurate O(n) Solution to the PnP Problem</a>。</p><p>问题描述如PnP，更加具体的，我们已知一组特征点，对于每个特征点$i$，我们有如下信息：</p><ul><li><p>特征点 $i$ 在世界坐标系的坐标<script type="math/tex">P_{i}^{w}=\left[\begin{array}{c}{x_{i}^{w}} \\ {y_{i}^{w}} \\ {z_{i}^{w}}\end{array}\right]</script></p></li><li><p>特征点在成像平面上的坐标<script type="math/tex">p_{i}=\left[\begin{array}{l}{u_{i}} \\ {v_{i}}\end{array}\right]</script></p></li><li>已知相机内参$K$</li></ul><p>求：世界坐标系到相机系的变换矩阵<script type="math/tex">T_{c w}=\left[\begin{array}{cc}{R_{c w}} & {t} \\ {0} & {1}\end{array}\right]</script></p><h3 id="算法假设"><a href="#算法假设" class="headerlink" title="算法假设"></a>算法假设</h3><p><code>EPnP</code>的思想是无论世界系还是相机系下的$3D$点都可以由<strong>4个控制点线性组合</strong>，记：</p><ul><li>世界系下4个控制点表示为:$\mathbf{c}_{j}^{w}, j=1, \cdots, 4$</li><li>相机系下4个控制点表示为:$\mathbf{c}_{j}^{c}, j=1, \cdots, 4$</li></ul><p>EPnP算法将参考点的坐标表示为控制点坐标的加权和：</p><script type="math/tex; mode=display">\mathbf{p}_{i}^{w}=\sum_{j=1}^{4} \alpha_{i j} \mathbf{c}_{j}^{w}, \text { with } \sum_{j=1}^{4} \alpha_{i j}=1</script><p>其中<script type="math/tex">\alpha_{i, j}, j=1, \cdots, 4</script>是加权系数，一旦虚拟控制点确定后，且满足4个控制点不共面的前提，<script type="math/tex">\alpha_{i, j}</script>是唯一的。</p><h3 id="控制点的存在性"><a href="#控制点的存在性" class="headerlink" title="控制点的存在性"></a>控制点的存在性</h3><p>现在讨论控制点的存在性，上式可以写成：</p><script type="math/tex; mode=display">\left[\begin{array}{c}{p_{i}^{w}} \\ {1}\end{array}\right]=\left[\begin{array}{cccc}{C_{1}^{w}} & {C_{2}^{w}} & {C_{3}^{w}} & {C_{4}^{w}} \\ {1} & {1} & {1} & {1}\end{array}\right] \alpha_{i} \stackrel{令}{=} C \alpha_{i}</script><p>可见只要$C$非奇异，就一定可以找到满足条件的$\alpha_{i} $，即：</p><script type="math/tex; mode=display">\left[\begin{array}{l}{\alpha_{i 1}} \\ {\alpha_{i 2}} \\ {\alpha_{i 3}} \\ {\alpha_{i 4}}\end{array}\right]=C^{-1}\left[\begin{array}{c}{\mathbf{p}_{i}^{w}} \\ {1}\end{array}\right]</script><p>接下来，我们讨论相机坐标系下，控制点和参考$3D$点之间的关系：</p><script type="math/tex; mode=display">p_{i}^{c}=R_{c w} p_{i}^{w}+t=R_{c w}\left(\sum_{j=1}^{4} \alpha_{i j} c_{i}^{w}\right)+t</script><p>由于<script type="math/tex">\sum_{j=1}^{4} \alpha_{i j}=1$，因此$t=\sum_{j=1}^{4} \alpha_{i j} t</script>,带入上式，得：</p><script type="math/tex; mode=display">p_{i}^{c}=\sum_{j=1}^{4} \alpha_{i j}\left(R_{c w} c_{i}^{w}\right)+t )=\sum_{j=1}^{4} \alpha_{i j} c_{i}^{c}</script><p>可见系数<script type="math/tex">\alpha_{i}</script>具有不变性，如果我们能够求出控制点在相机坐标系中的坐标<script type="math/tex">c_{1}^{c}, c_{2}^{c},c_{3}^{c},c_{4}^{c}</script>，那么对于任意一个3D点k，我们可以求得其在相机系下的坐标：<script type="math/tex">p_{k}^{c}=\sum_{j=1}^{4} \alpha_{k j} c_{i}^{c}</script>，这就变成了如P3P同样的问题了，即求解<code>3D-3D</code>位姿估计问题。</p><h3 id="如何选择控制点"><a href="#如何选择控制点" class="headerlink" title="如何选择控制点"></a>如何选择控制点</h3><p>记世界系下所有3D点集为<script type="math/tex">\left\{\mathbf{p}_{i}^{w}, i=1, \cdots, n\right\}</script>,第一个控制点是所有3D点的重心:</p><script type="math/tex; mode=display">\mathbf{c}_{1}^{w}=\frac{1}{n} \sum_{i=1}^{n} \mathbf{p}_{i}^{w}</script><p>对所有3D点去中心化，这些点罗列成矩阵形式：</p><script type="math/tex; mode=display">A=\left[\begin{array}{c}{\mathbf{p}_{1}^{w^{T}}-\mathbf{c}_{1}^{w^{T}}} \\ {\cdots} \\ {\mathbf{p}_{n}^{w^{T}}-\mathbf{c}_{1}^{w^{T}}}\end{array}\right]</script><p>对$A^TA$进行特征值分解（注意此时并非对A进行<code>SVD</code>分解，是为了减低时间复杂度，<code>SVD</code>分解的复杂度为$SO(3)$），其特征值为<script type="math/tex">\lambda_{c, i}, i=1,2,3</script>，对应的特征向量为<script type="math/tex">\mathbf{v}_{c, i}, i=1,2,3</script>，则剩余的3个控制点表示为如下公式：</p><script type="math/tex; mode=display">\mathbf{c}_{j}^{w}=\mathbf{c}_{1}^{w}+\lambda_{c, j-1}^{\frac{1}{2}} \mathbf{v}_{c, j-1}, j=2,3,4</script><h3 id="求解控制点在相机系下的坐标"><a href="#求解控制点在相机系下的坐标" class="headerlink" title="求解控制点在相机系下的坐标"></a>求解控制点在相机系下的坐标</h3><p>记<script type="math/tex">\left\{\mathbf{u}_{i}\right\}_{i=1, \cdots, n}</script>为相机下$3D$点<script type="math/tex">\left\{\mathbf{p}^c_{i}\right\}_{i=1, \cdots, n}</script>的图像坐标，则：</p><script type="math/tex; mode=display">\forall i, \quad w_{i}\left[\begin{array}{c}{\mathbf{u}_{i}} \\ {1}\end{array}\right]=K \mathbf{p}_{i}^{c}=K \sum_{j=1}^{4} \alpha_{i j} \mathbf{c}_{j}^{c}</script><p>其中<script type="math/tex">w_i</script>是尺度因子，将控制点<script type="math/tex">\mathbf{c}_{j}^{c}=\left[x_{j}^{c}, y_{j}^{c}, z_{j}^{c}\right]^{T}</script>带入上式，得：</p><script type="math/tex; mode=display">\forall i, \quad w_{i}\left[\begin{array}{c}{u_{i}} \\ {v_{i}} \\ {1}\end{array}\right]=\left[\begin{array}{ccc}{f_{u}} & {0} & {u_{c}} \\ {0} & {f_{v}} & {v_{c}} \\ {0} & {0} & {1}\end{array}\right] \sum_{j=1}^{4} \alpha_{i j}\left[\begin{array}{c}{x_{j}^{c}} \\ {y_{j}^{c}} \\ {z_{j}^{c}}\end{array}\right]</script><p>上式可以得到两个线性方程：</p><script type="math/tex; mode=display">\begin{array}{l}{\sum_{j=1}^{4} \alpha_{i j} f_{u} x_{j}^{c}+\alpha_{i j}\left(u_{c}-u_{i}\right) z_{j}^{c}=0} \\ {\sum_{j=1}^{4} \alpha_{i j} f_{v} y_{j}^{c}+\alpha_{i j}\left(v_{c}-v_{j}\right) z_{j}^{c}=0}\end{array}</script><p>把这N个点的约束罗列在一起，我们就可以得到如下矩阵：</p><script type="math/tex; mode=display">M\mathbf{x} = \mathbf{0}</script><p>其中<script type="math/tex">\mathbf{x}=\left[\mathbf{c}_{1}^{c \top}, \mathbf{c}_{2}^{c \top}, \mathbf{c}_{3}^{c \top}, \mathbf{c}_{4}^{c \top}\right]^{\top}</script>为<strong>12</strong>维向量，<script type="math/tex">\mathbf{M}</script>维度<script type="math/tex">2n\times 12</script>，如下形式:</p><p><img width="75%" data-src="https://cdn.mathpix.com/snip/images/XRV8zRc_TojEnL-nnDpP-eSDrWxXmwLRJ0zOt5FmAzg.original.fullsize.png"></p><h3 id="未完待续…"><a href="#未完待续…" class="headerlink" title="未完待续…"></a>未完待续…</h3><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://blog.csdn.net/jessecw79/article/details/82945918" target="_blank" rel="noopener">深入EPnP算法</a></li><li><a href="https://zhuanlan.zhihu.com/p/46695068" target="_blank" rel="noopener">3d-2d位姿估计之EPnP算法</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;PNP&lt;/code&gt;即“Perspective-N-Points”，是求解 3D 到 2D 点对运动的方法。它描述了当我们知道n个3D空间点以及它们在图像上的位置时，如何估计相机所在的位姿。PnP 问题有很多种求解方法，例如用三对点估计位姿的 &lt;code&gt;P3P&lt;/code&gt;（通常需要额外一个点进行验证结果），直接线性变换（&lt;code&gt;DLT&lt;/code&gt;），&lt;code&gt;EPnP&lt;/code&gt;（Efficient PnP，已知内参时用），&lt;code&gt;UPnP&lt;/code&gt;（内参未知时用） 等等）。此外，还能用非线性优化的方式，构建最小二乘问题并迭代求解，也就是万金油式的 &lt;code&gt;Bundle Adjustment&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="位姿" scheme="https://www.vincentqin.tech/tags/%E4%BD%8D%E5%A7%BF/"/>
    
      <category term="PNP" scheme="https://www.vincentqin.tech/tags/PNP/"/>
    
  </entry>
  
  <entry>
    <title>SLAM常见问题(二)：重定位Relocalisation</title>
    <link href="https://www.vincentqin.tech/posts/slam-common-issues-relocalisation/"/>
    <id>https://www.vincentqin.tech/posts/slam-common-issues-relocalisation/</id>
    <published>2019-08-08T14:51:14.000Z</published>
    <updated>2019-09-10T13:50:17.457Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>可以说整个重定位就是一个精心设计的解算当前帧位姿的模块，秉持着不抛弃不放弃的精神，ORB-SLAM的作者简直把特征匹配压榨到了极致，仿佛在说“小伙子你有很多匹配点的，不要放弃，我们优化一下位姿再找找匹配点呗”。</p><a id="more"></a><p>原理如下流程图：</p><p><img alt="重定位" data-src="//www.vincentqin.tech/posts/slam-common-issues-relocalisation/relocalisation.svg"></p><p>代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bool</span> Tracking::Relocalization()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Compute Bag of Words Vector</span></span><br><span class="line">    <span class="comment">// 步骤1：计算当前帧特征点的Bow映射，能够得到当前帧的词袋向量以及featureVector</span></span><br><span class="line">    <span class="comment">// 可用于SearchByBoW寻找匹配特征点</span></span><br><span class="line">    mCurrentFrame.ComputeBoW();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Relocalization is performed when tracking is lost</span></span><br><span class="line">    <span class="comment">// Track Lost: Query KeyFrame Database for keyframe candidates for relocalisation</span></span><br><span class="line">    <span class="comment">// 步骤2：找到与当前帧相似的候选关键帧，</span></span><br><span class="line">    <span class="comment">// 这里会通过查询关键帧数据库进行快速查找与当前帧相似的候选重定位帧vpCandidateKFs</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;KeyFrame*&gt; vpCandidateKFs = mpKeyFrameDB-&gt;DetectRelocalizationCandidates(&amp;mCurrentFrame);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(vpCandidateKFs.empty())</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> nKFs = vpCandidateKFs.size();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We perform first an ORB matching with each candidate</span></span><br><span class="line">    <span class="comment">// If enough matches are found we setup a PnP solver</span></span><br><span class="line">    <span class="function">ORBmatcher <span class="title">matcher</span><span class="params">(<span class="number">0.75</span>,<span class="literal">true</span>)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">vector</span>&lt;PnPsolver*&gt; vpPnPsolvers;</span><br><span class="line">    vpPnPsolvers.resize(nKFs);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;MapPoint*&gt; &gt; vvpMapPointMatches;</span><br><span class="line">    vvpMapPointMatches.resize(nKFs);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; vbDiscarded;</span><br><span class="line">    vbDiscarded.resize(nKFs);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> nCandidates=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nKFs; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        KeyFrame* pKF = vpCandidateKFs[i];</span><br><span class="line">        <span class="keyword">if</span>(pKF-&gt;isBad())</span><br><span class="line">            vbDiscarded[i] = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 步骤3：通过BoW进行匹配</span></span><br><span class="line">            <span class="comment">// 利用SearchByBoW查找当前帧与关键帧的匹配点vvpMapPointMatches</span></span><br><span class="line">            <span class="keyword">int</span> nmatches = matcher.SearchByBoW(pKF,mCurrentFrame,vvpMapPointMatches[i]);</span><br><span class="line">            <span class="comment">// 如果匹配点数小于15个点，跳过</span></span><br><span class="line">            <span class="keyword">if</span>(nmatches&lt;<span class="number">15</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                vbDiscarded[i] = <span class="literal">true</span>;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 如果匹配点数大于15个点，建立当前帧与关键帧之间的PNP求解器；</span></span><br><span class="line">            <span class="comment">// 仅仅如建立这个求解器，还未求解</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 初始化PnPsolver</span></span><br><span class="line">                PnPsolver* pSolver = <span class="keyword">new</span> PnPsolver(mCurrentFrame,vvpMapPointMatches[i]);</span><br><span class="line">                pSolver-&gt;SetRansacParameters(<span class="number">0.99</span>,<span class="number">10</span>,<span class="number">300</span>,<span class="number">4</span>,<span class="number">0.5</span>,<span class="number">5.991</span>);</span><br><span class="line">                vpPnPsolvers[i] = pSolver;</span><br><span class="line">                nCandidates++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Alternatively perform some iterations of P4P RANSAC</span></span><br><span class="line">    <span class="comment">// Until we found a camera pose supported by enough inliers</span></span><br><span class="line">    <span class="keyword">bool</span> bMatch = <span class="literal">false</span>;</span><br><span class="line">    <span class="function">ORBmatcher <span class="title">matcher2</span><span class="params">(<span class="number">0.9</span>,<span class="literal">true</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果候选关键帧数大于0且没有重定位成功</span></span><br><span class="line">    <span class="keyword">while</span>(nCandidates&gt;<span class="number">0</span> &amp;&amp; !bMatch)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nKFs; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 若当前候选关键帧与当前帧匹配数量小于15，跳过</span></span><br><span class="line">            <span class="keyword">if</span>(vbDiscarded[i])</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Perform 5 Ransac Iterations</span></span><br><span class="line">            <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; vbInliers;</span><br><span class="line">            <span class="keyword">int</span> nInliers;</span><br><span class="line">            <span class="keyword">bool</span> bNoMore;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 步骤4：通过EPnP算法估计初始位姿</span></span><br><span class="line">            PnPsolver* pSolver = vpPnPsolvers[i];</span><br><span class="line">            cv::Mat Tcw = pSolver-&gt;iterate(<span class="number">5</span>,bNoMore,vbInliers,nInliers);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// If Ransac reachs max. iterations discard keyframe</span></span><br><span class="line">            <span class="comment">// 若RANSAC失败，当前候选关键帧被提出候选帧</span></span><br><span class="line">            <span class="keyword">if</span>(bNoMore)</span><br><span class="line">            &#123;</span><br><span class="line">                vbDiscarded[i]=<span class="literal">true</span>;</span><br><span class="line">                nCandidates--;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// If a Camera Pose is computed, optimize</span></span><br><span class="line">            <span class="comment">// PNP求解出了一个比较初始的位姿，比较粗糙，需要进一步优化</span></span><br><span class="line">            <span class="keyword">if</span>(!Tcw.empty())</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 把刚刚PNP求解的位姿赋给当前帧位姿</span></span><br><span class="line">                Tcw.copyTo(mCurrentFrame.mTcw);</span><br><span class="line"></span><br><span class="line">                <span class="built_in">set</span>&lt;MapPoint*&gt; sFound;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> np = vbInliers.size();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;np; j++)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">if</span>(vbInliers[j])</span><br><span class="line">                    &#123;</span><br><span class="line">                        mCurrentFrame.mvpMapPoints[j]=vvpMapPointMatches[i][j];</span><br><span class="line">                        sFound.insert(vvpMapPointMatches[i][j]);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                        mCurrentFrame.mvpMapPoints[j]=<span class="literal">NULL</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 步骤5：通过PoseOptimization对姿态进行优化求解</span></span><br><span class="line">                <span class="keyword">int</span> nGood = Optimizer::PoseOptimization(&amp;mCurrentFrame);</span><br><span class="line">                <span class="comment">// 内点小于10，跳过</span></span><br><span class="line">                <span class="keyword">if</span>(nGood&lt;<span class="number">10</span>)</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                <span class="comment">// 刚才的PO优化会滤除一些外点</span></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> io =<span class="number">0</span>; io&lt;mCurrentFrame.N; io++)</span><br><span class="line">                    <span class="keyword">if</span>(mCurrentFrame.mvbOutlier[io])</span><br><span class="line">                        mCurrentFrame.mvpMapPoints[io]=<span class="keyword">static_cast</span>&lt;MapPoint*&gt;(<span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// If few inliers, search by projection in a coarse window and optimize again</span></span><br><span class="line">                <span class="comment">// 步骤6：如果内点较少，则通过投影的方式对之前未匹配的点进行匹配，再进行优化求解</span></span><br><span class="line">                <span class="comment">// 作者认为10&lt;=nGood&lt;50时仍有可能重定位成功，由于PO调整了位姿，</span></span><br><span class="line">                <span class="comment">// 可以通过位姿投影的方式将候选关键帧上的地图点投影在当前帧上进行搜索匹配点，</span></span><br><span class="line">                <span class="comment">// 从而增加匹配，然后再优化以得到足够多的内点</span></span><br><span class="line">                <span class="keyword">if</span>(nGood&lt;<span class="number">50</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">int</span> nadditional =matcher2.SearchByProjection(mCurrentFrame,vpCandidateKFs[i],sFound,<span class="number">10</span>,<span class="number">100</span>);</span><br><span class="line">                    <span class="comment">// 新增的点与之前PO内点之和大于50，我们考虑再进行一遍优化</span></span><br><span class="line">                    <span class="keyword">if</span>(nadditional+nGood&gt;=<span class="number">50</span>)</span><br><span class="line">                    &#123;</span><br><span class="line">                        nGood = Optimizer::PoseOptimization(&amp;mCurrentFrame);</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// If many inliers but still not enough, search by projection again in a narrower window</span></span><br><span class="line">                        <span class="comment">// the camera has been already optimized with many points</span></span><br><span class="line">                        <span class="comment">// 不够多呀，不要放弃，再来一遍</span></span><br><span class="line">                        <span class="keyword">if</span>(nGood&gt;<span class="number">30</span> &amp;&amp; nGood&lt;<span class="number">50</span>)</span><br><span class="line">                        &#123;</span><br><span class="line">                            sFound.clear();</span><br><span class="line">                            <span class="keyword">for</span>(<span class="keyword">int</span> ip =<span class="number">0</span>; ip&lt;mCurrentFrame.N; ip++)</span><br><span class="line">                                <span class="keyword">if</span>(mCurrentFrame.mvpMapPoints[ip])</span><br><span class="line">                                    sFound.insert(mCurrentFrame.mvpMapPoints[ip]);</span><br><span class="line">                            nadditional =matcher2.SearchByProjection(mCurrentFrame,vpCandidateKFs[i],sFound,<span class="number">3</span>,<span class="number">64</span>);</span><br><span class="line"></span><br><span class="line">                            <span class="comment">// Final optimization</span></span><br><span class="line">                            <span class="comment">// 最后一次优化啦~</span></span><br><span class="line">                            <span class="keyword">if</span>(nGood+nadditional&gt;=<span class="number">50</span>)</span><br><span class="line">                            &#123;</span><br><span class="line">                                nGood = Optimizer::PoseOptimization(&amp;mCurrentFrame);</span><br><span class="line"></span><br><span class="line">                                <span class="keyword">for</span>(<span class="keyword">int</span> io =<span class="number">0</span>; io&lt;mCurrentFrame.N; io++)</span><br><span class="line">                                    <span class="keyword">if</span>(mCurrentFrame.mvbOutlier[io])</span><br><span class="line">                                        mCurrentFrame.mvpMapPoints[io]=<span class="literal">NULL</span>;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// If the pose is supported by enough inliers stop ransacs and continue</span></span><br><span class="line">                <span class="comment">// 只要找到一个候选关键帧与当前帧的匹配点数大于50就重定位成功！</span></span><br><span class="line">                <span class="keyword">if</span>(nGood&gt;=<span class="number">50</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    bMatch = <span class="literal">true</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(!bMatch)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        mnLastRelocFrameId = mCurrentFrame.mnId;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;可以说整个重定位就是一个精心设计的解算当前帧位姿的模块，秉持着不抛弃不放弃的精神，ORB-SLAM的作者简直把特征匹配压榨到了极致，仿佛在说“小伙子你有很多匹配点的，不要放弃，我们优化一下位姿再找找匹配点呗”。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="ORB" scheme="https://www.vincentqin.tech/tags/ORB/"/>
    
      <category term="Relocalisation" scheme="https://www.vincentqin.tech/tags/Relocalisation/"/>
    
      <category term="重定位" scheme="https://www.vincentqin.tech/tags/%E9%87%8D%E5%AE%9A%E4%BD%8D/"/>
    
  </entry>
  
  <entry>
    <title>SLAM常见问题(一)：SearchByBoW</title>
    <link href="https://www.vincentqin.tech/posts/slam-common-issues-SearchbyBoW/"/>
    <id>https://www.vincentqin.tech/posts/slam-common-issues-SearchbyBoW/</id>
    <published>2019-08-04T15:18:14.000Z</published>
    <updated>2019-08-04T16:12:16.670Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>ORB-SLAM</strong>中使用了多种特征匹配的奇技淫巧，其中之一就是利用<strong>词袋信息</strong>进行引导匹配<code>SearchByBoW</code>：利用了<code>BOW</code>里的正向引导进行两帧之间的匹配，核心点在于位于同一个节点处的特征才有可能属于同一匹配，相较于暴力匹配匹配速度更快。</p><a id="more"></a><p>注意：每幅图像都可以通过<code>ComputeBoW</code>得到其对应的词袋向量。<code>featureVector</code>存储的是节点的索引值以及对应图像feature对应的索引向量，即<code>map&lt;node_id,vector&lt;featureID&gt;</code>。这样的话就可以根据两帧图像的<code>node_id</code>来初步确定二者共有的特征点，然后根据该<code>id</code>取出<code>vector&lt;featureID&gt;</code>，根据featureID找到图像上的特征点以及描述子，通过比较二者描述子距离来判定该特征点是否为匹配点，若距离小于某一阈值，则二者为匹配对。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief Bag of Words Representation</span></span><br><span class="line"><span class="comment"> * 计算词袋mBowVec和mFeatVec</span></span><br><span class="line"><span class="comment"> * @see CreateInitialMapMonocular() TrackReferenceKeyFrame() Relocalization()</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//同样的，关键帧也有构造函数void KeyFrame::ComputeBoW()</span></span><br><span class="line"><span class="keyword">void</span> Frame::ComputeBoW() </span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(mBowVec.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 将描述子mDescriptors转换为DBOW要求的输入格式</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;cv::Mat&gt; vCurrentDesc = Converter::toDescriptorVector(mDescriptors);</span><br><span class="line">        <span class="comment">// 转换成词袋向量mBowVec以及特征向量mFeatVec</span></span><br><span class="line">        <span class="comment">// mBowVec存储着单词及其对应的权重TF-IDF值</span></span><br><span class="line">        <span class="comment">// mFeatVec存储节点ID以及对应对应图像feature对应的索引向量</span></span><br><span class="line">        mpORBvocabulary-&gt;transform(vCurrentDesc,mBowVec,mFeatVec,<span class="number">4</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该函数在<code>Tracking</code>线程中的<code>TrackReferenceKeyFrame()</code>/<code>Relocalization()</code>进行调用（注意：<code>LoopClosing</code>线程中<code>ComputeSim3()</code>也会调用该函数，与上述二者的区别在于，ComputeSim3()中的SearchByBoW是寻找关键帧之间的匹配，而非关键帧与当前帧之间的匹配）。</p><p>下面给出<code>ORB-SLAM2</code>中用于<strong>关键帧与当前帧</strong>进行词袋引导匹配的源码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief 通过词袋，对关键帧的特征点进行跟踪</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 通过bow对pKF和F中的特征点进行快速匹配（不属于同一node的特征点直接跳过匹配） \n</span></span><br><span class="line"><span class="comment"> * 对属于同一node的特征点通过描述子距离进行匹配 \n</span></span><br><span class="line"><span class="comment"> * 根据匹配，用pKF中特征点对应的MapPoint去更新F中特征点对应的MapPoints \n</span></span><br><span class="line"><span class="comment"> * 每个特征点都对应一个MapPoint，因此pKF中每个特征点的MapPoint也就是F中对应点的MapPoint \n</span></span><br><span class="line"><span class="comment"> * 通过距离阈值、比例阈值和角度投票进行剔除误匹配</span></span><br><span class="line"><span class="comment"> * @param  pKF               KeyFrame</span></span><br><span class="line"><span class="comment"> * @param  F                 Current Frame</span></span><br><span class="line"><span class="comment"> * @param  vpMapPointMatches F中MapPoints对应的匹配，NULL表示未匹配</span></span><br><span class="line"><span class="comment"> * @return                   成功匹配的数量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> <span class="comment">//const int ORBmatcher::TH_HIGH = 100;</span></span><br><span class="line"> <span class="comment">//const int ORBmatcher::TH_LOW = 50;</span></span><br><span class="line"> <span class="comment">//const int ORBmatcher::HISTO_LENGTH = 30;</span></span><br><span class="line"><span class="keyword">int</span> ORBmatcher::SearchByBoW(KeyFrame* pKF,Frame &amp;F, <span class="built_in">vector</span>&lt;MapPoint*&gt; &amp;vpMapPointMatches)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// vpMapPointsKF：获取输入关键帧匹配到的地图点</span></span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;MapPoint*&gt; vpMapPointsKF = pKF-&gt;GetMapPointMatches();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化当前帧MapPoints对应的匹配NULL</span></span><br><span class="line">    vpMapPointMatches = <span class="built_in">vector</span>&lt;MapPoint*&gt;(F.N,<span class="keyword">static_cast</span>&lt;MapPoint*&gt;(<span class="literal">NULL</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// FeatureVector数据类型 map&lt;node_id,vector&lt;featureID&gt;，</span></span><br><span class="line">    <span class="comment">// 可以快速根据node_id找到属于该node的特征点</span></span><br><span class="line">    <span class="keyword">const</span> DBoW2::FeatureVector &amp;vFeatVecKF = pKF-&gt;mFeatVec;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> nmatches=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; rotHist[HISTO_LENGTH];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;HISTO_LENGTH;i++)</span><br><span class="line">        rotHist[i].reserve(<span class="number">500</span>);</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span> factor = HISTO_LENGTH/<span class="number">360.0f</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We perform the matching over ORB that belong to the same vocabulary node </span></span><br><span class="line">    <span class="comment">// (at a certain level)</span></span><br><span class="line">    <span class="comment">// 建立迭代器，将属于同一节点(特定层)的ORB特征进行匹配</span></span><br><span class="line">    DBoW2::FeatureVector::const_iterator KFit = vFeatVecKF.begin();</span><br><span class="line">    DBoW2::FeatureVector::const_iterator Fit = F.mFeatVec.begin();</span><br><span class="line">    DBoW2::FeatureVector::const_iterator KFend = vFeatVecKF.end();</span><br><span class="line">    DBoW2::FeatureVector::const_iterator Fend = F.mFeatVec.end();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(KFit != KFend &amp;&amp; Fit != Fend)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 步骤1：分别取出属于同一node的ORB特征点(只有属于同一node，才有可能是匹配点)</span></span><br><span class="line">        <span class="comment">// first表示node_id，只有node_id相同才表示这些特征点位于同一层</span></span><br><span class="line">        <span class="keyword">if</span>(KFit-&gt;first == Fit-&gt;first) </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// second中记录了这些特征对应图像中的ID</span></span><br><span class="line">            <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt; vIndicesKF = KFit-&gt;second;</span><br><span class="line">            <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt; vIndicesF = Fit-&gt;second;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 步骤2：遍历KF中属于该node的特征点</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">size_t</span> iKF=<span class="number">0</span>; iKF&lt;vIndicesKF.size(); iKF++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 获取关键帧上某一个特征点的ID</span></span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> realIdxKF = vIndicesKF[iKF];</span><br><span class="line">                <span class="comment">// 根据该ID得到该特征对应的MapPoint</span></span><br><span class="line">                MapPoint* pMP = vpMapPointsKF[realIdxKF]; </span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(!pMP) <span class="comment">//不存在</span></span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(pMP-&gt;isBad())<span class="comment">//被标记为坏点</span></span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 根据该ID取出KF中该特征对应的描述子</span></span><br><span class="line">                <span class="keyword">const</span> cv::Mat &amp;dKF= pKF-&gt;mDescriptors.row(realIdxKF); </span><br><span class="line"></span><br><span class="line">                <span class="keyword">int</span> bestDist1=<span class="number">256</span>; <span class="comment">// 最好的距离（最小距离）</span></span><br><span class="line">                <span class="keyword">int</span> bestIdxF =<span class="number">-1</span> ;</span><br><span class="line">                <span class="keyword">int</span> bestDist2=<span class="number">256</span>; <span class="comment">// 倒数第二好距离（倒数第二小距离）</span></span><br><span class="line"></span><br><span class="line">                <span class="comment">// 步骤3：遍历当前帧中属于该node的特征点，找到了最佳匹配点</span></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">size_t</span> iF=<span class="number">0</span>; iF&lt;vIndicesF.size(); iF++)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// 取出当前帧上位于该node上的某一个特征点的ID</span></span><br><span class="line">                    <span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> realIdxF = vIndicesF[iF];</span><br><span class="line">                    <span class="comment">// 表明这个点已经被匹配过了，不再匹配，加快速度</span></span><br><span class="line">                    <span class="keyword">if</span>(vpMapPointMatches[realIdxF])</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    <span class="comment">// 取出F中该特征对应的描述子</span></span><br><span class="line">                    <span class="keyword">const</span> cv::Mat &amp;dF = F.mDescriptors.row(realIdxF); </span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 计算描述子距离，这里是汉明距离，若非二进制描述子可选择用其他距离</span></span><br><span class="line">                    <span class="keyword">const</span> <span class="keyword">int</span> dist =  DescriptorDistance(dKF,dF); </span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 下面的操作就是分别获得最小bestDist1以及次小bestDist2的描述子距离</span></span><br><span class="line">                    <span class="comment">// dist &lt; bestDist1 &lt; bestDist2，更新bestDist1 bestDist2</span></span><br><span class="line">                    <span class="keyword">if</span>(dist&lt;bestDist1)</span><br><span class="line">                    &#123;</span><br><span class="line">                        bestDist2=bestDist1;</span><br><span class="line">                        bestDist1=dist;</span><br><span class="line">                        bestIdxF=realIdxF;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span>(dist&lt;bestDist2)<span class="comment">// bestDist1 &lt; dist &lt; bestDist2，更新bestDist2</span></span><br><span class="line">                    &#123;</span><br><span class="line">                        bestDist2=dist;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 步骤4：根据描述子距离阈值和角度投票剔除误匹配</span></span><br><span class="line">                <span class="comment">// 最小的描述子距离小于一个阈值 </span></span><br><span class="line">                <span class="keyword">if</span>(bestDist1&lt;=TH_LOW) </span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// trick!</span></span><br><span class="line">                    <span class="comment">// 最佳匹配比次佳匹配明显要好，那么最佳匹配才真正靠谱</span></span><br><span class="line">                    <span class="keyword">if</span>(<span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(bestDist1)&lt;</span><br><span class="line">                    mfNNratio*<span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(bestDist2))</span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="comment">// 步骤5：更新当前帧特征点的MapPoint</span></span><br><span class="line">                        <span class="comment">// 记录了特征点ID以及对应的特征点</span></span><br><span class="line">                        vpMapPointMatches[bestIdxF]=pMP;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 获得关键帧上的特征点位置</span></span><br><span class="line">                        <span class="keyword">const</span> cv::KeyPoint &amp;kp = pKF-&gt;mvKeysUn[realIdxKF];</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//</span></span><br><span class="line">                        <span class="keyword">if</span>(mbCheckOrientation)</span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="comment">// trick!</span></span><br><span class="line">                            <span class="comment">// angle：每个特征点在提取描述子时的旋转主方向角度，</span></span><br><span class="line">                            <span class="comment">// 如果图像旋转了，这个角度将发生改变</span></span><br><span class="line">                            <span class="comment">// 所有的特征点的角度变化应该是一致的，</span></span><br><span class="line">                            <span class="comment">// 通过直方图统计得到最准确的角度变化值</span></span><br><span class="line">                            <span class="comment">// 该特征点的角度变化值</span></span><br><span class="line">                            <span class="keyword">float</span> rot = kp.angle-F.mvKeys[bestIdxF].angle;</span><br><span class="line">                            <span class="keyword">if</span>(rot&lt;<span class="number">0.0</span>)</span><br><span class="line">                                rot+=<span class="number">360.0f</span>;</span><br><span class="line">                            <span class="keyword">int</span> bin = round(rot*factor);<span class="comment">// 将rot分配到bin组</span></span><br><span class="line">                            <span class="keyword">if</span>(bin==HISTO_LENGTH)</span><br><span class="line">                                bin=<span class="number">0</span>;</span><br><span class="line">                            assert(bin&gt;=<span class="number">0</span> &amp;&amp; bin&lt;HISTO_LENGTH);</span><br><span class="line">                            rotHist[bin].push_back(bestIdxF);</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="comment">// 匹配点+1</span></span><br><span class="line">                        nmatches++;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            KFit++;</span><br><span class="line">            Fit++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(KFit-&gt;first &lt; Fit-&gt;first)</span><br><span class="line">        &#123;</span><br><span class="line">            KFit = vFeatVecKF.lower_bound(Fit-&gt;first);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            Fit = F.mFeatVec.lower_bound(KFit-&gt;first);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据方向剔除误匹配的点，即删除那些不属于特征点角度变化最多的三个类别的匹配点</span></span><br><span class="line">    <span class="keyword">if</span>(mbCheckOrientation)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> ind1=<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">int</span> ind2=<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">int</span> ind3=<span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 计算rotHist中最大的三个的index</span></span><br><span class="line">        ComputeThreeMaxima(rotHist,HISTO_LENGTH,ind1,ind2,ind3);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;HISTO_LENGTH; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果特征点的旋转角度变化量属于这三个组，则保留</span></span><br><span class="line">            <span class="keyword">if</span>(i==ind1 || i==ind2 || i==ind3)</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 将除了ind1 ind2 ind3以外的匹配点去掉</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">size_t</span> j=<span class="number">0</span>, jend=rotHist[i].size(); j&lt;jend; j++)</span><br><span class="line">            &#123;</span><br><span class="line">                vpMapPointMatches[rotHist[i][j]]=<span class="keyword">static_cast</span>&lt;MapPoint*&gt;(<span class="literal">NULL</span>);</span><br><span class="line">                nmatches--;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nmatches;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外，<code>LoopClosing</code>线程中<code>ComputeSim3()</code>调用的<code>SearchByBoW</code>的函数声明为：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> ORBmatcher::SearchByBoW(KeyFrame *pKF1, KeyFrame *pKF2, <span class="built_in">vector</span>&lt;MapPoint *&gt; &amp;vpMatches12)</span><br></pre></td></tr></table></figure></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://blog.csdn.net/qq_24893115/article/details/52629248" target="_blank" rel="noopener">https://blog.csdn.net/qq_24893115/article/details/52629248</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;ORB-SLAM&lt;/strong&gt;中使用了多种特征匹配的奇技淫巧，其中之一就是利用&lt;strong&gt;词袋信息&lt;/strong&gt;进行引导匹配&lt;code&gt;SearchByBoW&lt;/code&gt;：利用了&lt;code&gt;BOW&lt;/code&gt;里的正向引导进行两帧之间的匹配，核心点在于位于同一个节点处的特征才有可能属于同一匹配，相较于暴力匹配匹配速度更快。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="ORB" scheme="https://www.vincentqin.tech/tags/ORB/"/>
    
      <category term="特征匹配" scheme="https://www.vincentqin.tech/tags/%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D/"/>
    
  </entry>
  
  <entry>
    <title>2019年浙大CADCG暑假SLAM培训部分课件</title>
    <link href="https://www.vincentqin.tech/posts/slam-summer-courses-CADCG-Lab/"/>
    <id>https://www.vincentqin.tech/posts/slam-summer-courses-CADCG-Lab/</id>
    <published>2019-07-20T13:52:15.000Z</published>
    <updated>2020-04-17T13:35:50.309Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>由浙江大学CAD&amp;CG国家重点实验室主办、浙江大学-商汤三维视觉联合实验室协办的“SLAM技术及应用”暑期学校于7月20日如期拉开序幕。<br>今天（2019/07/20）看到直播的时候已经是下午4点半了，只听到刘浩敏讲到末尾的一段，幸好主办方提供了讲座课件，Download下来慢慢看。</p><a id="more"></a><h2 id="2019年课件"><a href="#2019年课件" class="headerlink" title="2019年课件"></a>2019年课件</h2><ul><li><p>2019年7月20日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/Camera-model-and-Projection-Transformation.pdf" target="_blank" rel="noopener">相机模型与投影变换</a>（讲者：章国锋）</p></li><li><p>2019年7月20日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/SfM-GuofengZhang.pdf" target="_blank" rel="noopener">运动恢复结构</a>（讲者：章国锋）</p></li><li><p>2019年7月20日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/BA-haominLiu.pdf" target="_blank" rel="noopener">集束<strong>Bundle Adjustment</strong>调整</a>（讲者：刘浩敏）</p></li><li><p>2019年7月21日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/3D-Tutorial-ShuhanShen.pdf" target="_blank" rel="noopener">三维重建</a>（讲者：申抒含）</p></li><li><p>2019年7月21日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/V-SLAM-GuofengZhang.pdf" target="_blank" rel="noopener">视觉SLAM</a>（讲者：章国锋）</p></li><li><p>2019年7月21日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/RGB-D-SLAM-HanqingJiang.pdf" target="_blank" rel="noopener">RGB-D SLAM</a>（讲者：姜翰青）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/VI-SLAM.pdf" target="_blank" rel="noopener">视觉惯性SLAM</a>（讲者：黄国权）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/3D-recognition-track-XueyingQin.pdf" target="_blank" rel="noopener">三维物体的识别与跟踪</a>（讲者：秦学英）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/R2VR-VR-Fusion-Technology.pdf" target="_blank" rel="noopener">从现实到虚拟现实-虚实融合呈现技术</a>（讲者：王锐）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/Mav-DanpingZou.pdf" target="_blank" rel="noopener">面向SLAM研究的无人机快速入门与平台选择</a>（讲者：邹丹平）</p></li><li><p>2019年7月23日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/MobileVR-System-Design-Application.pdf" target="_blank" rel="noopener">移动增强现实系统的设计与应用案例解析</a>（讲者：章国锋）</p></li><li><p>2019年7月23日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/AR-Applications.pdf" target="_blank" rel="noopener">AR应用开发</a>（讲者：盛崇山）</p></li><li><p><strong><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2019-SLAM-Summer-School-slides.zip" target="_blank" rel="noopener">打包下载</a></strong></p></li></ul><h2 id="2018年课件"><a href="#2018年课件" class="headerlink" title="2018年课件"></a>2018年课件</h2><ul><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E7%9B%B8%E6%9C%BA%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%8A%95%E5%BD%B1%E5%8F%98%E6%8D%A2-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">相机模型与投影变换-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E8%BF%90%E5%8A%A8%E6%81%A2%E5%A4%8D%E7%BB%93%E6%9E%84-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">运动恢复结构-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E9%9B%86%E6%9D%9F%E8%B0%83%E6%95%B4-%E5%88%98%E6%B5%A9%E6%95%8F.pdf" target="_blank" rel="noopener">集束调整-刘浩敏</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E6%B7%B1%E5%BA%A6%E6%81%A2%E5%A4%8D%E4%B8%8E%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">深度恢复与三维重建-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E8%A7%86%E8%A7%89SLAM-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">视觉SLAM-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E8%A7%86%E8%A7%89%E6%83%AF%E5%AF%BCSLAM-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">视觉惯导SLAM-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/Visual-Inertial%20SLAM-%E6%9D%8E%E5%90%8D%E6%9D%A8.pdf" target="_blank" rel="noopener">Visual-Inertial SLAM-李名杨</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/RGB-D%20SLAM-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">RGB-D SLAM-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E5%9C%B0%E9%9D%A2%E6%97%A0%E4%BA%BA%E5%B9%B3%E5%8F%B0%E4%B8%AD%E7%9A%84SLAM%E6%8A%80%E6%9C%AF-%E5%88%98%E5%8B%87.pdf" target="_blank" rel="noopener">地面无人平台中的SLAM技术-刘勇</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E5%9F%BA%E4%BA%8E%E7%8E%AF%E5%A2%83%E7%BB%93%E6%9E%84%E5%8C%96%E7%89%B9%E6%80%A7%E7%9A%84%E8%A7%86%E8%A7%89SLAM%E6%96%B9%E6%B3%95-%E9%82%B9%E4%B8%B9%E5%B9%B3.pdf" target="_blank" rel="noopener">基于环境结构化特性的视觉SLAM方法-邹丹平</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E7%A7%BB%E5%8A%A8%E5%A2%9E%E5%BC%BA%E7%8E%B0%E5%AE%9E%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B%E8%A7%A3%E6%9E%90-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">移动增强现实系统设计与应用案例解析-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/VR-Ruiwang1.pdf" target="_blank" rel="noopener">虚实融合显示与绘制技术-王锐</a></p></li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://mp.weixin.qq.com/s/PV_xLmuE-HpnUgnJ5GyMOA" target="_blank" rel="noopener">第二届“SLAM技术及应用” 暑期学校与研讨会圆满落幕</a></li><li>章国锋主页：<a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank" rel="noopener">地址</a></li><li>CAD&amp;CG实验室主页，<a href="http://www.zjucvg.net/" target="_blank" rel="noopener">地址</a></li><li><a href="https://github.com/zju3dv" target="_blank" rel="noopener">CAD&amp;CG Github</a></li><li><a href="http://www.zjucvg.net/senseslam/" target="_blank" rel="noopener">SenseSLAM</a>,浙大-商汤三维视觉联合实验室</li><li>Shuhan Shen (申抒含)主页，<a href="http://vision.ia.ac.cn/Faculty/shshen/index.htm" target="_blank" rel="noopener">地址</a></li><li>姜翰青，<a href="https://www.linkedin.com/in/%E7%BF%B0%E9%9D%92-%E5%A7%9C-1194b411b/`" target="_blank" rel="noopener">Linkedin</a></li><li>讲座直播地址：<a href="https://www.douyu.com/7275221" target="_blank" rel="noopener">https://www.douyu.com/7275221</a></li><li>商汤泰坦公开课，<a href="https://cloud.xylink.com/live/v/2c9497116bb8b075016c082adec66ea7" target="_blank" rel="noopener">直播地址</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由浙江大学CAD&amp;amp;CG国家重点实验室主办、浙江大学-商汤三维视觉联合实验室协办的“SLAM技术及应用”暑期学校于7月20日如期拉开序幕。&lt;br&gt;今天（2019/07/20）看到直播的时候已经是下午4点半了，只听到刘浩敏讲到末尾的一段，幸好主办方提供了讲座课件，Download下来慢慢看。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>Filebrowser：一款轻量级个人网盘</title>
    <link href="https://www.vincentqin.tech/posts/build-filebrowser/"/>
    <id>https://www.vincentqin.tech/posts/build-filebrowser/</id>
    <published>2019-07-14T13:48:47.000Z</published>
    <updated>2020-03-31T15:27:36.717Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><br><br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/filebrowser-banner.png"><br><br></p><a id="more"></a><h1 id="个人网盘-Filebrowser"><a href="#个人网盘-Filebrowser" class="headerlink" title="个人网盘 Filebrowser"></a>个人网盘 Filebrowser</h1><p>服务器仅仅用于科学上网未免有些浪费了，是时候尝试一下自建个人网盘和图床了。</p><h2 id="如何安装"><a href="#如何安装" class="headerlink" title="如何安装"></a>如何安装</h2><p><a href="https://filebrowser.xyz/installation" target="_blank" rel="noopener">官方</a>给出了一键安装大法，进入服务器输入以下命令就可以了。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://filebrowser.xyz/get.sh | bash</span><br></pre></td></tr></table></figure></p><h2 id="首次配置"><a href="#首次配置" class="headerlink" title="首次配置"></a>首次配置</h2><p>当安装好之后，你并不能立即使用它，需要修改一些配置(以下内容参考[<a href="https://www.mivm.cn/filebrowser/" target="_blank" rel="noopener">米V米</a>]的教程)。</p><ul><li>创建配置数据库：<code>filebrowser -d /etc/filebrowser.db config init</code></li><li>设置监听地址：<code>filebrowser -d /etc/filebrowser.db config set --address 0.0.0.0</code></li><li>设置监听端口：<code>filebrowser -d /etc/filebrowser.db config set --port 8088</code></li><li>设置语言环境(中文)：<code>filebrowser -d /etc/filebrowser.db config set --locale zh-cn</code></li><li>设置日志位置：<code>filebrowser -d /etc/filebrowser.db config set --log /var/log/filebrowser.log</code></li><li>添加一个用户：<code>filebrowser -d /etc/filebrowser.db users add root password --perm.admin</code>，其中的root和password分别是用户名和密码，根据自己的需求更改。</li></ul><p>有关更多配置的选项，可以参考官方文档：<a href="https://docs.filebrowser.xyz/" target="_blank" rel="noopener">https://docs.filebrowser.xyz/</a><br>配置修改好以后，就可以启动FileBrowser了，使用-d参数指定配置数据库路径。示例：<code>filebrowser -d /etc/filebrowser.db</code><br>启动成功就可以使用浏览器访问FileBrowser了，在浏览器输入 <code>服务器IP:端口</code>，示例：<code>http://192.168.1.1:8088</code></p><p>然后会看到 FileBrowser 的登陆界面：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/filebrowser-login.png"></p><p>用刚刚创建的用户登陆，最后就可以放心使用啦~<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/filebrowser-demo.gif"></p><h2 id="后续配置"><a href="#后续配置" class="headerlink" title="后续配置"></a>后续配置</h2><p>完成以上过程之后已经可以正常访问个人网盘了，但是假如服务器重启之后就必须重新输入<code>filebrowser -d /etc/filebrowser.db</code>才能运行，为了省去这一步，我们需要进行设置服务器开机自动启动FileBrowser。<br>这里我们使用的是systemd 大法：<br>首先下载 FileBrowser 的 <code>service</code>文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl https://cdn.mivm.cn/www.mivm.cn/archives/filebrowser/filebrowser.service -o /lib/systemd/system/filebrowser.service</span><br></pre></td></tr></table></figure><p>如果你的运行命令不是<code>/usr/local/bin/filebrowser -d /etc/filebrowser.db</code>，需要对 service 文件进行修改，将文件的 ExecStart 改为你的运行命令，更改完成后需要输入<code>systemctl daemon-reload</code>。</p><p>下面祭出常用的命令：</p><ul><li>运行：<code>systemctl start filebrowser.service</code></li><li>停止运行：<code>systemctl stop filebrowser.service</code></li><li>开机启动：<code>systemctl enable filebrowser.service</code></li><li>取消开机启动：<code>systemctl disable filebrowser.service</code></li><li>查看运行状态：<code>systemctl status filebrowser.service</code></li></ul><p>这里有个<a href="https://youtu.be/sE31MBvOjxk" target="_blank" rel="noopener">视频教程</a>，需要科学上网查看。</p><p><br></p><h1 id="个人图床Chevereto"><a href="#个人图床Chevereto" class="headerlink" title="个人图床Chevereto"></a>个人图床Chevereto</h1><p>先给出安装好的样子~<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/Chevereto.png"></p><p>安装教程这里<a href="https://gist.github.com/biezhi/f90923b48863c7d745481ccdd678ccab" target="_blank" rel="noopener">install_chevereto.md</a>已经写得非常详细了，在此不做详细介绍。这里有个<a href="https://youtu.be/kShgzNkXRak" target="_blank" rel="noopener">视频教程</a>，我主要按照这个教程进行配置的，需要科学上网查看。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://youtu.be/sE31MBvOjxk" target="_blank" rel="noopener">把玩我的 VPS 主机 - 分分钟搭建时尚简洁的在线网盘</a></li><li><a href="https://youtu.be/kShgzNkXRak" target="_blank" rel="noopener">搭建漂亮的私人图床</a></li><li><a href="https://biezhi.me/" target="_blank" rel="noopener">王爵 nice的主页</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;img src=&quot;https://vincentqin.gitee.io/blogresource-1/build-filebrowser/filebrowser-banner.png&quot; alt&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="个人网盘" scheme="https://www.vincentqin.tech/tags/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%9B%98/"/>
    
  </entry>
  
  <entry>
    <title>SuperPoint: Self-Supervised Interest Point Detection and Description</title>
    <link href="https://www.vincentqin.tech/posts/superpoint/"/>
    <id>https://www.vincentqin.tech/posts/superpoint/</id>
    <published>2019-06-23T06:02:06.000Z</published>
    <updated>2020-05-18T15:17:42.554Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文出自近几年备受瞩目的创业公司<a href="https://www.magicleap.com/" target="_blank" rel="noopener">MagicLeap</a>，发表在CVPR 2018,一作<a href="http://www.danieldetone.com/" target="_blank" rel="noopener">Daniel DeTone</a>，<strong>[<a href="https://arxiv.org/abs/1712.07629" target="_blank" rel="noopener">paper</a>]</strong>，<strong>[<a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork/blob/master/assets/DL4VSLAM_talk.pdf" target="_blank" rel="noopener">slides</a>]</strong>，<strong>[<a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork" target="_blank" rel="noopener">code</a>]</strong>。</p><p>这篇文章设计了一种自监督网络框架，能够同时提取特征点的位置以及描述子。相比于patch-based方法，本文提出的算法能够在原始图像提取到像素级精度的特征点的位置及其描述子。<br>本文提出了一种单映性适应（<code>Homographic Adaptation</code>）的策略以增强特征点的复检率以及跨域的实用性（这里跨域指的是synthetic-to-real的能力，网络模型在虚拟数据集上训练完成，同样也可以在真实场景下表现优异的能力）。</p><a id="more"></a><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>诸多应用（诸如SLAM/SfM/相机标定/立体匹配）的首要一步就是特征点提取，这里的特征点指的是<strong>能够在不同光照&amp;不同视角下都能够稳定且可重复检测的2D图像点位置</strong>。</p><p>基于CNN的算法几乎在以图像作为输入的所有领域表现出相比于人类特征工程更加优秀的表达能力。目前已经有一些工作做类似的任务，例如人体位姿估计,目标检测以及室内布局估计等。这些算法以通常以大量的人工标注作为GT，这些精心设计的网络用来训练以得到人体上的角点，例如嘴唇的边缘点亦或人体的关节点，但是这里的问题是这里的点实际是ill-defined（我的理解是，这些点有可能是特征点，但仅仅是一个大概的位置，是特征点的子集，并没有真正的把特征点的概念定义清楚）。</p><p>本文采用了非人工监督的方法提取真实场景的特征点。本文设计了一个由特征点检测器监督的具有伪真值数据集，而非是大量的人工标记。为了得到伪真值，本文首先在大量的虚拟数据集上训练了一个全卷积网络（FCNN），这些虚拟数据集由一些基本图形组成，例如有线段、三角形、矩形和立方体等，这些基本图形具有没有争议的特征点位置，文中称这些特征点为<code>MagicPoint</code>，这个pre-trained的检测器就是<code>MagicPoint</code>检测器。这些<code>MagicPoint</code>在虚拟场景的中检测特征点的性能明显优于传统方式，但是在真实的复杂场景中表现不佳，此时作者提出了一种多尺度多变换的方法<code>Homographic Adaptation</code>。对于输入图像而言，<code>Homographic Adaptation</code>通过对图像进行多次不同的尺度/角度变换来帮助网络能够在不同视角不同尺度观测到特征点。<br>综上：<strong>SuperPoint = MagicPoint+Homographic Adaptation</strong></p><h1 id="算法优劣对比"><a href="#算法优劣对比" class="headerlink" title="算法优劣对比"></a>算法优劣对比</h1><p><img alt="fig1_table1" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/tab_1.png"></p><ul><li>基于图像块的算法导致特征点位置精度不够准确；</li><li>特征点与描述子分开进行训练导致运算资源的浪费，网络不够精简，实时性不足；或者仅仅训练特征点或者描述子的一种，不能用同一个网络进行联合训练；</li></ul><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img alt="fig3" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_3.png"></p><p>上图可见特征点检测器以及描述子网络共享一个单一的前向encoder，只是在decoder时采用了不同的结构，根据任务的不同学习不同的网络参数。这也是本框架与其他网络的不同之处：其他网络采用的是先训练好特征点检测网络，然后再去进行对特征点描述网络进行训练。<br>网络共分成以下4个主要部分，在此进行详述：</p><h2 id="1-Shared-Encoder-共享的编码网络"><a href="#1-Shared-Encoder-共享的编码网络" class="headerlink" title="1. Shared Encoder 共享的编码网络"></a>1. Shared Encoder 共享的编码网络</h2><p>从上图可以看到，整体而言，本质上有两个网络，只是前半部分共享了一部分而已。本文利用了VGG-style的encoder以用于降低图像尺寸，encoder包括卷积层，max-pooling层，以及非线性激活层。通过3个max-pooling层将图像的尺寸变成$H_c = H/8$和$H_c = H/8$，经过encoder之后，图像由$I \in \mathcal{R}^{H \times W}$变为张量$\mathcal{B} \in \mathbb{R}^{H_c \times W_c \times F}$</p><h2 id="2-Interest-Point-Decoder"><a href="#2-Interest-Point-Decoder" class="headerlink" title="2. Interest Point Decoder"></a>2. Interest Point Decoder</h2><p><img alt="fig_10_magicPoint1" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_10_magicPoint1.png"></p><p>这里介绍的是特征点的解码端。每个像素的经过该解码器的输出是该像素是特征点的概率（probability of “point-ness”）。<br>通常而言，我们可以通过反卷积得到上采样的图像，但是这种操作会导致计算量的骤增以及会引入一种“checkerboard artifacts”。因此本文设计了一种带有“特定解码器”（这种解码器没有参数）的特征点检测头以减小模型计算量（子像素卷积）。<br>例如：输入张量的维度是$\mathbb{R}^{H_c \times W_c \times 65}$，输出维度$\mathbb{R}^{H \times W}$，即图像的尺寸。这里的65表示原图$8 \times 8$的局部区域，加上一个非特征点<code>dustbin</code>。通过在channel维度上做softmax，非特征点dustbin会被删除，同时会做一步图像的<code>reshape</code>：$\mathbb{R}^{H_c \times W_c \times 64} \Rightarrow \mathbb{R}^{H \times W}$ 。（这就是<strong><a href="https://blog.csdn.net/leviopku/article/details/84975282" target="_blank" rel="noopener">子像素卷积</a></strong>的意思，俗称像素洗牌）</p><h2 id="3-Descriptor-Decoder"><a href="#3-Descriptor-Decoder" class="headerlink" title="3. Descriptor Decoder"></a>3. Descriptor Decoder</h2><p>首先利用类似于UCN的网络得到一个半稠密的描述子（此处参考文献<a href="https://arxiv.org/abs/1606.03558" target="_blank" rel="noopener">UCN</a>），这样可以减少算法训练内存开销同时减少算法运行时间。之后通过双三次多项式插值得到其余描述，然后通过<code>L2-normalizes</code>归一化描述子得到统一的长度描述。特征维度由$\mathcal{D} \in \mathbb{R}^{H_c \times W_c \times D}$变为$\mathbb{R}^{H\times W \times D}$ 。</p><p><img alt="fig_11_des_decoder" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_11_des_decoder.png"></p><p>由特征点得到其描述子的过程文中没有细讲，看了一下<a href="https://github.com/pytorch/pytorch/blob/f064c5aa33483061a48994608d890b968ae53fb5/aten/src/THNN/generic/SpatialGridSamplerBilinear.c" target="_blank" rel="noopener">源代码</a>就明白了。其实该过程主要用了一个函数即<code>grid_sample</code>，画了一个草图作为解释。</p><ul><li>图像尺寸归一化：首先对图像的尺寸进行归一化，(-1,-1)表示原来图像的(0,0)位置，(1,1)表示原来图像的(H-1,W-1)位置，这样一来，特征点的位置也被归一化到了相应的位置。</li><li>构建grid：将归一化后的特征点罗列起来，构成一个尺度为1*1*K*2的张量，其中K表示特征数量，2分别表示xy坐标。</li><li>特征点位置反归一化：根据输入张量的H与W对grid(1,1,0,:)（表示第一个特征点，其余特征点类似）进行反归一化，其实就是按照比例进行缩放+平移，得到反归一化特征点在张量某个slice（通道）上的位置；但是这个位置可能并非为整像素，此时要对其进行双线性插值补齐，然后其余slice按照同样的方式进行双线性插值。注：代码中实际的就是双线性插值，并非文中讲的双三次插值；</li><li>输出维度：1*C*1*K。</li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/grid_sample.png"></p><h2 id="4-误差构建"><a href="#4-误差构建" class="headerlink" title="4. 误差构建"></a>4. 误差构建</h2><script type="math/tex; mode=display">\begin{array}{l}{\mathcal{L}\left(\mathcal{X}, \mathcal{X}^{\prime}, \mathcal{D}, \mathcal{D}^{\prime} ; Y, Y^{\prime}, S\right)=} \\ {\qquad \mathcal{L}_{p}(\mathcal{X}, Y)+\mathcal{L}_{p}\left(\mathcal{X}^{\prime}, Y^{\prime}\right)+\lambda \mathcal{L}_{d}\left(\mathcal{D}, \mathcal{D}^{\prime}, S\right)}\end{array}</script><p>可见损失函数由两项组成，其中一项为特征点检测loss$\mathcal{L}_{p}$ ，另外一项是描述子的loss$\mathcal{L}_{d}$。</p><p>对于检测项loss，此时采用了交叉熵损失函数:</p><script type="math/tex; mode=display">\mathcal{L}_{p}(\mathcal{X}, Y)=\frac{1}{H_{c} W_{c}} \sum_{h=1 \atop w=1}^{H_{c}, W_{c}} l_{p}\left(\mathbf{x}_{h w} ; y_{h w}\right)</script><p>其中：</p><script type="math/tex; mode=display">l_{p}\left(\mathbf{x}_{h w} ; y\right)=-\log \left(\frac{\exp \left(\mathbf{x}_{h w y}\right)}{\sum_{k=1}^{65} \exp \left(\mathbf{x}_{h w k}\right)}\right)</script><p>描述子的损失函数:</p><script type="math/tex; mode=display">\mathcal{L}_{d}\left(\mathcal{D}, \mathcal{D}^{\prime}, S\right)=\frac{1}{\left(H_{c} W_{c}\right)^{2}} \sum_{h=1 \atop w=1}^{H_{c}, W_{c}} \sum_{h^{\prime}=1 \atop w^{\prime}=1}^{H_{c}, W_{c}} l_{d}\left(\mathbf{d}_{h w}, \mathbf{d}_{h^{\prime} w^{\prime}}^{\prime} ; s_{h w h^{\prime} w^{\prime}}\right)</script><p>其中<script type="math/tex">l_{d}</script>为<code>Hinge-loss</code>（合页损失函数，用于SVM，如支持向量的软间隔，可以保证最后解的稀疏性）；</p><script type="math/tex; mode=display">l_{d}\left(\mathbf{d}, \mathbf{d}^{\prime} ; s\right)=\lambda_{d} * s * \max \left(0, m_{p}-\mathbf{d}^{T} \mathbf{d}^{\prime}\right)+(1-s) * \max \left(0, \mathbf{d}^{T} \mathbf{d}^{\prime}-m_{n}\right)</script><p>同时指示函数为<script type="math/tex">s_{h w h^{\prime} w^{\prime}}</script>,$S$表示所有正确匹配对集合:</p><script type="math/tex; mode=display">s_{h w h^{\prime} w^{\prime}}=\left\{\begin{array}{ll}{1,} & {\text { if }\left\|\widehat{\mathcal{H} \mathbf{p}_{h w}}-\mathbf{p}_{h^{\prime} w^{\prime}}\right\| \leq 8} \\ {0,} & {\text { otherwise }}\end{array}\right.</script><h1 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h1><p><img alt="fig2" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_2.png"></p><p>本文一共设计了两个网络，一个是<code>BaseDetector</code>，用于检测角点（注意，此处提取的并不是最终输出的特征点，可以理解为候选的特征点），另一个是<code>SuperPoint</code>网络，输出特征点和描述子。</p><p>网络的训练共分为三个步骤：</p><ol><li>第一步是采用虚拟的三维物体作为数据集，训练网络去提取角点，这里得到的是<code>BaseDetector</code>即，<code>MagicPoint</code>；</li><li>使用真实场景图片，用第一步训练出来的网络<code>MagicPoint</code> +<code>Homographic Adaptation</code>提取角点，这一步称作兴趣点自标注（Interest Point Self-Labeling）</li><li>对第二步使用的图片进行几何变换得到新的图片，这样就有了已知位姿关系的图片对，把这两张图片输入SuperPoint网络，提取特征点和描述子。</li></ol><h2 id="预训练Magic-Point"><a href="#预训练Magic-Point" class="headerlink" title="预训练Magic Point"></a>预训练Magic Point</h2><p>此处参考作者之前发表的一篇论文<strong>[<a href="https://arxiv.org/abs/1707.07410" target="_blank" rel="noopener">Toward Geometric Deep SLAM</a>]</strong>，其实就是<code>MagicPoint</code>，在此不做展开介绍。<br><img alt="fig2" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_10_magicPoint1.png"></p><p><img alt="fig4" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_4.png"></p><h2 id="Homographic-Adaptation"><a href="#Homographic-Adaptation" class="headerlink" title="Homographic Adaptation"></a>Homographic Adaptation</h2><p>算法在虚拟数据集上表现极其优秀，但是在真实场景下表示没有达到预期，此时本文进行了<code>Homographic Adaptation</code>。<br>作者使用的数据集是<code>MS-COCO</code>，为了使网络的泛化能力更强，本文不仅使用原始了原始图片，而且对每张图片进行随机的旋转和缩放形成新的图片，新的图片也被用来进行识别。这一步其实就类似于训练里常用的数据增强。经过一系列的单映变换之后特征点的复检率以及普适性得以增强。值得注意的是，在实际训练时，这里采用了迭代使用单映变换的方式，例如使用优化后的特征点检测器重新进行单映变换进行训练，然后又可以得到更新后的检测器，如此迭代优化，这就是所谓的self-supervisd。<br><img alt="fig5" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_5.png"></p><p><img alt="fig_9_HA" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_9_HA.png"></p><p>最后的关键点检测器，即<script type="math/tex">\hat{F}\left(I ; f_{\theta}\right)</script>，可以表示为再所有随机单映变换/反变换的聚合：</p><script type="math/tex; mode=display">\hat{F}\left(I ; f_{\theta}\right)=\frac{1}{N_{h}} \sum_{i=1}^{N_{h}} \mathcal{H}_{i}^{-1} f_{\theta}\left(\mathcal{H}_{i}(I)\right)</script><p><img alt="fig_6" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_6.png"></p><h2 id="构建残差，迭代优化描述子以及检测器"><a href="#构建残差，迭代优化描述子以及检测器" class="headerlink" title="构建残差，迭代优化描述子以及检测器"></a>构建残差，迭代优化描述子以及检测器</h2><p>利用上面网络得到的关键点位置以及描述子表示构建残差，利用<code>ADAM</code>进行优化。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><img alt="fig_8" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_8.png"></p><p><img alt="tab_3" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/tab_3.png"></p><p><img alt="tab_4" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/tab_4.png"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol><li>it is possible to transfer knowledge from a synthetic dataset onto real-world images</li><li>sparse interest point detection and description can be cast as a single, efficient convolutional neural network</li><li>the resulting system works well for geometric computer vision matching tasks such as Homography Estimation</li></ol><p>未来工作:</p><ol><li>研究Homographic Adaptation能否在语义分割任务或者目标检测任务中有提升作用</li><li>兴趣点提取以及描述这两个任务是如何影响彼此的</li></ol><p>作者最后提到，他相信该网络能够解决SLAM或者SfM领域的数据关联<em>，并且</em><code>learning-based</code>前端可以使得诸如机器人或者AR等应用获得更加鲁棒。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文出自近几年备受瞩目的创业公司&lt;a href=&quot;https://www.magicleap.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MagicLeap&lt;/a&gt;，发表在CVPR 2018,一作&lt;a href=&quot;http://www.danieldetone.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Daniel DeTone&lt;/a&gt;，&lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/abs/1712.07629&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;，&lt;strong&gt;[&lt;a href=&quot;https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork/blob/master/assets/DL4VSLAM_talk.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;slides&lt;/a&gt;]&lt;/strong&gt;，&lt;strong&gt;[&lt;a href=&quot;https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;code&lt;/a&gt;]&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这篇文章设计了一种自监督网络框架，能够同时提取特征点的位置以及描述子。相比于patch-based方法，本文提出的算法能够在原始图像提取到像素级精度的特征点的位置及其描述子。&lt;br&gt;本文提出了一种单映性适应（&lt;code&gt;Homographic Adaptation&lt;/code&gt;）的策略以增强特征点的复检率以及跨域的实用性（这里跨域指的是synthetic-to-real的能力，网络模型在虚拟数据集上训练完成，同样也可以在真实场景下表现优异的能力）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="CV" scheme="https://www.vincentqin.tech/categories/CV/"/>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="Deep Learning" scheme="https://www.vincentqin.tech/tags/Deep-Learning/"/>
    
      <category term="特征提取" scheme="https://www.vincentqin.tech/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
      <category term="SuperPoint" scheme="https://www.vincentqin.tech/tags/SuperPoint/"/>
    
      <category term="MagicLeap" scheme="https://www.vincentqin.tech/tags/MagicLeap/"/>
    
      <category term="深度学习" scheme="https://www.vincentqin.tech/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Bygones</title>
    <link href="https://www.vincentqin.tech/posts/bygones/"/>
    <id>https://www.vincentqin.tech/posts/bygones/</id>
    <published>2019-05-28T16:57:39.000Z</published>
    <updated>2020-03-02T15:15:31.025Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><!-- <div id="dplayer1" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer1"),"loop":true,"video":{"url":"https://www.bilibili.com/video/av92924509/"},"danmaku":{"id":"bbe4286bf164ef6a1497f18a7b42ff944e684b821","api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> --><iframe src="//player.bilibili.com/player.html?aid=92924509&cid=158651721&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><a id="more"></a><p><strong><center>致我们逝去的青春！</center></strong></p>]]></content>
    
    <summary type="html">
    
      &lt;!-- &lt;div id=&quot;dplayer1&quot; class=&quot;dplayer hexo-tag-dplayer-mark&quot; style=&quot;margin-bottom: 20px;&quot;&gt;&lt;/div&gt;&lt;script&gt;(function(){var player = new DPlayer({&quot;container&quot;:document.getElementById(&quot;dplayer1&quot;),&quot;loop&quot;:true,&quot;video&quot;:{&quot;url&quot;:&quot;https://www.bilibili.com/video/av92924509/&quot;},&quot;danmaku&quot;:{&quot;id&quot;:&quot;bbe4286bf164ef6a1497f18a7b42ff944e684b821&quot;,&quot;api&quot;:&quot;https://api.prprpr.me/dplayer/&quot;}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()&lt;/script&gt; --&gt;
&lt;iframe src=&quot;//player.bilibili.com/player.html?aid=92924509&amp;cid=158651721&amp;page=1&quot; scrolling=&quot;no&quot; border=&quot;0&quot; frameborder=&quot;no&quot; framespacing=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt; &lt;/iframe&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Black Hole</title>
    <link href="https://www.vincentqin.tech/posts/first-black-hole/"/>
    <id>https://www.vincentqin.tech/posts/first-black-hole/</id>
    <published>2019-04-10T15:55:01.000Z</published>
    <updated>2020-03-31T14:55:18.422Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/first-black-hole/big-blackhole.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Awesome CV Works</title>
    <link href="https://www.vincentqin.tech/posts/awesome-works/"/>
    <id>https://www.vincentqin.tech/posts/awesome-works/</id>
    <published>2019-03-31T12:15:41.000Z</published>
    <updated>2019-05-26T02:26:30.494Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>The post contains papers-with-code about SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc. <a href="https://github.com/Vincentqyw/Recent-Stars-2019" target="_blank" rel="noopener"><img alt="GitHub stars" data-src="https://img.shields.io/github/stars/Vincentqyw/Recent-Stars-2019.svg?logo=github&amp;label=Stars"></a></p><a id="more"></a><h2 id="SLAM-related"><a href="#SLAM-related" class="headerlink" title="SLAM related"></a>SLAM related</h2><ul><li><a href="https://github.com/kiran-mohan/SLAM-Algorithms-Octave" target="_blank" rel="noopener">Solutions to assignments of Robot Mapping Course WS 2013/14 by Dr. Cyrill Stachniss at University of Freiburg</a>,SLAM算法学习课后作业答案</li><li><a href="https://github.com/RonaldSun/VI-Stereo-DSO" target="_blank" rel="noopener">Direct sparse odometry combined with stereo cameras and IMU</a>,双目DSO+IMU</li><li><a href="https://github.com/HorizonAD/stereo_dso" target="_blank" rel="noopener">Direct Sparse Odometry with Stereo Cameras</a>,双目DSO</li><li><a href="https://github.com/uoip/g2opy" target="_blank" rel="noopener">Python binding of SLAM graph optimization framework g2o</a>,python版本的g2o实现</li><li><a href="https://github.com/mihaidusmanu/d2-net" target="_blank" rel="noopener">D2-Net: A Trainable CNN for Joint Description and Detection of Local Features</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1905.03561" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://dsmn.ml/publications/d2-net.html" target="_blank" rel="noopener">Project Page</a>]</strong>, 深度学习描述子</li><li><a href="https://github.com/ethz-asl/orb_slam_2_ros" target="_blank" rel="noopener">ROS interface for ORBSLAM2</a>,ROS版本的ORBSLAM2</li><li><a href="https://github.com/yan99033/CNN-SVO" target="_blank" rel="noopener">CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction</a>， <strong>[<a href="https://arxiv.org/pdf/1810.01011.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/ManiiXu/VINS-Mono-Learning" target="_blank" rel="noopener">VINS-Mono-Learning</a>，代码注释版VINS-Mono，初学者学习</li><li><a href="https://github.com/xdspacelab/openvslam" target="_blank" rel="noopener">OpenVSLAM: Versatile Visual SLAM Framework</a>,  <strong>[<a href="https://openvslam.readthedocs.io/" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/fabianschenk/RESLAM" target="_blank" rel="noopener">RESLAM: A real-time robust edge-based SLAM system</a>, <strong>[<a href="https://github.com/fabianschenk/fabianschenk.github.io/raw/master/files/schenk_icra_2019.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/rubengooj/pl-slam" target="_blank" rel="noopener">PL-SLAM: a Stereo SLAM System through the Combination of Points and Line Segments</a>, <strong>[<a href="https://arxiv.org/abs/1705.09479" target="_blank" rel="noopener">Paper</a>]</strong>，线特征SLAM</li><li><a href="https://github.com/YipuZhao/GF_PL_SLAM" target="_blank" rel="noopener">Good Line Cutting: towards Accurate Pose Tracking of Line-assisted VO/VSLAM</a>, ECCV 2018, <strong>[<a href="https://sites.google.com/site/zhaoyipu/good-feature-visual-slam" target="_blank" rel="noopener">Project Page</a>]</strong>, 改进的PL-SLAM</li><li><a href="https://github.com/leoshine/Spherical_Regression" target="_blank" rel="noopener">Spherical Regression: Learning Viewpoints, Surface Normals and 3D Rotations on n-Spheres</a>, CVPR 2019, <strong>[<a href="http://arxiv.org/abs/1904.05404" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/icsl-Jeon/traj_gen_vis" target="_blank" rel="noopener">svo_edgelet</a>, 在线轨迹生成</li><li><a href="https://github.com/TimboKZ/caltech_samaritan" target="_blank" rel="noopener">Drone SLAM project for Caltech’s ME 134 Autonomy class</a>, <strong>[<a href="https://github.com/TimboKZ/caltech_samaritan/blob/master/CS134_Final_Project_Report.pdf" target="_blank" rel="noopener">PDF</a>]</strong></li><li><a href="https://github.com/icsl-Jeon/traj_gen_vis" target="_blank" rel="noopener">Online Trajectory Generation of a MAV for Chasing a Moving Target in 3D Dense Environments</a>, <strong>[<a href="https://arxiv.org/pdf/1904.03421.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/AtsushiSakai/PythonRobotics" target="_blank" rel="noopener">PythonRobotics</a>,<strong>[<a href="https://arxiv.org/abs/1808.10703" target="_blank" rel="noopener">Paper</a>]</strong>, <a href="https://github.com/onlytailei/CppRobotics" target="_blank" rel="noopener">CppRobotics</a></li><li><a href="https://github.com/izhengfan/ba_demo_ceres" target="_blank" rel="noopener">Bundle adjustment demo using Ceres Solver</a>,  <strong>[<a href="https://fzheng.me/2018/01/23/ba-demo-ceres/" target="_blank" rel="noopener">Blog</a>]</strong>, ceres实现BA</li><li><a href="https://github.com/shichaoy/cube_slam" target="_blank" rel="noopener">CubeSLAM: Monocular 3D Object Detection and SLAM</a>, <strong>[<a href="https://arxiv.org/abs/1806.00557" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/sshaoshuai/PointRCNN" target="_blank" rel="noopener">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.04244" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/nrupatunga/GIST-global-Image-Descripor" target="_blank" rel="noopener">GIST-Global Image Descriptor</a>, GIST描述子</li><li><a href="https://github.com/ethz-asl/mav_voxblox_planning" target="_blank" rel="noopener">mav voxblox planning</a>, MAV planning tools using voxblox as the map representation.</li><li><a href="https://github.com/zziz/kalman-filter" target="_blank" rel="noopener">Python Kalman Filter</a>, 30行实现卡尔曼滤波</li><li><a href="https://github.com/arpg/vicalib" target="_blank" rel="noopener">vicalib</a>, 视觉惯导系统标定工具</li><li><a href="https://github.com/simondlevy/BreezySLAM" target="_blank" rel="noopener">BreezySLAM</a>, 基于雷达的SLAM，支持Python(&amp;Matlab, C++, and Java)</li><li><a href="https://github.com/Yvon-Shong/Probabilistic-Robotics" target="_blank" rel="noopener">Probabilistic-Robotics</a>, 《概率机器人》中文版，书和课后习题</li><li><a href="https://github.com/emmjaykay/stanford_self_driving_car_code" target="_blank" rel="noopener">Stanford Self Driving Car Code</a>, <strong>[<a href="http://robots.stanford.edu/papers/junior08.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, 斯坦福自动驾驶车代码</li><li><a href="https://github.com/ndrplz/self-driving-car" target="_blank" rel="noopener">Udacity Self-Driving Car Engineer Nanodegree projects</a></li><li><a href="https://github.com/TUMFTM/Lecture_AI_in_Automotive_Technology" target="_blank" rel="noopener">Artificial Intelligence in Automotive Technology</a>, TUM自动驾驶技术中的人工智能课程</li><li><a href="https://github.com/hlzz/DeepMatchVO" target="_blank" rel="noopener">DeepMatchVO: Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation</a>,ICRA 2019, <strong>[<a href="https://arxiv.org/abs/1902.09103" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/zdzhaoyong/GSLAM" target="_blank" rel="noopener">GSLAM: A General SLAM Framework and Benchmark</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1902.07995" target="_blank" rel="noopener">Paper</a>]</strong>, 集成了各种传感器输入的SLAM统一框架</li><li><a href="https://github.com/izhengfan/se2lam" target="_blank" rel="noopener">Visual-Odometric Localization and Mapping for Ground Vehicles Using SE(2)-XYZ Constraints</a>，ICRA 2019,基于SE(2)-XYZ约束的VO系统</li><li><a href="https://github.com/nicolov/simple_slam_loop_closure" target="_blank" rel="noopener">Simple bag-of-words loop closure for visual SLAM</a>, <strong>[<a href="https://nicolovaligi.com/bag-of-words-loop-closure-visual-slam.html" target="_blank" rel="noopener">Blog</a>]</strong>, 回环</li><li><a href="https://github.com/rmsalinas/fbow" target="_blank" rel="noopener">FBOW (Fast Bag of Words), an extremmely optimized version of the DBow2/DBow3 libraries</a>,优化版本的DBow2/DBow3</li><li><a href="https://github.com/tomas789/tonav" target="_blank" rel="noopener">Multi-State Constraint Kalman Filter (MSCKF) for Vision-aided Inertial Navigation(master’s thesis)</a></li><li><a href="https://github.com/yuzhou42/MSCKF" target="_blank" rel="noopener">MSCKF</a>, MSCKF中文注释版</li><li><a href="https://github.com/hbtang/calibcamodo" target="_blank" rel="noopener">Calibration algorithm for a camera odometry system</a>, VO系统的标定程序</li><li><a href="https://github.com/cggos/vins_mono_cg" target="_blank" rel="noopener">Modified version of VINS-Mono</a>, 注释版本VINS Mono</li><li><a href="https://github.com/zhenpeiyang/RelativePose" target="_blank" rel="noopener">Extreme Relative Pose Estimation for RGB-D Scans via Scene Completion</a>,<strong>[<a href="https://arxiv.org/abs/1901.00063" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/jessecw/EPnP_Eigen" target="_blank" rel="noopener">Implementation of EPnP algorithm with Eigen</a>,利用Eigen编写的EPnP</li><li><a href="https://github.com/jiexiong2016/GCNv2_SLAM" target="_blank" rel="noopener">Real-time SLAM system with deep features</a>, 深度学习描述子(ORB vs. GCNv2)</li><li><a href="https://github.com/Huangying-Zhan/Depth-VO-Feat" target="_blank" rel="noopener">Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction</a>, CVPR 2018, 无监督单目深度恢复以及VO</li><li><a href="https://github.com/Phylliida/orbslam-windows" target="_blank" rel="noopener">ORB-SLAM-windows</a>, Windows版本的ORB-SLAM</li><li><a href="https://github.com/danping/structvio" target="_blank" rel="noopener">StructVIO : Visual-inertial Odometry with Structural Regularity of Man-made Environments</a>,<strong>[<a href="http://drone.sjtu.edu.cn/dpzou/project/structvio.html" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/irvingzhang/KalmanFiltering" target="_blank" rel="noopener">KalmanFiltering</a>, 各种卡尔曼滤波器的demo</li><li><a href="https://github.com/ZhenghaoFei/visual_odom" target="_blank" rel="noopener">Stereo Odometry based on careful Feature selection and Tracking</a>, <strong>[<a href="https://lamor.fer.hr/images/50020776/Cvisic2017.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, C++ OpenCV实现SOFT</li><li><a href="https://github.com/dzunigan/zSLAM" target="_blank" rel="noopener">Visual SLAM with RGB-D Cameras based on Pose Graph Optimization</a></li><li><a href="https://github.com/drsrinathsridhar/GRANSAC" target="_blank" rel="noopener">Multi-threaded generic RANSAC implemetation</a>, 多线程RANSAC</li><li><a href="https://github.com/PyojinKim/OPVO" target="_blank" rel="noopener">Visual Odometry with Drift-Free Rotation Estimation Using Indoor Scene Regularities</a>, BMVC 2017, <strong>[<a href="http://pyojinkim.me/pub/Visual-Odometry-with-Drift-Free-Rotation-Estimation-Using-Indoor-Scene-Regularities/" target="_blank" rel="noopener">Project Page</a>]</strong>，利用平面正交信息进行VO</li><li><a href="https://github.com/baidu/ICE-BA" target="_blank" rel="noopener">ICE-BA</a>, CVPR 2018, <strong>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_ICE-BA_Incremental_Consistent_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/AIBluefisher/GraphSfM" target="_blank" rel="noopener">GraphSfM: Robust and Efficient Graph-based Structure from Motion</a>, <strong>[<a href="https://aibluefisher.github.io/GraphSfM/" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/cuitaixiang/LOAM_NOTED" target="_blank" rel="noopener">LOAM_NOTED</a>, loam中文注解版</li><li><a href="https://github.com/Ethan-Zhou/MWO" target="_blank" rel="noopener">Divide and Conquer: Effcient Density-Based Tracking of 3D Sensors in Manhattan Worlds</a>,ACCV 2016,<strong>[<a href="http://users.cecs.anu.edu.au/~u5535909/" target="_blank" rel="noopener">Project Page</a>]</strong>,曼哈顿世界利用深度传感器进行旋转量平移量分离优化</li><li><a href="https://github.com/jstraub/rtmf" target="_blank" rel="noopener">Real-time Manhattan World Rotation Estimation in 3D</a>,IROS 2015,实时曼哈顿世界旋转估计</li></ul><h2 id="Pose-Object-tracking"><a href="#Pose-Object-tracking" class="headerlink" title="Pose/Object tracking"></a>Pose/Object tracking</h2><ul><li><a href="https://github.com/cbsudux/Human-Pose-Estimation-101" target="_blank" rel="noopener">Basics of 2D and 3D Human Pose Estimation</a>,人体姿态估计入门</li><li><a href="https://github.com/OceanPang/Libra_R-CNN" target="_blank" rel="noopener">Libra R-CNN: Towards Balanced Learning for Object Detection</a></li><li><a href="https://github.com/HRNet/HRNet-Object-Detection" target="_blank" rel="noopener">High-resolution networks (HRNets) for object detection</a>, <strong>[<a href="https://arxiv.org/pdf/1904.04514.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/xiaolonw/TimeCycle" target="_blank" rel="noopener">Learning Correspondence from the Cycle-Consistency of Time</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1903.07593" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/zju3dv/pvnet" target="_blank" rel="noopener">PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.11788" target="_blank" rel="noopener">Paper</a>], [<a href="https://zju3dv.github.io/pvnet" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/mkocabas/EpipolarPose" target="_blank" rel="noopener">Self-Supervised Learning of 3D Human Pose using Multi-view Geometry</a>, CVPR 2018, <strong>[<a href="https://arxiv.org/abs/1903.02330" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/vita-epfl/openpifpaf" target="_blank" rel="noopener">PifPaf: Composite Fields for Human Pose Estimation</a>, <strong>[<a href="https://arxiv.org/abs/1903.06593" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" target="_blank" rel="noopener">Deep High-Resolution Representation Learning for Human Pose Estimation</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09212.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://jingdongwang2017.github.io/Projects/HRNet/PoseEstimation.html" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/YuliangXiu/PoseFlow" target="_blank" rel="noopener">PoseFlow: Efficient Online Pose Tracking)</a>, BMVC 2018, <strong>[<a href="https://arxiv.org/abs/1802.00977" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/vana77/Bottom-up-Clustering-Person-Re-identification" target="_blank" rel="noopener">A Bottom-Up Clustering Approach to Unsupervised Person Re-identification</a>，AAAI 2019, 重定位</li><li><a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener">Fast Online Object Tracking and Segmentation: A Unifying Approach</a>,CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1812.05050" target="_blank" rel="noopener">Paper</a>] [<a href="https://youtu.be/I_iOVrcpEBw" target="_blank" rel="noopener">Video</a>] [<a href="http://www.robots.ox.ac.uk/~qwang/SiamMask" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/TuSimple/simpledet" target="_blank" rel="noopener">SimpleDet - A Simple and Versatile Framework for Object Detection and Instance Recognition</a>,<strong>[<a href="https://arxiv.org/abs/1903.05831" target="_blank" rel="noopener">Paper</a>]</strong> </li></ul><h2 id="Depth-Disparity-amp-Flow-estimation"><a href="#Depth-Disparity-amp-Flow-estimation" class="headerlink" title="Depth/Disparity &amp; Flow estimation"></a>Depth/Disparity &amp; Flow estimation</h2><ul><li><a href="https://github.com/muskie82/AR-Depth-cpp" target="_blank" rel="noopener">Fast Depth Densification for Occlusion-aware Augmented Reality</a>, SIGGRAPH-Asia 2018, <strong>[<a href="https://homes.cs.washington.edu/~holynski/publications/occlusion/index.html" target="_blank" rel="noopener">Project Page</a>]</strong>,<a href="https://github.com/facebookresearch/AR-Depth" target="_blank" rel="noopener">another version</a></li><li><a href="https://github.com/CVLAB-Unibo/Learning2AdaptForStereo" target="_blank" rel="noopener">Learning To Adapt For Stereo</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1904.02957" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/JiaRenChang/PSMNet" target="_blank" rel="noopener">Pyramid Stereo Matching Network</a>,<strong>[<a href="https://arxiv.org/abs/1803.08669" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/lelimite4444/BridgeDepthFlow" target="_blank" rel="noopener">Bridging Stereo Matching and Optical Flow via Spatiotemporal Correspondence</a>, <strong>[<a href="https://arxiv.org/abs/1905.09265" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/wvangansbeke/Sparse-Depth-Completion" target="_blank" rel="noopener">Sparse Depth Completion</a>, <strong>[<a href="https://arxiv.org/pdf/1902.05356.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, RGB图像辅助雷达深度估计</li><li><a href="https://github.com/sshan-zhao/GASDA" target="_blank" rel="noopener">GASDA</a>, CVPR 2019, <strong>[<a href="https://sshan-zhao.github.io/papers/gasda.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/xy-guo/MVSNet_pytorch" target="_blank" rel="noopener">MVSNet: Depth Inference for Unstructured Multi-view Stereo</a>, <strong>[<a href="https://arxiv.org/abs/1804.02505" target="_blank" rel="noopener">Paper</a>]</strong>, 非官方实现版本的MVSNet</li><li><a href="https://github.com/HKUST-Aerial-Robotics/Stereo-RCNN" target="_blank" rel="noopener">Stereo R-CNN based 3D Object Detection for Autonomous Driving</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09738.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo" target="_blank" rel="noopener">Real-time self-adaptive deep stereo</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1810.05424" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/ialhashim/DenseDepth" target="_blank" rel="noopener">High Quality Monocular Depth Estimation via Transfer Learning</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.11941" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://ialhashim.github.io/publications/index.html" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/xy-guo/GwcNet" target="_blank" rel="noopener">Group-wise Correlation Stereo Network</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1903.04025" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/phuang17/DeepMVS" target="_blank" rel="noopener">DeepMVS: Learning Multi-View Stereopsis</a>, CVPR 2018,<strong>[<a href="https://phuang17.github.io/DeepMVS/index.html" target="_blank" rel="noopener">Project Page</a>]</strong>,多目深度估计</li><li><a href="https://github.com/sampepose/flownet2-tf" target="_blank" rel="noopener">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</a>, CVPR 2017, 深度学习光流恢复</li><li><a href="https://github.com/DLuensch/StereoVision-ADCensus" target="_blank" rel="noopener">StereoVision-ADCensus</a>,深度恢复代码集合(<strong>ADCensus, SGBM, BM</strong>)</li><li><a href="https://github.com/yangguorun/SegStereo" target="_blank" rel="noopener">SegStereo: Exploiting Semantic Information for Disparity Estimation</a>, 探究语义信息在深度估计中的作用</li><li><a href="https://github.com/kuantingchen04/Light-Field-Depth-Estimation" target="_blank" rel="noopener">Light Filed Depth Estimation using GAN</a>，利用GAN进行光场深度恢复</li><li><a href="https://github.com/daniilidis-group/EV-FlowNet" target="_blank" rel="noopener">EV-FlowNet: Self-Supervised Optical Flow for Event-based Cameras</a>,Proceedings of Robotics 2018,<strong>[<a href="https://arxiv.org/abs/1802.06898" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/vt-vl-lab/DF-Net" target="_blank" rel="noopener">DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency</a>, ECCV 2018, <strong>[<a href="https://arxiv.org/abs/1809.01649" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/yzcjtr/GeoNet" target="_blank" rel="noopener">GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose</a>, CVPR 2018, <strong>[<a href="https://arxiv.org/abs/1803.02276" target="_blank" rel="noopener">Paper</a>]</strong> </li></ul><h2 id="3D-amp-Graphic"><a href="#3D-amp-Graphic" class="headerlink" title="3D &amp; Graphic"></a>3D &amp; Graphic</h2><ul><li><a href="https://github.com/PRBonn/refusion" target="_blank" rel="noopener">ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals</a>, <strong>[<a href="https://arxiv.org/pdf/1905.02082.pdf" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/Lotayou/densebody_pytorch" target="_blank" rel="noopener">densebody_pytorch</a>, <strong>[<a href="https://arxiv.org/abs/1903.10153v3" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/svip-lab/PlanarReconstruction" target="_blank" rel="noopener">Single-Image Piece-wise Planar 3D Reconstruction via Associative Embedding</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09777.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, 单目3D重建</li><li><a href="https://github.com/sunset1995/HorizonNet" target="_blank" rel="noopener">HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch Data Augmentation</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1901.03861" target="_blank" rel="noopener">Paper</a>]</strong>, 深度学习全景转3D</li><li><a href="https://github.com/Microsoft/O-CNN" target="_blank" rel="noopener">Adaptive O-CNN: A Patch-based Deep Representation of 3D Shapes</a>,SIGGRAPH Asia 2018, <strong>[<a href="https://wang-ps.github.io/AO-CNN.html" target="_blank" rel="noopener">Project Page</a>]</strong></li></ul><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><ul><li><a href="https://live.bilibili.com/7332534?visit_id=9ytrx9lpsy80" target="_blank" rel="noopener">End-to-end Adversarial Learning for Generative Conversational Agents</a>，2017，介绍了一种端到端的基于GAN的聊天机器人</li><li><a href="https://github.com/yulunzhang/RNAN" target="_blank" rel="noopener">Residual Non-local Attention Networks for Image Restoration</a>,ICLR 2019.</li><li><a href="https://github.com/HelenMao/MSGAN" target="_blank" rel="noopener">MSGAN: Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis</a>, CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1903.05628" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/NVlabs/SPADE" target="_blank" rel="noopener">SPADE: Semantic Image Synthesis with Spatially-Adaptive Normalization</a>,CVPR 2019, <strong>[<a href="https://nvlabs.github.io/SPADE/" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/Oldpan/Faceswap-Deepfake-Pytorch" target="_blank" rel="noopener">Faceswap with Pytorch or DeepFake with Pytorch</a>, 假脸</li></ul><h2 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h2><ul><li><a href="https://github.com/RemoteML/bestofml" target="_blank" rel="noopener">The best resources around Machine Learning</a></li><li><a href="https://github.com/cydonia999/VGGFace2-pytorch" target="_blank" rel="noopener">VGGFace2: A dataset for recognising faces across pose and age</a></li><li><a href="https://github.com/SmirkCao/Lihang" target="_blank" rel="noopener">Statistical learning methods</a></li></ul><h2 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h2><ul><li><a href="https://github.com/ZhaoJ9014/face.evoLVe.PyTorch" target="_blank" rel="noopener">High-Performance Face Recognition Library on PyTorch</a>，人脸识别库</li><li><a href="https://github.com/enggen/Deep-Learning-Coursera" target="_blank" rel="noopener">Deep-Learning-Coursera</a>，深度学习教程（deeplearning.ai）</li></ul><h2 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h2><ul><li><a href="https://github.com/JuliaLang/julia" target="_blank" rel="noopener">Julia</a></li><li><a href="https://github.com/alan-turing-institute/MLJ.jl" target="_blank" rel="noopener">A Julia machine learning framework</a>，一种基于Julia的机器学习框架</li></ul><p><img alt data-src="https://github.com/alan-turing-institute/MLJ.jl/blob/master/doc/two_model_stack.png"></p><h2 id="Collections"><a href="#Collections" class="headerlink" title="Collections"></a>Collections</h2><ul><li><a href="https://github.com/wwxFromTju/awesome-reinforcement-learning-zh" target="_blank" rel="noopener">awesome-reinforcement-learning-zh</a>,强化学习从入门到放弃的资料</li><li><a href="https://github.com/uzh-rpg/event-based_vision_resources" target="_blank" rel="noopener">Event-based Vision Resources</a>，关于事件相机的资源</li><li><a href="https://github.com/DeepTecher/AutonomousVehiclePaper" target="_blank" rel="noopener">AutonomousVehiclePaper</a>，无人驾驶相关论文速递</li><li><a href="https://github.com/wutianyiRosun/Segmentation.X" target="_blank" rel="noopener">Segmentation.X</a>, Segmentation相关论文&amp;代码</li><li><a href="https://github.com/amusi/CVPR2019-Code" target="_blank" rel="noopener">CVPR-2019</a>, CVPR 2019 论文开源项目合集</li><li><a href="https://github.com/kanster/awesome-slam" target="_blank" rel="noopener">awesome-slam</a>, SLAM合集</li><li><a href="https://github.com/tzutalin/awesome-visual-slam" target="_blank" rel="noopener">awesome-visual-slam</a>, 视觉SLAM合集</li><li><a href="https://github.com/zziz/pwc" target="_blank" rel="noopener">Papers with code</a>, 周更论文with代码</li><li><a href="https://github.com/cbsudux/awesome-human-pose-estimation" target="_blank" rel="noopener">Awesome Human Pose Estimation</a>,<a href="https://github.com/nkalavak/awesome-object-pose" target="_blank" rel="noopener">awesome-object-pose</a>, 位姿估计合集</li><li><a href="https://github.com/mrgloom/awesome-semantic-segmentation" target="_blank" rel="noopener">Awesome Semantic Segmentation</a>, 语义分割集合</li><li><a href="https://github.com/mengyuest/iros2018-slam-papers" target="_blank" rel="noopener">IROS2018 SLAM Collections</a>, IROS 2018集合</li><li><a href="https://github.com/TerenceCYJ/VP-SLAM-SC-papers" target="_blank" rel="noopener">VP-SLAM-SC-papers</a>,Visual Positioning &amp; SLAM &amp; Spatial Cognition 论文统计与分析</li><li><a href="https://github.com/HuaizhengZhang/Awesome-System-for-Machine-Learning" target="_blank" rel="noopener">Awesome System for Machine Learning</a></li><li><a href="https://github.com/Thinkgamer/Machine-Learning-With-Python" target="_blank" rel="noopener">Machine-Learning-With-Python</a>, 《机器学习实战》python代码实现</li><li><a href="https://github.com/qqfly/how-to-learn-robotics" target="_blank" rel="noopener">How to learn robotics</a>, 开源机器人学学习指南</li><li><a href="https://github.com/kjw0612/awesome-deep-vision" target="_blank" rel="noopener">Awesome Deep Vision</a>,DL在CV领域的应用</li><li><a href="https://github.com/YapengTian/Single-Image-Super-Resolution" target="_blank" rel="noopener">Single-Image-Super-Resolution</a>, 一个有关<strong>图像超分辨</strong>的合集</li><li><a href="https://github.com/wifity/ai-report" target="_blank" rel="noopener">ai report</a>, AI相关的研究报告</li><li><a href="https://paperswithcode.com/sota" target="_blank" rel="noopener">State-of-the-art papers and code</a>,搜集了目前sota的论文以及代码</li><li><a href="https://github.com/extreme-assistant/cvpr2019" target="_blank" rel="noopener">CVPR 2019 (Papers/Codes/Project/Paper reading)</a></li><li><a href="https://github.com/openMVG/awesome_3DReconstruction_list" target="_blank" rel="noopener">A curated list of papers &amp; resources linked to 3D reconstruction from images</a>,有关三维重建的论文汇总</li><li><a href="https://github.com/nebula-beta/SLAM-Jobs" target="_blank" rel="noopener">SLAM-Jobs</a>, SLAM/SFM求职指南</li></ul><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><ul><li><a href="https://github.com/cszn/DPSR" target="_blank" rel="noopener">Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels</a>,CVPR 2019,超分辨</li><li><a href="https://github.com/lzhbrian/Cool-Fashion-Papers" target="_blank" rel="noopener">Cool Fashion Papers</a>, Cool resources about Fashion + AI.</li><li><a href="https://github.com/nbei/Deep-Flow-Guided-Video-Inpainting" target="_blank" rel="noopener">Deep Flow-Guided Video Inpainting</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1806.10447.pdf" target="_blank" rel="noopener">Paper</a>]</strong> ,图像修复</li><li><a href="https://github.com/dbolya/yolact" target="_blank" rel="noopener">YOLACT: Real-time Instance Segmentation</a></li><li><a href="https://github.com/lyl8213/Plate_Recognition-LPRnet" target="_blank" rel="noopener">LPRNet: License Plate Recognition via Deep Neural Networks</a>, <strong>[<a href="https://arxiv.org/pdf/1806.10447.pdf" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/xiaofengShi/CHINESE-OCR" target="_blank" rel="noopener">CHINESE-OCR</a>, 运用tf实现自然场景文字检测</li><li><a href="https://github.com/PerpetualSmile/BeautyCamera" target="_blank" rel="noopener">BeautyCamera</a>, 美颜相机，具有人脸检测、磨皮美白人脸、滤镜、调节图片、摄像功能</li><li><a href="https://github.com/zhengzhugithub/CV-arXiv-Daily" target="_blank" rel="noopener">CV-arXiv-Daily</a>, 分享计算机视觉每天的arXiv文章</li><li>Pluralistic-Inpainting, <a href="https://arxiv.org/abs/1903.04227" target="_blank" rel="noopener">ArXiv</a> | <a href="http://www.chuanxiaz.com/publication/pluralistic/" target="_blank" rel="noopener">Project Page</a> | <a href="http://www.chuanxiaz.com/project/pluralistic/" target="_blank" rel="noopener">Online Demo</a> | <a href="https://www.youtube.com/watch?v=9V7rNoLVmSs" target="_blank" rel="noopener">Video(demo)</a></li><li><a href="https://github.com/Jezzamonn/fourier" target="_blank" rel="noopener">An Interactive Introduction to Fourier Transforms</a>, 超棒的傅里叶变换图形化解释</li><li><a href="https://github.com/datawhalechina/pumpkin-book" target="_blank" rel="noopener">pumpkin-book</a>, 《机器学习》（西瓜书）公式推导解析</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The post contains papers-with-code about SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc. &lt;a href=&quot;https://github.com/Vincentqyw/Recent-Stars-2019&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/Vincentqyw/Recent-Stars-2019.svg?logo=github&amp;amp;label=Stars&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="disparity" scheme="https://www.vincentqin.tech/tags/disparity/"/>
    
      <category term="pose-tracking" scheme="https://www.vincentqin.tech/tags/pose-tracking/"/>
    
      <category term="object-tracking" scheme="https://www.vincentqin.tech/tags/object-tracking/"/>
    
      <category term="depth-estimation" scheme="https://www.vincentqin.tech/tags/depth-estimation/"/>
    
      <category term="flow-estimation" scheme="https://www.vincentqin.tech/tags/flow-estimation/"/>
    
      <category term="3D-graphics" scheme="https://www.vincentqin.tech/tags/3D-graphics/"/>
    
  </entry>
  
  <entry>
    <title>开启SSR模式</title>
    <link href="https://www.vincentqin.tech/posts/build-ssr-server/"/>
    <id>https://www.vincentqin.tech/posts/build-ssr-server/</id>
    <published>2019-03-31T05:56:15.000Z</published>
    <updated>2020-04-15T14:39:28.955Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>关于科学上网，食用别人调配的酸酸乳总觉味道不对，自食其力心里才会感到踏实。受<a href="https://newdee.cf/" target="_blank" rel="noopener">Newdee老贼</a>指点，鄙人成功在服务器上开启了酸酸乳服务。详细过程Newdee已经在博文“<a href="https://newdee.cf/posts/1420aa47/" target="_blank" rel="noopener">SS服务器搭建</a>”介绍地相当详细。本人记性不好，遂本文将记录几个关键步骤，以备后续不时之需。<br><strong><font color="#FF0000" face="宋体">注意：本文仅供个人学习使用，不可用于商业或者违法行为！</font></strong></p><a id="more"></a><h2 id="购买服务器"><a href="#购买服务器" class="headerlink" title="购买服务器"></a>购买服务器</h2><ul><li>购买服务器(支持alipay &amp; wechat pay)，地址: <a href="https://www.vultr.com/?ref=7996819" target="_blank" rel="noopener">https://www.vultr.com/</a></li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/vultr.png"></p><ul><li>经过几个步骤：1. Server Location, 2. Server Type, 3. Server Size, 4. Additional Features,5,6,7可以忽略，最后点击右下角的<strong>Deploy New</strong>即可部署。</li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/buy-vultr.png"></p><p>后台是这样的：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/vultr-backend.png"></p><p>然后根据IP以及用户名利用SSH在本地进行远程连接，进行以下步骤。</p><h2 id="安装SSR"><a href="#安装SSR" class="headerlink" title="安装SSR"></a>安装SSR</h2><p>如果是单用户使用，进入服务器直接执行下述命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -N --no-check-certificate https://raw.githubusercontent.com/Vincentqyw/doubi/master/ssr.sh &amp;&amp; chmod +x ssr.sh &amp;&amp; bash ssr.sh</span><br></pre></td></tr></table></figure><p>关于加密协议以及混淆的设置参见下图：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/account.png"></p><p>多用户使用的版本（可配置多个账号）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/ssrmu.sh &amp;&amp; chmod +x ssrmu.sh &amp;&amp; bash ssrmu.sh</span><br></pre></td></tr></table></figure><p>设置完毕之后，后续进行管理直接运行<code>bash ssrmu.sh</code>选择不同的功能项即可。</p><h2 id="封禁某些端口-可选"><a href="#封禁某些端口-可选" class="headerlink" title="封禁某些端口(可选)"></a>封禁某些端口(可选)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -N --no-check-certificate https://raw.githubusercontent.com/Vincentqyw/doubi/master/ban_iptables.sh &amp;&amp; chmod +x ban_iptables.sh &amp;&amp; bash ban_iptables.sh</span><br></pre></td></tr></table></figure><p>选择封禁垃圾邮件端口就行。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/ban-mails.png"></p><h2 id="BBR加速-可选"><a href="#BBR加速-可选" class="headerlink" title="BBR加速(可选)"></a>BBR加速(可选)</h2><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/bbr.png"></p><h2 id="安装SSRR-可选"><a href="#安装SSRR-可选" class="headerlink" title="安装SSRR(可选)"></a>安装SSRR(可选)</h2><p>接下来的链接给出了SSRR的安装教程，不再赘述，<a href="https://gist.github.com/biezhi/45fac901f02f7c867e46aecd41076d70#kcp-%E5%AE%A2%E6%88%B7%E7%AB%AF" target="_blank" rel="noopener">Link</a>。</p><h2 id="建立快照"><a href="#建立快照" class="headerlink" title="建立快照"></a>建立快照</h2><p>建立系统快照就是将系统某个状态下的各种数据记录在一个文件里，下一次新建完主机后恢复快照就能够恢复成之前系统的样子。</p><p>若已有了主机，点击下图所示的<code>Snapshots</code>对该系统建立快照。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/snapshots-step1.png"></p><p>随后就会出现下图所示的页面，在<code>Label</code>一栏输入这个快照的标签，方便区分不同的快照。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/snapshots-step2.png"></p><p>若想对新建的系统恢复以前建立的快照，可以点击上图中的类似于<code>循环</code>的标志。</p><h2 id="不可描述"><a href="#不可描述" class="headerlink" title="不可描述"></a>不可描述</h2><ul><li><a href="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/SSR-WIN-ANDROID-IOS.7z" target="_blank" rel="noopener">不可描述</a></li><li>PC终端可自行挑选，鄙人推荐<a href="https://www.termius.com/windows" target="_blank" rel="noopener">termius</a>, <a href="https://git-scm.com/downloads" target="_blank" rel="noopener">Git Bash</a></li><li>参考链接：<a href="https://gist.github.com/biezhi/45fac901f02f7c867e46aecd41076d70" target="_blank" rel="noopener">ShadowsocksR 协议插件文档</a></li></ul><p>手机端以及电脑端输入对应的IP/端口/混淆/加密等信息进行连接即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于科学上网，食用别人调配的酸酸乳总觉味道不对，自食其力心里才会感到踏实。受&lt;a href=&quot;https://newdee.cf/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Newdee老贼&lt;/a&gt;指点，鄙人成功在服务器上开启了酸酸乳服务。详细过程Newdee已经在博文“&lt;a href=&quot;https://newdee.cf/posts/1420aa47/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SS服务器搭建&lt;/a&gt;”介绍地相当详细。本人记性不好，遂本文将记录几个关键步骤，以备后续不时之需。&lt;br&gt;&lt;strong&gt;&lt;font color=&quot;#FF0000&quot; face=&quot;宋体&quot;&gt;注意：本文仅供个人学习使用，不可用于商业或者违法行为！&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="科学上网" scheme="https://www.vincentqin.tech/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/"/>
    
  </entry>
  
  <entry>
    <title>虚实:「未麻的部屋」</title>
    <link href="https://www.vincentqin.tech/posts/recent-status/"/>
    <id>https://www.vincentqin.tech/posts/recent-status/</id>
    <published>2019-01-13T09:55:31.000Z</published>
    <updated>2020-03-31T14:51:06.334Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/recent-status/Perfect-Blue-cover.png"></p><a id="more"></a><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/recent-status/post-2.jpg"></p><p>过去，到底是哪一条支线造就了现在的自己。虚实之间，到底是谁在支配？</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://vincentqin.gitee.io/blogresource-1/recent-status/Perfect-Blue-cover.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>笔记：李群与李代数求导</title>
    <link href="https://www.vincentqin.tech/posts/LieAlgebra/"/>
    <id>https://www.vincentqin.tech/posts/LieAlgebra/</id>
    <published>2018-12-04T15:40:56.000Z</published>
    <updated>2020-03-31T15:37:28.963Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note primary">            <p>最近一段时间在推$Jacobian$，会用到一些关于李代数求导的知识。参考高博《视觉slam十四讲》一书，在此总结一些常用的关于李群与李代数相关的知识点。</p>          </div><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在SLAM中位姿是未知的，而我们需要解决什么样的相机位姿最符合当前观测数据这样的问题。一种典型的方式是把它构建成一个优化问题，求解最优的$R$,$t$，使得误差最小化。<br>旋转矩阵自身是带有约束的（正交且行列式为 1）。它们作为优化变量时，会引入额外的约束，使优化变得困难。通过李群——李代数间的转换关系，我们希望把位姿估计变成无约束的优化问题，简化求解方式。群（Group）是一种集合加上一种运算的代数结构，李群是指具有连续（光滑）性质的群，李群在相机姿态估计时具有重要意义，接下来主要讨论特殊正交群$SO(n)$与特殊欧式群$SE(n)$。</p><h2 id="特殊正交群与特殊欧式群"><a href="#特殊正交群与特殊欧式群" class="headerlink" title="特殊正交群与特殊欧式群"></a>特殊正交群与特殊欧式群</h2><script type="math/tex; mode=display">SO(n) = \{ \mathbf{R} \in \mathbb{R}^{n \times n} | \mathbf{R R}^T = \mathbf{I}, det(\mathbf{R})=1 \}</script><script type="math/tex; mode=display">SE(3) = \left\{ \mathbf{T} = \left[ {\begin{array}{*{20}{c}} \mathbf{R} & \mathbf{t} \\ \mathbf{0}^T & 1 \end{array}} \right]  \in \mathbb{R}^{4 \times 4} | \mathbf{R} \in SO(3), \mathbf{t} \in \mathbb{R}^3\right\}</script><p>在李群中，我们使用矩阵来表达一个旋转和平移，这存在冗余的自由度。三维空间的旋转只有三自由度，旋转+平移有六自由度。因此，我们希望寻找一个没有冗余自由度（但是相应的存在奇异性）的表示，也就是李代数$\mathfrak{so}(3)$和$\mathfrak{se}(3)$。且无论是旋转还是变换矩阵，它们都是对加法不封闭的，但是对乘法是封闭的。</p><h2 id="李代数的引出"><a href="#李代数的引出" class="headerlink" title="李代数的引出"></a>李代数的引出</h2><p>对于任意旋转矩阵$R$，它必定满足：</p><script type="math/tex; mode=display">\mathbf{R} \mathbf{R}^T＝\mathbf{I}.</script><p>考虑它随时间发生变化，即从$\mathbf{R}$变为$\mathbf{R(t)}$，它仍然满足如下如下等式：</p><script type="math/tex; mode=display">\mathbf{R}(t) \mathbf{R}(t) ^T = \mathbf{I}</script><p>对两侧同时对时间求导数得：</p><script type="math/tex; mode=display">\mathbf{\dot{R}} (t) \mathbf{R} {(t)^T} + \mathbf{R} (t) \mathbf{\dot{R}} {(t)^T} = 0</script><p>则有：</p><script type="math/tex; mode=display">\mathbf{\dot{R}} (t) \mathbf{R} {(t)^T} = - \left(  \mathbf{\dot{R}} (t) \mathbf{R} {(t)^T} \right)^T</script><p>可见$\mathbf{\dot{R}} (t) \mathbf{R} {(t)^T}$是一个反对称矩阵，将其记作$\mathbf{A}$，于是$\mathbf{A}^T=-\mathbf{A}$,所以它主对角线元素必为，而非对角线元素则只有三个自由度。我们一定可以找到一个这样的向量$\mathbf{a}=[a_1, a_2, a_3]^T$使得：</p><script type="math/tex; mode=display">{\mathbf{a}^ \wedge } = \mathbf{A} = \left[ {\begin{array}{*{20}{c}} 0& -a_3 & a_2\\ {a_3}&0& - {a_1}\\  - {a_2}&{a_1}&0 \end{array}} \right]</script><p>其中$^{\wedge}$符号表示由向量转换为矩阵，反之我们也可以用$^{\vee}$符号定义由矩阵转换为向量的方式:</p><script type="math/tex; mode=display">{ \mathbf{A}^ \vee } = \mathbf{a}</script><p>现在，由于$\mathbf{\dot{R}} (t) \mathbf{R} {(t)^T}$是一个反对称矩阵，所以我们一定可以找到一个三维向量$\mathbf{\phi} (t) \in \mathbb{R}^3$与之对应。于是我们有：</p><script type="math/tex; mode=display">\mathbf{ \dot{R} } (t) \mathbf{R}(t)^T = \mathbf{\phi} (t) ^ {\wedge}</script><p>左右各右乘$\mathbf{R}(t)$，由于其正交性，有：</p><script type="math/tex; mode=display">\mathbf{ \dot{R} } (t)  = \mathbf{\phi} (t)^{\wedge} \mathbf{R}(t) =   \left[ {\begin{array}{*{20}{c}} 0&- {\phi _3}&{\phi _2}\\ {\phi _3}&0& - {\phi _1}\\ { - \phi _2}&{\phi _1}&0 \end{array}} \right] \mathbf{R} (t)</script><p>可以看到，每对旋转矩阵求一次导数，只需左乘一个矩阵$\mathbf{\phi} (t)^{\wedge}$即可。由于$\mathbf{\phi} (t)^{\wedge}$反映了的导数性质，故称它在的正切空间(tangent space)上。同时在$t_0$附近，设$\mathbf{\phi}$保持为常数$\mathbf{\phi}(t_0)=\mathbf{\phi}_0$，我们有：</p><script type="math/tex; mode=display">\mathbf{ \dot{R} } (t)  = \mathbf{\phi} (t_0)^{\wedge} \mathbf{R}(t)= \mathbf{\phi}_0^{\wedge} \mathbf{R}(t)</script><p>又因为初始值$\mathbf{ R } (0) = \mathbf{I} $对上式进行求解可得：</p><script type="math/tex; mode=display">\label{eq:so3ode} \mathbf{R}(t) = \exp \left( \mathbf{\phi}_0^{\wedge} t\right) .</script><p>上式描述$\mathbf{R}$在局部的导数关系。</p><h2 id="李代数-mathfrak-so-3"><a href="#李代数-mathfrak-so-3" class="headerlink" title="李代数 $\mathfrak{so}(3)$"></a>李代数 $\mathfrak{so}(3)$</h2><p>上文提及的$\mathbf{\phi}$是一种李代数，$SO(3)$对应的李代数是定义在$\mathbb{R}^3$上的向量，我们记作$\mathbf{\phi}$，它 对应与一个反对称矩阵：</p><script type="math/tex; mode=display">\label{eq:phi} \mathbf{\Phi} = \mathbf{\phi}^{\wedge} = \left[ {\begin{array}{*{20}{c}}     0&{ - \phi _3}&{\phi _2}\\     {\phi _3}&0&{ - \phi _1}\\     { - \phi _2}&{\phi _1}&0     \end{array}} \right] \in \mathbb{R}^{3 \times 3}</script><p>由于它与反对称矩阵关系很紧密，在不引起歧义的情况下，就说的元素是3维向量或者3维反对称矩阵，不加区别：</p><script type="math/tex; mode=display">\bbox[5px,border:2px solid red]{\mathfrak{so}(3) = \left\{ \Phi = \mathbf{\phi^\wedge} \in \mathbb{R}^{3 \times 3} | \mathbf{\phi} \in \mathbb{R}^3 \right\}}</script><h2 id="李代数-mathfrak-se-3"><a href="#李代数-mathfrak-se-3" class="headerlink" title="李代数 $\mathfrak{se}(3)$"></a>李代数 $\mathfrak{se}(3)$</h2><p>$SE(3)$对应的李代数为$\mathfrak{se}(3)$，$\mathfrak{se}(3)$定义在$\mathbb{R}^{6}$空间，其具体形式如下：</p><script type="math/tex; mode=display">\bbox[5px,border:2px solid red]{\mathfrak{se}(3) = \left\{ \mathbf{ \xi } = \left[ \begin{array}{l}     \mathbf{\rho} \\     \mathbf{\phi}      \end{array} \right] \in \mathbb{R}^{6}, \mathbf{\rho} \in \mathbb{R}^{3},\mathbf{\phi} \in \mathfrak{so}(3),\mathbf{\xi}^\wedge  = \left[ {\begin{array}{*{20}{c}}     \mathbf{\phi} ^ \wedge &\mathbf{\rho} \\ \mathbf{0}^T&0 \end{array}} \right] \in \mathbb{R}^{4 \times 4} \right\}}</script><p>$\mathfrak{se}(3)$是一个这样的六维向量，前三维表示平移，记作$\mathbf{\rho}$；后三维表示旋转，记作$\mathbf{\phi}$（有时候这两个参数会反过来，可也可以的）。</p><h2 id="指数映射"><a href="#指数映射" class="headerlink" title="指数映射"></a>指数映射</h2><p>$\mathfrak{so}(3)$以及$\mathfrak{se}(3)$的指数映射分别对应于$SO(3)$以及$SE(3)$，它们之间的转换关系可以由下图表示：</p><p><img alt="lieGroup" data-src="https://vincentqin.gitee.io/blogresource-3/LieAlgebra/lieGroup.png"></p><h2 id="李代数求导"><a href="#李代数求导" class="headerlink" title="李代数求导"></a>李代数求导</h2><h3 id="对旋转矩阵李代数求导"><a href="#对旋转矩阵李代数求导" class="headerlink" title="对旋转矩阵李代数求导"></a>对旋转矩阵李代数求导</h3><p>对$\mathbf{R}$进行一次扰动$\Delta \mathbf{R}$，假设左扰动$\Delta \mathbf{R}$对应的李代数为$ {\boldsymbol \varphi}$，对$ {\boldsymbol \varphi}$求导，得到：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial ({\boldsymbol Rp})}{\partial {\boldsymbol \varphi}}&= \lim_{\boldsymbol \varphi \to 0}\frac{ \overbrace{ \exp ({\boldsymbol \varphi}^{\land}) }^{\color{Red}{可作泰勒展开}} \exp ({\boldsymbol \phi}^{\land}) {\boldsymbol p} - \exp ({\boldsymbol \phi}^{\land}) {\boldsymbol p}}{ {\boldsymbol \varphi} }\\&\approx \lim_{\boldsymbol \varphi \to 0}\frac{({\boldsymbol I} + {\boldsymbol \varphi}^{\land}) \exp ({\boldsymbol \phi}^{\land}) {\boldsymbol p} - \exp ({\boldsymbol \phi}^{\land}) {\boldsymbol p}}{ {\boldsymbol \varphi} }  \\&= \lim_{\boldsymbol \varphi \to 0}\frac{ {\boldsymbol \varphi}^{\land} {\boldsymbol {Rp}} }{ {\boldsymbol \varphi} }  \\&= \lim_{\boldsymbol \varphi \to 0}\frac{ -({\boldsymbol {Rp}})^{\land} {\boldsymbol \varphi} }{ {\boldsymbol \varphi} } \\&= -({\boldsymbol {Rp}})^{\land}\end{aligned}</script><h3 id="变换转矩阵李代数求导"><a href="#变换转矩阵李代数求导" class="headerlink" title="变换转矩阵李代数求导"></a>变换转矩阵李代数求导</h3><p>假设空间点${\boldsymbol p}$经过一次变换${\boldsymbol T}$（对应的李代数为${\boldsymbol \xi}$）后变为 ${\boldsymbol Tp}$ 。当给${\boldsymbol T}$左乘一个扰动$\Delta {\boldsymbol T} = \exp (\delta {\boldsymbol \xi}^{\land})$，设扰动项的李代数为$\delta {\boldsymbol \xi} = [\delta {\boldsymbol \rho}, \delta {\boldsymbol \phi}]^{T}$，有：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial ({\boldsymbol {Tp}})}{\partial \delta{\boldsymbol \xi}}&= \lim_{\delta{\boldsymbol \xi} \to 0}\frac{ \overbrace{ \exp (\delta {\boldsymbol \xi}^{\land}) }^{\color{Red}{可作泰勒展开}}  \exp ({\boldsymbol \xi}^{\land}) {\boldsymbol p} - \exp ({\boldsymbol \xi}^{\land}) {\boldsymbol p}}{ \delta {\boldsymbol \xi} } \\&\approx \lim_{\delta{\boldsymbol \xi} \to 0}\frac{ ({\boldsymbol I} + \delta {\boldsymbol \xi}^{\land}) \exp ({\boldsymbol \xi}^{\land}) {\boldsymbol p} - \exp ({\boldsymbol \xi}^{\land}) {\boldsymbol p} }{ \delta {\boldsymbol \xi} } \\&= \lim_{\delta{\boldsymbol \xi} \to 0}\frac{  \delta {\boldsymbol \xi}^{\land} \exp ({\boldsymbol \xi}^{\land}) {\boldsymbol p}  }{ \delta {\boldsymbol \xi} } \\&= \lim_{\delta{\boldsymbol \xi} \to 0}\frac{\begin{bmatrix} \delta {\boldsymbol \phi}^{\land}  &   \delta {\boldsymbol \rho}     \\     {\boldsymbol 0}^{T}                 &                      1                           \\\end{bmatrix}\begin{bmatrix}   {\boldsymbol {Rp}} +  {\boldsymbol t}     \\                                     1                                \\\end{bmatrix}}{ \delta {\boldsymbol \xi} } \\&= \lim_{\delta{\boldsymbol \xi} \to 0}\frac{\begin{bmatrix}   \delta {\boldsymbol \phi}^{\land} ({\boldsymbol {Rp}} + {\boldsymbol t}) + \delta {\boldsymbol \rho}     \\                                     0                                \\\end{bmatrix}}{ \delta {\boldsymbol \xi} } \\&=\overbrace{\begin{bmatrix} {\boldsymbol I}            &   -({\boldsymbol {Rp}} + {\boldsymbol t})^{\land}    \\ {\boldsymbol 0}^{T}    &     {\boldsymbol 0}^{T}             \\\end{bmatrix}}^{\color{Red}{上式分块求导}}\\&= ({\boldsymbol {Tp}})^{\bigodot}\end{aligned}</script><p>上式中运算符号$\bigodot$的含义：将一个齐次坐标的空间点变换成一个$4 \times 6$的矩阵。</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><h3 id="SE-3-左扰"><a href="#SE-3-左扰" class="headerlink" title="$SE(3)$左扰"></a>$SE(3)$左扰</h3><script type="math/tex; mode=display">\begin{aligned}\rm{exp}\left( {\Delta {\xi}^{\land} } \right){\rm{exp}}\left( {\xi}^{\land}  \right)& \approx \left( {I + {\left[ {\Delta \xi } \right]}_ \times } \right){\rm{exp}}(\xi) \\&= \left( I_{4 \times 4} +\left[\begin{array}{*{20}{c}}{  \left[ \Delta \phi  \right]}_{\times}   &   \Delta \rho    \\0&0\end{array}\right]\right)\left[\begin{array}{*{20}{c}} R   &   t    \\0&1\end{array}\right] \\&=\left[\begin{array}{*{20}{c}}{  \left[ \Delta \phi  \right]}_{\times}+I_{3 \times 3}   &   \Delta \rho    \\0&1\end{array}\right]\left[\begin{array}{*{20}{c}} R   &   t    \\0&1\end{array}\right] \\&=\left[\begin{array}{*{20}{c}} \left( \left[ \Delta \phi  \right]_{\times}+I_{3 \times 3} \right) R  &    \left( \left[ \Delta \phi  \right]_{\times}+I_{3 \times 3} \right) t + \Delta \rho    \\0&1\end{array}\right]\end{aligned}</script><h3 id="SE-3-右扰"><a href="#SE-3-右扰" class="headerlink" title="$SE(3)$右扰"></a>$SE(3)$右扰</h3><script type="math/tex; mode=display">\begin{aligned}\rm{exp}\left( {\xi}^{\land}  \right) \rm{exp}\left( {\Delta {\xi}^{\land} } \right)& \approx {\rm{exp}}({\xi}^{\land}) \left( {I + {\left[ {\Delta \xi } \right]}_ \times } \right)\\&=\left[\begin{array}{*{20}{c}} R   &   t    \\0&1\end{array}\right]\left( I_{4 \times 4} +\left[\begin{array}{*{20}{c}}{  \left[ \Delta \phi  \right]}_{\times}   &   \Delta \rho    \\0&0\end{array}\right]\right)\\&=\left[\begin{array}{*{20}{c}} R   &   t    \\0&1\end{array}\right]\left[\begin{array}{*{20}{c}}{  \left[ \Delta \phi  \right]}_{\times}+I_{3 \times 3}   &   \Delta \rho    \\0&1\end{array}\right] \\&=\left[\begin{array}{*{20}{c}} R \left( \left[ \Delta \phi  \right]_{\times}+I_{3 \times 3} \right)  &   R \Delta \rho + t   \\0&1\end{array}\right]\end{aligned}</script>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note primary&quot;&gt;
            &lt;p&gt;最近一段时间在推$Jacobian$，会用到一些关于李代数求导的知识。参考高博《视觉slam十四讲》一书，在此总结一些常用的关于李群与李代数相关的知识点。&lt;/p&gt;
          &lt;/div&gt;
    
    </summary>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/categories/SLAM/"/>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="computer vision" scheme="https://www.vincentqin.tech/tags/computer-vision/"/>
    
      <category term="李代数" scheme="https://www.vincentqin.tech/tags/%E6%9D%8E%E4%BB%A3%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>资料：ORB SLAM2 阅读报告</title>
    <link href="https://www.vincentqin.tech/posts/orb-slam/"/>
    <id>https://www.vincentqin.tech/posts/orb-slam/</id>
    <published>2018-11-30T15:14:30.000Z</published>
    <updated>2020-03-31T15:50:48.086Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>首先，解释下SLAM的概念，借鉴高博《视觉 SLAM 十四讲》中的一句话：SLAM 是 Simultaneous Localization and Mapping 的缩写，中文译作“同时定位与地图构建”。它是指搭载特定传感器的主体，在没有环境先验信息的情况下，于运动过程中建立环境的模型，同时估计自己的运动。如果这里的传感器主要为相机，那就称为“视觉 SLAM”。</p><a id="more"></a><p>先来张图，下图就是利用相机作为传感器在环境中采集一系列的图像，经过SLAM系统建立的点云图以及相机轨迹。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/orb-slam/MH01-traj.gif"></p><p>SLAM自1986年提出之后，一直以来是机器人领域的热点问题。这里总结一些常用开源SLAM方案。</p><style>table th:nth-of-type(1) {    width: 150px;}table th:nth-of-type(2) {    width: 200px;}</style><div class="table-container"><table><thead><tr><th>方案名称</th><th>传感器形式</th><th>地址</th></tr></thead><tbody><tr><td>MonoSLAM</td><td>单目</td><td><a href="https://github.com/hanmekim/SceneLib2" target="_blank" rel="noopener">https://github.com/hanmekim/SceneLib2</a></td></tr><tr><td>PTAM</td><td>单目</td><td><a href="http://www.robots.ox.ac.uk/~gk/PTAM/" target="_blank" rel="noopener">http://www.robots.ox.ac.uk/~gk/PTAM/</a></td></tr><tr><td>ORB-SLAM</td><td>单目</td><td><a href="http://webdiis.unizar.es/~raulmur/orbslam/" target="_blank" rel="noopener">http://webdiis.unizar.es/~raulmur/orbslam/</a></td></tr><tr><td>ORB-SLAM2</td><td>单目/双目/RGB-D</td><td><a href="https://github.com/raulmur/ORB_SLAM2" target="_blank" rel="noopener">https://github.com/raulmur/ORB_SLAM2</a></td></tr><tr><td>LSD-SLAM</td><td>单目为主</td><td><a href="http://vision.in.tum.de/research/vslam/lsdslam" target="_blank" rel="noopener">http://vision.in.tum.de/research/vslam/lsdslam</a></td></tr><tr><td>SVO</td><td>单目</td><td><a href="https://github.com/uzh-rpg/rpg_svo" target="_blank" rel="noopener">https://github.com/uzh-rpg/rpg_svo</a></td></tr><tr><td>DTAM</td><td>RGB-D</td><td><a href="https://github.com/anuranbaka/OpenDTAM" target="_blank" rel="noopener">https://github.com/anuranbaka/OpenDTAM</a></td></tr><tr><td>DVO</td><td>RGB-D</td><td><a href="https://github.com/tum-vision/dvo_slam" target="_blank" rel="noopener">https://github.com/tum-vision/dvo_slam</a></td></tr><tr><td>DSO</td><td>单目</td><td><a href="https://github.com/JakobEngel/dso" target="_blank" rel="noopener">https://github.com/JakobEngel/dso</a></td></tr><tr><td>RTAB-MAP</td><td>双目/RGB-D</td><td><a href="https://github.com/introlab/rtabmap" target="_blank" rel="noopener">https://github.com/introlab/rtabmap</a></td></tr><tr><td>RGBD-SLAM-V2</td><td>RGB-D</td><td><a href="https://github.com/felixendres/rgbdslam_v2" target="_blank" rel="noopener">https://github.com/felixendres/rgbdslam_v2</a></td></tr><tr><td>Elastic Fusion</td><td>RGB-D</td><td><a href="https://github.com/mp3guy/ElasticFusion" target="_blank" rel="noopener">https://github.com/mp3guy/ElasticFusion</a></td></tr><tr><td>Hector SLAM</td><td>激光</td><td><a href="http://wiki.ros.org/hector_slam" target="_blank" rel="noopener">http://wiki.ros.org/hector_slam</a></td></tr><tr><td>GMapping</td><td>激光</td><td><a href="http://wiki.ros.org/gmapping" target="_blank" rel="noopener">http://wiki.ros.org/gmapping</a></td></tr><tr><td>OKVIS</td><td>多目+IMU</td><td><a href="https://github.com/ethz-asl/okvis" target="_blank" rel="noopener">https://github.com/ethz-asl/okvis</a></td></tr><tr><td>ROVIO</td><td>多目+IMU</td><td><a href="https://github.com/ethz-asl/rovio" target="_blank" rel="noopener">https://github.com/ethz-asl/rovio</a></td></tr><tr><td>VINS</td><td>单目+IMU</td><td><a href="https://github.com/HKUST-Aerial-Robotics/VINS-Mono" target="_blank" rel="noopener">https://github.com/HKUST-Aerial-Robotics/VINS-Mono</a></td></tr></tbody></table></div><p>ORB-SLAM应该是SLAM最具有代表性的算法，<a href="https://arxiv.org/abs/1610.06475" target="_blank" rel="noopener">ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras；</a> <a href="https://github.com/raulmur/ORB_SLAM2" target="_blank" rel="noopener"><strong>code；</strong></a> <a href="http://webdiis.unizar.es/~raulmur/orbslam/" target="_blank" rel="noopener"><strong>主页；</strong></a></p><p>ORB-SLAM 是PTAM 的继承者们中非常有名的一位。它提出于 2015 年，是现代 SLAM 系统中做的非常完善，非常易用的系统之一（如果不是最完善和易用的话）。ORB-SLAM 代表着主流的特征点 SLAM 的一个高峰。相比于之前的工作，ORB-SLAM 具有以下几条明显的优势：</p><ul><li>支持单目、双目、RGB-D 三种模式。这使得无论我们拿到了任何一种常见的传感器，都可以先放到 ORB-SLAM 上测试一下，它具有良好的泛用性。</li><li>整个系统围绕 ORB 特征进行计算，包括视觉里程计与回环检测的 ORB 字典。它体现出 ORB 特征是现阶段计算平台的一种优秀的效率与精度之间的折衷方式。ORB不像 SIFT 或 SURF 那样费时，在 CPU 上面即可实时计算；相比 Harris 角点等简单角点特征，又具有良好的旋转和缩放不变性。并且，ORB 提供描述子，使我们在大范围运动时能够进行回环检测和重定位。</li><li>ORB 的回环检测是它的亮点。优秀的回环检测算法保证了 ORB-SLAM 有效地防止累计误差，并且在丢失之后还能迅速找回，这在许多现有的 SLAM 系统中都不够完善。为此，ORB-SLAM 在运行之前必须加载一个很大的 ORB 字典文件。</li><li>ORB-SLAM 创新式地使用了三个线程完成 SLAM：实时跟踪特征点的 Tracking 线程，局部 Bundle Adjustment 的优化线程（Co-visibility Graph，俗称小图），以及全局 Pose Graph 的回环检测与优化线程（Essential Graph 俗称大图）。其中，Tracking线程负责对每张新来的图像提取 ORB 特征点，并与最近的关键帧进行比较，计算特征点的位置并粗略估计相机位姿。小图线程求解一个Bundle Adjustment 问题，它包括局部空间内的特征点与相机位姿。这个线程负责求解更精细的相机位姿与特征点空间位置。不过，仅有前两个线程，只完成了一个比较好的视觉里程计。第三个线程，也就是大图线程，对全局的地图与关键帧进行回环检测，消除累积误差。由于全局地图中的地图点太多，所以这个线程的优化不包括地图点，而只有相机位姿组成的位姿图。继 PTAM 的双线程结构之后，ORB-SLAM 的三线程结构取得了非常好的跟踪和建图效果，能够保证轨迹与地图的全局一致性。这种三线程结构亦将被后续的研究者认同和采用。</li><li>ORB-SLAM围绕特征点进行了不少的优化。例如，在OpenCV的特征提取基础上保证了特征点的均匀分布；在优化位姿时使用了一种循环优化四遍以得到更多正确匹配的方法；比PTAM更为宽松的关键帧选取策略等等。这些细小的改进使得 ORB-SLAM 具有远超其他方案的鲁棒性：即使对于较差的场景，较差的标定内参，ORB-SLAM 都能够顺利地工作。</li></ul><p>整个ORB-SLAM系统包括三个部分组成，分别是跟踪（Tracking）、局部建图（Local Mapping）以及回环检测（Loop Closing）模块，它们分别被三个线程并行地进行处理。接下来对这个系统进行介绍。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/orb-slam/orb-slam2-mainflow.png"></p><h1 id="理论篇"><a href="#理论篇" class="headerlink" title="理论篇"></a>理论篇</h1><h2 id="跟踪模块"><a href="#跟踪模块" class="headerlink" title="跟踪模块"></a>跟踪模块</h2><p>跟踪（Tracking）是在每帧中粗略地定位相机位姿以及决定何时插入新的关键帧。算法设计了运动模型以及跟踪参考帧模型去大致预测出相机的位姿。如相机跟踪失败（由于遮挡、大幅度运动等），就启动重定位模块对相机进行位置查找。如果已经有了初始位姿以及特征匹配，利用关键帧的Covisibility Graph恢复出局部可见图。之后，局部地图点的匹配可利用重投影实现，随后相机的位姿利用BA来优化。最后，Tracking线程决定是否插入新的关键帧。</p><ul><li>初始位姿估计：利用运动模型或者关键帧模型去预测相机位姿。如果运动模型已经跟踪到了当前帧，会利用引导匹配（Guided Search）在上一帧中寻找地图点。如果没有找到足够的匹配（如，运动模型不适用的情况），我们就在上一帧中更大的范围中寻找地图点。如不满足运动模型条件，导致运动模型失败，则采用参考关键帧模型利用参考帧模型对当前帧进行跟踪。通过以上两个模型即可对相机位姿进行初步定位。</li><li>跟踪局部地图：一旦我们已经估计了相机位姿以及我们得到一系列匹配的特征。我们可以将地图点投影到该帧上以搜索更多的匹配地图点。为了减小计算大图的超大复杂度，我们仅将其投影局部小图。局部地图包括，一系列关键帧<script type="math/tex">K_1</script>，这些关键帧与当前帧共享着相同的地图点；还有与<script type="math/tex">K_1</script>有共视关系的关键帧<script type="math/tex">K_2</script>们。局部图还有一个参考帧<script type="math/tex">K_{ref}</script>，这个关键帧与当前帧有最多的匹配点。</li><li>重定位：当运动模型以及跟踪关键帧失败时，可利用重定位来恢复得到相机位姿。应该从历史关键帧中选取和当前帧相似的图片，对当前帧进行位姿估计以及位姿优化。</li></ul><h2 id="局部建图模块"><a href="#局部建图模块" class="headerlink" title="局部建图模块"></a>局部建图模块</h2><p>局部建图（Local Mapping）的主要任务：当跟踪当前帧成功之后，需要利用局部建图更新其运动模型同时更新地图点。等待跟踪过程判断是否应该插入一个新的关键帧，并把关键帧插入到地图中，并对局部地图进行局部BA优化。这个线程能够获得更为精细的相机位姿以及点云。</p><ul><li>处理关键帧：跟踪成功之后，需要对关键帧进行处理以得到地图。具体而言：从关键帧队列中获得一帧，计算出其特征点的BoW映射向量（表示）。关键帧和其对应的地图点进行绑定，更新地图点的平均观测方向以及观测距离范围。更新关键帧之间的连接关系（共视关系），最后将关键帧插入地图中。</li><li>精选地图点：由于跟踪过程引入地图点的策略较为宽松，此时需要检查最近加入的地图点，并将一些冗余的地图点从最近地图点的列表中剔除。</li><li>创建新地图点：由于上一步已经剔除了一些冗余地图点，该模块需要通过当前关键帧及其共视关键帧利用三角化得到更多高质量的3D地图点并添加地图点的属性。</li><li>Local BA：该步骤通过局部BA优化局部地图点以及局部关键帧的位姿。</li><li>精选关键帧：剔除冗余的关键帧，这样不至于增加后期BA的压力，而且可以保证在相同的环境下，关键帧的数目不会无限制的增长，同时减小存储压力。</li></ul><h2 id="回环检测模块"><a href="#回环检测模块" class="headerlink" title="回环检测模块"></a>回环检测模块</h2><p>回环检测（Loop Closing）的主要目标是检测当前关键帧是否经过历史位置。如有经过，则利用回环检测得到的回环帧去修正整个SLAM长期跟踪过程中带来的累积误差、尺度漂移等。如果仅有前两个线程的话，仅仅完成了一个很好的视觉里程计（VO），这个线程会对全局地图以及关键帧进行回环检测，以消除上述累积误差。</p><ul><li>候选关键帧检测：当前关键帧仅有与历史关键帧足够相似才可能成为回环候选帧，该模块通过一定的筛选策略对当前关键帧进行筛选，判断其是否为闭环候选关键帧。由于在实际闭环检测过程中，回环候选帧及其共视关键帧，在一定连续的时间内都可能被观测到。该模块主要通过利用这一条件，对闭环候选关键帧进一步地筛选，通过筛选条件的候选关键帧将进行下一步的判断。</li><li>相似性变换计算：考虑到单目SLAM的尺度漂移，当前帧和回环帧之间的相对位姿应是一个相似变换，并且，二者之间应具有足够多的匹配点。该模块主要是通过循环计算当前帧和上述经过筛选后的候选关键帧之间的相似变换，直到找到一个和当前帧具有足够多匹配点的相似变换，对应的候选关键帧即为最终的回环帧。</li><li>回环修正：受累积误差的影响，时间越久，越接近当前帧的关键帧及相应的地图点，误差将越大。若寻找到的回环帧，当前帧位姿及其对应的地图点会更精确。该模块就是为了修正累积误差，利用回环帧及其共视关键帧，以及对应的地图点，来修正当前帧及其共视关键帧的位姿以及对应的地图点的世界坐标。紧接着进行地图点融合，更新共视图，然后通过本质图优化相机位姿，最后进行全局BA来修正整个SLAM的累积误差（相机位姿以及地图点）。</li></ul><p>牛吹完了，说下缺点。<br>当然，ORB-SLAM 也存在一些不足之处。首先，由于整个 SLAM 系统都采用特征点进行计算，我们必须对每张图像都计算一遍 ORB 特征，这是非常耗时的。ORB-SLAM 的三线程结构也对 CPU 带来了较重的负担，使得它只有在当前 PC 架构的 CPU 上才能实时运算，移植到嵌入式端则有一定困难。其次，ORB-SLAM 的建图为稀疏特征点，目前还没有开放存储和读取地图后重新定位的功能（虽然从实现上来讲并不困难）。根据我们在建图章节的分析，稀疏特征点地图只能满足我们对定位的需求，而无法提供导航、避障、交互等诸多功能。然而，如果我们仅用 ORB-SLAM 处理定位问题，似乎又嫌它有些过于重量级了。相比之下，另外一些方案提供了更为轻量级的定位，使我们能够在低端的处理器上运行 SLAM，或者让 CPU 有余力处理其他的事务。</p><h1 id="实践篇"><a href="#实践篇" class="headerlink" title="实践篇"></a>实践篇</h1><p>在这里下载ORB-SLAM2的源码，然后参考ORB-SLAM2项目的说明文档，安装一些必要的第三方软件：</p><ul><li>pangolin：<a href="http://eigen.tuxfamily.org/index.php?title=Main_Page" target="_blank" rel="noopener">http://eigen.tuxfamily.org/index.php?title=Main_Page</a></li><li>Eigen：<a href="http://eigen.tuxfamily.org/index.php?title=Main_Page" target="_blank" rel="noopener">http://eigen.tuxfamily.org/index.php?title=Main_Page</a></li><li>opencv 3.4.2 ：<a href="https://blog.csdn.net/haoqimao_hard/article/details/82049565" target="_blank" rel="noopener">https://blog.csdn.net/haoqimao_hard/article/details/82049565</a></li><li>ROS：<a href="http://wiki.ros.org/melodic/Installation/Ubuntu#Ubuntu_install_of_ROS_Melodic" target="_blank" rel="noopener">http://wiki.ros.org/melodic/Installation/Ubuntu#Ubuntu_install_of_ROS_Melodic</a></li></ul><p>注意，其中有坑，务必安装正确。安装好之后顺便在Euroc数据集中的MH01上测试，得到下面的轨迹地图。</p><p><img alt data-src="//www.vincentqin.tech/posts/orb-slam/MH01-traj.png"></p><h2 id="EuRoC数据集"><a href="#EuRoC数据集" class="headerlink" title="EuRoC数据集"></a>EuRoC数据集</h2><p>EuRoC数据集包含11个双目序列，这个序列由小型无人机在两个房间（V1/V2, Vicon Room）以及一个大工厂环境(MH, Machine Hall)中拍摄得到。相机的基线长约为11cm，以20Hz速度拍摄图片。序列被分成了三种（根据MAV的速度，光照以及场景纹理）easy , medium, difficult。<br>$ATE$表示绝对轨迹误差，是衡量相机位姿的标准之一。假设有真实位姿序列：<script type="math/tex">P_1,P_2,P_3,...,P_n</script>以及估计的位姿序列：<script type="math/tex">Q_1,Q_2,Q_3,...,Q_n</script> ，它们已经做了包括时间戳对齐等操作。实际场景中，这两个序列可能有不同的采样率、长度亦或数据可能丢失，此时需要进行数据关联和插值。首先得的在第i时刻的轨迹误差：</p><script type="math/tex; mode=display">F_i := Q_i^{-1}SP_i</script><p>其中$S$是从<script type="math/tex">P_{1:n}</script>到<script type="math/tex">Q_{1:n}</script>的最小二乘刚体变换，通过求取以上误差在所有位置时刻的均方根我们得到APE的具体形式：</p><script type="math/tex; mode=display">RMSE(F_{1:n}):=\left(\frac{1}{n}\sum_{i=1}^n||trans(F_i)||^2\right)^{\frac{1}{2}}</script><p>其中的$trans(·)$表示求取该位姿的平移分量算子。</p><p>文中大部分内容来自网络以及高博十四讲。因本人水平有限，如有错误，谢谢指出。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="http://www.slamcn.org/index.php/%E9%A6%96%E9%A1%B5" target="_blank" rel="noopener">http://www.slamcn.org/index.php/%E9%A6%96%E9%A1%B5</a></li><li><a href="https://blog.csdn.net/qinruiyan/article/details/50918504" target="_blank" rel="noopener">https://blog.csdn.net/qinruiyan/article/details/50918504</a></li><li>Sturm J, Engelhard N, Endres F, et al. A benchmark for the evaluation of RGB-D SLAM systems[C]. Ieee International Conference on Intelligent Robots and Systems. IEEE, 2012:573-580.</li><li>Horn B K P. Closed-form solution of absolute orientation using unit quaternions[J]. J.opt.soc.am.a, 1987, 5(7):1127-1135.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;首先，解释下SLAM的概念，借鉴高博《视觉 SLAM 十四讲》中的一句话：SLAM 是 Simultaneous Localization and Mapping 的缩写，中文译作“同时定位与地图构建”。它是指搭载特定传感器的主体，在没有环境先验信息的情况下，于运动过程中建立环境的模型，同时估计自己的运动。如果这里的传感器主要为相机，那就称为“视觉 SLAM”。&lt;/p&gt;
    
    </summary>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/categories/SLAM/"/>
    
    
  </entry>
  
  <entry>
    <title>资料：SLAM草稿</title>
    <link href="https://www.vincentqin.tech/posts/slam/"/>
    <id>https://www.vincentqin.tech/posts/slam/</id>
    <published>2018-11-29T16:53:00.000Z</published>
    <updated>2020-03-31T14:59:59.921Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="ECCV-2018-视觉定位综述"><a href="#ECCV-2018-视觉定位综述" class="headerlink" title="ECCV 2018 视觉定位综述"></a>ECCV 2018 视觉定位综述</h2><p>以下是今年ECCV上几位大牛介绍SLAM技术的tutorial pdfs，涉及基于特征点以及基于学习的SLAM算法介绍，并在最后探究了SLAM领域的主要问题以及未来的发展趋势。</p><ul><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/1.Intro.pdf" target="_blank" rel="noopener">Feature-based vs. Learned Approaches</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/2.Sattler-Feature-Based-3D-Localization.pdf" target="_blank" rel="noopener">Current State of Feature-based Localization</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/3.Learning-Based Localization_Upload.pdf" target="_blank" rel="noopener">Learning-based Localization</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/4.Failure_Cases.pdf" target="_blank" rel="noopener">Failure Cases of Feature-based and Learning-based Methods</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/5.Long_Term_Localization.pdf" target="_blank" rel="noopener">Long-term Localization: Towards Higher-level Scene Understanding</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/6.Learning Problems_Upload.pdf" target="_blank" rel="noopener">Open Problems of Learning-based Methods</a></li></ul><p>整理还未完备，先祭出这几本书&amp;博客，方便随时查看。</p><a id="more"></a><h2 id="教程"><a href="#教程" class="headerlink" title="教程"></a>教程</h2><ul><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/slambook14.pdf" target="_blank" rel="noopener">slambook</a></li><li><a href="https://github.com/gaoxiang12/slambook/tree/master/project/0.3" target="_blank" rel="noopener">slambook code</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/OpenCV3book.pdf" target="_blank" rel="noopener">OpenCV3.0</a></li><li><a href="https://blog.csdn.net/OnafioO/article/details/73175835" target="_blank" rel="noopener">SLAM前世今生</a></li><li><a href="https://github.com/RainerKuemmerle/g2o" target="_blank" rel="noopener">g2o Github</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/g2o-details.pdf" target="_blank" rel="noopener">g2o details</a></li><li><a href="https://blog.csdn.net/u012700322/article/details/52857244" target="_blank" rel="noopener">g2o译文</a></li><li><a href="https://www.cnblogs.com/gaoxiang12/p/5304272.html" target="_blank" rel="noopener">深入理解图优化与g2o：g2o篇</a></li><li><a href="https://github.com/RainerKuemmerle/g2o" target="_blank" rel="noopener">深入理解图优化与g2o：g2o篇 code</a></li><li><a href="https://me.csdn.net/heyijia0327" target="_blank" rel="noopener">白巧克力亦唯心的博客</a></li><li><a href="http://www.360doc.com/content/17/0718/14/44420101_672315705.shtml" target="_blank" rel="noopener">Graph slam学习</a></li></ul><h2 id="g2o漫谈"><a href="#g2o漫谈" class="headerlink" title="g2o漫谈"></a><a href="https://openslam-org.github.io/g2o.html" target="_blank" rel="noopener">g2o</a>漫谈</h2><p>g2o里面有各种各样的求解器，而它的顶点、边的类型多种多样。通过自定义顶点和边，事实上，只要一个优化问题能够表达成图，就可以用g2o去求解它。常见的，比如bundle adjustment，ICP，数据拟合等。g2o是一个C++项目，其中矩阵数据结构多来自Eigen。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-4/slam/g2o.png"></p><p>先看上部分，SparseOptimizer是我们需要维护的东西，是一个Optimizable Graph，也是一个Hyper Graph。一个SparseOptimizer含有很多个顶点（继承与Base Vertex）和多条边（继承自BaseUnaryEdge，BaseBinaryEdge或BaseMultiEdge）。这些Base Vertex和Base Edge都是抽象的基类，而实际用的顶点和边，都是它们的派生类。</p><p>我们用SparseOptimizer.addVertex 和 SparseOptimizer.addEdge 向图中添加顶点和边，然后调用SpaseOptimizer.optimize来优化。</p><p>在优化前，需要指定我们用的求解器和迭代算法。从图下半部分来看，一个SparseOptimization拥有一个Optimization Algorithm,继承自Gusss-Newton，Levernberg-Marquardt，Powell’s dogleg 三者之一，同时拥有一个Solver，含有俩个部分。一个是SparseBlockMatrix，用于计算稀疏的雅克比和海塞；一个用于计算 <script type="math/tex">H\Delta x = -b</script>，需要一个线性方程的求解器。而这个求解器，可以从PCG，CSparse，Choldmod三者选一。<br>则一共三个步骤：</p><ol><li>选择一个线性方程求解器，从 PCG, CSparse, Choldmod中选</li><li>选择一个 BlockSolver</li><li>选择一个迭代策略，从GN, LM, Doglog中选</li></ol><h3 id="BlockSolver，块求解器"><a href="#BlockSolver，块求解器" class="headerlink" title="BlockSolver，块求解器"></a>BlockSolver，块求解器</h3><p>块求解器是包含线性求解器的存在，之所以是包含，是因为块求解器会构建好线性求解器所需要的矩阵块（也就是<script type="math/tex">H</script>和<script type="math/tex">b</script>，<script type="math/tex">H\Delta x = -b</script>），之后给线性求解器让它进行运算，边的jacobian也就是在这个时候发挥了自己的光和热。</p><p>这里再记录下一个比较容易混淆的问题，也就是在初始化块求解器的时候的参数问题。大部分的例程在初始化块求解器的时候都会使用如下的程序代码：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;g2o::BlockSolver_6_3::LinearSolverType&gt; linearSolver = g2o::make_unique&lt;g2o::LinearSolverCholmod&lt;g2o::BlockSolver_6_3::PoseMatrixType&gt;&gt;();</span><br></pre></td></tr></table></figure></p><p>其中的BlockSolver_6_3有两个参数，分别是6和3，在定义的时候可以看到这是一个模板的重命名（模板类的重命名只能用using）<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">int</span> p, <span class="keyword">int</span> l&gt;  </span><br><span class="line"><span class="keyword">using</span> BlockSolverPL = BlockSolver&lt; BlockSolverTraits&lt;p, l&gt; &gt;;</span><br></pre></td></tr></table></figure></p><p>其中<strong>p代表pose的维度，l表示landmark的维度</strong>，且这里都表示的是增量的维度。</p><h3 id="g2o的顶点（Vertex）"><a href="#g2o的顶点（Vertex）" class="headerlink" title="g2o的顶点（Vertex）"></a>g2o的顶点（Vertex）</h3><ul><li><code>g2o::BaseVertex&lt; D, T &gt;</code> 其中 <code>int D, typename T</code></li></ul><p>首先记录一下定义模板的两个参数D和T，两个类型分别是int和typename的类型，D表示的是维度，g2o源码里面是这个注释的:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> Dimension = D; <span class="comment">//&lt; dimension of the estimate (minimal) in the manifold space</span></span><br></pre></td></tr></table></figure><p>可以看到这个D并非是顶点（更确切的说是状态变量）的维度，而是其在流形空间（manifold）的最小表示，这里一定要区别开；之后是T，源码里面也给出了T的作用:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> T EstimateType;</span><br><span class="line">EstimateType _estimate;</span><br></pre></td></tr></table></figure><p>可以看到，这里T就是顶点（状态变量）的类型。在顶点的继承中，这两个参数是直接面向我们的，所以务必要定义妥当。</p><h3 id="g2o的边（Edge）"><a href="#g2o的边（Edge）" class="headerlink" title="g2o的边（Edge）"></a>g2o的边（Edge）</h3><ul><li><code>g2o::BaseBinaryEdge&lt; D, E, VertexXi, VertexXj &gt;</code> 其中 <code>int D, typename E</code></li></ul><p>首先还是介绍这两个参数，还是从源码上来看：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> Dimension = D;</span><br><span class="line"><span class="keyword">typedef</span> E Measurement;</span><br><span class="line"><span class="keyword">typedef</span> Eigen::Matrix&lt;<span class="keyword">number_t</span>, D, <span class="number">1</span>, Eigen::ColMajor&gt; ErrorVector;</span><br></pre></td></tr></table></figure><p>可以看到，D决定了误差的维度，从映射的角度讲，三维情况下就是2维的，二维的情况下是1维的；然后E是measurement的类型，也就是测量值是什么类型的，这里E就是什么类型的（一般都是Eigen::VectorN表示的，N是自然数）。</p><ul><li><code>typename VertexXi</code>, <code>typename VertexXj</code></li></ul><p>这两个参数就是边连接的两个顶点的类型，这里特别注意一下，这两个必须一定是顶点的类型，也就是继承自<code>BaseVertex</code>等基础类的类！不是顶点的数据类！例如必须是<code>VertexSE3Expmap</code>而不是<code>VertexSE3Expmap</code>的数据类型类<code>SE3Quat</code>。原因的话源码里面也很清楚，因为后面会用到一系列顶点的维度等等的属性，这些属性是数据类型类里面没有的。</p><ul><li><code>_jacobianOplusXi</code>，<code>_jacobianOplusXj</code></li></ul><p>在成员函数<code>linearizeOplus()</code>（线性化函数）中维护着这两个量，<code>_jacobianOplusXi</code>和<code>_jacobianOplusXj</code>就是所谓的雅可比矩阵，如果是二元边的话二者都有；若为一元边，只有<code>_jacobianOplusXi</code>。</p><p>这两个变量本质上是Eigen::Matrix类型的，具体定义在这里：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">typename</span> Eigen::Matrix&lt;<span class="keyword">number_t</span>, D, Di, D==<span class="number">1</span>?Eigen::RowMajor:Eigen::ColMajor&gt;::AlignedMapType JacobianXiOplusType;</span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">typename</span> Eigen::Matrix&lt;<span class="keyword">number_t</span>, D, Dj, D==<span class="number">1</span>?Eigen::RowMajor:Eigen::ColMajor&gt;::AlignedMapType JacobianXjOplusType;</span><br><span class="line">JacobianXiOplusType _jacobianOplusXi;</span><br><span class="line">JacobianXjOplusType _jacobianOplusXj;</span><br></pre></td></tr></table></figure></p><hr><p>to be continued…</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li><a href="https://blog.csdn.net/Hansry/article/details/78080807" target="_blank" rel="noopener">RGB-D SLAM——g2o篇（三）</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ECCV-2018-视觉定位综述&quot;&gt;&lt;a href=&quot;#ECCV-2018-视觉定位综述&quot; class=&quot;headerlink&quot; title=&quot;ECCV 2018 视觉定位综述&quot;&gt;&lt;/a&gt;ECCV 2018 视觉定位综述&lt;/h2&gt;&lt;p&gt;以下是今年ECCV上几位大牛介绍SLAM技术的tutorial pdfs，涉及基于特征点以及基于学习的SLAM算法介绍，并在最后探究了SLAM领域的主要问题以及未来的发展趋势。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://vincentqin.gitee.io/blogresource-4/slam/1.Intro.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Feature-based vs. Learned Approaches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://vincentqin.gitee.io/blogresource-4/slam/2.Sattler-Feature-Based-3D-Localization.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Current State of Feature-based Localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://vincentqin.gitee.io/blogresource-4/slam/3.Learning-Based Localization_Upload.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Learning-based Localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://vincentqin.gitee.io/blogresource-4/slam/4.Failure_Cases.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Failure Cases of Feature-based and Learning-based Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://vincentqin.gitee.io/blogresource-4/slam/5.Long_Term_Localization.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Long-term Localization: Towards Higher-level Scene Understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://vincentqin.gitee.io/blogresource-4/slam/6.Learning Problems_Upload.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Open Problems of Learning-based Methods&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;整理还未完备，先祭出这几本书&amp;amp;博客，方便随时查看。&lt;/p&gt;
    
    </summary>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/categories/SLAM/"/>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="cv" scheme="https://www.vincentqin.tech/tags/cv/"/>
    
      <category term="computer vision" scheme="https://www.vincentqin.tech/tags/computer-vision/"/>
    
      <category term="AR" scheme="https://www.vincentqin.tech/tags/AR/"/>
    
  </entry>
  
</feed>
