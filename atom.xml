<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RealCat</title>
  
  <subtitle>Turn on, Tune in, Drop out</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.vincentqin.tech/"/>
  <updated>2020-08-26T15:06:45.137Z</updated>
  <id>https://www.vincentqin.tech/</id>
  
  <author>
    <name>Vincent Qin</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>📝笔记：AdaLAM: Revisiting Handcrafted Outlier Detection 超强外点滤除算法</title>
    <link href="https://www.vincentqin.tech/posts/adalam/"/>
    <id>https://www.vincentqin.tech/posts/adalam/</id>
    <published>2020-08-17T12:37:12.000Z</published>
    <updated>2020-08-26T15:06:45.137Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note success">            <p><code>AdaLAM</code>的全称是<code>Adaptive Locally-Affine Matching</code>(自适应局部仿射匹配)，本文提出了一种高效快速外点滤除算法。</p>          </div><a id="more"></a><h2 id="原有技术问题"><a href="#原有技术问题" class="headerlink" title="原有技术问题"></a>原有技术问题</h2><p>在图像匹配任务中初始匹配中外点较多，目前难以高效快速地滤除外点。</p><h2 id="新技术创新点"><a href="#新技术创新点" class="headerlink" title="新技术创新点"></a>新技术创新点</h2><ol><li>基于目前已有的外点滤除算法（spatial matching），提出了现有的鲁棒快速的图像一致性空域验证算法；</li><li>本框架基于一种几何假设（局部仿射），场景实用性较强；经实验验证，该算法目前达到了SOTA（很自信啊）。</li></ol><h2 id="新技术主要框架以及关键技术点"><a href="#新技术主要框架以及关键技术点" class="headerlink" title="新技术主要框架以及关键技术点"></a>新技术主要框架以及关键技术点</h2><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/adalam/fig-1.png"></p><p>总共分四步：</p><ol><li>找到初始匹配（最近邻top1）;</li><li>找到置信度高且分布较好的点作为“种子点”；</li><li>在初始匹配中选择与该种子点在同一个区域的匹配点；</li><li>保留那些局部一致较好匹配；</li></ol><p>接下来重点介绍后3点。</p><h3 id="种子点选择"><a href="#种子点选择" class="headerlink" title="种子点选择"></a>种子点选择</h3><p>将<code>ratio-test</code>得到的<code>最优次优比</code>作为左图上匹配点的匹配置信度，选择那些在半径$R$内匹配置信度最大的点作为种子点。由于每个匹配点都是独立的，此时可用GPU对该过程进行并行加速。</p><h3 id="局部选择与过滤"><a href="#局部选择与过滤" class="headerlink" title="局部选择与过滤"></a>局部选择与过滤</h3><p>接下来要寻找一些可以支持种子匹配的匹配对。</p><p>令$S_{i}=\left(\mathbf{x}_{1}^{S_{i}}, \mathbf{x}_{2}^{S_{i}}\right)$，其中$\mathbf{x}_{1}^{S_{i}}, \mathbf{x}_{2}^{S_{i}}$分别表示两张图上的第$i$个种子匹配对，它们之间符合<strong>相似变换</strong>（即旋转+缩放，其中旋转$\alpha^{S_{i}}=\alpha_{2}^{S_{i}}-\alpha_{1}^{S_{i}}$, 缩放为$\sigma^{S_{i}}=\sigma_{2}^{S_{i}} / \sigma_{1}^{S_{i}}$）。那么对于任意匹配$\left(p_{1}, p_{2}\right)=\left(\left(\mathbf{x}_{1}, \mathbf{d}_{1}, \sigma_{1}, \alpha_{1}\right),\left(\mathbf{x}_{2}, \mathbf{d}_{2}, \sigma_{2}, \alpha_{2}\right)\right) \in \mathcal{M}$，其中$\mathbf{d}$表示描述子，如果上述匹配满足如下约束关系，就能够被纳入到支持种子点的匹配集合$\mathcal{N}_{i} \subseteq \mathcal{M}$中，该约束关系为：</p><script type="math/tex; mode=display">\begin{array}{c}\left(\left\|\mathbf{x}_{1}^{S_{i}}-\mathbf{x}_{1}\right\| \leq \lambda R_{1}\right) \wedge \left(\left\|\mathbf{x}_{2}^{S_{i}}-\mathbf{x}_{2}\right\| \leq \lambda R_{2}\right)\end{array}</script><script type="math/tex; mode=display">\begin{array}{c}\left(\left|\alpha^{S_{i}}-\alpha^{p}\right| \leq t_{\alpha}\right) \wedge\left(\left|\ln \left(\frac{\sigma^{S_{i}}}{\sigma^{p}}\right)\right| \leq t_{\sigma}\right)\end{array}</script><p>上式中$\alpha^{p}=\alpha_{2}-\alpha_{1}, \sigma^{p}=\sigma_{2} / \sigma_{1}$表示两个匹配点之间的角度与尺度差异；$R_1$与$R_2$分别表示图像$I_1$与$I_2$的种子点扩散半径；$\lambda$表示邻域圈圈的覆盖程度的正则项。</p><p>上面的第一个式子表示：初始匹配中与种子点相对位置差不多且在半径在$\lambda R$的匹配会加入到$\mathcal{N}_{i}$；第二个式子告诉我们：上面加入的这些匹配对需要满足角度以及尺度一致性才能够被加入，否则面谈。上面的两个条件需要同时满足才能得到$\mathcal{N}_{i}$。</p><h3 id="自适应仿射校验"><a href="#自适应仿射校验" class="headerlink" title="自适应仿射校验"></a>自适应仿射校验</h3><p>我们假设匹配对之间符合<strong>局部仿射变换</strong>，即上述的每个$\mathcal{N}_{i}$都满足该假设，那么接下来可利用该假设去滤除一些错误的匹配对：使用RANSAC的思想找到最小解集去拟合仿射矩阵，然后滤除置信度低的匹配对。</p><p>由于仅<strong>使用2对匹配点就可以得到仿射矩阵</strong>，那么即使对每个圈圈求仿射也并不耗时。对于第$j$次迭代，我们可以得到匹配关系$k$，对于在集合$\mathcal{N}_{i}$，我们可以随机选择一对匹配$\mathbf{x}_{1}^{k}, \mathbf{x}_{2}^{k}$，进而得到二者之间的仿射矩阵$A_{i}^{j}$，然后我们就可以得到<strong>匹配关系对两个匹配对产生的残差</strong>，如下式：</p><script type="math/tex; mode=display">r_{k}\left(A_{i}^{j}\right)=\left\|A_{i}^{j} \mathbf{x}_{1}^{k}-\mathbf{x}_{2}^{k}\right\|</script><p>然后作者参考文献<sup><a href="#fn_36" id="reffn_36">36</a></sup>，设计了置信度$c_{k}$（不展开讲），当置信度大于某个阈值，表示该模型对该匹配关系拟合的较好，视该匹配被视为内点；否则为外点。</p><p>注意每次迭代需要更新上述残差/置信度以及内点，后一次利用前一次得到的内点去拟合新的仿射矩阵，然后做校验，直至达到最大迭代次数，最后输出内点。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/adalam/tab-1.png"></p><p>看到没，竟然超过了效果极好的<strong>GMS</strong>。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/adalam/fig-2.png"></p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/adalam/fig-3.png"></p><p>实时性：在RTX2080Ti平台下处理4000~8000个点耗时15~25ms。</p><h2 id="借鉴意义"><a href="#借鉴意义" class="headerlink" title="借鉴意义"></a>借鉴意义</h2><p>本文提出了一种高效快速的外点滤除算法，可加入到任何特征匹配算法中，预期能够提高位姿解算的精度。但上述实验结果中对GPU的要求较高，目前不清楚在低配版GPU或者在CPU平台下的表现。</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><ol><li><strong><a href="https://arxiv.org/pdf/2006.04250" target="_blank" rel="noopener">Paper: AdaLAM: Revisiting Handcrafted Outlier Detection</a></strong></li><li><a href="https://github.com/cavalli1234/AdaLAM" target="_blank" rel="noopener">Github Code</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note success&quot;&gt;
            &lt;p&gt;&lt;code&gt;AdaLAM&lt;/code&gt;的全称是&lt;code&gt;Adaptive Locally-Affine Matching&lt;/code&gt;(自适应局部仿射匹配)，本文提出了一种高效快速外点滤除算法。&lt;/p&gt;
          &lt;/div&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="CV" scheme="https://www.vincentqin.tech/tags/CV/"/>
    
      <category term="Matching" scheme="https://www.vincentqin.tech/tags/Matching/"/>
    
  </entry>
  
  <entry>
    <title>🔨工具：使用vercel加速Hexo静态博客访问</title>
    <link href="https://www.vincentqin.tech/posts/speedup-gitpage/"/>
    <id>https://www.vincentqin.tech/posts/speedup-gitpage/</id>
    <published>2020-07-28T15:33:41.000Z</published>
    <updated>2020-08-26T15:07:23.719Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">            <p>估计现在有很多同学使用了Hexo博客框架白嫖一套主题并部署在了Github上，这一切看起来很容易上手，于是开开心心地去写博客了，但到后面才发现这才是“苦难”的开始：原本以为是要写博客，但更多的时间是被用来优化网站。因为强迫症患者总是对网站各种不满意，于是自己挖坑又填坑。</p>          </div><p>本文属于日常折腾篇，尝试用<a href="https://vercel.com/docs" target="_blank" rel="noopener">vercel</a>加速博客访问。</p><a id="more"></a><h2 id="网站代码-repo-导入-vercel"><a href="#网站代码-repo-导入-vercel" class="headerlink" title="网站代码 repo 导入 vercel"></a>网站代码 repo 导入 vercel</h2><h3 id="注册账号"><a href="#注册账号" class="headerlink" title="注册账号"></a>注册账号</h3><p>进入登陆页面：<a href="https://vercel.com/login" target="_blank" rel="noopener">https://vercel.com/login</a>，使用 GitHub 账号登陆即可。</p><h3 id="导入项目"><a href="#导入项目" class="headerlink" title="导入项目"></a>导入项目</h3><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/speedup-gitpage/fig1.png"></p><p>点击<code>Continue</code>，进入如下界面。输入项目URL。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/speedup-gitpage/fig2.png"></p><h3 id="部署网站"><a href="#部署网站" class="headerlink" title="部署网站"></a>部署网站</h3><p>导入过程中，选择 other 模板即可，一切选择默认即可，导入完成后自动部署。最后在部署卡片中就可以看到vercel生成的URL。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/speedup-gitpage/fig3.png"></p><p>接下来就可以通过这几个链接来访问你的网站：</p><ul><li><a href="https://realcat-git-master.realcat.vercel.app" target="_blank" rel="noopener">realcat-git-master.realcat.vercel.app</a></li><li><a href="https://realcat.realcat.vercel.app" target="_blank" rel="noopener">realcat.realcat.vercel.app</a></li><li><a href="https://realcat.vercel.app" target="_blank" rel="noopener">realcat.vercel.app</a></li></ul><h2 id="修改DNS"><a href="#修改DNS" class="headerlink" title="修改DNS"></a>修改DNS</h2><p>如果你有自己的域名，需要在域名服务提供商（阿里云、GoDaddy 等）的 DNS 解析中增加一条 CNAME 记录（将你的域名指向另一个域名）：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/speedup-gitpage/fig5.png"></p><h2 id="设置域名"><a href="#设置域名" class="headerlink" title="设置域名"></a>设置域名</h2><p>添加了<code>vincentqin.tech</code>以及<code>www.vincentqin.tech</code>重定向。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/speedup-gitpage/fig4.png"></p><h2 id="尝试一下"><a href="#尝试一下" class="headerlink" title="尝试一下"></a>尝试一下</h2><p>现在网站访问速度快得飞起！</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://www.qtmuniao.com/2020/03/15/hexo-to-zeit-co/" target="_blank" rel="noopener">使用 zeit.co 托管 Hexo 静态博客</a></li><li><a href="https://fwbo.me/2020/05/29/%E5%85%B6%E4%BB%96/20200529-%E8%AF%95%E7%94%A8vercel%E6%89%98%E7%AE%A1%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">试用vercel托管博客</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note info&quot;&gt;
            &lt;p&gt;估计现在有很多同学使用了Hexo博客框架白嫖一套主题并部署在了Github上，这一切看起来很容易上手，于是开开心心地去写博客了，但到后面才发现这才是“苦难”的开始：原本以为是要写博客，但更多的时间是被用来优化网站。因为强迫症患者总是对网站各种不满意，于是自己挖坑又填坑。&lt;/p&gt;
          &lt;/div&gt;
&lt;p&gt;本文属于日常折腾篇，尝试用&lt;a href=&quot;https://vercel.com/docs&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;vercel&lt;/a&gt;加速博客访问。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="vercel" scheme="https://www.vincentqin.tech/tags/vercel/"/>
    
      <category term="hexo" scheme="https://www.vincentqin.tech/tags/hexo/"/>
    
      <category term="blog" scheme="https://www.vincentqin.tech/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title>📝笔记：ORB-SLAM3论文阅读</title>
    <link href="https://www.vincentqin.tech/posts/orb-slam3/"/>
    <id>https://www.vincentqin.tech/posts/orb-slam3/</id>
    <published>2020-07-25T14:19:38.000Z</published>
    <updated>2020-08-26T15:05:49.623Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文全称：ORB-SLAM3：An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM.</p><p>首先回顾一下历史：ORB-SLAM首次在2015年被提出，它的改进版ORB-SLAM2在2017年被提出，同年提出了ORB-SLAM-VI，时隔3年，ORB-SLAM3横空出世，朋友圈、学术群里到处都在热议这个挂在ARXIV才不到3天的论文。好奇心的驱使下，本人偷瞄了一下论文，就在这里总结一下吧。</p><p><strong><a href="http://xxx.itp.ac.cn/pdf/2007.11898.pdf" target="_blank" rel="noopener">论文</a></strong>,  <strong><a href="https://github.com/UZ-SLAMLab/ORB_SLAM3" target="_blank" rel="noopener">Code Github</a></strong>,  <strong><a href="https://gitee.com/vincentqin/ORB_SLAM3" target="_blank" rel="noopener">Code国内镜像</a></strong>,  <strong><a href="https://github.com/Vincentqyw/Recent-Stars-2020" target="_blank" rel="noopener">SLAM资源站</a></strong></p><a id="more"></a><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>ORB-SLAM3实现了基于视觉惯导紧耦合，同时能够对多地图进行复用；另外支持单目/双目/RGB-D作为输入，支持针孔以及鱼眼相机模型。</p><p>本文第一个创新点是提出了一种<strong>基于特征点法的视觉惯导紧耦合SLAM系统</strong>，这套系统在很大使用了最大后验估计对状态量进行求解，即使是在<strong>系统初始化</strong>阶段亦是如此。这套系统可在大/小/室内/室外各种环境下鲁棒实时运行，相较于之前的算法有2~5倍的精度提升。</p><p>本文第二个主要创新点在于提出了<strong>多地图复用系统</strong>，这套系统使用了一种高召回率的场景识别算法。正是依赖于这一点，ORB-SLAM3能够有效应对长时弱纹理的环境：若系统丢失，它重新开始建图并在当经过之前走过的地点时与原来的地图无缝融合。另外，相比于其它仅用到之前一段时间内信息的VO方案，本文提出的方案用到之前所有时刻的信息。这样可以利用到很久之前或者来自不同地图的信息。</p><p>实验表明，本文提出的算法在学术领域已有的SLAM系统中表现最优（这句话说的很严谨，没有提到工业界，因为没开源）。</p><h1 id="系统总览"><a href="#系统总览" class="headerlink" title="系统总览"></a>系统总览</h1><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-fig-1.png"></p><p>ORB-SLAM3基于ORB-SLAM2以及ORB-SLAM-VI进行改进，上图是其系统框图。这基本上保持了与ORB-SLAM2类似的框架，同时增加了前述的几个创新点，接下来对其中主要步骤进行简要说明。</p><h2 id="Atlas-地图集"><a href="#Atlas-地图集" class="headerlink" title="Atlas(地图集)"></a>Atlas(地图集)</h2><p>所谓Atlas就是一种多地图的表示形式，多地图由不相连的地图组成。可以看到，它由active map以及non-active map组成。其中active map被Tracking thread（追踪线程）用作定位，它被连续优化同时会加入新的关键帧。系统会构建基于DBoW2的关键帧数据库，可以用来做重定位/闭环以及地图融合。</p><h2 id="Tracking-thread-追踪线程"><a href="#Tracking-thread-追踪线程" class="headerlink" title="Tracking thread(追踪线程)"></a>Tracking thread(追踪线程)</h2><p>与ORB-SLAM2类似，该线程用来处理传感器信息，计算当前帧相对于active map的位姿以及最小化匹配到的地图点的重投影误差。该线程决定何时当前帧被判定为关键帧。在VI模式下，机体的速度以及IMU的bias通过优化惯导残差被估计。当系统追踪丢失后，会触发重定位模式，即当前帧在所有的Altas进行重定位；若重定位成功，当前帧恢复追踪状态；否则，经过一段时间（超过5秒），当前的active map会被存储为non-active map，同时开启新的建图过程。</p><h2 id="Local-Mapping-thread-局部建图线程"><a href="#Local-Mapping-thread-局部建图线程" class="headerlink" title="Local Mapping thread(局部建图线程)"></a>Local Mapping thread(局部建图线程)</h2><p>向active map中新增/删减/优化关键帧以及地图点，上述的操作是通过维护一个靠近当前帧的局部窗口的关键帧进行实现。与此同时，IMU的参数被初始化然后被该线程通过本文提出的最大后验估计技术进行求解。</p><h2 id="Loop-and-map-merging-thread-闭环和地图融合线程"><a href="#Loop-and-map-merging-thread-闭环和地图融合线程" class="headerlink" title="Loop and map merging thread(闭环和地图融合线程)"></a>Loop and map merging thread(闭环和地图融合线程)</h2><p>该线程检测active map与整个Atlas是否有共同的区域，若共同的区域同属于active map，此时进行闭环矫正；若共同的区域属于不同的map，那么这些map就会被融合（融合为1个）,并变为active map。BA被一个独立的线程执行（不损害实时性），用来修正地图。</p><h1 id="相机模型"><a href="#相机模型" class="headerlink" title="相机模型"></a>相机模型</h1><h2 id="重定位"><a href="#重定位" class="headerlink" title="重定位"></a>重定位</h2><p>ORB-SLAM假设所有系统组件都符合针孔模型。对于重定位这项任务，ORB-SLAM基于EPnP算法建立PnP求解器进行解算位姿，其中所有的公式都假设相机为标定好的针孔相机。为适配本文的方法，作者使用了一种独立于相机模型的求解器：Maximum Likelihood Perspective-n-Point algorithm (<a href="http://xxx.itp.ac.cn/pdf/1607.08112v1" target="_blank" rel="noopener">MLPnP</a>)（最大后验PnP算法）。该算法实现了相机模型与求解算法的解耦，相机模型只是用来提供投影射线作为输入。</p><h2 id="非矫正双目SLAM"><a href="#非矫正双目SLAM" class="headerlink" title="非矫正双目SLAM"></a>非矫正双目SLAM</h2><p>几乎所有的SLAM系统都假设双目图像是已经被<strong>矫正</strong>的，这里的矫正是指，使用相同的焦距将两个图像转换为针孔投影，且像平面共面，同时与水平对极线对齐，从而可以通过查看图像中的同一行进行特征匹配。然而，这个假设是及其严格的，在很多应用中，这个假设通常难以满足或则不合适使用。例如，矫正（一对基线较大的相机）双目相机或双目鱼眼镜头将需要对其拍摄的图像进行严格的图像裁剪，从而失去了大视场角的优势，所以我们需要更快的环境映射方式以应对遮挡等情况。</p><p>本文设计的系统并不依赖于图像的矫正，而是将双目设备看作两个单目相机，于是有如下约束：</p><ol><li>两个单目相机之间存在一个固有的SE(3)变换（即双目外参）；</li><li>两个单目相机之间存在共视；</li></ol><p>上述两个约束可以用来有效地估计尺度。按照这种思路，ORB-SLAM3可估计6DOF刚体机体位姿，注意，机体位于某一个相机或者IMU上。另外，若两个相机之间存在共视，我们可以在通过三角化恢复路标点的尺度。注意：仅在首次看到该其区域的路标点时进行三角化，其它时刻只使用单目信息。</p><h1 id="VI-SLAM"><a href="#VI-SLAM" class="headerlink" title="VI SLAM"></a>VI SLAM</h1><p>之前的ORB-SLAM-VI受限于针孔相机/初始化时间太长以及无法应对某些挑战场景。本文设计了一种快速准确IMU初始化技术，以及开发了一种单目/双目惯导SLAM，支持针孔以及鱼眼相机图像作为输入。</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>纯视觉SLAM的状态量仅有当前帧的位姿，而在VI-SLAM中，增加了一些需要被估计的状态量。除了机体在世界系下的位姿$\mathbf{T}_{i}=\left[\mathbf{R}_{i}, \mathbf{p}_{i}\right] \in \operatorname{SE}(3)$（这里是指$\mathbf{T}_{WC_i}$）还有速度$\mathbf{v}_{i}$，以及陀螺仪和加速度计的bias: $\mathbf{b}_{i}^{g}, \mathbf{b}_{i}^{a}$。我们将这些状态量合在一起构成了如下状态向量$\eqref{eq1}$：</p><script type="math/tex; mode=display">\begin{equation} \mathcal{S}_{i} \doteq\left\{\mathbf{T}_{i}, \mathbf{v}_{i}, \mathbf{b}_{i}^{g}, \mathbf{b}_{i}^{a}\right\} \label{eq1} \end{equation}</script><p>对于视觉惯导SLAM而言，我们通过对IMU的测量进行预积分，可以获得连续两帧（如第$i$到第$i+1$帧）之间的相对位姿测量：$\Delta \mathbf{p}_{i, i+1}$，$\Delta \mathbf{v}_{i, i+1}$，$\Delta \mathbf{R}_{i, i+1}$以及整个测量向量的信息矩阵$\Sigma_{\mathcal{I}_{i, i+1}}$。于是给定上述预积分项以及状态向量$\mathcal{S}_{i}$以及$\mathcal{S}_{i+1}$，我们就可以得到如下IMU残差$\mathbf{r}_{\mathcal{I}_{i, i+1}}$： </p><script type="math/tex; mode=display">\begin{equation} \begin{aligned}\mathbf{r}_{\mathcal{I}_{i, i+1}} &=\left[\mathbf{r}_{\Delta \mathrm{R}_{i, i+1}}, \mathbf{r}_{\Delta \mathrm{v}_{i, i+1}}, \mathbf{r}_{\Delta \mathrm{p}_{i, i+1}}\right] \\\mathbf{r}_{\Delta \mathrm{R}_{i, i+1}} &=\log \left(\Delta \mathbf{R}_{i, i+1}^{\mathrm{T}} \mathbf{R}_{i}^{\mathrm{T}} \mathbf{R}_{i+1}\right) \\\mathbf{r}_{\Delta \mathrm{v}_{i, i+1}} &=\mathbf{R}_{i}^{\mathrm{T}}\left(\mathbf{v}_{i+1}-\mathbf{v}_{i}-\mathbf{g} \Delta t_{i, i+1}\right)-\Delta \mathbf{v}_{i, i+1} \\\mathbf{r}_{\Delta \mathrm{p}_{i, i+1}} &=\mathbf{R}_{i}^{\mathrm{T}}\left(\mathbf{p}_{j}-\mathbf{p}_{i}-\mathbf{v}_{i} \Delta t-\frac{1}{2} \mathbf{g} \Delta t^{2}\right)-\Delta \mathbf{p}_{i, i+1}\end{aligned}\label{eq2}\end{equation}</script><p>此外，除了IMU残差还有帧$i$与3D点$\mathbf{x}_j$之间的视觉残差，即重投影误差$\mathbf{r}_{i j}$：</p><script type="math/tex; mode=display">\begin{equation} \mathbf{r}_{i j}=\mathbf{u}_{i j}-\Pi\left(\mathbf{T}_{\mathrm{CB}} \mathbf{T}_{i}^{-1} \oplus \mathbf{x}_{j}\right)\label{eq3}\end{equation}</script><p>其中$\mathbf{u}_{i j}$是3D路标点$\mathbf{x}_j$在当前帧上的投影，信息矩阵为$\Sigma_{i, j}$，$\mathbf{T}_{CB} \in \operatorname{SE}(3)$表示机体系到相机的刚体变换。给定$k+1$个关键帧及其状态量$\bar{S}_{k} \doteq\left\{\mathcal{S}_{0} \ldots \mathcal{S}_{k}\right\}$，同时给定$l$个3D点集，它的状态量为$\mathcal{X} \doteq\left\{\mathbf{x}_{0} \ldots \mathbf{x}_{l-1}\right\}$，于是该优化问题可表示为IMU残差以及重投影误差的组合，具体如下形式：</p><script type="math/tex; mode=display">\begin{equation} \min _{\overline{\mathcal{S}}_{k}, \mathcal{X}}\left(\sum_{i=1}^{k}\left\|\mathbf{r}_{\mathcal{I}_{i-1, i}}\right\|_{\Sigma_{\mathcal{I}_{i, i+1}}^{2}}+\sum_{j=0}^{l-1} \sum_{i \in \mathcal{K}^{j}} \rho_{\text {Hub }}\left(\left\|\mathbf{r}_{i j}\right\|_{\Sigma_{i j}}\right)\right)\label{eq4}\end{equation}</script><p>其中$\mathcal{K}^{j}$表示观测到第$j$个3D点的关键帧集合。这个优化可以用因子图进行表示，如图2a。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-fig-2.png"></p><h2 id="IMU初始化"><a href="#IMU初始化" class="headerlink" title="IMU初始化"></a>IMU初始化</h2><p>这一步的目的是获取IMU参数较好的初始值：速度/重力向以及Bias。本文的工作重点是设计<strong>一种快速准确的IMU初始化算法</strong>。文中提到如下3个见解：</p><ol><li>纯单目SLAM可以提供非常准确的初始地图，问题是尺度未知。首先求解vision-only问题可以提升IMU初始化的性能。</li><li><strong>如果将尺度scale显式表示为优化变量，而不是使用BA的隐式表示，则scale收敛得更快</strong>。</li><li>若在IMU初始化过程中忽略传感器不确定性会产生大量不可预测的错误。</li></ol><p>根据以上三点，本文设计了如下基于最大后验估计的IMU初始化算法，主要分为如下三个步骤：</p><h3 id="Step1-Vision-only-MAP-Estimation"><a href="#Step1-Vision-only-MAP-Estimation" class="headerlink" title="Step1: Vision-only MAP Estimation"></a>Step1: Vision-only MAP Estimation</h3><p>进行单目SLAM，按照关键帧速率4Hz持续运行2s，然后我们可以得到按比例缩放的地图，包括$k=10$个关键帧以及上百个地图点，然后通过Visual-Only BA进行优化，因子图如图2b。于是可以得到优化后的轨迹$\overline{\mathbf{T}}_{0: k}=[\mathbf{R}, \overline{\mathbf{p}}]_{0: k}$，其中上划线表示按比例缩放的变量（即尺度未定）。</p><h3 id="Step2-Inertial-only-MAP-Estimation"><a href="#Step2-Inertial-only-MAP-Estimation" class="headerlink" title="Step2: Inertial-only MAP Estimation"></a>Step2: Inertial-only MAP Estimation</h3><p>这一步的目的是获得IMU参数最优估计。利用$\overline{\mathbf{T}}_{0: k}$以及这些关键帧之间的IMU测量，IMU的参数可以放在一起构成状态向量：</p><script type="math/tex; mode=display">\begin{equation} \mathcal{Y}_{k}=\left\{s, \mathbf{R}_{\mathrm{w} g}, \mathbf{b}, \overline{\mathbf{v}}_{0: k}\right\}\label{eq5}\end{equation}</script><p>其中的$s \in \mathbb{R}^{+}$表示尺度因子，$\mathbf{R}_{\mathrm{w} g} \in \mathrm{SO}(3)$表示重力向，由两个角度表示，重力向量在世界系下的表示为$\mathbf{g}=\mathbf{R}_{\mathrm{w} g} \mathbf{g}_{\mathrm{I}}$，其中$\mathbf{g}_{\mathrm{I}}=(0,0, G)^{\mathrm{T}}$，$G$是重力大小模值；$\mathbf{b}=\left(\mathbf{b}^{a}, \mathbf{b}^{g}\right) \in \mathbb{R}^{6}$表示加速度计以及陀螺仪的bias，在初始化阶段可以假设该值为常数；$\overline{\mathbf{v}}_{0: k}$表示从首帧到末尾帧的无尺度机体速度。此时，我们有IMU的测量$\dot{\mathcal{I}}_{0: k} \doteq\left\{\mathcal{I}_{0,1} \ldots \mathcal{I}_{k-1, k}\right\}$，于是MAP估计问题中的需要被最大化的后验分布为：</p><script type="math/tex; mode=display">\begin{equation} p\left(\mathcal{Y}_{k} \mid \mathcal{I}_{0: k}\right) \propto p\left(\mathcal{I}_{0: k} \mid \mathcal{Y}_{k}\right) p\left(\mathcal{Y}_{k}\right)\label{eq6}\end{equation}</script><p>具体地，$p\left(\mathcal{I}_{0: k} \mid \mathcal{Y}_{k}\right)$表示似然，$p\left(\mathcal{Y}_{k}\right)$表示先验，考虑到IMU测量之间iid，于是最大后验概率估计问题可以表示为如下形式：</p><script type="math/tex; mode=display">\begin{equation} \mathcal{Y}_{k}^{*}=\underset{\mathcal{Y}_{k}}{\arg \max }\left(p\left(\mathcal{Y}_{k}\right) \prod_{i=1}^{k} p\left(\mathcal{I}_{i-1, i} \mid s, \mathbf{g}_{d i r}, \mathbf{b}, \overline{\mathbf{v}}_{i-1}, \overline{\mathbf{v}}_{i}\right)\right)\label{eq7}\end{equation}</script><p>对上式取负对数并且假设IMU预积分以及先验分布服从高斯分布，于是最终的优化问题可以表示为：</p><script type="math/tex; mode=display">\begin{equation} \mathcal{Y}_{k}^{*}=\underset{\mathcal{Y}_{k}}{\arg \min }\left(\left\|\mathbf{r}_{\mathbf{p}}\right\|_{\Sigma_{p}}^{2}+\sum_{i=1}^{k}\left\|\mathbf{r}_{\mathcal{I}_{i-1, i}}\right\|_{\Sigma_{\mathcal{I}_{i-1, i}}}^{2}\right)\label{eq8}\end{equation}</script><p>这个优化问题的因子图表示为图2c，可以看到上式中不包含视觉残差，而是多了一个先验的残差项$\mathbf{r}_{p}$用来约束IMU的bias需要<strong>接近于0</strong>。<br>由于上述优化在流形上进行，重力向的在优化过程中的更新需要构建一个”retraction”：</p><script type="math/tex; mode=display">\begin{equation} \mathbf{R}_{\mathrm{wg}}^{\mathrm{new}}=\mathbf{R}_{\mathrm{wg}}^{\mathrm{old}} \operatorname{Exp}\left(\delta \alpha_{\mathrm{g}}, \delta \beta_{\mathrm{g}}, 0\right)\label{eq9}\end{equation}</script><p>其中$\operatorname{Exp}\left(.\right)$表示从$\mathfrak{s o}(3)$到$\mathrm{SO}(3)$指数映射。<br>为了保证优化过程中尺度因子保持正数，尺度因子的更新形式为如下形式：</p><script type="math/tex; mode=display">\begin{equation} s^{\text {new }}=s^{\text {old }} \exp (\delta s)\label{eq10}\end{equation}</script><p>Inertial-only MAP Estimation完成之后，帧位姿/速度以及3D点根据估计的尺度进行调整，同时将$z$轴对齐重力向。</p><h3 id="Step3-Visual-Inertial-MAP-Estimation"><a href="#Step3-Visual-Inertial-MAP-Estimation" class="headerlink" title="Step3: Visual-Inertial MAP Estimation"></a>Step3: Visual-Inertial MAP Estimation</h3><p>一旦视觉以及IMU有了较好的估计后，进行一个VI联合优化进一步对这些参数进行精化，优化因子图见图2a。<br>作者在EuRoC数据集上进行测试发现上述初始化方式非常有效，可达到2秒内仅5%的误差。为了进一步提升初始估计精度，初始化后会进行5~15秒的VI BA优化，这样可以收敛到仅1%的尺度误差。相较于ORB-SLAM-VI需要15秒才获得首个尺度因子更加快速。经过了这些BA操作之后，我们就认为这个map是mature（成熟的），意思就是尺度/IMU参数/重力向已经被较好地估计完成。</p><h2 id="追踪与建图"><a href="#追踪与建图" class="headerlink" title="追踪与建图"></a>追踪与建图</h2><p>追踪过程借鉴了VI-SLAM-VI的思路：追踪过程解决了简化版的VI优化问题，只优化最后两帧位姿，同时保持地图点固定。</p><p>建图过程是为了解决全图优化问题，若图的规模比较大，这个问题会变得很棘手。本文采用了滑动窗口的思想，即维护了关键帧与地图点的滑动窗口，同时包括它们的共视关键帧，只是在优化时需要保持这些关键帧固定状态。</p><p>慢速运动不能为IMU参数的提供好的可观性，会使初始化无法在仅15秒内收敛到精确的结果。为了 应对这种情况，本文基于修改版“Inertial-only MAP Estimation”的提出了一种尺度精化技术：所有插入的关键帧都参与优化，但优化量只有重力向以及尺度因子，优化因子图见图2d，在这种情况下bias为常数的假设就不再成立。我们使用了每帧的估计量并固定它们。上述的这种操作非常高效，在Local Mapping线程每隔10秒执行一次，直到Map中有100个关键帧或者从初始化起已经过了75秒。</p><h2 id="鲁棒性以及追踪丢失"><a href="#鲁棒性以及追踪丢失" class="headerlink" title="鲁棒性以及追踪丢失"></a>鲁棒性以及追踪丢失</h2><p>快速运动/无纹理/遮挡等会导致系统丢失，ORB-SLAM采用了基于词袋的场景识别进行重定位。本文的VI系统在追踪到少于15个特征点时就会视觉丢失（visually lost），鲁棒性体现在如下两点：</p><ol><li>短期丢失：通过IMU读数对当前帧状态进行估计，地图点根据估计的位姿投影到当前帧后设置较大搜索窗口进行匹配。大多数情况下，通过这种方式能够恢复系统位姿。若超过5秒仍未重定位成功，则进入下一个状态；</li><li>长期丢失：新的VI地图会被重新初始化，并成为active map。</li></ol><h1 id="地图融合与闭环"><a href="#地图融合与闭环" class="headerlink" title="地图融合与闭环"></a>地图融合与闭环</h1><p>前文的介绍可知，短期以及中期数据关联可以通过Tracking以及Local Mapping进行完成。而对于长期的数据关联，可通过重定位以及闭环实现。</p><p>ORB-SLAM采用的基于视觉词袋场景识别的重定位，若候选关键帧只有1个，召回率为50~80%。为了应对假阳性的出现，算法使用了时域校验以及几何校验，这两种手段能够使精确率达到100%的同时召回率为30~40%。至关重要的是，时域连续性检测将使场景识别滞后至少3个关键帧，同时召回率较低，这都是目前存在的问题。</p><p>为了应对这个问题，本文提出一种新的场景识别（召回率得到改善）以及多地图数据关联算法。一旦Local Mapping线程创建了关键帧，场景识别算法就会被激活并且寻找该帧在Atlas中的数据关联。若匹配的关键帧在active map中，则进行闭环；否则，则进行多地图间的数据关联，即将active map与匹配的map进行融合。一旦这个新的关键帧与匹配地图间的相对位姿被计算出，就定义一个在局部窗口，这个局部窗口包括匹配的关键帧以及这个关键帧的共视关键帧。在这个局部窗口中，我们会寻找中期数据关联，以提高闭环以及地图融合的精度。这个改进使得ORB-SLAM3比ORB-SLAM2具有更高的精度。</p><h2 id="场景识别"><a href="#场景识别" class="headerlink" title="场景识别"></a>场景识别</h2><p>此处较为简单，与ORB-SLAM2基本类似，只是增加了重力向校验的步骤。具体的，计算出了当前关键帧在matched map（可能是active map或者其它地图）中的位姿$\mathbf{T}_{a m} \in \operatorname{SE}(3)$，检验pitch(俯仰角)和roll(横滚角)是否小于一定阈值来对场景识别结果进行校验。</p><h2 id="视觉地图融合"><a href="#视觉地图融合" class="headerlink" title="视觉地图融合"></a>视觉地图融合</h2><p>当场景识别成功后，位于actimve map $M_a$中的当前关键帧$K_a$与位于active map $M_m$中的当前关键帧$K_m$产生了多地图的数据关联，此时会进行地图融合。在此过程中，必须格外小心，以确保跟踪线程可以迅速重用$M_m$中的信息，以避免地图重复。</p><p>由于$M_a$可能包含许多元素，融合它们可能需要很长时间，因此融合分为两个步骤。首先，在由$K_a$和$K_m$的邻域定义的welding window（焊接窗口）中执行融合，随后在第二阶段，通过位姿图优化将校正传播到融合图的其余部分。具体过程如下：</p><h3 id="step1-Welding-window-assembly"><a href="#step1-Welding-window-assembly" class="headerlink" title="step1: Welding window assembly"></a>step1: Welding window assembly</h3><p>Welding window包括当前关键帧$K_a$及其共视帧，$K_m$以及其共视帧，以及被这些关键帧观测的地图点。在将它们包含在Welding window中之前，属于$M_a$的关键帧和地图点通过$\mathbf{T}_{m a}$进行变换，以使其与$M_m$对齐。</p><h3 id="step2-融合"><a href="#step2-融合" class="headerlink" title="step2: 融合"></a>step2: 融合</h3><p>$M_a$与$M_m$融合为一个新的active map。为了要删除重复的点，$M_m$关键帧主动搜索匹配$M_a$中的点。对于每个匹配点，都会删除$M_a$中的点，并保留$M_m$中的点，并累积删除点的所有观测值，同时更新共视图以及本质图。</p><h3 id="step3-Welding-bundle-adjustment"><a href="#step3-Welding-bundle-adjustment" class="headerlink" title="step3: Welding bundle adjustment"></a>step3: Welding bundle adjustment</h3><p>Welding window范围内的所有关键帧进行局部BA优化（见下图）。为了确定尺度自由度，<strong>观测到$M_m$的那些关键帧需要保持固定</strong>。优化完成后，Welding window区域中包含的所有关键帧都可以用于跟踪，实现地图$M_m$的快速/准确复用。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-fig-3a.png"></p><h3 id="位姿图优化"><a href="#位姿图优化" class="headerlink" title="位姿图优化"></a>位姿图优化</h3><p>位姿图利用本质图在融合后的地图范围进行优化，Welding window范围内的关键帧保持固定。这一步的意义在于分摊误差。</p><h2 id="视觉惯导地图融合"><a href="#视觉惯导地图融合" class="headerlink" title="视觉惯导地图融合"></a>视觉惯导地图融合</h2><p>与上述纯视觉地图融合步骤类似，只是修改了步骤1和3，具体如下：</p><h3 id="step1-VI-welding-window-assembly"><a href="#step1-VI-welding-window-assembly" class="headerlink" title="step1: VI welding window assembly"></a>step1: VI welding window assembly</h3><p>若active map是mature的，与纯视觉类似，将对地图$M_a$进行$\mathbf{T}_{m a} \in \operatorname{SE}(3)$变换，与$M_m$对齐。若active map不是mature的，将对地图进行$\mathbf{T}_{m a} \in \operatorname{Sim}(3)$变换，与$M_m$对齐。</p><h3 id="step2-同上"><a href="#step2-同上" class="headerlink" title="step2: 同上"></a>step2: 同上</h3><h3 id="step3-VI-welding-bundle-adjustment"><a href="#step3-VI-welding-bundle-adjustment" class="headerlink" title="step3: VI welding bundle adjustment"></a>step3: VI welding bundle adjustment</h3><p>active关键帧$K_a$及其前5个时间连续的关键帧的位姿，速度和bias参与优化，对于$M_m$中的$K_m$及其前5个时间连续的关键帧的位姿，速度和bias也参与优化，详细见下图。对于$M_m$，优化中包括但不固定在紧接局部窗口之前的那一关键帧位姿，而对于$M_a$，局部窗口之前的那一关键帧，但其姿势仍然参与优化。所有这些关键帧看到的所有地图点以及观测这些地图点的关键帧姿势也都进行优化。所有关键帧和地图点都通过重投影误差项（作为约束因子，下图中的蓝色小方块）进行关联。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-fig-3b.png"></p><h2 id="闭环"><a href="#闭环" class="headerlink" title="闭环"></a>闭环</h2><p>闭环校正算法类似于地图融合，但是在这种情况下，当前关键帧以及匹配关键帧都同属于active map。welding window是由匹配的关键帧组合而成/重复的3D点被融合/更新共视图以及本质图的连接关系。下一步是姿势图优化（PG），以均分误差。由于闭环增加了中期/长期数据关联，此时进行全局BA。在VI情况下，仅在关键帧的数量低于阈值时才执行全局BA，以避免巨大的计算量。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>实验主要分为如下部分：</p><ul><li>EuRoC单一会话（地图）：11个场景中的每个序列产生一个地图；传感器配置：单目，单目+IMU，双目以及双目+IMU；</li><li>TUM-VI数据：比较单目/双目鱼眼VI配置下的表现；</li><li>多次会话，上述所有数据；</li></ul><!-- <iframe width="2543" height="1128" src="https://www.youtube.com/embed/kZbpIbaBnZ0?list=PLXwmI2zWtHsY0iCQDOuI-PVroFGLCYHpC" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><iframe width="2543" height="1128" src="https://www.youtube.com/embed/E-7fsq7en2g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> --><h2 id="EuRoC单次会话"><a href="#EuRoC单次会话" class="headerlink" title="EuRoC单次会话"></a>EuRoC单次会话</h2><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-tab-2.png"></p><h2 id="VI-SLAM以及TUM-VI数据集"><a href="#VI-SLAM以及TUM-VI数据集" class="headerlink" title="VI SLAM以及TUM-VI数据集"></a>VI SLAM以及TUM-VI数据集</h2><p><img width="80%" data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-tab-3.png"></p><p><img width="70%" data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-fig-4.png"></p><h2 id="多地图"><a href="#多地图" class="headerlink" title="多地图"></a>多地图</h2><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-fig-5.png"></p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-fig-6.png"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>基于ORB-SLAM/ORB-SLAM2以及ORB-SLAM-VI，本文提出了ORB-SLAM3，它是一个功能更加完整，性能更加出众的SLAM系统。这使得该系统更加适合长时/大规模SLAM实际应用。</p><p>实验结果表明，ORB-SLAM3是第一个能够有效利用短期，中期，长期和多地图数据关联的视觉和视觉惯性系统，其精度水平已经超出了现有系统。 实验结果还表明，关于精度，使用所有这些类型的数据关联的能力会超过其他选择，如使用直接方法代替特征点法或对局部BA执行关键帧边缘化，而不是像我们那样假设一组外部静态关键帧。</p><p>关于鲁棒性，直接法在低纹理环境中可能更鲁棒，但仅限于短期和中期数据关联。另一方面，匹配特征描述符可以成功解决长期和多地图数据关联问题，但与使用光度信息的Lucas-Kanade相比，跟踪功能似乎不那么可靠。在直接法中使用这四种数据关联方式是一个非常有趣的研究领域，我们目前正在根据这个想法探索从人体内部的内窥镜图像构建地图。</p><p>在四种不同的传感器配置中，双目惯导SLAM提供了最可靠，最准确的解决方案。此外，惯性传感器允许以IMU速率估算姿势，IMU速率比帧速率高几个数量级，这可能也会在某些领域发挥优势（如AR/MR等领域）。对于设备体积/成本受限等应用，可以选择使用单目惯导方案，精度与鲁棒性并不会下降多少。</p><p>在慢速运动或没有旋转和俯仰旋转的应用中，例如平坦区域中的汽车，IMU传感器可能难以初始化。在这些情况下，推荐使用双目SLAM。或者，使用CNN进行单目深度恢复的最新研究成果为单目SLAM恢复尺度提供了良好的前景，但是需要保证在同样的环境中对网络进行了训练（泛化性问题）。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文全称：ORB-SLAM3：An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM.&lt;/p&gt;
&lt;p&gt;首先回顾一下历史：ORB-SLAM首次在2015年被提出，它的改进版ORB-SLAM2在2017年被提出，同年提出了ORB-SLAM-VI，时隔3年，ORB-SLAM3横空出世，朋友圈、学术群里到处都在热议这个挂在ARXIV才不到3天的论文。好奇心的驱使下，本人偷瞄了一下论文，就在这里总结一下吧。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://xxx.itp.ac.cn/pdf/2007.11898.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文&lt;/a&gt;&lt;/strong&gt;,  &lt;strong&gt;&lt;a href=&quot;https://github.com/UZ-SLAMLab/ORB_SLAM3&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code Github&lt;/a&gt;&lt;/strong&gt;,  &lt;strong&gt;&lt;a href=&quot;https://gitee.com/vincentqin/ORB_SLAM3&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code国内镜像&lt;/a&gt;&lt;/strong&gt;,  &lt;strong&gt;&lt;a href=&quot;https://github.com/Vincentqyw/Recent-Stars-2020&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SLAM资源站&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/categories/SLAM/"/>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="CV" scheme="https://www.vincentqin.tech/tags/CV/"/>
    
      <category term="笔记" scheme="https://www.vincentqin.tech/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文" scheme="https://www.vincentqin.tech/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>🔨工具：解决Github挂图及龟速访问</title>
    <link href="https://www.vincentqin.tech/posts/manage-pc-hosts/"/>
    <id>https://www.vincentqin.tech/posts/manage-pc-hosts/</id>
    <published>2020-06-03T14:56:38.000Z</published>
    <updated>2020-08-26T14:51:50.439Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>对于经常刷Github的同学而言，是否会经常遇到图片加载不出来/GitHub访问慢等情况？反正我是经常遇到！为了解决这个问题，<a href="https://github.com/521xueweihan" target="_blank" rel="noopener">削微寒</a>公布了解决方案：修改本机<code>hosts</code>，无需安装任何程序。下面是详细说明以及使用方法（修改自项目<em>README</em>）。</p><a id="more"></a><h1 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h1><h2 id="复制如下内容"><a href="#复制如下内容" class="headerlink" title="复制如下内容"></a>复制如下内容</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GitHub520 Host Start</span></span><br><span class="line">185.199.108.154                                   github.githubassets.com</span><br><span class="line">199.232.68.133                                    camo.githubusercontent.com</span><br><span class="line">199.232.68.133                                    github.map.fastly.net</span><br><span class="line">199.232.69.194                                    github.global.ssl.fastly.net</span><br><span class="line">140.82.112.3                                      github.com</span><br><span class="line">140.82.112.6                                      api.github.com</span><br><span class="line">52.136.127.111                                    raw.githubusercontent.com</span><br><span class="line">199.232.68.133                                    favicons.githubusercontent.com</span><br><span class="line">199.232.68.133                                    avatars5.githubusercontent.com</span><br><span class="line">199.232.68.133                                    avatars4.githubusercontent.com</span><br><span class="line">199.232.68.133                                    avatars3.githubusercontent.com</span><br><span class="line">199.232.68.133                                    avatars2.githubusercontent.com</span><br><span class="line">199.232.68.133                                    avatars1.githubusercontent.com</span><br><span class="line">199.232.68.133                                    avatars0.githubusercontent.com</span><br><span class="line"><span class="comment"># GitHub520 Host End</span></span><br></pre></td></tr></table></figure><p>上面内容会<strong>自动定时更新</strong>，这里是<a href="https://raw.githubusercontent.com/521xueweihan/GitHub520/master/hosts" target="_blank" rel="noopener">最新地址</a>，保证最新有效。</p><p>以下介绍两种方式：<a href="#手动方式">手动方式</a>和<a href="#自动方式">自动方式</a>，强烈推荐<a href="#自动方式">自动方式</a>。</p><h2 id="手动方式"><a href="#手动方式" class="headerlink" title="手动方式"></a>手动方式</h2><h3 id="修改-hosts-文件"><a href="#修改-hosts-文件" class="headerlink" title="修改 hosts 文件"></a>修改 hosts 文件</h3><p>hosts 文件在每个系统的位置不一，详情如下：</p><ul><li>Windows 系统：<code>C:\Windows\System32\drivers\etc\hosts</code></li><li>Linux 系统：<code>/etc/hosts</code></li><li>Mac（苹果电脑）系统：<code>/etc/hosts</code></li><li>Android（安卓）系统：<code>/system/etc/hosts</code></li><li>iPhone（iOS）系统：<code>/etc/hosts</code></li></ul><p>修改方法，把第一步的内容复制到文本末尾：</p><ul><li>Windows 使用记事本。</li><li>Linux、Mac 使用 Root 权限：<code>sudo vi /etc/hosts</code>。</li><li>iPhone、iPad 须越狱、Android 必须要 root。</li></ul><h3 id="激活生效"><a href="#激活生效" class="headerlink" title="激活生效"></a>激活生效</h3><p>大部分情况下是直接生效，如未生效可尝试下面的办法，刷新 DNS：</p><ul><li><p>Windows：在 CMD 窗口输入：<code>ipconfig /flushdns</code></p></li><li><p>Linux 命令：<code>sudo rcnscd restart</code></p></li><li><p>Mac 命令：<code>sudo killall -HUP mDNSResponder</code></p></li></ul><p><strong>Tips：</strong> 上述方法无效可以尝试重启机器。</p><h2 id="自动方式"><a href="#自动方式" class="headerlink" title="自动方式"></a>自动方式</h2><h3 id="下载Hosts切换工具"><a href="#下载Hosts切换工具" class="headerlink" title="下载Hosts切换工具"></a>下载Hosts切换工具</h3><p><strong>Tip</strong>：推荐 <a href="https://github.com/oldj/SwitchHosts" target="_blank" rel="noopener">SwitchHosts</a> 工具管理 <code>hosts</code>。根据自己的系统选择对应的版本进行下载，<strong>[<a href="https://github.com/oldj/SwitchHosts/releases" target="_blank" rel="noopener">下载页面</a>]</strong>。</p><h3 id="配置工具"><a href="#配置工具" class="headerlink" title="配置工具"></a>配置工具</h3><p>以SwitchHosts为例，看一下怎么使用的，配置参考下面：</p><div class="note default">            <p><strong>Title</strong>: 随意<br><strong>Type</strong>: <code>Remote</code><br><strong>URL</strong>: <code>https://raw.githubusercontent.com/521xueweihan/GitHub520/master/hosts</code><br><strong>Auto Refresh</strong>: 最好选 <code>1 hour</code></p>          </div><p>配置页面如下图：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/manage-pc-hosts/switch-hosts-realcat-1.png"></p><p>配置好的页面是这样的：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/manage-pc-hosts/switch-hosts-realcat-2.png"></p><p>这样每次 <code>hosts</code> 有更新都能及时进行更新，免去手动更新。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://github.com/521xueweihan/GitHub520" target="_blank" rel="noopener">让你“爱”上 GitHub，解决访问时图裂、加载慢的问题</a></li><li><a href="https://github.com/oldj/SwitchHosts" target="_blank" rel="noopener">系统hosts管理器</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于经常刷Github的同学而言，是否会经常遇到图片加载不出来/GitHub访问慢等情况？反正我是经常遇到！为了解决这个问题，&lt;a href=&quot;https://github.com/521xueweihan&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;削微寒&lt;/a&gt;公布了解决方案：修改本机&lt;code&gt;hosts&lt;/code&gt;，无需安装任何程序。下面是详细说明以及使用方法（修改自项目&lt;em&gt;README&lt;/em&gt;）。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Github" scheme="https://www.vincentqin.tech/tags/Github/"/>
    
      <category term="hosts" scheme="https://www.vincentqin.tech/tags/hosts/"/>
    
  </entry>
  
  <entry>
    <title>📝笔记：图解卡尔曼滤波</title>
    <link href="https://www.vincentqin.tech/posts/kalman-filter-in-pictures/"/>
    <id>https://www.vincentqin.tech/posts/kalman-filter-in-pictures/</id>
    <published>2020-05-24T11:41:50.000Z</published>
    <updated>2020-08-26T15:07:55.773Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>译者注</strong>：这恐怕是全网有关卡尔曼滤波最简单易懂的解释，如果你认真的读完本文，你将对卡尔曼滤波有一个更加清晰的认识，并且可以手推卡尔曼滤波。原文作者使用了漂亮的图片和颜色来阐明它的原理（读起来并不会因公式多而感到枯燥），所以请勇敢地读下去！</p><p>本人翻译水平有限，如有疑问，请阅读原文；如有错误，请在评论区指出。</p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="关于滤波"><a href="#关于滤波" class="headerlink" title="关于滤波"></a>关于滤波</h3><p>首先援引来自知乎大神的解释。</p><div class="note info">            <p>一位专业课的教授给我们上课的时候，曾谈到：filtering is weighting（滤波即加权）。滤波的作用就是给不同的信号分量不同的权重。最简单的loss pass filter， 就是直接把低频的信号给1权重，而给高频部分0权重。对于更复杂的滤波，比如维纳滤波, 则要根据信号的统计知识来设计权重。</p><p>从统计信号处理的角度，降噪可以看成滤波的一种。降噪的目的在于突出信号本身而抑制噪声影响。从这个角度，降噪就是给信号一个高的权重而给噪声一个低的权重。维纳滤波就是一个典型的降噪滤波器。</p>          </div><h3 id="关于卡尔曼滤波"><a href="#关于卡尔曼滤波" class="headerlink" title="关于卡尔曼滤波"></a>关于卡尔曼滤波</h3><p>Kalman Filter 算法，是一种递推预测滤波算法，算法中涉及到滤波，也涉及到对下一时刻数据的预测。Kalman Filter 由一系列递归数学公式描述。它提供了一种高效可计算的方法来估计过程的状态，并使估计均方误差最小。卡尔曼滤波器应用广泛且功能强大：它可以估计信号的过去和当前状态，甚至能估计将来的状态，即使并不知道模型的确切性质。</p><p>Kalman Filter 也可以被认为是一种数据融合算法（Data fusion algorithm），已有50多年的历史，是当今使用最重要和最常见的数据融合算法之一。Kalman Filter 的巨大成功归功于其小的计算需求，优雅的递归属性以及作为具有高斯误差统计的一维线性系统的最优估计器的状态<sup><a href="#fn_4" id="reffn_4">4</a></sup>。</p><p>Kalman Filter 只能减小均值为0的测量噪声带来的影响。只要噪声期望为0，那么不管方差多大，只要迭代次数足够多，那效果都很好。反之，噪声期望不为0，那么估计值就还是与实际值有偏差。</p><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>上面的描述可能把大家绕晕了，下面来点轻松的。</p><p>我们会有一个疑问：卡尔曼滤波到底是如何解决实际问题的呢？</p><p>我们以机器人为例介绍卡尔曼滤波的原理，我们的任务是要预测机器人的状态$\vec{x}$，包括位置$p$与速度$v$，即可表示为：</p><script type="math/tex; mode=display">\vec{x} = \begin{bmatrix}  p\\  v  \end{bmatrix}</script><p>某个时刻，我们不知道真实的位置与速度到底是多少，二者存在一个所有可能性的组合，大致如下图所示。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_0.png"></p><p><strong>卡尔曼滤波假设状态所有的变量都是随机的且都服从高斯分布，每个变量都有其对应的均值$\mu$以及方差$\sigma^2$（它代表了不确定性）</strong>。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_1.png"></p><p>在上图中，位置和速度是不相关（<strong>uncorrelated</strong>）的，这意味着某个变量的状态不会告诉你其他变量的状态是怎样的。即虽然我们知道现在的速度，但是无法从现在的速度推测出现在的位置。但实际上并非如此，我们知道速度和位置是有关系的（<strong>correlated</strong>），这样一来二者之间的组合关系变成了如下图所示的情况。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_3.png"></p><p>这种情况是很容易发生的，例如，如果速度很快，我们可能会走得更远，所以我们的位置会更大。如果我们走得很慢，我们就不会走得太远。</p><p>这种状态变量之间的关系很重要，因为它可以为我们提供<strong>更多信息</strong>：One measurement tells us something about what the others could be。这就是卡尔曼滤波器的目标，我们希望从不确定的测量中尽可能多地获取信息！</p><p>这种状态量的相关性可以由协方差矩阵表示。简而言之，矩阵$\Sigma_{ij}$的每个元素是第i个状态变量和第j个状态变量之间的相关度。（显然地可以知道协方差矩阵是对称的，这意味着您交换i和j都没关系）。协方差矩阵通常标记为“ $\Sigma$”，因此我们将它们的元素称为“$\Sigma_{ij}$”。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_2.png"></p><h2 id="状态预测"><a href="#状态预测" class="headerlink" title="状态预测"></a>状态预测</h2><h3 id="问题的矩阵形式表示"><a href="#问题的矩阵形式表示" class="headerlink" title="问题的矩阵形式表示"></a>问题的矩阵形式表示</h3><p>我们把状态建模成高斯分布（Gaussian blob，由于二维高斯分布长得像一个个小泡泡，之所以长这个样子，可参考链接<sup><a href="#fn_2" id="reffn_2">2</a></sup>）。我们需要求解/估计在时间$k$时刻的两个信息：1. 最优估计$\mathbf{\hat{x}_k}$以及它的协方差矩阵$\mathbf{P_k}$，我们可以写成下面矩阵形式：</p><script type="math/tex; mode=display">\begin{equation} \label{eq:statevars}  \begin{aligned}  \mathbf{\hat{x}}_k &= \begin{bmatrix}  \text{position}\\  \text{velocity}  \end{bmatrix}\\  \mathbf{P}_k &=  \begin{bmatrix}  \Sigma_{pp} & \Sigma_{pv} \\  \Sigma_{vp} & \Sigma_{vv} \\  \end{bmatrix}  \end{aligned}  \end{equation}</script><p>（当然，这里我们仅使用位置和速度，但是请记住状态可以包含任意数量的变量，并且可以表示所需的任何变量）</p><p>接下来，我们需要某种方式来查看当前状态（$k-1$时刻）并<strong>预测</strong>在时刻$k$处的状态。请记住，我们不知道哪个状态是“真实”状态，但是我们提到的<strong>预测</strong>（prediction）并不在乎这些。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_7.jpg"></p><p>我们可以用一个矩阵$\mathbf{F_k}$来表示这个预测过程：</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_8.jpg"></p><p>这个矩阵$\mathbf{F_k}$将原始估计中的每个点移动到新的预测位置。</p><p>那么问题来了，应该如何使用上述矩阵来预测下一时刻的位置和速度呢？为了阐述这个过程，我们使用了一个非常基础的运动学公式（初中物理中就学过）进行描述：</p><script type="math/tex; mode=display">\begin{split} \color{deeppink}{p_k} &= \color{royalblue}{p_{k-1}} + \Delta t &\color{royalblue}{v_{k-1}} \\ \color{deeppink}{v_k} &= &\color{royalblue}{v_{k-1}} \end{split}</script><p>写成矩阵形式：</p><script type="math/tex; mode=display">\begin{align} \color{deeppink}{\mathbf{\hat{x}}_k} &= \begin{bmatrix} 1 & \Delta t \\ 0 & 1 \end{bmatrix} \color{royalblue}{\mathbf{\hat{x}}_{k-1}} \\ &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} \label{statevars} \end{align}</script><p>现在我们有了一个<strong>预测矩阵</strong>或者叫做<strong>状态转移矩阵</strong>，该矩阵可以帮助我们计算下一个时刻的状态。但我们仍然不知道如何更新状态的协方差矩阵，其实过程也是很简单，如果我们将分布中的每个点乘以矩阵$A$，那么其协方差矩阵$\Sigma$会发生什么？</p><script type="math/tex; mode=display">\begin{equation} \begin{split} Cov(x) &= \Sigma\\ Cov(\color{firebrick}{\mathbf{A}}x) &= \color{firebrick}{\mathbf{A}} \Sigma \color{firebrick}{\mathbf{A}}^T \end{split} \label{covident} \end{equation}</script><p>将公式$\eqref{covident}$带入公式$\eqref{statevars}$，我们可以得到：</p><script type="math/tex; mode=display">\begin{equation} \begin{split} \color{deeppink}{\mathbf{\hat{x}}_k} &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} \\ \color{deeppink}{\mathbf{P}_k} &= \mathbf{F_k} \color{royalblue}{\mathbf{P}_{k-1}} \mathbf{F}_k^T \end{split} \end{equation}</script><h3 id="External-influence"><a href="#External-influence" class="headerlink" title="External influence"></a>External influence</h3><p>不过我们并没有考虑到所有的影响因素。可能有一些与状态本身无关的变化——如外界因素可能正在影响系统。</p><p>例如，我们用状态对列车的运动进行建模，如果列车长加大油门，火车就加速。同样，在我们的机器人示例中，导航系统软件可能会发出使车轮转动或停止的命令。如果我们很明确地知道这些因素，我们可以将其放在一起构成一个向量$\color{darkorange}{\vec{\mathbf{u}_k}}$，对这个量进行处理，然后将其添加到我们的预测中对状态进行更正。</p><p>假设我们知道由于油门设置或控制命令而产生的预期加速度$\color{darkorange}{a}$。根据基本运动学，我们得到下式：</p><script type="math/tex; mode=display">\begin{split} \color{deeppink}{p_k} &= \color{royalblue}{p_{k-1}} + {\Delta t} &\color{royalblue}{v_{k-1}} + &\frac{1}{2} \color{darkorange}{a} {\Delta t}^2 \\ \color{deeppink}{v_k} &= &\color{royalblue}{v_{k-1}} + & \color{darkorange}{a} {\Delta t} \end{split}</script><p>矩阵形式：</p><script type="math/tex; mode=display">\begin{equation} \begin{split} \color{deeppink}{\mathbf{\hat{x}}_k} &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} + \begin{bmatrix} \frac{\Delta t^2}{2} \\ \Delta t \end{bmatrix} \color{darkorange}{a} \\ &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} + \mathbf{B}_k \color{darkorange}{\vec{\mathbf{u}_k}} \end{split} \end{equation}</script><p>其中$\mathbf{B}_k$被称为<strong>控制矩阵</strong>，$\color{darkorange}{\vec{\mathbf{u}_k}}$被称为<strong>控制向量</strong>。（注意：对于没有外部影响的简单系统，可以忽略该控制项）。</p><p>如果我们的预测并不是100％准确模型，这会发生什么呢？</p><h3 id="External-uncertainty"><a href="#External-uncertainty" class="headerlink" title="External uncertainty"></a>External uncertainty</h3><p>如果状态仅仅依赖其自身的属性进行演进，那一切都很好。如果状态受到外部因素进行演进，我们只要知道那些外部因素是什么，那么一切仍然很好。</p><p>但在实际使用中，我们有时不知道的那些外部因素到底是如何被建模的。例如，我们要跟踪四轴飞行器，它可能会随风摇晃；如果我们跟踪的是轮式机器人，则车轮可能会打滑，或者因地面颠簸导致其减速。我们无法跟踪这些外部因素，如果发生任何这些情况，我们的预测可能会出错，因为我们并没有考虑这些因素。</p><p>通过在每个预测步骤之后添加一些新的不确定性，我们可以对与“世界”相关的不确定性进行建模（如我们无法跟踪的事物）：</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_9.jpg"></p><p>这样一来，由于新增的不确定性<strong>原始估计中的每个状态都可能迁移到多个状态</strong>。 因为我们非常喜欢用高斯分布进行建模，此时也不例外。我们可以说$\color{royalblue}{\mathbf{\hat{x}}_{k-1}}$的每个点都移动到具有协方差$\color{mediumaquamarine}{\mathbf{Q}_k}$的高斯分布内的某个位置，如下图所示：</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_10a.jpg"></p><p>这将产生一个新的高斯分布，其协方差不同（但均值相同）：</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_10b.jpg"></p><p>所以呢，我们在状态量的协方差中增加了额外的协方差$\color{mediumaquamarine}{\mathbf{Q}_k}$，所以预测阶段完整的状态转移方程为：</p><script type="math/tex; mode=display">\begin{equation}  \begin{split}  \color{deeppink}{\mathbf{\hat{x}}_k} &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} + \mathbf{B}_k \color{darkorange}{\vec{\mathbf{u}_k}} \\  \color{deeppink}{\mathbf{P}_k} &= \mathbf{F_k} \color{royalblue}{\mathbf{P}_{k-1}} \mathbf{F}_k^T + \color{mediumaquamarine}{\mathbf{Q}_k}  \end{split}  \label{kalpredictfull}  \end{equation}</script><p>换句话说：<strong><font color="deeppink">新的最佳估计</font></strong>是根据<strong><font color="royalblue">先前的最佳估计</font></strong>做出的<strong>预测</strong>，再加上对<strong><font color="darkorange">已知外部影响</font></strong>的校正。</p><p><strong><font color="deeppink">新的不确定度</font></strong>是根据<strong><font color="royalblue">先前的不确定度</font></strong>做出的<strong>预测</strong>，再加上<strong><font color="mediumaquamarine">来自环境额外的不确定度</font></strong>。</p><p>上述过程描绘了状态预测过程，那么当我们从传感器中获取一些测量数据时会发生什么呢？</p><h2 id="状态更新"><a href="#状态更新" class="headerlink" title="状态更新"></a>状态更新</h2><h3 id="利用测量进一步修正状态"><a href="#利用测量进一步修正状态" class="headerlink" title="利用测量进一步修正状态"></a>利用测量进一步修正状态</h3><p>假设我们有几个传感器，这些传感器可以向我们提供有关系统状态的信息。就目前而言，测量什么量都无关紧要，也许一个读取位置，另一个读取速度。每个传感器都告诉我们有关状态的一些间接信息（换句话说，传感器在状态下运作并产生一组测量读数）。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_12.jpg"></p><p>请注意，测量的单位可能与状态量的单位不同。我们使用矩阵$\mathbf{H}_k$对传感器的测量进行建模。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_13.jpg"></p><p>所以我们期望传感器的度数可以被建模成如下形式：</p><script type="math/tex; mode=display">\begin{equation}  \begin{aligned}  \vec{\mu}_{\text{expected}} &= \mathbf{H}_k \color{deeppink}{\mathbf{\hat{x}}_k} \\  \mathbf{\Sigma}_{\text{expected}} &= \mathbf{H}_k \color{deeppink}{\mathbf{P}_k} \mathbf{H}_k^T  \end{aligned}  \end{equation}</script><p>卡尔曼滤波器的伟大之处就在于它能够处理传感器噪声。换句话说，传感器本身的测量是不准确的，且原始估计中的每个状态都可能导致一定范围的传感器读数，而卡尔曼滤波能够在这些不确定性存在的情况下找到最优的状态。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_14.jpg"></p><p>根据传感器的读数，我们会猜测系统正处于某个特定状态。但是由于不确定性的存在，<strong>某些状态比其他状态更可能产生我们看到的读数</strong>：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_11.jpg"></p><p>我们将这种不确定性（如传感器噪声）的<strong>协方差</strong>表示为$\color{mediumaquamarine}{\mathbf{R}_k}$，读数的分布<strong>均值</strong>等于我们观察到传感器的读数，我们将其表示为$\color{yellowgreen}{\vec{\mathbf{z}_k}}$</p><p>这样一来，我们有了两个高斯分布：一个围绕通过状态转移预测的平均值，另一个围绕实际传感器读数。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_4.jpg"></p><p>因此，我们需要将基于预测状态（<strong><font color="deeppink">粉红色</font></strong>）的推测读数与基于实际观察到的传感器读数（<strong><font color="yellowgreen">绿色</font></strong>）进行融合。</p><p>那么融合后<strong>最有可能的新状态</strong>是什么？ 对于任何可能的读数$(z_1,z_2)$，我们都有两个相关的概率：（1）我们的传感器读数$\color{yellowgreen}{\vec{\mathbf{z}_k}}$是$(z_1,z_2)$的（误-）测量值的概率，以及（2）先前估计值的概率认为$(z_1,z_2)$是我们应该看到的读数。</p><p>如果我们有两个概率，并且想知道两个概率都为真的机会，则将它们相乘。因此，我们对两个高斯分布进行了相乘处理：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_6a.png"></p><p>两个概率分布相乘得到的就是上图中的重叠部分。而且重叠部分的概率分布会比我们之前的任何一个估计值/读数都精确得多，这个分布的均值就是两种估计最有可能配置（得到的状态）。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_6.png"></p><p>事实证明，<strong>两个独立的高斯分布相乘之后会得到一个新的具有其均值和协方差矩阵的高斯分布</strong>！下面开始推公式。</p><h3 id="合并两个高斯分布"><a href="#合并两个高斯分布" class="headerlink" title="合并两个高斯分布"></a>合并两个高斯分布</h3><p>首先考虑一维高斯情况：一个均值为$\mu$，方差为$\sigma^2$的高斯分布的形式为：</p><script type="math/tex; mode=display">\begin{equation} \label{gaussformula} \mathcal{N}(x, \mu,\sigma) = \frac{1}{ \sigma \sqrt{ 2\pi } } e^{ -\frac{ (x – \mu)^2 }{ 2\sigma^2 } } \end{equation}</script><p>我们想知道将两个高斯曲线相乘会发生什么。下图中的蓝色曲线表示两个高斯总体的（未归一化）交集：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_joint.png"></p><script type="math/tex; mode=display">\begin{equation} \label{gaussequiv} \mathcal{N}(x, \color{fuchsia}{\mu_0}, \color{deeppink}{\sigma_0}) \cdot \mathcal{N}(x, \color{yellowgreen}{\mu_1}, \color{mediumaquamarine}{\sigma_1}) \stackrel{?}{=} \mathcal{N}(x, \color{royalblue}{\mu’}, \color{mediumblue}{\sigma’}) \end{equation}</script><p>将公式$\eqref{gaussformula}$代入公式$\eqref{gaussequiv}$，我们可以得到新的高斯分布的均值和方差如下所示：</p><script type="math/tex; mode=display">\begin{equation} \label{fusionformula} \begin{aligned} \color{royalblue}{\mu’} &= \mu_0 + \frac{\sigma_0^2 (\mu_1 – \mu_0)} {\sigma_0^2 + \sigma_1^2}\\ \color{mediumblue}{\sigma’}^2 &= \sigma_0^2 – \frac{\sigma_0^4} {\sigma_0^2 + \sigma_1^2} \end{aligned} \end{equation}</script><p>我们将其中的一小部分重写为$\color{purple}{\mathbf{k}}$：</p><script type="math/tex; mode=display">\begin{equation} \label{gainformula} \color{purple}{\mathbf{k}} = \frac{\sigma_0^2}{\sigma_0^2 + \sigma_1^2} \end{equation}</script><script type="math/tex; mode=display">\begin{equation} \begin{split} \color{royalblue}{\mu’} &= \mu_0 + &\color{purple}{\mathbf{k}} (\mu_1 – \mu_0)\\ \color{mediumblue}{\sigma’}^2 &= \sigma_0^2 – &\color{purple}{\mathbf{k}} \sigma_0^2 \end{split} \label{update} \end{equation}</script><p>这样一来，公式的形式就简单多了！我们顺势将公式$\eqref{gainformula}$和$\eqref{update}$的矩阵形式写在下面：</p><script type="math/tex; mode=display">\begin{equation} \label{matrixgain} \color{purple}{\mathbf{K}} = \Sigma_0 (\Sigma_0 + \Sigma_1)^{-1} \end{equation}</script><script type="math/tex; mode=display">\begin{equation} \begin{split} \color{royalblue}{\vec{\mu}’} &= \vec{\mu_0} + &\color{purple}{\mathbf{K}} (\vec{\mu_1} – \vec{\mu_0})\\ \color{mediumblue}{\Sigma’} &= \Sigma_0 – &\color{purple}{\mathbf{K}} \Sigma_0 \end{split} \label{matrixupdate} \end{equation}</script><p>其中$\Sigma$表示新高斯分布的协方差矩阵，$\vec{\mu}$是每个维度的均值，$\color{purple}{\mathbf{K}}$就是大名鼎鼎的“<strong>卡尔曼增益</strong>”（<strong>Kalman gain</strong>）。</p><h3 id="公式汇总"><a href="#公式汇总" class="headerlink" title="公式汇总"></a>公式汇总</h3><p>我们有两个高斯分布，一个是我们预测的观测$(\color{fuchsia}{\mu_0}, \color{deeppink}{\Sigma_0}) = (\color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k}, \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T})$，另外一个是实际的观测(传感器读数)$(\color{yellowgreen}{\mu_1}, \color{mediumaquamarine}{\Sigma_1}) = (\color{yellowgreen}{\vec{\mathbf{z}_k}}, \color{mediumaquamarine}{\mathbf{R}_k})$，我们将这两个高斯分布带入公式$\eqref{matrixupdate}$中就可以得到二者的重叠区域：</p><script type="math/tex; mode=display">\begin{equation} \begin{aligned} \mathbf{H}_k \color{royalblue}{\mathbf{\hat{x}}_k’} &= \color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k} & + & \color{purple}{\mathbf{K}} ( \color{yellowgreen}{\vec{\mathbf{z}_k}} – \color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k} ) \\ \mathbf{H}_k \color{royalblue}{\mathbf{P}_k’} \mathbf{H}_k^T &= \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} & – & \color{purple}{\mathbf{K}} \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} \end{aligned} \label {kalunsimplified} \end{equation}</script><p>从公式$\eqref{matrixgain}$我们可以知道，卡尔曼增益是：</p><script type="math/tex; mode=display">\begin{equation} \label{eq:kalgainunsimplified} \color{purple}{\mathbf{K}} = \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} ( \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} + \color{mediumaquamarine}{\mathbf{R}_k})^{-1} \end{equation}</script><p>然后我们将公式$\eqref{kalunsimplified}$与公式$\eqref{eq:kalgainunsimplified}$中的$\mathbf{H}_k$去除，同时将$\color{royalblue}{\mathbf{P}_k’}$后面的$\mathbf{H}_k^T$去除，我们可以得到最终的化简形式的更新方程：</p><script type="math/tex; mode=display">\begin{equation} \begin{split} \color{royalblue}{\mathbf{\hat{x}}_k’} &= \color{fuchsia}{\mathbf{\hat{x}}_k} & + & \color{purple}{\mathbf{K}’} ( \color{yellowgreen}{\vec{\mathbf{z}_k}} – \color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k} ) \\ \color{royalblue}{\mathbf{P}_k’} &= \color{deeppink}{\mathbf{P}_k} & – & \color{purple}{\mathbf{K}’} \color{deeppink}{\mathbf{H}_k \mathbf{P}_k} \end{split} \label{kalupdatefull} \end{equation}</script><script type="math/tex; mode=display">\begin{equation} \color{purple}{\mathbf{K}’} = \color{deeppink}{\mathbf{P}_k \mathbf{H}_k^T} ( \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} + \color{mediumaquamarine}{\mathbf{R}_k})^{-1} \label{kalgainfull} \end{equation}</script><h2 id="图说"><a href="#图说" class="headerlink" title="图说"></a>图说</h2><p>大功告成，$\color{royalblue}{\mathbf{\hat{x}}_k’}$就是更新后的最优状态！接下来我们可以继续进行预测，然后更新，重复上述过程！下图给出卡尔曼滤波信息流。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/kalflow.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在上述所有数学公式中，你需要实现的只是公式$\eqref{kalpredictfull}, \eqref{kalupdatefull}$和$\eqref{kalgainfull}$。（或者，如果你忘记了这些，可以从等式$\eqref{covident}$和$\eqref{matrixupdate}$重新推导所有内容。）</p><p>这将使你能够准确地对任何线性系统建模。对于非线性系统，我们使用<strong>扩展卡尔曼滤波器</strong>，该滤波器通过简单地线性化预测和测量值的均值进行工作。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><blockquote id="fn_1"><sup>1</sup>. <a href="http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/#mathybits" target="_blank" rel="noopener">How a Kalman filter works, in pictures, 图解卡尔曼滤波是如何工作的</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. <a href="https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/" target="_blank" rel="noopener">A geometric interpretation of the covariance matrix, 协方差矩阵的几何解释</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. <a href="https://sikasjc.github.io/2018/05/08/kalman_filter" target="_blank" rel="noopener">Kalman Filter 卡尔曼滤波</a><a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. R. Faragher, “Understanding the Basis of the Kalman Filter Via a Simple and Intuitive Derivation [Lecture Notes]”, IEEE Signal Processing Magazine, vol. 29, no. 5, pp. 128–132, Sep. 2012.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. G. Welch and G. Bishop, “<a href="http://www.cs.unc.edu/~welch/media/pdf/kalman_intro.pdf" target="_blank" rel="noopener">An Introduction to the Kalman Filter</a>”, p. 16, 2006.<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. Fitzgerald, Robert J. “Divergence of the Kalman filter”, Automatic Control IEEE Transactions on 16.6(1971):736-747.<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;译者注&lt;/strong&gt;：这恐怕是全网有关卡尔曼滤波最简单易懂的解释，如果你认真的读完本文，你将对卡尔曼滤波有一个更加清晰的认识，并且可以手推卡尔曼滤波。原文作者使用了漂亮的图片和颜色来阐明它的原理（读起来并不会因公式多而感到枯燥），所以请勇敢地读下去！&lt;/p&gt;
&lt;p&gt;本人翻译水平有限，如有疑问，请阅读原文；如有错误，请在评论区指出。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>🔨工具：国内加速访问arxiv</title>
    <link href="https://www.vincentqin.tech/posts/redirect-arxiv/"/>
    <id>https://www.vincentqin.tech/posts/redirect-arxiv/</id>
    <published>2020-05-20T14:44:56.000Z</published>
    <updated>2020-08-26T15:07:36.816Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note warning">            <p><strong>注意</strong>：<code>arxiv</code>读音与<code>archive</code>一样，英[ˈɑːkaɪv]，美[ˈɑːrkaɪv]</p>          </div><p>国内访问论文预发布平台<code>arxiv</code>巨慢无比，让人闹心！网上找了一个很好用的方法，按照这个方法配置之后<code>arxiv</code>就秒开了。原理就是将<code>arxiv</code>重定向到<code>xxx.itp.ac.cn</code>（中科院理论物理研究所镜像）。<br>如果此时你找到了一篇文章，地址是<code>arxiv.org/abs/1911.11763</code>，只需要把<code>arxiv.org</code>换成<code>xxx.itp.ac.cn</code>即可。但每次都手动配置就很麻烦，为了贯彻将懒惰进行到底的精神，我们需要将上述过程自动化。配置如下：</p><a id="more"></a><ol><li><p>安装<a href="https://chrome.google.com/webstore/detail/tampermonkey/dhdgffkkebhmkfjojejmpbldmpobfkfo?hl=en" target="_blank" rel="noopener">Tampermonkey</a>油猴插件，自行google。</p></li><li><p>添加如下脚本</p></li></ol><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// ==UserScript==</span></span><br><span class="line"><span class="comment">// @name        redirect_arxiv</span></span><br><span class="line"><span class="comment">// @namespace   redirect_arxiv</span></span><br><span class="line"><span class="comment">// @include     *</span></span><br><span class="line"><span class="comment">// @include     https://*github.io*</span></span><br><span class="line"><span class="comment">// @include     https://*arxiv.org/*</span></span><br><span class="line"><span class="comment">// @include     https://*google.c*</span></span><br><span class="line"><span class="comment">// @include     https://*semanticscholar.org/*</span></span><br><span class="line"><span class="comment">// @include     https://*github.com*</span></span><br><span class="line"><span class="comment">// @include     https://*zhihu.com*</span></span><br><span class="line"><span class="comment">// @include     https://*brainpp.cn*</span></span><br><span class="line"><span class="comment">// @include     https://*outlook.cn*</span></span><br><span class="line"><span class="comment">// @version     1.0</span></span><br><span class="line"><span class="comment">// @grant       none</span></span><br><span class="line"><span class="comment">// ==/UserScript==</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 重定向 arxiv.org 到 xxx.itp.ac.cn（中科院理论物理研究所镜像）</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">findFatherNode</span>(<span class="params">node, nodeName=<span class="string">'A'</span>, maxDeep=<span class="number">1000</span></span>)</span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; maxDeep; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (! node)&#123;<span class="keyword">return</span> node&#125;</span><br><span class="line">        <span class="keyword">if</span> (node.nodeName == nodeName)&#123;</span><br><span class="line">            <span class="keyword">return</span> node</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            node = node.parentElement</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">document</span>.body.addEventListener(<span class="string">'mousedown'</span>, <span class="function"><span class="keyword">function</span>(<span class="params">e</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">var</span> targ = e.target || e.srcElement;</span><br><span class="line">    <span class="keyword">var</span> aTag = findFatherNode(targ, <span class="string">'A'</span>, <span class="number">10</span>);</span><br><span class="line">    <span class="keyword">if</span> (!aTag || !(aTag.href))&#123;<span class="keyword">return</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> headN = <span class="number">17</span>;</span><br><span class="line">    <span class="keyword">var</span> hrefHead = aTag.href.slice(<span class="number">0</span>, headN);</span><br><span class="line">    <span class="keyword">var</span> hrefTail = aTag.href.slice(headN);</span><br><span class="line">    <span class="keyword">if</span> ( (hrefHead.indexOf(<span class="string">'arxiv.org'</span>)==<span class="number">-1</span>))&#123;<span class="keyword">return</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ( hrefHead.match(<span class="regexp">/https?:\/\/arxiv\.org/</span>) ) &#123;</span><br><span class="line">        hrefHead = hrefHead.replace(<span class="regexp">/https?:\/\/arxiv\.org/</span>, <span class="string">'http://xxx.itp.ac.cn'</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    aTag.href = hrefHead + hrefTail</span><br><span class="line">    <span class="comment">// console.log(targ, targ.href);</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><ul><li><a href="https://www.zhihu.com/question/58912862/answer/695125360" target="_blank" rel="noopener">参考来自小磊知乎的回答</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note warning&quot;&gt;
            &lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：&lt;code&gt;arxiv&lt;/code&gt;读音与&lt;code&gt;archive&lt;/code&gt;一样，英[ˈɑːkaɪv]，美[ˈɑːrkaɪv]&lt;/p&gt;
          &lt;/div&gt;
&lt;p&gt;国内访问论文预发布平台&lt;code&gt;arxiv&lt;/code&gt;巨慢无比，让人闹心！网上找了一个很好用的方法，按照这个方法配置之后&lt;code&gt;arxiv&lt;/code&gt;就秒开了。原理就是将&lt;code&gt;arxiv&lt;/code&gt;重定向到&lt;code&gt;xxx.itp.ac.cn&lt;/code&gt;（中科院理论物理研究所镜像）。&lt;br&gt;如果此时你找到了一篇文章，地址是&lt;code&gt;arxiv.org/abs/1911.11763&lt;/code&gt;，只需要把&lt;code&gt;arxiv.org&lt;/code&gt;换成&lt;code&gt;xxx.itp.ac.cn&lt;/code&gt;即可。但每次都手动配置就很麻烦，为了贯彻将懒惰进行到底的精神，我们需要将上述过程自动化。配置如下：&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="arxiv" scheme="https://www.vincentqin.tech/tags/arxiv/"/>
    
      <category term="redirect" scheme="https://www.vincentqin.tech/tags/redirect/"/>
    
  </entry>
  
  <entry>
    <title>📝笔记：CVPR2020图像匹配挑战赛，新数据集+新评测方法，SOTA正瑟瑟发抖！</title>
    <link href="https://www.vincentqin.tech/posts/2020-image-matching-cvpr/"/>
    <id>https://www.vincentqin.tech/posts/2020-image-matching-cvpr/</id>
    <published>2020-05-17T04:23:09.000Z</published>
    <updated>2020-08-26T15:08:50.909Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>从一系列的图像中恢复物体的3D结构是计算机视觉研究中一个热门课题，这使得我们可以相隔万里从google map中看到复活节岛的风景。这得益于图像来自于可控的条件，使得最终的重建效果一致性且质量都很高，但是这却限制了采集设备以及视角。畅想一下，假如我们不使用专业设备，而是利用sfm技术根据互联网上大量的图片重建出这个复杂世界。</p><!-- ![A 3D reconstruction generated from over 3000 images, including those from the previous figure](https://1.bp.blogspot.com/-loSqCB3NnM0/XoTiOGP9SYI/AAAAAAAAFlE/rs8iCTq63FYapA7HbljF8iWa7fyHvh3UgCLcBGAsYHQ/s400/image3.gif) --><!-- ![](https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/image_sfm.gif) --><p><img alt data-src="https://vincentqin.gitee.io/posts/2020-image-matching-cvpr/image_sfm.gif"></p><p>为了加快这个领域的研究，更好地利用图像数据有效信息，谷歌联合 <a href="https://www.uvic.ca/" target="_blank" rel="noopener">UVIC</a>, <a href="https://www.cvut.cz/en" target="_blank" rel="noopener">CTU</a>以及EPFL发表了这篇文章 “<a href="https://arxiv.org/abs/2003.01587" target="_blank" rel="noopener">Image Matching across Wide Baselines: From Paper to Practice</a>”，[<strong><a href="http://xxx.itp.ac.cn/pdf/2003.01587v2" target="_blank" rel="noopener">PDF</a></strong>]，旨在公布一种新的衡量用于3D重建方法的标准模块+数据集，这里主要是指2D图像间的匹配。这个评价模块可以很方便地集成并评估现有流行的特征匹配算法，包括传统方法或者基于机器学习的方法。</p><p>谷歌公布2020图像匹配挑战的数据集：<a href="https://image-matching-workshop.github.io/" target="_blank" rel="noopener">官网</a>，<a href="http://ai.googleblog.com/2020/04/announcing-2020-image-matching.html" target="_blank" rel="noopener">博客</a>，文末有排行榜。</p><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>图像特征匹配是计算机视觉的基础+核心问题之一，包括image retrieval <sup><a href="#fn_48" id="reffn_48">48</a></sup> <sup><a href="#fn_7" id="reffn_7">7</a></sup> <sup><a href="#fn_69" id="reffn_69">69</a></sup> <sup><a href="#fn_91" id="reffn_91">91</a></sup> <sup><a href="#fn_63" id="reffn_63">63</a></sup>, 3D reconstruction<sup><a href="#fn_3" id="reffn_3">3</a></sup> <sup><a href="#fn_43" id="reffn_43">43</a></sup> <sup><a href="#fn_79" id="reffn_79">79</a></sup> <sup><a href="#fn_106" id="reffn_106">106</a></sup>，re-localization <sup><a href="#fn_74" id="reffn_74">74</a></sup> <sup><a href="#fn_75" id="reffn_75">75</a></sup> <sup><a href="#fn_51" id="reffn_51">51</a></sup>以及 SLAM <sup><a href="#fn_61" id="reffn_61">61</a></sup> <sup><a href="#fn_30" id="reffn_30">30</a></sup> <sup><a href="#fn_31" id="reffn_31">31</a></sup>等在内的诸多研究领域都会用到特征匹配。这个问题已经研究了几十年，但仍未被很好地解决。特征匹配面临的问题很多，主要包括以下挑战：视角，尺度，旋转，光照，遮挡以及相机渲染等。</p><p>近些年来，研究者开始将视线转移到端到端的学习方法（图像-&gt;位姿），但是这些方法甚至没有达到传统的方法（图像-&gt;匹配-&gt;BA优化）的性能。我们可以看到，传统的方法将3D重建问题拆分成为2个子问题：特征匹配与位姿解算。解决每个子问题的新方法，诸如特征匹配/位姿解算，都使用了“临时指标”，但是单独地评价单个子问题的性能不足以说明整体性能。例如，一些研究仅在某个数据集上展现了相较于手工特征SIFT的优势，但是这些算法是否能够在真实应用中仍然展现出优势呢？我们通过后续实验说明传统算法经过调整之后也可匹敌现有的标称“sota”的算法（着实打脸）。</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig1.png"></p><p>是时候换一种方式进行评价了，本文不去过多关注在临时指标上的表现，而关注在下游任务上的表现。本文贡献：</p><ol><li>30k图像+深度图+真实位姿（posed image）</li><li>模块化流水线处理流程，结合了数十种经典的和最新的特征提取和匹配以及姿态估计方法，以及多种启发式方法，可以分别交换和调整</li><li>两个下游任务，双目/多视角重建</li><li>全面研究了手工特征以及学习特征数十种方法和技术，以及它们的结合以及超参数选择的过程</li></ol><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="局部特征"><a href="#局部特征" class="headerlink" title="局部特征"></a>局部特征</h3><p>在引入SIFT特征之后，局部特征变成了主流。它的处理流程主要分为几个步骤：特征提取，旋转估计，描述子提取。除了SIFT，手工特征还有SURF <sup><a href="#fn_15" id="reffn_15">15</a></sup>, ORB <sup><a href="#fn_73" id="reffn_73">73</a></sup>, 以及 AKAZE <sup><a href="#fn_4" id="reffn_4">4</a></sup>等。</p><p>现代描述子通常在SIFT关键点（即DoG）的预裁剪图像块上训练深度网络，其中包括：Deepdesc <sup><a href="#fn_82" id="reffn_82">82</a></sup>, TFeat <sup><a href="#fn_11" id="reffn_11">11</a></sup>, L2-Net <sup><a href="#fn_89" id="reffn_89">89</a></sup>, Hardnet <sup><a href="#fn_57" id="reffn_57">57</a></sup>, SOSNet [90]以及 LogPolarDesc <sup><a href="#fn_34" id="reffn_34">34</a></sup>（它们中绝大多数都是在同一个数据集上进行的训练）。</p><p>最近有一些工作利用了其它线索，诸如几何或全局上下文信息进行训练，其中包括GeoDesc [50] and ContextDesc <sup><a href="#fn_49" id="reffn_49">49</a></sup>。</p><p>另外还有一些方法将特征点以及描述子进行单独训练，例如TILDE <sup><a href="#fn_95" id="reffn_95">95</a></sup>, TCDet <sup><a href="#fn_103" id="reffn_103">103</a></sup>, QuadNet <sup><a href="#fn_78" id="reffn_78">78</a></sup>, and Key.Net <sup><a href="#fn_13" id="reffn_13">13</a></sup>。当前还有一些算法将二者联合起来训练，例如LIFT <sup><a href="#fn_99" id="reffn_99">99</a></sup>,DELF <sup><a href="#fn_63" id="reffn_63">63</a></sup>, SuperPoint <sup><a href="#fn_31" id="reffn_31">31</a></sup>, LF-Net <sup><a href="#fn_64" id="reffn_64">64</a></sup>, D2-Net <sup><a href="#fn_33" id="reffn_33">33</a></sup>,R2D2 <sup><a href="#fn_72" id="reffn_72">72</a></sup>。</p><h3 id="鲁棒匹配"><a href="#鲁棒匹配" class="headerlink" title="鲁棒匹配"></a>鲁棒匹配</h3><p>大基线的双目匹配的外点内点率可低至10%，甚至更低。要做匹配的话需要从中选择出能够解算出位姿的算法。常用的方式包括基于随机一致采样RANSAC的5-<sup><a href="#fn_62" id="reffn_62">62</a></sup>，7-<sup><a href="#fn_41" id="reffn_41">41</a></sup>，8-point<sup><a href="#fn_39" id="reffn_39">39</a></sup>算法。它的改进算法包括local optimization <sup><a href="#fn_24" id="reffn_24">24</a></sup>, MLESAC <sup><a href="#fn_92" id="reffn_92">92</a></sup>, PROSAC <sup><a href="#fn_23" id="reffn_23">23</a></sup>, DEGENSAC <sup><a href="#fn_26" id="reffn_26">26</a></sup>, GC-RANSAC <sup><a href="#fn_12" id="reffn_12">12</a></sup>,  MAGSAC <sup><a href="#fn_29" id="reffn_29">29</a></sup>，CNe (Context Networks) <sup><a href="#fn_100" id="reffn_100">100</a></sup>+RANSAC，同样还有<sup><a href="#fn_70" id="reffn_70">70</a></sup> <sup><a href="#fn_104" id="reffn_104">104</a></sup> <sup><a href="#fn_85" id="reffn_85">85</a></sup> <sup><a href="#fn_102" id="reffn_102">102</a></sup>。作者最后加了一句“Despite their promise, it remains unclear how well they perform in real settings”（质疑中，哈哈）。</p><h3 id="运动恢复结构（SfM）"><a href="#运动恢复结构（SfM）" class="headerlink" title="运动恢复结构（SfM）"></a>运动恢复结构（SfM）</h3><p>方法 <sup><a href="#fn_3" id="reffn_3">3</a></sup> <sup><a href="#fn_43" id="reffn_43">43</a></sup> <sup><a href="#fn_27" id="reffn_27">27</a></sup> <sup><a href="#fn_37" id="reffn_37">37</a></sup> <sup><a href="#fn_106" id="reffn_106">106</a></sup>，最流行的包括VisualSFM <sup><a href="#fn_98" id="reffn_98">98</a></sup>以及COLMAP <sup><a href="#fn_79" id="reffn_79">79</a></sup>（作为真值）。</p><h3 id="数据集和标准"><a href="#数据集和标准" class="headerlink" title="数据集和标准"></a>数据集和标准</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig2.jpg"></p><p>以前的特征匹配数据集如下：</p><ul><li>Oxford dataset <sup><a href="#fn_54" id="reffn_54">54</a></sup>, 48张图像+真值单应矩阵</li><li>HPatches <sup><a href="#fn_9" id="reffn_9">9</a></sup>, 696张光照以及视角变化，无遮挡平面图像</li><li>DTU <sup><a href="#fn_1" id="reffn_1">1</a></sup>, Edge Foci <sup><a href="#fn_107" id="reffn_107">107</a></sup>, Webcam <sup><a href="#fn_95" id="reffn_95">95</a></sup>, AMOS <sup><a href="#fn_67" id="reffn_67">67</a></sup>, 以及 Strecha’s <sup><a href="#fn_83" id="reffn_83">83</a></sup></li></ul><p>上述数据集都有其限制：窄基线，真值噪声大，图像数量少。基于学习的描述子通常在<sup><a href="#fn_21" id="reffn_21">21</a></sup>上进行训练，它们之所以比SIFT好的原因可能在于过拟合了（作者看到会不会脸红）。<br>另外，用于导航/重定位以及slam的数据集包括Kitti <sup><a href="#fn_38" id="reffn_38">38</a></sup>, Aachen <sup><a href="#fn_76" id="reffn_76">76</a></sup>, Robotcar <sup><a href="#fn_52" id="reffn_52">52</a></sup>以及CMU seasons <sup><a href="#fn_75" id="reffn_75">75</a></sup> <sup><a href="#fn_8" id="reffn_8">8</a></sup>，但并不包含Phototourism数据中的多种变换。</p><h2 id="Phototourism-数据集"><a href="#Phototourism-数据集" class="headerlink" title="Phototourism 数据集"></a>Phototourism 数据集</h2><p>上述数据集这么“烂”，于是作者搞出了他们心目中最好的公开数据集——Phototourism 数据集。作者从<sup><a href="#fn_43" id="reffn_43">43</a></sup> <sup><a href="#fn_88" id="reffn_88">88</a></sup>中选择的25个受欢迎的地标集合（共30k）为基础，每个地标都有成百上千的图像。论文中，作者从中选择出11个场景，其中9个测试集和2个验证集做实验。将它们缩减为最大尺寸为1024像素，并使用COLMAP <sup><a href="#fn_79" id="reffn_79">79</a></sup>对其进行求解位姿以及点云和深度，通过建立好的模型去除遮挡物。</p><p>具体地，如下2个表格所示：</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab1.png"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab2.png"></p><h2 id="处理流程图Pipeline"><a href="#处理流程图Pipeline" class="headerlink" title="处理流程图Pipeline"></a>处理流程图Pipeline</h2><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig7.png"></p><p>流程如上图，蓝色框就是要进行的几个处理，分别介绍一下。</p><h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>作者选择了3大类特征：</p><ol><li>完全手工特征:<br>SIFT <sup><a href="#fn_48" id="reffn_48">48</a></sup> (以及RootSIFT <sup><a href="#fn_6" id="reffn_6">6</a></sup>), SURF <sup><a href="#fn_15" id="reffn_15">15</a></sup>, ORB <sup><a href="#fn_73" id="reffn_73">73</a></sup>, AKAZE <sup><a href="#fn_4" id="reffn_4">4</a></sup>，FREAK <sup><a href="#fn_107" id="reffn_107">107</a></sup>描述子+BRISK <sup><a href="#fn_108" id="reffn_108">108</a></sup>特征点，使用OpenCV的实现，除了ORB特征，降低特征提取阈值以多提取一些特征；<br>除此之外，也考虑VLFeat<sup><a href="#fn_94" id="reffn_94">94</a></sup>中DoG的一些变种：(VL-)DoG, Hessian <sup><a href="#fn_16" id="reffn_16">16</a></sup>, Hessian-Laplace <sup><a href="#fn_55" id="reffn_55">55</a></sup>, Harris-Laplace <sup><a href="#fn_55" id="reffn_55">55</a></sup>, MSER <sup><a href="#fn_53" id="reffn_53">53</a></sup>; 以及它们的仿射变种: DoG-Affine, Hessian-Affine <sup><a href="#fn_55" id="reffn_55">55</a></sup> <sup><a href="#fn_14" id="reffn_14">14</a></sup>, DoG-AffNet <sup><a href="#fn_59" id="reffn_59">59</a></sup>, Hessian-AffNet <sup><a href="#fn_59" id="reffn_59">59</a></sup></li><li>描述子从DoG特征学习得到的特征：<br>L2-Net <sup><a href="#fn_89" id="reffn_89">89</a></sup>, Hardnet <sup><a href="#fn_57" id="reffn_57">57</a></sup>,Geodesc <sup><a href="#fn_50" id="reffn_50">50</a></sup>, SOSNet <sup><a href="#fn_90" id="reffn_90">90</a></sup>, ContextDesc <sup><a href="#fn_49" id="reffn_49">49</a></sup>, LogPolarDesc <sup><a href="#fn_34" id="reffn_34">34</a></sup></li><li>端到端学习来的特征：<br>Superpoint <sup><a href="#fn_31" id="reffn_31">31</a></sup>, LF-Net <sup><a href="#fn_64" id="reffn_64">64</a></sup>, and D2-Net <sup><a href="#fn_33" id="reffn_33">33</a></sup>以及它们的多尺度变种：single- (SS) 以及 multi-scale (MS)</li></ol><h3 id="特征匹配"><a href="#特征匹配" class="headerlink" title="特征匹配"></a>特征匹配</h3><p>此处用的是最近邻。</p><h3 id="外点滤除"><a href="#外点滤除" class="headerlink" title="外点滤除"></a>外点滤除</h3><p>Context Networks <sup><a href="#fn_100" id="reffn_100">100</a></sup>+RANSAC<sup><a href="#fn_100" id="reffn_100">100</a></sup> <sup><a href="#fn_85" id="reffn_85">85</a></sup>，简称CNe，效果如下：</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig3.png"></p><h3 id="Stereo-task"><a href="#Stereo-task" class="headerlink" title="Stereo task"></a>Stereo task</h3><p>给定图像$\mathbf{I}_i$以及$\mathbf{I}_j$，解算基础矩阵 $\mathbf{F}_{i,j}$，除了现有的OpenCV<sup><a href="#fn_19" id="reffn_19">19</a></sup>以及sklearn<sup><a href="#fn_65" id="reffn_65">65</a></sup>中实现的RANSAC <sup><a href="#fn_36" id="reffn_36">36</a></sup> <sup><a href="#fn_25" id="reffn_25">25</a></sup>，作者也用到了DEGENSAC <sup><a href="#fn_26" id="reffn_26">26</a></sup>, GC-RANSAC <sup><a href="#fn_12" id="reffn_12">12</a></sup> and MAGSAC <sup><a href="#fn_29" id="reffn_29">29</a></sup>。最后通过OpenCV的<code>recoverPose</code>函数解算位姿。</p><h3 id="Multi-view-task"><a href="#Multi-view-task" class="headerlink" title="Multi-view task"></a>Multi-view task</h3><p>由于是评价<strong>特征的好坏</strong>而不是SfM算法，作者从几个大场景中<strong>随机选择</strong>出图片构成几个小的数据集，称为”bags”。其中包含3/5张图像的各有100bags，10张图像的各有50bags，25张图像的各有25bags，总共275个bags。将外点滤除后的结果送入COLMAP <sup><a href="#fn_79" id="reffn_79">79</a></sup>作为输入进行SfM重建。</p><h3 id="误差指标"><a href="#误差指标" class="headerlink" title="误差指标"></a>误差指标</h3><ol><li>mAA(mean Average Accuracy): Stereo task/Multi-view task</li><li>ATE(Absolute Trajectory Error): Multi-view task</li></ol><h2 id="实验开始——配置细节很重要"><a href="#实验开始——配置细节很重要" class="headerlink" title="实验开始——配置细节很重要"></a>实验开始——配置细节很重要</h2><p>首先比较了RANSAC在不同参数配置（置信度，极线对齐误差阈值以及最大迭代次数）下的表现：<br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig8.png"><br>总体来说，MAGSAC表现最好，DEGENSAC表现次之。另外，作者提到“default settings can be woefully inadequate. For example, OpenCV sets τ = 0.99 and η = 3 pixels, which results in a mAP at 10o of 0.5292 on the validation set – a performance drop of 23.9% relative.” 所以在日常使用OpenCV的RANSAC函数时需要自己调整下参数。</p><p>作者认为RANSAC的内点阈值对于每种局部特征也是不同的，作者做了如下实验。<br><img alt="Figure 5. RANSAC – Inlier threshold $\eta$" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig9.png"><br>上图可以直观看到从DOG学习的特征都聚集在了一起，其它特征比较分散，这也是太难选择了，于是作者使用了其他论文作者推荐的配置参数或者一些合理的参数作为内点阈值。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>作者列出了很多结果以及结论，我们仅去关注几个感兴趣的。</p><h3 id="8k特征"><a href="#8k特征" class="headerlink" title="8k特征"></a>8k特征</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab5.png"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab6.png"></p><p>大家期待已久的真的sota到底是谁呢？作者在以上特征的超参调整到最优后进行了实验，测试结果如下：</p><ol><li>mAA指标上DoG特征点占据了Top的位置，其中SOSNet排名#1，紧随其后的是HardNet。</li><li>‘HardNetAmos+’ <sup><a href="#fn_56" id="reffn_56">56</a></sup>,它在更多的数据(Brown <sup><a href="#fn_20" id="reffn_20">20</a></sup>, HPatches <sup><a href="#fn_9" id="reffn_9">9</a></sup>, AMOS <sup><a href="#fn_67" id="reffn_67">67</a></sup>)上进行了训练，但是效果却比不上在Brown的‘Liberty’上训练模型的效果。</li><li>multi-view任务中，DoG+HardNet表现属于top水平，略优于ContextDesc, SOSNet，LogpolarDesc；</li><li>R2D2是表现最好的端到端方法，同样在multi-view任务中表现较好（#6），但是在stereo任务中不如SIFT；</li><li>D2-net表现并不太好，可能由于图像下采样造成了较差的定位误差；</li><li>适当调整参数后的SIFT尤其是RootSIFT能够在stereo任务中排名#9，multi-view任务中排名#9，与所谓sota相差13.1%以及4.9%.（真为咱传统特征争气！）</li></ol><h3 id="2k特征"><a href="#2k特征" class="headerlink" title="2k特征"></a>2k特征</h3><p>这样做的理由是能够与LF-Net与Superpoint进行比较，结果如下图：</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab7.png"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab8.png"></p><p>结论：</p><ol><li>Key.Net+HardNet获得最好的表现，第二名是LogPolarDesc；</li><li>R2D2在stereo任务中排名#2，multi-view任务中排名#7</li></ol><h3 id="8k-vs-2k"><a href="#8k-vs-2k" class="headerlink" title="8k vs. 2k"></a>8k vs. 2k</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig16.png"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig17.png"></p><p>结论：</p><ol><li><strong>基于DoG的方法容易受益于多个特征，而学习的方法收益于重新训练</strong>（该结论来自于Key.Net+Hardnet的组合，作者进行了重新训练，表现优异）</li><li>整体来说基于学习的特征KeyNet, SuperPoint, R2D2, LF-Net在multi-view任务配置下比stereo任务配置下表现更好；(作者的假设是它们的鲁棒性好，但定位精度低)</li></ol><h3 id="光照变化"><a href="#光照变化" class="headerlink" title="光照变化"></a>光照变化</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig26.png"></p><p>作者用了直方图均衡化（CLAHE<sup><a href="#fn_66" id="reffn_66">66</a></sup>）去调整图像光度，结果如上图，可以看到几乎所有的基于学习的方法的测试效果都下降了，这可能由于没有专门地在这种场景中进行训练。而SIFT也没有得到明显提升，可能在于SIFT描述子是在某些假设条件下最佳表现。</p><h3 id="新指标-vs-传统指标"><a href="#新指标-vs-传统指标" class="headerlink" title="新指标 vs. 传统指标"></a>新指标 vs. 传统指标</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig18.png"></p><p>这里要说明的是传统的评价方式与本文提出方式的关系。</p><ol><li>matching score的选择还是比较明智的，它似乎与mAA相关，但也很难保证高的匹配得分就一定有助于提升mAA，例如RootSIFT vs ContextDesc；</li><li>repeatability则比较难去诠释它对最后位姿解算的效果。AKAZE的repeatability最好但是matching score和pose mAA都非常差，作者的原话(arxiv版本1)就是<strong>descriptor may hurt its performance</strong>。</li><li>Key.Net获得最好的repeatability，但是在mAA指标上弱于DoG的方法，即使使用了相同的描述子HardNet;</li></ol><p><strong>注意</strong>，以上结果都是论文发布在arxiv平台时给出的结果，最新结果参考这个官网<a href="https://vision.uvic.ca/image-matching-challenge/leaderboard/" target="_blank" rel="noopener">排行榜</a>。</p><p>由于目前正在使用superpoint特征（SuperPoint (2k features, NMS=4), DEGENSAC），所以比较关注它的表现。感觉在2k特征阵营，它的表现并不好（屈居#35,目前共52个算法），然而SuperPoint + SuperGlue + DEGENSAC以及SuperPoint+GIFT+Graph Motion Coherence Network+DEGENSAC分别位列#1以及#2，这也是结果很让人欣慰！</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/leadboard_superglue.png"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/leadboard_superpoint.png"></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><blockquote id="fn_1"><sup>1</sup>. H. Aanaes, A. L. Dahl, and K. Steenstrup Pedersen. Interesting Interest Points. IJCV, 97:18–35, 2012. 2<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. H. Aanaes and F. Kahl. Estimation of Deformable Structure and Motion. In Vision and Modelling of Dynamic Scenes Workshop, 2002. 6<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. S. Agarwal, N. Snavely, I. Simon, S.M. Seitz, and R. Szeliski. Building Rome in One Day. In ICCV, 2009. 1, 2<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. P. F. Alcantarilla, J. Nuevo, and A. Bartoli. Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces. In BMVC, 2013. 2, 3<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. Anonymous. DeepSFM: Structure From Motion Via Deep Bundle Adjustment. In Submission to ICLR, 2020. 2<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. Relja Arandjelovic. Three things everyone should know to improve object retrieval. In CVPR, 2012. 3<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote><blockquote id="fn_7"><sup>7</sup>. Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. NetVLAD: CNN Architecture for Weakly Supervised Place Recognition. In CVPR, 2016. 1<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a></blockquote><blockquote id="fn_8"><sup>8</sup>. Hernan Badino, Daniel Huber, and Takeo Kanade. The CMU Visual Localization Data Set. <a href="http://3dvis" target="_blank" rel="noopener">http://3dvis</a>. ri.cmu.edu/data-sets/localization, 2011. 2<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a></blockquote><blockquote id="fn_9"><sup>9</sup>. V. Balntas, K. Lenc, A. Vedaldi, and K. Mikolajczyk. HPatches: A Benchmark and Evaluation of Handcrafted and Learned Local Descriptors. In CVPR, 2017. 2, 7<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a></blockquote><blockquote id="fn_10"><sup>10</sup>. Vassileios Balntas, Shuda Li, and Victor Prisacariu. RelocNet: Continuous Metric Learning Relocalisation using Neural Nets. In The European Conference on Computer Vision (ECCV), September 2018. 1<a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a></blockquote><blockquote id="fn_11"><sup>11</sup>. V. Balntas, E. Riba, D. Ponsa, and K. Mikolajczyk. Learning Local Feature Descriptors with Triplets and Shallow Convolutional Neural Networks. In BMVC, 2016. 2<a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a></blockquote><blockquote id="fn_12"><sup>12</sup>. Daniel Barath and Ji Matas. Graph-cut ransac. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 2, 4<a href="#reffn_12" title="Jump back to footnote [12] in the text."> &#8617;</a></blockquote><blockquote id="fn_13"><sup>13</sup>. Axel Barroso-Laguna, Edgar Riba, Daniel Ponsa, and Krystian Mikolajczyk. Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters. In Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision, 2019. 2, 3<a href="#reffn_13" title="Jump back to footnote [13] in the text."> &#8617;</a></blockquote><blockquote id="fn_14"><sup>14</sup>. A. Baumberg. Reliable Feature Matching Across Widely Separated Views. In CVPR, pages 774–781, 2000. 3, 6<a href="#reffn_14" title="Jump back to footnote [14] in the text."> &#8617;</a></blockquote><blockquote id="fn_15"><sup>15</sup>. H. Bay, T. Tuytelaars, and L. Van Gool. SURF: Speeded Up Robust Features. In ECCV, 2006. 2, 3<a href="#reffn_15" title="Jump back to footnote [15] in the text."> &#8617;</a></blockquote><blockquote id="fn_16"><sup>16</sup>. P. R. Beaudet. Rotationally invariant image operators. In Proceedings of the 4th International Joint Conference on Pattern Recognition, pages 579–583, Kyoto, Japan, Nov. 1978. 3, 6<a href="#reffn_16" title="Jump back to footnote [16] in the text."> &#8617;</a></blockquote><blockquote id="fn_17"><sup>17</sup>. Jia-Wang Bian, Yu-Huan Wu, Ji Zhao, Yun Liu, Le Zhang, Ming-Ming Cheng, and Ian Reid. An Evaluation of Feature Matchers for Fundamental Matrix Estimation. In BMVC, 2019. 2<a href="#reffn_17" title="Jump back to footnote [17] in the text."> &#8617;</a></blockquote><blockquote id="fn_18"><sup>18</sup>. Eric Brachmann and Carsten Rother. Neural- Guided RANSAC: Learning Where to Sample Model Hypotheses. In ICCV, 2019. 2<a href="#reffn_18" title="Jump back to footnote [18] in the text."> &#8617;</a></blockquote><blockquote id="fn_19"><sup>19</sup>. G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000. 4<a href="#reffn_19" title="Jump back to footnote [19] in the text."> &#8617;</a></blockquote><blockquote id="fn_20"><sup>20</sup>. M. Brown, G. Hua, and S. Winder. Discriminative Learning of Local Image Descriptors. PAMI, 2011. 1, 2, 7<a href="#reffn_20" title="Jump back to footnote [20] in the text."> &#8617;</a></blockquote><blockquote id="fn_21"><sup>21</sup>. M. Brown and D. Lowe. Automatic Panoramic Image Stitching Using Invariant Features. IJCV, 74:59–73, 2007. 2<a href="#reffn_21" title="Jump back to footnote [21] in the text."> &#8617;</a></blockquote><blockquote id="fn_22"><sup>22</sup>. Mai Bui, Christoph Baur, Nassir Navab, Slobodan Ilic, and Shadi Albarqouni. Adversarial Networks for Camera Pose Regression and Reﬁnement. In The IEEE International Conference on Computer Vision (ICCV) Workshops, Oct 2019. 1<a href="#reffn_22" title="Jump back to footnote [22] in the text."> &#8617;</a></blockquote><blockquote id="fn_23"><sup>23</sup>. Ondˇrej Chum and Jiˇr´ı Matas. Matching with PROSAC Progressive Sample Consensus. In CVPR, pages 220–226, June 2005. 2<a href="#reffn_23" title="Jump back to footnote [23] in the text."> &#8617;</a></blockquote><blockquote id="fn_24"><sup>24</sup>. Ondˇrej Chum, Jiˇr´ı Matas, and Josef Kittler. Locally Optimized RANSAC. In PR, 2003. 2<a href="#reffn_24" title="Jump back to footnote [24] in the text."> &#8617;</a></blockquote><blockquote id="fn_25"><sup>25</sup>. Ondˇrej Chum, Jiˇr´ı Matas, and Josef Kittler. Locally optimized ransac. In Pattern Recognition, 2003. 4<a href="#reffn_25" title="Jump back to footnote [25] in the text."> &#8617;</a></blockquote><blockquote id="fn_26"><sup>26</sup>. Ondrej Chum, Tomas Werner, and Jiri Matas. Two-View Geometry Estimation Unaffected by a Dominant Plane. In CVPR, 2005. 2, 4<a href="#reffn_26" title="Jump back to footnote [26] in the text."> &#8617;</a></blockquote><blockquote id="fn_27"><sup>27</sup>. Hainan Cui, Xiang Gao, Shuhan Shen, and Zhanyi Hu. Hsfm: Hybrid structure-from-motion. In CVPR, July 2017. 2<a href="#reffn_27" title="Jump back to footnote [27] in the text."> &#8617;</a></blockquote><blockquote id="fn_28"><sup>28</sup>. Zheng Dang, Kwang Moo Yi, Yinlin Hu, Fei Wang, Pascal Fua, and Mathieu Salzmann. Eigendecomposition-Free Training of Deep Networks with Zero Eigenvalue-Based Losses. In ECCV, 2018. 4<a href="#reffn_28" title="Jump back to footnote [28] in the text."> &#8617;</a></blockquote><blockquote id="fn_29"><sup>29</sup>. Jana Noskova Daniel Barath, Jiri Matas. MAGSAC: marginalizing sample consensus. In CVPR, 2019. 1, 2, 4<a href="#reffn_29" title="Jump back to footnote [29] in the text."> &#8617;</a></blockquote><blockquote id="fn_30"><sup>30</sup>. D. Detone, T. Malisiewicz, and A. Rabinovich. Toward Geometric Deep SLAM. arXiv preprint arXiv:1707.07410, 2017. 1<a href="#reffn_30" title="Jump back to footnote [30] in the text."> &#8617;</a></blockquote><blockquote id="fn_31"><sup>31</sup>. D. Detone, T. Malisiewicz, and A. Rabinovich. Superpoint: Self-Supervised Interest Point Detection and Description. CVPR Workshop on Deep Learning for Visual SLAM, 2018. 1, 2, 3, 8<a href="#reffn_31" title="Jump back to footnote [31] in the text."> &#8617;</a></blockquote><blockquote id="fn_32"><sup>32</sup>. J. Dong and S. Soatto. Domain-Size Pooling in Local Descriptors: DSP-SIFT. In CVPR, 2015. 6<a href="#reffn_32" title="Jump back to footnote [32] in the text."> &#8617;</a></blockquote><blockquote id="fn_33"><sup>33</sup>. M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, and T. Sattler. D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. In CVPR, 2019. 1, 2, 3, 8<a href="#reffn_33" title="Jump back to footnote [33] in the text."> &#8617;</a></blockquote><blockquote id="fn_34"><sup>34</sup>. Patrick Ebel, Anastasiia Mishchuk, Kwang Moo Yi, Pascal Fua, and Eduard Trulls. Beyond Cartesian Representations for Local Descriptors. In ICCV, 2019. 2, 3, 6<a href="#reffn_34" title="Jump back to footnote [34] in the text."> &#8617;</a></blockquote><blockquote id="fn_35"><sup>35</sup>. Vassileios Balntas et.al. SILDa: A Multi-Task Dataset for Evaluating Visual Localization. <a href="https://github" target="_blank" rel="noopener">https://github</a>. com/scape-research/silda, 2018. 2<a href="#reffn_35" title="Jump back to footnote [35] in the text."> &#8617;</a></blockquote><blockquote id="fn_36"><sup>36</sup>. M.A Fischler and R.C. Bolles. Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography. Communications ACM, 24(6):381–395, 1981. 1, 2, 4<a href="#reffn_36" title="Jump back to footnote [36] in the text."> &#8617;</a></blockquote><blockquote id="fn_37"><sup>37</sup>. P. Gay, V. Bansal, C. Rubino, and A. D. Bue. Probabilistic Structure from Motion with Objects (PSfMO). In ICCV, 2017. 2<a href="#reffn_37" title="Jump back to footnote [37] in the text."> &#8617;</a></blockquote><blockquote id="fn_38"><sup>38</sup>. Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite. In CVPR, 2012. 2<a href="#reffn_38" title="Jump back to footnote [38] in the text."> &#8617;</a></blockquote><blockquote id="fn_39"><sup>39</sup>. R.I. Hartley. In Defense of the Eight-Point Algorithm. PAMI, 19(6):580–593, June 1997. 2<a href="#reffn_39" title="Jump back to footnote [39] in the text."> &#8617;</a></blockquote><blockquote id="fn_40"><sup>40</sup>. R. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, 2000. 1<a href="#reffn_40" title="Jump back to footnote [40] in the text."> &#8617;</a></blockquote><blockquote id="fn_41"><sup>41</sup>. R. I. Hartley. Projective reconstruction and invariants from multiple images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(10):1036–1041, Oct 1994. 1, 2<a href="#reffn_41" title="Jump back to footnote [41] in the text."> &#8617;</a></blockquote><blockquote id="fn_42"><sup>42</sup>. K. He, Y. Lu, and S. Sclaroff. Local Descriptors Optimized for Average Precision. In CVPR, 2018. 1<a href="#reffn_42" title="Jump back to footnote [42] in the text."> &#8617;</a></blockquote><blockquote id="fn_43"><sup>43</sup>. J. Heinly, J.L. Schoenberger, E. Dunn, and J-M. Frahm. Reconstructing the World in Six Days. In CVPR, 2015. 1, 2, 3<a href="#reffn_43" title="Jump back to footnote [43] in the text."> &#8617;</a></blockquote><blockquote id="fn_44"><sup>44</sup>. Karel Lenc and Varun Gulshan and Andrea Vedaldi. VLBenchmarks. <a href="http://www.vlfeat.org/" target="_blank" rel="noopener">http://www.vlfeat.org/</a> benchmarks/, 2011. 2<a href="#reffn_44" title="Jump back to footnote [44] in the text."> &#8617;</a></blockquote><blockquote id="fn_45"><sup>45</sup>. A. Kendall, M. Grimes, and R. Cipolla. Posenet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization. In ICCV, pages 2938–2946, 2015. 1<a href="#reffn_45" title="Jump back to footnote [45] in the text."> &#8617;</a></blockquote><blockquote id="fn_46"><sup>46</sup>. J. Krishna Murthy, Ganesh Iyer, and Liam Paull. gradSLAM: Dense SLAM meets Automatic Differentiation. arXiv, 2019. 2<a href="#reffn_46" title="Jump back to footnote [46] in the text."> &#8617;</a></blockquote><blockquote id="fn_47"><sup>47</sup>. Zhengqi Li and Noah Snavely. MegaDepth: Learning Single-View Depth Prediction from Internet Photos. In CVPR, 2018. 2<a href="#reffn_47" title="Jump back to footnote [47] in the text."> &#8617;</a></blockquote><blockquote id="fn_48"><sup>48</sup>. David G. Lowe. Distinctive Image Features from ScaleInvariant Keypoints. IJCV, 20(2):91–110, November 2004. 1, 2, 3, 4, 6, 8, 15<a href="#reffn_48" title="Jump back to footnote [48] in the text."> &#8617;</a></blockquote><blockquote id="fn_49"><sup>49</sup>. Zixin Luo, Tianwei Shen, Lei Zhou, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan. ContextDesc: Local Descriptor Augmentation with Cross-Modality Context. In CVPR, 2019. 2, 3<a href="#reffn_49" title="Jump back to footnote [49] in the text."> &#8617;</a></blockquote><blockquote id="fn_50"><sup>50</sup>. Z. Luo, T. Shen, L. Zhou, S. Zhu, R. Zhang, Y. Yao, T. Fang, and L. Quan. Geodesc: Learning Local Descriptors by Integrating Geometry Constraints. In ECCV, 2018. 2, 3<a href="#reffn_50" title="Jump back to footnote [50] in the text."> &#8617;</a></blockquote><blockquote id="fn_51"><sup>51</sup>. Simon Lynen, Bernhard Zeisl, Dror Aiger, Michael Bosse, Joel Hesch, Marc Pollefeys, Roland Siegwart, and Torsten Sattler. Large-scale, real-time visual-inertial localization revisited. arXiv Preprint, 2019. 1<a href="#reffn_51" title="Jump back to footnote [51] in the text."> &#8617;</a></blockquote><blockquote id="fn_52"><sup>52</sup>. Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year, 1000 km: The Oxford RobotCar dataset. IJRR, 36(1):3–15, 2017. 2<a href="#reffn_52" title="Jump back to footnote [52] in the text."> &#8617;</a></blockquote><blockquote id="fn_53"><sup>53</sup>. J. Matas, O. Chum, M. Urban, and T. Pajdla. Robust WideBaseline Stereo from Maximally Stable Extremal Regions. IVC, 22(10):761–767, 2004. 3, 6<a href="#reffn_53" title="Jump back to footnote [53] in the text."> &#8617;</a></blockquote><blockquote id="fn_54"><sup>54</sup>. K. Mikolajczyk and C. Schmid. A Performance Evaluation of Local Descriptors. PAMI, 27(10):1615–1630, 2004. 2<a href="#reffn_54" title="Jump back to footnote [54] in the text."> &#8617;</a></blockquote><blockquote id="fn_55"><sup>55</sup>. K. Mikolajczyk, C. Schmid, and A. Zisserman. Human Detection Based on a Probabilistic Assembly of Robust Part Detectors. In ECCV, pages 69–82, 2004. 3, 6<a href="#reffn_55" title="Jump back to footnote [55] in the text."> &#8617;</a></blockquote><blockquote id="fn_56"><sup>56</sup>. Jiri Matas Milan Pultar, Dmytro Mishkin. Leveraging Outdoor Webcams for Local Descriptor Learning. In Proceedings of CVWW 2019, 2019. 7<a href="#reffn_56" title="Jump back to footnote [56] in the text."> &#8617;</a></blockquote><blockquote id="fn_57"><sup>57</sup>. A. Mishchuk, D. Mishkin, F. Radenovic, and J. Matas. Working Hard to Know Your Neighbor’s Margins: Local Descriptor Learning Loss. In NeurIPS, 2017. 2, 3, 6<a href="#reffn_57" title="Jump back to footnote [57] in the text."> &#8617;</a></blockquote><blockquote id="fn_58"><sup>58</sup>. Dmytro Mishkin, Jiri Matas, and Michal Perdoch. MODS: PAMI, 19(6):580–593, June 1997. 2 Fast and robust method for two-view matching. CVIU, 2015. 6, 15<a href="#reffn_58" title="Jump back to footnote [58] in the text."> &#8617;</a></blockquote><blockquote id="fn_59"><sup>59</sup>. D. Mishkin, F. Radenovic, and J. Matas. Repeatability is Not Enough: Learning Affine Regions via Discriminability. In ECCV, 2018. 3, 6<a href="#reffn_59" title="Jump back to footnote [59] in the text."> &#8617;</a></blockquote><blockquote id="fn_60"><sup>60</sup>. Arun Mukundan, Giorgos Tolias, and Ondrej Chum. Explicit Spatial Encoding for Deep Local Descriptors. In CVPR, 2019. 1<a href="#reffn_60" title="Jump back to footnote [60] in the text."> &#8617;</a></blockquote><blockquote id="fn_61"><sup>61</sup>. R. Mur-Artal, J. Montiel, and J. Tardos. Orb-Slam: A Versatile and Accurate Monocular Slam System. IEEE Transactions on Robotics, 31(5):1147–1163, 2015. 1<a href="#reffn_61" title="Jump back to footnote [61] in the text."> &#8617;</a></blockquote><blockquote id="fn_62"><sup>62</sup>. D. Nister. An Efficient Solution to the Five-Point Relative Pose Problem. In CVPR, June 2003. 2<a href="#reffn_62" title="Jump back to footnote [62] in the text."> &#8617;</a></blockquote><blockquote id="fn_63"><sup>63</sup>. Hyeonwoo Noh, Andre Araujo, Jack Sim, and Tobias Weyanda nd Bohyung Han. Large-Scale Image Retrieval with Attentive Deep Local Features. In ICCV, 2017. 1, 2<a href="#reffn_63" title="Jump back to footnote [63] in the text."> &#8617;</a></blockquote><blockquote id="fn_64"><sup>64</sup>. Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi. LF-Net: Learning Local Features from Images. In NeurIPS, 2018. 2, 3<a href="#reffn_64" title="Jump back to footnote [64] in the text."> &#8617;</a></blockquote><blockquote id="fn_65"><sup>65</sup>. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. 4<a href="#reffn_65" title="Jump back to footnote [65] in the text."> &#8617;</a></blockquote><blockquote id="fn_66"><sup>66</sup>. Stephen M. Pizer, E. Philip Amburn, John D. Austin, Robert Cromartie, Ari Geselowitz, Trey Greer, Bart ter Haar Romeny, John B. Zimmerman, and Karel Zuiderveld. Adaptive histogram equalization and its variations. Computer vision, graphics, and image processing, 1987. 15<a href="#reffn_66" title="Jump back to footnote [66] in the text."> &#8617;</a></blockquote><blockquote id="fn_67"><sup>67</sup>. M. Pultar, D. Mishkin, and J. Matas. Leveraging Outdoor Webcams for Local Descriptor Learning. In Computer Vision Winter Workshop, 2019. 2, 7<a href="#reffn_67" title="Jump back to footnote [67] in the text."> &#8617;</a></blockquote><blockquote id="fn_68"><sup>68</sup>. C.R. Qi, H. Su, K. Mo, and L.J. Guibas. Pointnet: Deep Learning on Point Sets for 3D Classiﬁcation and Segmentation. In CVPR, 2017. 4<a href="#reffn_68" title="Jump back to footnote [68] in the text."> &#8617;</a></blockquote><blockquote id="fn_69"><sup>69</sup>. Filip Radenovic, Georgios Tolias, and Ondra Chum. CNN image retrieval learns from BoW: Unsupervised ﬁne-tuning with hard examples. In ECCV, 2016. 1<a href="#reffn_69" title="Jump back to footnote [69] in the text."> &#8617;</a></blockquote><blockquote id="fn_70"><sup>70</sup>. R. Ranftl and V. Koltun. Deep Fundamental Matrix Estimation. In ECCV, 2018. 2, 4<a href="#reffn_70" title="Jump back to footnote [70] in the text."> &#8617;</a></blockquote><blockquote id="fn_71"><sup>71</sup>. J. Revaud, P. Weinzaepfel, C. De Souza, N. Pion, G. Csurka, Y. Cabon, and M. Humenberger. R2D2: Repeatable and Reliable Detector and Descriptor. In arXiv Preprint, 2019. 8<a href="#reffn_71" title="Jump back to footnote [71] in the text."> &#8617;</a></blockquote><blockquote id="fn_72"><sup>72</sup>. J´erˆome Revaud, Philippe Weinzaepfel, C´esar Roberto de Souza, Noe Pion, Gabriela Csurka, Yohann Cabon, and Martin Humenberger. R2D2: Repeatable and Reliable Detector and Descriptor. In NeurIPS, 2019. 2<a href="#reffn_72" title="Jump back to footnote [72] in the text."> &#8617;</a></blockquote><blockquote id="fn_73"><sup>73</sup>. E. Rublee, V. Rabaud, K. Konolidge, and G. Bradski. ORB: An Efﬁcient Alternative to SIFT or SURF. In ICCV, 2011. 2, 3, 6<a href="#reffn_73" title="Jump back to footnote [73] in the text."> &#8617;</a></blockquote><blockquote id="fn_74"><sup>74</sup>. Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Improving Image-Based Localization by Active Correspondence Search. In ECCV, 2012. 1<a href="#reffn_74" title="Jump back to footnote [74] in the text."> &#8617;</a></blockquote><blockquote id="fn_75"><sup>75</sup>. T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg, D. Safari, M. Okutomi, M. Pollefeys, J. Sivic, F. Kahl, and T. Pajdla. Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions. In CVPR, 2018. 1, 2<a href="#reffn_75" title="Jump back to footnote [75] in the text."> &#8617;</a></blockquote><blockquote id="fn_76"><sup>76</sup>. Torsten Sattler, Tobias Weyand, Bastian Leibe, and Leif Kobbelt. Image Retrieval for Image-Based Localization Revisited. In BMVC, 2012. 2<a href="#reffn_76" title="Jump back to footnote [76] in the text."> &#8617;</a></blockquote><blockquote id="fn_77"><sup>77</sup>. Torsten Sattler, Qunjie Zhou, Marc Pollefeys, and Laura Leal-Taixe. Understanding the Limitations of CNN-based Absolute Camera Pose Regression. In CVPR, 2019. 1<a href="#reffn_77" title="Jump back to footnote [77] in the text."> &#8617;</a></blockquote><blockquote id="fn_78"><sup>78</sup>. N. Savinov, A. Seki, L. Ladicky, T. Sattler, and M. Pollefeys. Quad-Networks: Unsupervised Learning to Rank for Interest Point Detection. CVPR, 2017. 2<a href="#reffn_78" title="Jump back to footnote [78] in the text."> &#8617;</a></blockquote><blockquote id="fn_79"><sup>79</sup>. J.L. Sch¨onberger and J.M. Frahm. Structure-From-Motion Revisited. In CVPR, 2016. 1, 2, 3, 4, 6<a href="#reffn_79" title="Jump back to footnote [79] in the text."> &#8617;</a></blockquote><blockquote id="fn_80"><sup>80</sup>. J.L. Sch¨onberger, H. Hardmeier, T. Sattler, and M. Pollefeys. Comparative Evaluation of Hand-Crafted and Learned Local Features. In CVPR, 2017. 2<a href="#reffn_80" title="Jump back to footnote [80] in the text."> &#8617;</a></blockquote><blockquote id="fn_81"><sup>81</sup>. Yunxiao Shi, Jing Zhu, Yi Fang, Kuochin Lien, and Junli Gu. Self-Supervised Learning of Depth and Ego-motion with Differentiable Bundle Adjustment. arXiv Preprint, 2019. 2<a href="#reffn_81" title="Jump back to footnote [81] in the text."> &#8617;</a></blockquote><blockquote id="fn_82"><sup>82</sup>. E. Simo-serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. Moreno-Noguer. Discriminative Learning of Deep Convolutional Feature Point Descriptors. In ICCV, 2015. 2<a href="#reffn_82" title="Jump back to footnote [82] in the text."> &#8617;</a></blockquote><blockquote id="fn_83"><sup>83</sup>. C. Strecha, W.V. Hansen, L. Van Gool, P. Fua, and U. Thoennessen. On Benchmarking Camera Calibration and Multi-View Stereo for High Resolution Imagery. In CVPR, 2008. 2<a href="#reffn_83" title="Jump back to footnote [83] in the text."> &#8617;</a></blockquote><blockquote id="fn_84"><sup>84</sup>. J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. A Benchmark for the Evaluation of RGB-D SLAM Systems. In IROS, 2012. 4<a href="#reffn_84" title="Jump back to footnote [84] in the text."> &#8617;</a></blockquote><blockquote id="fn_85"><sup>85</sup>. Weiwei Sun, Wei Jiang, Eduard Trulls, Andrea Tagliasacchi, and Kwang Moo Yi. Attentive Context Normalization for Robust Permutation-Equivariant Learning. In arXiv Preprint, 2019. 2, 4, 8<a href="#reffn_85" title="Jump back to footnote [85] in the text."> &#8617;</a></blockquote><blockquote id="fn_86"><sup>86</sup>. Chengzhou Tang and Ping Tan. Ba-Net: Dense Bundle Adjustment Network. In ICLR, 2019. 2<a href="#reffn_86" title="Jump back to footnote [86] in the text."> &#8617;</a></blockquote><blockquote id="fn_87"><sup>87</sup>. Keisuke Tateno, Federico Tombari, Iro Laina, and Nassir Navab. Cnn-slam: Real-time dense monocular slam with learned depth prediction. In CVPR, July 2017. 2<a href="#reffn_87" title="Jump back to footnote [87] in the text."> &#8617;</a></blockquote><blockquote id="fn_88"><sup>88</sup>. B. Thomee, D.A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L. Li. YFCC100M: the New Data in Multimedia Research. In CACM, 2016. 3<a href="#reffn_88" title="Jump back to footnote [88] in the text."> &#8617;</a></blockquote><blockquote id="fn_89"><sup>89</sup>. Y. Tian, B. Fan, and F. Wu. L2-Net: Deep Learning of Discriminative Patch Descriptor in Euclidean Space. In CVPR, 2017. 2, 3<a href="#reffn_89" title="Jump back to footnote [89] in the text."> &#8617;</a></blockquote><blockquote id="fn_90"><sup>90</sup>. Yurun Tian, Xin Yu, Bin Fan, Fuchao Wu, Huub Heijnen, and Vassileios Balntas. SOSNet: Second Order Similarity Regularization for Local Descriptor Learning. In CVPR, 2019. 1, 2, 3<a href="#reffn_90" title="Jump back to footnote [90] in the text."> &#8617;</a></blockquote><blockquote id="fn_91"><sup>91</sup>. Giorgos Tolias, Yannis Avrithis, and Herv´e J´egou. Image Search with Selective Match Kernels: Aggregation Across Single and Multiple Images. IJCV, 116(3):247–261, Feb 2016. 1<a href="#reffn_91" title="Jump back to footnote [91] in the text."> &#8617;</a></blockquote><blockquote id="fn_92"><sup>92</sup>. P.H.S. Torr and A. Zisserman. MLESAC: A New Robust Estimator with Application to Estimating Image Geometry. CVIU, 78:138–156, 2000. 2<a href="#reffn_92" title="Jump back to footnote [92] in the text."> &#8617;</a></blockquote><blockquote id="fn_93"><sup>93</sup>. B. Triggs, P. Mclauchlan, R. Hartley, and A. Fitzgibbon. Bundle Adjustment – A Modern Synthesis. In Vision Algorithms: Theory and Practice, pages 298–372, 2000. 1<a href="#reffn_93" title="Jump back to footnote [93] in the text."> &#8617;</a></blockquote><blockquote id="fn_94"><sup>94</sup>. Andrea Vedaldi and Brian Fulkerson. Vlfeat: An open and portable library of computer vision algorithms. In Proceedings of the 18th ACM International Conference on Multimedia, MM ’10, pages 1469–1472, 2010. 3<a href="#reffn_94" title="Jump back to footnote [94] in the text."> &#8617;</a></blockquote><blockquote id="fn_95"><sup>95</sup>. Y. Verdie, K. M. Yi, P. Fua, and V. Lepetit. TILDE: A Temporally Invariant Learned DEtector. In CVPR, 2015. 2<a href="#reffn_95" title="Jump back to footnote [95] in the text."> &#8617;</a></blockquote><blockquote id="fn_96"><sup>96</sup>. S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, and K. Fragkiadaki. Sfm-Net: Learning of Structure and Motion from Video. arXiv Preprint, 2017. 2<a href="#reffn_96" title="Jump back to footnote [96] in the text."> &#8617;</a></blockquote><blockquote id="fn_97"><sup>97</sup>. X. Wei, Y. Zhang, Y. Gong, and N. Zheng. Kernelized Subspace Pooling for Deep Local Descriptors. In CVPR, 2018. 1<a href="#reffn_97" title="Jump back to footnote [97] in the text."> &#8617;</a></blockquote><blockquote id="fn_98"><sup>98</sup>. Changchang Wu. Towards Linear-Time Incremental Structure from Motion. In 3DV, 2013. 2, 6<a href="#reffn_98" title="Jump back to footnote [98] in the text."> &#8617;</a></blockquote><blockquote id="fn_99"><sup>99</sup>. Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal Fua. LIFT: Learned Invariant Feature Transform. In ECCV, 2016. 2<a href="#reffn_99" title="Jump back to footnote [99] in the text."> &#8617;</a></blockquote><blockquote id="fn_100"><sup>100</sup>. K. M. Yi, E. Trulls, Y. Ono, V. Lepetit, M. Salzmann, and P. Fua. Learning to Find Good Correspondences. In CVPR, 2018. 2, 3, 4, 7, 13, 17<a href="#reffn_100" title="Jump back to footnote [100] in the text."> &#8617;</a></blockquote><blockquote id="fn_101"><sup>101</sup>. S. Zagoruyko and N. Komodakis. Learning to Compare Image Patches via Convolutional Neural Networks. In CVPR, 2015. 6<a href="#reffn_101" title="Jump back to footnote [101] in the text."> &#8617;</a></blockquote><blockquote id="fn_102"><sup>102</sup>. Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou, Tianwei Shen, Yurong Chen, Long Quan, and Hongen Liao. Learning Two-View Correspondences and Geometry Using Order-Aware Network. ICCV, 2019. 2, 3, 4<a href="#reffn_102" title="Jump back to footnote [102] in the text."> &#8617;</a></blockquote><blockquote id="fn_103"><sup>103</sup>. Xu Zhang, Felix X. Yu, Svebor Karaman, and Shih-Fu Chang. Learning Discriminative and Transformation Covariant Local Feature Detectors. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. 2<a href="#reffn_103" title="Jump back to footnote [103] in the text."> &#8617;</a></blockquote><blockquote id="fn_104"><sup>104</sup>. Chen Zhao, Zhiguo Cao, Chi Li, Xin Li, and Jiaqi Yang. NM-Net: Mining Reliable Neighbors for Robust Feature Correspondences. In CVPR, 2019. 2, 4<a href="#reffn_104" title="Jump back to footnote [104] in the text."> &#8617;</a></blockquote><blockquote id="fn_105"><sup>105</sup>. Qunjie Zhou, Torsten Sattler, Marc Pollefeys, and Laura Leal-Taixe. To learn or not to learn: Visual localization from essential matrices. arXiv Preprint, 2019. 1<a href="#reffn_105" title="Jump back to footnote [105] in the text."> &#8617;</a></blockquote><blockquote id="fn_106"><sup>106</sup>. Siyu Zhu, Runze Zhang, Lei Zhou, Tianwei Shen, Tian Fang, Ping Tan, and Long Quan. Very Large-Scale Global SfM by Distributed Motion Averaging. In CVPR, June 2018. 1, 2<a href="#reffn_106" title="Jump back to footnote [106] in the text."> &#8617;</a></blockquote><blockquote id="fn_107"><sup>107</sup>. C.L. Zitnick and K. Ramnath. Edge Foci Interest Points. In ICCV, 2011. 2<a href="#reffn_107" title="Jump back to footnote [107] in the text."> &#8617;</a></blockquote><blockquote id="fn_108"><sup>108</sup>. A. Alahi, R. Ortiz, and P. Vandergheynst. FREAK: Fast Retina Keypoint. In CVPR, 2012. 7, 11<a href="#reffn_108" title="Jump back to footnote [108] in the text."> &#8617;</a></blockquote><blockquote id="fn_109"><sup>109</sup>. S. Leutenegger, M. Chli, and R. Y. Siegwart. Brisk: Binary robust invariant scalable keypoints. In ICCV, pages 2548–2555, 2011.7<a href="#reffn_109" title="Jump back to footnote [109] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从一系列的图像中恢复物体的3D结构是计算机视觉研究中一个热门课题，这使得我们可以相隔万里从google map中看到复活节岛的风景。这得益于图像来自于可控的条件，使得最终的重建效果一致性且质量都很高，但是这却限制了采集设备以及视角。畅想一下，假如我们不使用专业设备，而是利用sfm技术根据互联网上大量的图片重建出这个复杂世界。&lt;/p&gt;
&lt;!-- ![A 3D reconstruction generated from over 3000 images, including those from the previous figure](https://1.bp.blogspot.com/-loSqCB3NnM0/XoTiOGP9SYI/AAAAAAAAFlE/rs8iCTq63FYapA7HbljF8iWa7fyHvh3UgCLcBGAsYHQ/s400/image3.gif) --&gt;
&lt;!-- ![](https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/image_sfm.gif) --&gt;
&lt;p&gt;&lt;img src=&quot;https://vincentqin.gitee.io/posts/2020-image-matching-cvpr/image_sfm.gif&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;为了加快这个领域的研究，更好地利用图像数据有效信息，谷歌联合 &lt;a href=&quot;https://www.uvic.ca/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;UVIC&lt;/a&gt;, &lt;a href=&quot;https://www.cvut.cz/en&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CTU&lt;/a&gt;以及EPFL发表了这篇文章 “&lt;a href=&quot;https://arxiv.org/abs/2003.01587&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Image Matching across Wide Baselines: From Paper to Practice&lt;/a&gt;”，[&lt;strong&gt;&lt;a href=&quot;http://xxx.itp.ac.cn/pdf/2003.01587v2&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PDF&lt;/a&gt;&lt;/strong&gt;]，旨在公布一种新的衡量用于3D重建方法的标准模块+数据集，这里主要是指2D图像间的匹配。这个评价模块可以很方便地集成并评估现有流行的特征匹配算法，包括传统方法或者基于机器学习的方法。&lt;/p&gt;
&lt;p&gt;谷歌公布2020图像匹配挑战的数据集：&lt;a href=&quot;https://image-matching-workshop.github.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官网&lt;/a&gt;，&lt;a href=&quot;http://ai.googleblog.com/2020/04/announcing-2020-image-matching.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;博客&lt;/a&gt;，文末有排行榜。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="ORB" scheme="https://www.vincentqin.tech/tags/ORB/"/>
    
      <category term="特征匹配" scheme="https://www.vincentqin.tech/tags/%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D/"/>
    
      <category term="SuperPoint" scheme="https://www.vincentqin.tech/tags/SuperPoint/"/>
    
      <category term="SIFT" scheme="https://www.vincentqin.tech/tags/SIFT/"/>
    
  </entry>
  
  <entry>
    <title>📝笔记：SuperGlue:Learning Feature Matching with Graph Neural Networks论文阅读</title>
    <link href="https://www.vincentqin.tech/posts/superglue/"/>
    <id>https://www.vincentqin.tech/posts/superglue/</id>
    <published>2020-04-17T17:21:56.000Z</published>
    <updated>2020-08-26T15:09:06.818Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>ETHZ ASL与Magicleap联名之作，CVPR 2020 Oral（论文见文末），一作是来自ETHZ的实习生，二作是当年CVPR2018 SuperPoint的作者Daniel DeTone。<br><!-- ![](/posts/superglue/freiburg_matches.gif) --></p><p><img alt data-src="https://vincentqin.gitee.io/posts/superglue/freiburg_matches.gif"></p><a id="more"></a><p>注：</p><ol><li>SuperPoint参见另外一篇文章<a href="https://www.vincentqin.tech/posts/superpoint/">《SuperPoint: Self-Supervised Interest Point Detection and Description》</a>，<a href="https://vincentqin.gitee.io/posts/superpoint/" target="_blank" rel="noopener">备用链接</a>。</li><li>后文中反复提到的self-attention/cross-attention，我暂时翻译成自我注意力/交叉注意力。</li><li>本人知识水平有限，如有错误请在评论区指出。当然，没有问题也可刷刷评论。</li></ol><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文提出了一种能够同时进行特征匹配以及滤除外点的网络。其中特征匹配是通过求解可微分最优化转移问题（ optimal transport problem）来解决，损失函数由GNN来构建；本文基于注意力机制提出了一种灵活的内容聚合机制，这使得SuperGlue能够同时感知潜在的3D场景以及进行特征匹配。该算法与传统的，手工设计的特征相比，能够在室内外环境中位姿估计任务中取得最好的结果，该网络能够在GPU上达到实时，预期能够集成到sfm以及slam算法中。</p><p><img alt="superglue_front" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_front.png"></p><p>SuperGlue是一种特征匹配网络，它的输入是2张图像中特征点以及描述子（手工特征或者深度学习特征均可），输出是图像特征之间的匹配关系。</p><p>作者认为学习特征匹配可以被视为找到两簇点的局部分配关系。作者受到了Transformer的启发，同时将self-和cross-attention利用特征点位置以及其视觉外观进行匹配。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="局部特征匹配"><a href="#局部特征匹配" class="headerlink" title="局部特征匹配"></a>局部特征匹配</h3><p>传统的特征可分5步走：1)提取特征点；2)计算描述子；3)最近邻匹配；4)滤除外点；5)求解几何约束；其中滤除外点一步包括点方法有：计算最优次优比，RANSAC，交叉验证以及neighborhood consensus。</p><p>最近的一些工作主要集中在设计特异性更好的稀疏特征上，而它们的匹配算法仍然依赖于NN等策略：在做匹配时并没有考虑特征的结构相似性以及外观相似性。</p><h3 id="图匹配"><a href="#图匹配" class="headerlink" title="图匹配"></a>图匹配</h3><p>这类方法将特征的匹配问题描述成“quadratic assignment problems”，这是一个NP-hard问题，求解这类问题需要复杂不切实际的算子。后来的研究者将这个问题化简成“linear assignment problems”，但仅仅用了一个浅层模型，相比之下SuperGlue利用深度神经网络构建了一种合适的代价进行求解。此处需要说明的是图匹配问题可以认为是一种“<em>optimal transport</em>”问题，<strong>它是一种有效但简单的近似解的广义线性分配，即Sinkhorn算法</strong>。</p><h3 id="深度点云匹配"><a href="#深度点云匹配" class="headerlink" title="深度点云匹配"></a>深度点云匹配</h3><p>点云匹配的目的是通过在元素之间聚集信息来设计置换等价或不变函数。一些算法同等的对待这些元素，还有一些算法主要关注于元素的局部坐标或者特征空间。注意力机制可以通过关注特定的元素和属性来实现全局以及依赖于数据的局部聚合，因而更加全面和灵活。SuperGlue借鉴了这种注意力机制。</p><h2 id="框架以及原理"><a href="#框架以及原理" class="headerlink" title="框架以及原理"></a>框架以及原理</h2><p>特征匹配必须满足的硬性要求是：i)至多有1个匹配点；ii)有些点由于遮挡等原因并没有匹配点。一个成熟的特征匹配模型应该做到：既能够找到特征之间的正确匹配，又可以鉴别错误匹配。</p><p><img alt="superglue_arch" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_arch.png"></p><p>整个框架由两个主要模块组成：注意力GNN以及最优匹配层。其中注意力GNN将特征点以及描述子编码成为一个向量（该向量可以理解为特征匹配向量），随后利用自我注意力以及交叉注意力来回增强（重复$L$次）这个向量$\mathbf{f}$的特征匹配性能；随后进入最优匹配层，通过计算特征匹配向量的内积得到匹配度得分矩阵，然后通过Sinkhorn算法（迭代$T$次）解算出最优特征分配矩阵。 </p><h3 id="公式化"><a href="#公式化" class="headerlink" title="公式化"></a>公式化</h3><p>该部分对特征匹配问题建模。给定两张图片$A,B$，每张图片上都有特征点位置$\mathbf{p}$以及对应的描述子$\mathbf{d}$，所以我们经常用$(\mathbf{p},\mathbf{d})$来表示图像特征。第$i$个特征可以表示为$\mathbf{p}_i:=(x,y,c)$，其中$c$表示特征点提取置信度，$(x,y)$表示特征坐标；描述子可以表示为$\mathbf{d}_i \in \mathbb{R}^{D}$，其中$D$表示特征维度，这里的特征可以是CNN特征，如SuperPoint，或者是传统特征SIFT。假设图像$A,B$分别有$M,N$个特征，可以表示为$\mathcal{A}:=\{1, \ldots, M\}$以及$\mathcal{B}:=\{1, \ldots, N\}$。</p><p><strong>部分分配矩阵</strong>：约束i）和ii）意味着对应关系来自两组关键点之间的部分分配。我们给出一个软分配矩阵$\mathbf{P} \in[0,1]^{M \times N}$，根据上述约束，我们有如下关系：</p><script type="math/tex; mode=display">\mathbf{P} \mathbf{1}_{N} \leq \mathbf{1}_{M} \quad \text { and } \quad \mathbf{P}^{\top} \mathbf{1}_{M} \leq \mathbf{1}_{N}</script><p>那我们设计网络的目标就是解算这个分配矩阵$\mathbf{P}$。</p><h3 id="注意力GNN"><a href="#注意力GNN" class="headerlink" title="注意力GNN"></a>注意力GNN</h3><p>这里有个有意思的说法：特征点的位置以及视觉外观能够提高其特异性。另外一个具有启发性的观点是人类在寻找匹配点过程是具有参考价值的。想一下人类是怎样进行特征匹配的，人类通过来回浏览两个图像试探性筛选匹配关键点，并进行来回检查（如果不是匹配的特征，观察一下周围有没有匹配的更好的点，直到找到匹配点/或没有匹配）。上述过程人们通过主动寻找上下文来增加特征点特异性，这样可以排除一些具有奇异性的匹配。本文的核心就是利用基于注意力机制的GNN实现上述过程，即模拟了人类进行特征匹配。</p><h4 id="特征点Encode"><a href="#特征点Encode" class="headerlink" title="特征点Encode"></a>特征点Encode</h4><p>首先根据上述说法，特征点位置+描述会获得更强的特征匹配特异性，所以这里将特征点的位置以及描述子合并成每个特征点$i$的初始表示$^{(0)} \mathbf{x}_{i}$，</p><script type="math/tex; mode=display">^{(0)} \mathbf{x}_{i}=\mathbf{d}_{i}+\mathbf{M L P}_{\mathrm{enc}}\left(\mathbf{p}_{i}\right)</script><p>其中MLP表示多层感知机（Multilayer Perceptron ，MLP）此处用于对低维特征升维，上式实际上是将视觉外观以及特征点位置进行了耦合，正因如此，这使得该Encode形式使得后续的注意力机制能够充分考虑到特征的外观以及位置相似度。</p><h4 id="多层GNN"><a href="#多层GNN" class="headerlink" title="多层GNN"></a>多层GNN</h4><p>考虑一个单一的完全图，它的节点是图像中每个特征点，这个图包括两种不同的无向边：一种是“Intra-image edges”（self edge）$\mathcal{E}_{\text {self }}$，它连接了来自图像内部特征点；另外一种是“Inter-image edges”（cross edge）$\mathcal{E}_{\text {cross }}$，它连接本图特征点$i$与另外一张图所有特征点（构成了该边）。</p><p>令$^{(\ell)} \mathbf{x}_{i}^{A}$表示为图像$A$上第$i$个元素在第$\ell$层的中间表达形式。信息（message）$\mathbf{m}_{\mathcal{E} \rightarrow i}$是聚合了所有特征点$\{j:(i, j) \in \mathcal{E}\}$之后点结果（它的具体形式后面的Attentional Aggregation会介绍，一句话来说就是将自我注意力以及交叉注意力进行聚合），其中$\mathcal{E} \in \{\mathcal{E}_{\text {self }},\mathcal{E}_{\text {self }}\}$，所以图像$A$中所有特征$i$传递更新的残差信息（residual message？）是：</p><script type="math/tex; mode=display">^{(\ell+1)} \mathbf{x}_{i}^{A}=^{(\ell)} \mathbf{x}_{i}^{A}+\operatorname{MLP}\left(\left[^{(\ell)} \mathbf{x}_{i}^{A} \| \mathbf{m}_{\mathcal{E} \rightarrow i}\right]\right)</script><p>其中$[\cdot | \cdot]$表示串联操作。同样的，图像$B$上所有特征有类似的更新形式。可以看到self 以及cross edges绑在一起并交替进行更新，先self后cross，作者提到共有固定数量的$L$层。</p><p>需要说明的是，这里的self-/cross-attention实际上就是模拟了人类来回浏览匹配的过程，其中self-attention是为了使得特征更加具有匹配特异性，而cross-attention是为了用这些具有特异性的点做图像间特征的相似度比较。</p><h4 id="Attentional-Aggregation"><a href="#Attentional-Aggregation" class="headerlink" title="Attentional Aggregation"></a>Attentional Aggregation</h4><p>文章的亮点之一就是将注意力机制用于特征匹配，这到底是如何实现的呢？作者提到，注意力机制将self以及cross信息聚合得到$\mathbf{m}_{\mathcal{E} \rightarrow i}$。其中self edge利用了self-attention[58]，cross edge利用了cross-attention。类似于数据库检索，我们想要查询$\mathbf{q}_i$基于元素的属性即键$\mathbf{k}_i$，检索到了某些元素的值$\mathbf{v}_j$。</p><script type="math/tex; mode=display">\mathbf{m}_{\mathcal{E} \rightarrow i}=\sum_{j:(i, j) \in \mathcal{E}} \alpha_{i j} \mathbf{v}_{j}</script><p>其中注意力权重${\alpha}_{ij}$是查询与检索到对象键值相似度的$\operatorname{Softmax}$即，$\alpha_{i j}=\operatorname{Softmax}_{j}\left(\mathbf{q}_{i}^{\top} \mathbf{k}_{j}\right)$。</p><p>这里需要解释一下键（key），query以及值（value）。令待查询点特征点$i$位于查询图像$Q$上，所有的源特征点位于图像$S$上，其中$(Q, S) \in\{A, B\}^{2}$，于是我们可以将key，query以及value写成下述形式：</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{q}_{i} &=\mathbf{W}_{1}^{(\ell)} \mathbf{x}_{i}^{Q}+\mathbf{b}_{1} \\\left[\begin{array}{l}\mathbf{k}_{j} \\ \mathbf{v}_{j}\end{array}\right] &=\left[\begin{array}{l}\mathbf{W}_{2} \\ \mathbf{W}_{3}\end{array}\right](\ell) \mathbf{x}_{i}^{S}+\left[\begin{array}{l}\mathbf{b}_{2} \\ \mathbf{b}_{3}\end{array}\right] \end{aligned}</script><p>每一层$\ell$都有其对应的一套投影参数，这些参数被所有的特征点共享。理解一下：此处的$\mathbf{q}_i$对应于待查询图像上某个特征点$i$的一种表示（self-attention映射），$\mathbf{k}_j$以及$\mathbf{v}_j$都是来自于召回的图像特征点$j$的一种表示（映射）；$\alpha_{i j}$表示这两个特征相似度，它是由$\mathbf{q}_i$以及$\mathbf{k}_j$计算得到（在这里体现了cross-attention的思想？），越大就表示这两个特征越相似，然后利用该相似度对$\mathbf{v}_j$加权求和得到$\mathbf{m}_{\mathcal{E} \rightarrow i}$，这就是所谓的<strong>特征聚合</strong>。</p><p>上面提到的这些概念有些难以理解，作者特意对上述过程进行了可视化，self-attention就是一张图像内部的边相连进行聚合，它能够更加关注具有特异性的所有点，且并不仅局限于其邻域位置特征（心心相依，何惧千里，逃…）；cross-attention做的就是匹配那些外观相似的两张图像见的特征。</p><p><img alt="superglue_fig_4" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_4.png"></p><p><img alt="superglue_fig_7" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_7.jpg"></p><p>下图展示了每层self-attention以及across-attention中权重${\alpha_{i j}}$的结果。按照匹配从难到易，文中画出了3个不同的特征点作为演示，绿色特征点（容易），蓝色特征点（中等）以及红色特征点（困难）。对于self-attention，初始时它（某个特征）关联了图像上所有的点（首行），然后逐渐地关注在与该特征相邻近的特征点（尾行）。同样地，cross-attention主要关注去匹配可能的特征点，随着层的增加，它逐渐减少匹配点集直到收敛。绿色特征点在第9层就已经趋近收敛，而红色特征直到最后才能趋紧收敛（匹配）。可以看到无论是self还是cross，它们关注的区域都会随着网络层深度的增加而逐渐缩小。</p><p><img alt="superglue_fig_15" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_15_1.jpg"></p><p>经过了$L$次self/cross-attention后就可以得到注意力GNN的输出，对于图像$A$我们有：</p><script type="math/tex; mode=display">\mathbf{f}_{i}^{A}=\mathbf{W} \cdot^{(L)} \mathbf{x}_{i}^{A}+\mathbf{b}, \quad \forall i \in \mathcal{A}</script><p>我们可以把$\mathbf{f}_{i}^{A}$理解为<strong>匹配描述子</strong>（类比特征描述子），专门为特征匹配服务，对于图像$B$具有类似的形式。</p><h3 id="匹配层（Optimal-matching-layer）"><a href="#匹配层（Optimal-matching-layer）" class="headerlink" title="匹配层（Optimal matching layer）"></a>匹配层（Optimal matching layer）</h3><p>接下来的任务就是去构建软分配矩阵$\mathbf{P}$。对于一般的图匹配流程，这个分配矩阵可以通过计算一个得分矩阵$\mathbf{S} \in \mathbb{R}^{M \times N}$（用来表示一些潜在的匹配）来实现。具体而言，通过最大化总体得分$\sum_{i, j} \mathbf{S}_{i, j} \mathbf{P}_{i, j}$即可得到这个分配矩阵$\mathbf{P}$，其中要注意的是$\mathbf{P}$是有约束的。</p><h4 id="匹配得分预测"><a href="#匹配得分预测" class="headerlink" title="匹配得分预测"></a>匹配得分预测</h4><p>去计算$M\times N$个潜在匹配得分是不可取的，于是作者就用GNN聚合得到的$\mathbf{f}_{i}^{A}$以及$\mathbf{f}_{i}^{B}$计算内积得到得分：</p><script type="math/tex; mode=display">\mathbf{S}_{i, j}=<\mathbf{f}_{i}^{A}, \mathbf{f}_{j}^{B}>, \forall(i, j) \in \mathcal{A} \times \mathcal{B}</script><h4 id="遮挡以及可见性"><a href="#遮挡以及可见性" class="headerlink" title="遮挡以及可见性"></a>遮挡以及可见性</h4><p>类似于SuperPoint在提取特征点时增加了一层dustbin通道，专门为了应对图像中没有特征点情况。本文借鉴了该思想，在得分矩阵$\mathbf{S}$的最后一列/行设置为dustbins可以得到$\overline{\mathbf{S}}$，这样做的作用在于可以滤出错误的匹配点。</p><script type="math/tex; mode=display">\overline{\mathbf{S}}_{i, N+1}=\overline{\mathbf{S}}_{M+1, j}=\overline{\mathbf{S}}_{M+1, N+1}=z \in \mathbb{R}</script><p>图像$A$上的特征点被分配到图像$B$上某个特征匹配或者被分配到dustbin，这就意味着每个dustbin有$N,M$个匹配，因此软分配矩阵有如下约束：</p><script type="math/tex; mode=display">\overline{\mathbf{P}} \mathbf{1}_{N+1}=\mathbf{a} \quad\text {  and } \quad \overline{\mathbf{P}}^{\top} \mathbf{1}_{M+1}=\mathbf{b}</script><p>其中$\mathbf{a}=\left[\begin{array}{ll}\mathbf{1}_{M}^{\top} &amp; N\end{array}\right]^{\top}$，$\mathbf{b}=\left[\begin{array}{ll}\mathbf{1}_{N}^{\top} &amp; M\end{array}\right]^{\top}$。（<font color="red">此处不太理解，最后一维为何为N？</font>）</p><h4 id="Sinkhorn-Algorithm"><a href="#Sinkhorn-Algorithm" class="headerlink" title="Sinkhorn Algorithm"></a>Sinkhorn Algorithm</h4><p>求解最大化总体得分可由“Sinkhorn Algorithm”[52,12]进行求解，此处并不作为重点讲解。</p><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>GNN网络以及最优匹配层都是可微的，这使得反向传播训练成为可能。网络训练使用了一种监督学习的方式，即有了匹配的真值$\mathcal{M}=\{(i, j)\} \subset \mathcal{A} \times \mathcal{B}$（如，由真值相对位姿变换得到的匹配关系），当然也可以获得一些没有匹配的特征点$\mathcal{I} \subseteq \mathcal{A}$以及$ \mathcal{J} \subseteq \mathcal{B}$。当给定真值标签，就可以去最小化分配矩阵$\overline{\mathbf{P}}$ 负对数似然函数：</p><script type="math/tex; mode=display">\begin{aligned} \operatorname{Loss}=&-\sum_{(i, j) \in \mathcal{M}} \log \overline{\mathbf{P}}_{i, j} \\ &-\sum_{i \in \mathcal{I}} \log \overline{\mathbf{P}}_{i, N+1}-\sum_{j \in \mathcal{J}} \log \overline{\mathbf{P}}_{M+1, j} \end{aligned}</script><p>这个监督学习的目标是同时最大化精度以及匹配的召回率，接下来的训练过程略过，直接开始实验阶段的介绍。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>特征匹配的目的是为了解算出两帧之间的相对位姿，所以实验对比的一个指标就是<strong>单应矩阵</strong>估计，另外还有室内外的位姿估计。只能说SuperGlue的效果太好了，直接放结果吧（本来论文7页就写完了，作者放了10页附录大招）。</p><h3 id="单应矩阵估计"><a href="#单应矩阵估计" class="headerlink" title="单应矩阵估计"></a>单应矩阵估计</h3><p>能够获得非常高的匹配召回率（98.3%）同时获得超高的精度，比传统的暴力匹配都好了一大截。</p><p><img alt="superglue_tb_1" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_tb_1.png"></p><h3 id="室内外位姿估计"><a href="#室内外位姿估计" class="headerlink" title="室内外位姿估计"></a>室内外位姿估计</h3><p>下表看来，大基线室内位姿估计也是相当棒，完胜传统算法。</p><p><img alt="superglue_tb_2" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_tb_2.png"></p><p><img alt="superglue_tb_3" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_tb_3.png"></p><h3 id="网络耗时"><a href="#网络耗时" class="headerlink" title="网络耗时"></a>网络耗时</h3><p>接下来放出大家比较关心的网络耗时，下图是在NVIDIA GeForce GTX 1080 GPU跑了500次的结果，512个点69ms（14.5fps），1024个点87ms（11.5fps）。</p><p><img alt="superglue_tb_3" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_11.png"></p><h3 id="更多匹配结果"><a href="#更多匹配结果" class="headerlink" title="更多匹配结果"></a>更多匹配结果</h3><p>第一列是SuperPoint+暴力匹配结果，第二列是SuperPoint+OAnet（ICCV 2019）结果，第三列是SuperPoint+SuperGlue结果。能看到SuperGlue惊人的特征匹配能力，尤其是在大视角变化时优势明显（红线表示错误匹配，绿线表示正确匹配）。</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_res_3.jpg"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_res_1.jpg"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_res_2.jpg"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文展示了基于注意力的图神经网络对局部特征匹配的强大功能。 SuperGlue的框架使用两种注意力：（i）自我注意力，可以增强局部描述符的接受力；以及（ii）交叉注意力，可以实现跨图像交流，并受到人类来回观察方式的启发进行匹配图像。文中方法通过解决<strong>最优运输问题</strong>，优雅地处理了特征分配问题以及遮挡点。实验表明，SuperGlue与现有方法相比有了显着改进，可以在极宽的基线室内和室外图像对上进行高精度的相对姿势估计。此外，SuperGlue可以实时运行，并且可以同时使用经典和深度学习特征。</p><p>总而言之，论文提出的可学习的中后端（middle-end）算法以功能强大的神经网络模型替代了手工启发式技术，该模型同时在单个统一体系结构中执行上下文聚合，匹配和过滤外点。作者最后提到：若与深度学习前端结合使用，SuperGlue是迈向端到端深度学习SLAM的重要里程碑。（when combined with a deep front-end, SuperGlue is a major milestone towards end-to-end deep SLAM）</p><p>这真是鼓舞SLAM研究人员的士气！</p><h2 id="附件"><a href="#附件" class="headerlink" title="附件"></a>附件</h2><ul><li><a href="https://github.com/magicleap/SuperGluePretrainedNetwork" target="_blank" rel="noopener">SuperGlue Github地址</a></li><li><a href="http://xxx.itp.ac.cn/pdf/1911.11763v2" target="_blank" rel="noopener">SuperGlue Paper</a></li><li>*<a href="https://image-matching-workshop.github.io/" target="_blank" rel="noopener">Image Matching: Local Features &amp; Beyond CVPR 2020 Workshop</a></li></ul><hr><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_01.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_02.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_03.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_04.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_05.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_06.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_07.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_08.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_09.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_10.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_11.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_12.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_13.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_14.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_15.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_16.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_17.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ETHZ ASL与Magicleap联名之作，CVPR 2020 Oral（论文见文末），一作是来自ETHZ的实习生，二作是当年CVPR2018 SuperPoint的作者Daniel DeTone。&lt;br&gt;&lt;!-- ![](/posts/superglue/freiburg_matches.gif) --&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://vincentqin.gitee.io/posts/superglue/freiburg_matches.gif&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="CV" scheme="https://www.vincentqin.tech/categories/CV/"/>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="Deep Learning" scheme="https://www.vincentqin.tech/tags/Deep-Learning/"/>
    
      <category term="特征提取" scheme="https://www.vincentqin.tech/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
      <category term="MagicLeap" scheme="https://www.vincentqin.tech/tags/MagicLeap/"/>
    
      <category term="深度学习" scheme="https://www.vincentqin.tech/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SuperGlue" scheme="https://www.vincentqin.tech/tags/SuperGlue/"/>
    
      <category term="笔记" scheme="https://www.vincentqin.tech/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文" scheme="https://www.vincentqin.tech/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>SLAM常见问题(五)：Singular Value Decomposition（SVD）分解</title>
    <link href="https://www.vincentqin.tech/posts/slam-common-issues-SVD/"/>
    <id>https://www.vincentqin.tech/posts/slam-common-issues-SVD/</id>
    <published>2019-08-18T11:12:28.000Z</published>
    <updated>2020-03-31T14:57:52.667Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>SVD分解就是一种矩阵拆解术，它能够把<strong>任意</strong>矩阵$A \in \mathbb{R}^{m \times n}$拆解成3个矩阵的乘积形式，即：</p><script type="math/tex; mode=display">A = U \Sigma V^T</script><p>其中，$U \in \mathbb{R}^{m \times m}$，$V \in \mathbb{R}^{n \times n}$都是正交矩阵，即列向量是正交的单位向量，$\Sigma \in \mathbb{R}^{m \times n}$的对角阵（奇异值）。搬运了来自MIT OpenCourseWare的在线课程并放在了B站，讲解得很清晰。</p><a id="more"></a><!-- <div id="dplayer2" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer2"),"loop":true,"video":{"url":"http://45.76.197.98:888/api/public/dl/0hkAga3Z/Singular-Value-Decomposition.mp4"},"danmaku":{"id":"bbe4286bf164ef6w1497f18a7b42ff944e6r4b821","api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> --><iframe src="//player.bilibili.com/player.html?aid=93275447&cid=159253510&page=15" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>刚才说了矩阵$U, \Sigma, V$的形式，视频中还提到了这三个矩阵的物理意义，即SVD分解可以理解为：任意矩阵都可以分解为<strong>(rotation)*(Stretch)*(rotation)</strong>的形式。接下来说明一下这三个矩阵是如何来的。</p><h2 id="计算-A-TA"><a href="#计算-A-TA" class="headerlink" title="计算 $A^TA$"></a>计算 $A^TA$</h2><script type="math/tex; mode=display">A^TA = (U \Sigma V^T)^TU \Sigma V^T = V{\Sigma}^TU^TU \Sigma V^T = V{\Sigma}^T \Sigma V^T</script><p>可见，$V$正是矩阵$A^TA$的特征向量，而${\Sigma}^T \Sigma $为矩阵$A^TA$的特征值。</p><h2 id="计算-AA-T"><a href="#计算-AA-T" class="headerlink" title="计算 $AA^T$"></a>计算 $AA^T$</h2><script type="math/tex; mode=display">AA^T = U \Sigma V^T(U \Sigma V^T)^T = U \Sigma V^TV{\Sigma}^TU^TU \Sigma V^T = U{\Sigma}^T \Sigma U^T</script><p>可见，$U$正是矩阵$AA^T$的特征向量，而${\Sigma}^T \Sigma $为矩阵$A^TA$的特征值。</p><p>所以$U, \Sigma, V$都可以通过上述方式来计算。</p><h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><script type="math/tex; mode=display">\Sigma = \left[    \begin{array}    {cccc|cccc}    {\sigma_{1}} & {0} & {\dots} & {0} & {0} & {0} & {\dots} & {0} \\     {0} & {\sigma_{2}} & {\dots} & {0} & {0} & {0} & {\dots} & {0} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {0} & {0} & {\dots} & {0} \\    {0} & {0} & {} & {\sigma_{k}} & {0} & {0} & {\dots} & {0} \\    \hline     {0} & {0} & {\dots} & {0} & {0} & {0} & {\dots} & {0} \\    {\vdots} & {\vdots} & {} & {\vdots} & {\vdots} & {\vdots} & {} & {\vdots} \\    {0} & {0} & {\dots} & {0} & {0} & {0} & {\dots} & {0}    \end{array}\right]_{m \times n}\Rightarrow\left[    \begin{array}    {cccc}    {\sigma_{1}} & {0} & {\dots} & {0} \\     {0} & {\sigma_{2}} & {\dots} & {0}  \\     {\vdots} & {\vdots} & {\ddots} & {\vdots}  \\    {0} & {0} & {} & {\sigma_{k}}    \end{array}\right]_{k \times k}</script><p>其中${\sigma}_1 \geq {\sigma}_2 \geq … {\sigma}_k &gt; 0 $，将$\Sigma$中主对角线为0的部分删去，同样的$U,V$对应的部分删去，SVD分解就变成了下图的形式。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-SVD/svd.png"></p><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><h3 id="数字例子"><a href="#数字例子" class="headerlink" title="数字例子"></a>数字例子</h3><p>有矩阵A，对其进行SVD分解，已知：</p><script type="math/tex; mode=display">A = \left[\begin{matrix}​    1 & 4 & 3 & 5 & 6  \cr ​    2 & 3 & {4} & 5 & 0  \cr ​    7 & 4 & 0 & 9 & 1  \cr  \end{matrix}\right]</script><p>计算$A^TA$以及$AA^T$：</p><script type="math/tex; mode=display">A^TA = \left[\begin{matrix}​    {54} & {38} & {11} & {78} & {13}  \cr ​    {38} & {41} & {24} & {71} & {28}  \cr ​    {11} & {24} & {25} & {35} & {18}  \cr ​    {78} & {71} & {35} & {131} & {39}  \cr ​    {13} & {28} & {18} & {39} & {37} \end{matrix}\right]\\A^TA = \left[\begin{matrix}​    {87} & {51} & {74}   \cr ​    {51} & {54} & {71}   \cr ​    {74} & {71} & {147}   \cr \end{matrix}\right]</script><p>对以上两式做特征值分解得到：</p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">V <span class="built_in">=</span></span><br><span class="line">    <span class="number">0.4269</span>    <span class="number">0.5222</span>    <span class="number">0.1760</span>   -<span class="number">0.5292</span>   -<span class="number">0.4839</span></span><br><span class="line">    <span class="number">0.4087</span>   -<span class="number">0.1757</span>   -<span class="number">0.0655</span>   -<span class="number">0.5258</span>    <span class="number">0.7221</span></span><br><span class="line">    <span class="number">0.2100</span>   -<span class="number">0.4474</span>   -<span class="number">0.7536</span>   -<span class="number">0.1512</span>   -<span class="number">0.4062</span></span><br><span class="line">    <span class="number">0.7389</span>    <span class="number">0.1520</span>   -<span class="number">0.0603</span>    <span class="number">0.6481</span>    <span class="number">0.0853</span></span><br><span class="line">    <span class="number">0.2464</span>   -<span class="number">0.6879</span>    <span class="number">0.6271</span>   -<span class="number">0.0258</span>   -<span class="number">0.2687</span></span><br><span class="line"></span><br><span class="line">U <span class="built_in">=</span></span><br><span class="line">    <span class="number">0.5095</span>    <span class="number">0.7999</span>    <span class="number">0.3171</span></span><br><span class="line">    <span class="number">0.4285</span>    <span class="number">0.0838</span>   -<span class="number">0.8997</span></span><br><span class="line">    <span class="number">0.7462</span>   -<span class="number">0.5942</span>    <span class="number">0.3001</span></span><br></pre></td></tr></table></figure><p>奇异值$\Sigma ^T \Sigma = \text{Diag}(238.2878, 37.3715, 12.3407) \Rightarrow \Sigma = \text{Diag}(15.4366, 6.1132, 3.5129)$</p><p>这与直接调用<code>svd(A)</code>结果是一致的（可能差个正负号）。</p><h3 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h3><p>祭上亲爱的Battle Angel Alita。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-SVD/alita_origin.jpg"></p><p>原始图像尺寸$1440\times 2560 $，我们可以对该图像做SVD分解，然后仅保留奇异值的前10，50，100重构图像，比较重构图像与原始图像的质量差异。可见仅仅保留其前10个奇异值时，图像质量遭到了极大破坏（此时仅保留原始图像信息的58.864%），随着奇异值数量的增多，图像质量也会逐渐提升，可以看到当奇异值个数为100时，基本上已经看不出与原图的差异（此时仅保留原始图像信息的87.37%）。由此，我们实现了图像压缩。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-SVD/alita_svd1.jpg"></p><p>下图是保留的奇异值数量与图像质量的关系图，保留的奇异值越多，图像质量越高，图像压缩效果越不明显；反之，奇异值越少，图像质量越差，图像压缩效果越明显。这只是一种非常简单的图像压缩算法，仅作原理验证使用，在实际中用到的概率不是很大。</p><p><img alt data-src="//www.vincentqin.tech/posts/slam-common-issues-SVD/alita_svd_quality.svg"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% simple test of using SVD decomposistion</span></span><br><span class="line">clear <span class="built_in">all</span>;</span><br><span class="line">close <span class="built_in">all</span>;</span><br><span class="line">clc;</span><br><span class="line"></span><br><span class="line"><span class="comment">% A = [1 4 3 5 6;</span></span><br><span class="line"><span class="comment">%          2 3 4 5 0;</span></span><br><span class="line"><span class="comment">%          7 4 0 9 1]</span></span><br><span class="line"><span class="comment">% A'*A;</span></span><br><span class="line"><span class="comment">% A*A';</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% [V,Dv] = eig(A'*A);</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% lambda = wrev(diag(Dv));</span></span><br><span class="line"><span class="comment">% V = fliplr(V)</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% [U,Du] = eig(A*A');</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% lambda = wrev(diag(Du));</span></span><br><span class="line"><span class="comment">% U = fliplr(U)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% LOADING IMAGE</span></span><br><span class="line">img <span class="built_in">=</span> imread(<span class="string">'alita_origin.png'</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ENERGE <span class="built_in">=</span> <span class="number">0</span>;</span><br><span class="line">for i <span class="built_in">=</span> <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">    [U(:,:,i) D(:,:,i) V(:,:,i)] <span class="built_in">=</span> svd(double(img(:,:,i)))  ;</span><br><span class="line">    ENERGE <span class="built_in">=</span> ENERGE +sum(diag(D(:,:,i)));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"><span class="comment">%% 10</span></span><br><span class="line">DIM <span class="built_in">=</span> <span class="number">10</span>;</span><br><span class="line">ENERGE10 <span class="built_in">=</span> <span class="number">0</span>;</span><br><span class="line">for i <span class="built_in">=</span> <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">    img_recons10(:,:,i) <span class="built_in">=</span> U(:,<span class="number">1</span>:DIM,i)*D(<span class="number">1</span>:DIM,<span class="number">1</span>:DIM,i)*V(:,<span class="number">1</span>:DIM,i)<span class="string">';</span></span><br><span class="line"><span class="string">     ENERGE10 = ENERGE10 +sum(diag(D(1:DIM,1:DIM,i)));</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">% figure;</span></span><br><span class="line"><span class="string">% imshow(mat2gray(img_recons10))</span></span><br><span class="line"><span class="string">% imwrite(mat2gray(img_recons10),'</span>alita_10.png<span class="string">');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">%% 50</span></span><br><span class="line"><span class="string">DIM = 50;</span></span><br><span class="line"><span class="string">ENERGE50 = 0;</span></span><br><span class="line"><span class="string">for i = 1:3</span></span><br><span class="line"><span class="string">    img_recons50(:,:,i) = U(:,1:DIM,i)*D(1:DIM,1:DIM,i)*V(:,1:DIM,i)'</span>;</span><br><span class="line">     ENERGE50 <span class="built_in">=</span> ENERGE50 +sum(diag(D(<span class="number">1</span>:DIM,<span class="number">1</span>:DIM,i)));</span><br><span class="line">end</span><br><span class="line"><span class="comment">% figure;</span></span><br><span class="line"><span class="comment">% imshow(mat2gray(img_recons50))</span></span><br><span class="line"><span class="comment">% imwrite(mat2gray(img_recons50),'alita_50.png');</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 100</span></span><br><span class="line">DIM <span class="built_in">=</span> <span class="number">100</span>;</span><br><span class="line">ENERGE100 <span class="built_in">=</span> <span class="number">0</span>;</span><br><span class="line">for i <span class="built_in">=</span> <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">    img_recons100(:,:,i) <span class="built_in">=</span> U(:,<span class="number">1</span>:DIM,i)*D(<span class="number">1</span>:DIM,<span class="number">1</span>:DIM,i)*V(:,<span class="number">1</span>:DIM,i)<span class="string">';</span></span><br><span class="line"><span class="string">    ENERGE100 = ENERGE100 +sum(diag(D(1:DIM,1:DIM,i)));</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">% figure;</span></span><br><span class="line"><span class="string">% imshow(mat2gray(img_recons100))</span></span><br><span class="line"><span class="string">% imwrite(mat2gray(img_recons100),'</span>alita_100.png<span class="string">');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">figure;</span></span><br><span class="line"><span class="string">set(gcf,'</span>pos<span class="string">',[ 986 414 1274 826])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">FONTSIZE = 15;</span></span><br><span class="line"><span class="string">h(1) = subplot(221);imshow(mat2gray(img)); </span></span><br><span class="line"><span class="string">xlabel('</span>origin Alita<span class="string">');set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">h(2) = subplot(222);imshow(mat2gray(img_recons10));</span></span><br><span class="line"><span class="string">xlabel(['</span>Using <span class="number">10</span> singular values: <span class="string">' num2str(ENERGE10/ENERGE)]);set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">h(3) = subplot(223);imshow(mat2gray(img_recons50));</span></span><br><span class="line"><span class="string">xlabel(['</span>Using <span class="number">50</span> singular values: <span class="string">' num2str(ENERGE50/ENERGE)]);set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">h(4) = subplot(224);imshow(mat2gray(img_recons100));</span></span><br><span class="line"><span class="string">xlabel(['</span>Using <span class="number">100</span> singular values: <span class="string">' num2str(ENERGE100/ENERGE)]);set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string">set(gcf,'</span>color<span class="string">',[1 1 1])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">%% SHOW ENERGY</span></span><br><span class="line"><span class="string">ENERGY_tmp = zeros(size(img,1),1);</span></span><br><span class="line"><span class="string">for DIM_ = 1:size(img,1)</span></span><br><span class="line"><span class="string">   for i = 1:3</span></span><br><span class="line"><span class="string">     ENERGY_tmp(DIM_,1) = ENERGY_tmp(DIM_,1) +sum(diag(D(1:DIM_,1:DIM_,i)));</span></span><br><span class="line"><span class="string">   end</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">figure;</span></span><br><span class="line"><span class="string">FONTSIZE = 30;</span></span><br><span class="line"><span class="string">ratio = ENERGY_tmp/ENERGE;</span></span><br><span class="line"><span class="string">X =   1:size(img,1);</span></span><br><span class="line"><span class="string">plot(X,ratio,'</span>linewidth<span class="string">',5,'</span>color<span class="string">','</span>r<span class="string">');</span></span><br><span class="line"><span class="string">set(gcf,'</span>color<span class="string">',[1 1 1])</span></span><br><span class="line"><span class="string">xlabel('</span>Number of Singular values<span class="string">');</span></span><br><span class="line"><span class="string">ylabel('</span>Image Quality<span class="string">');</span></span><br><span class="line"><span class="string">set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string">set(gcf,'</span>pos<span class="string">',[ 986 414 1274 826])</span></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="http://www-users.math.umn.edu/~lerman/math5467/svd.pdf" target="_blank" rel="noopener">A Singularly Valuable Decomposition The SVD of a Matrix</a></li><li>李宏毅关于SVD的介绍，<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/LA_2018/Lecture/SVD.pdf" target="_blank" rel="noopener">PPT</a>,<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_LA18.html" target="_blank" rel="noopener">课程列表</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SVD分解就是一种矩阵拆解术，它能够把&lt;strong&gt;任意&lt;/strong&gt;矩阵$A \in \mathbb{R}^{m \times n}$拆解成3个矩阵的乘积形式，即：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
A = U \Sigma V^T&lt;/script&gt;&lt;p&gt;其中，$U \in \mathbb{R}^{m \times m}$，$V \in \mathbb{R}^{n \times n}$都是正交矩阵，即列向量是正交的单位向量，$\Sigma \in \mathbb{R}^{m \times n}$的对角阵（奇异值）。搬运了来自MIT OpenCourseWare的在线课程并放在了B站，讲解得很清晰。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="SVD" scheme="https://www.vincentqin.tech/tags/SVD/"/>
    
      <category term="位姿" scheme="https://www.vincentqin.tech/tags/%E4%BD%8D%E5%A7%BF/"/>
    
  </entry>
  
  <entry>
    <title>SLAM常见问题(四)：求解ICP，利用SVD分解得到旋转矩阵</title>
    <link href="https://www.vincentqin.tech/posts/slam-common-issues-ICP/"/>
    <id>https://www.vincentqin.tech/posts/slam-common-issues-ICP/</id>
    <published>2019-08-18T03:43:04.000Z</published>
    <updated>2020-03-31T15:05:16.626Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>今天讲一篇关于利用<code>SVD</code>方法求解<code>ICP</code>问题的文献<a href="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-ICP/svd_rot.pdf" target="_blank" rel="noopener">《Least-Squares Rigid Motion Using SVD》</a>，这篇文章非常精彩地推导出将$3D$点对齐问题的解析解，同时总结了求解该问题的统一范式。</p><a id="more"></a><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>已知<script type="math/tex">{\mathcal{P}=\left\{\mathbf{p}_{1}, \mathbf{p}_{2}, \ldots, \mathbf{p}_{n}\right\}}</script>以及<script type="math/tex">{\mathcal{Q}=\left\{\mathbf{q}_{1}, \mathbf{q}_{2}, \ldots, \mathbf{q}_{n}\right\}}</script>是空间中（文中说的更加普适，<script type="math/tex">\mathbf{p}_i , \mathbf{q}_i \in \mathbb{R}^{d}</script>，可以表示$d$维空间）的匹配点集，我们试图找到这样的旋转矩阵$R$和平移向量$\mathbf{t}$最小化如下对齐误差（即<code>ICP</code>问题的形式）：</p><script type="math/tex; mode=display">(R, \mathbf{t})=\underset{R \in S O(d), \mathbf{t} \in \mathbb{R}^{d}}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left\|\left(R \mathbf{p}_{i}+\mathbf{t}\right)-\mathbf{q}_{i}\right\|^{2} \tag{1}</script><p>接下来文章分别推导了平移向量$\mathbf{t}$以及旋转矩阵$R$的解析解。</p><h2 id="计算平移量"><a href="#计算平移量" class="headerlink" title="计算平移量"></a>计算平移量</h2><p>此时假定旋转矩阵$R$是固定的，令<script type="math/tex">F(\mathbf{t}) = \sum_{i=1}^{n} w_{i}\left\|\left(R \mathbf{p}_{i}+\mathbf{t}\right)-\mathbf{q}_{i}\right\|^{2}</script>，我们可以通过$F$对$\mathbf{t}$求导的方式得到平移量的最优解，如下：</p><script type="math/tex; mode=display">\begin{aligned} 0 &=\frac{\partial F}{\partial \mathbf{t}}=\sum_{i=1}^{n} 2 w_{i}\left(R \mathbf{p}_{i}+\mathbf{t}-\mathbf{q}_{i}\right)=\\ &=2 \mathbf{t}\left(\sum_{i=1}^{n} w_{i}\right)+2 R\left(\sum_{i=1}^{n} w_{i} \mathbf{p}_{i}\right)-2 \sum_{i=1}^{n} w_{i} \mathbf{q}_{i}  \end{aligned} \tag{2}</script><p>令：</p><script type="math/tex; mode=display">\overline{\mathbf{p}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{p}_{i}}{\sum_{i=1}^{n} w_{i}},  \overline{\mathbf{q}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{q}_{i}}{\sum_{i=1}^{n} w_{i}} \tag{3}</script><p>于是我们得到$\mathbf{t}$的解：</p><script type="math/tex; mode=display">\mathbf{t} = \overline{\mathbf{q}} - R\overline{\mathbf{p}} \tag{4}</script><p>从上式看出最优的平移量$\mathbf{t}$将$\mathcal{P}$点集的加权中心映射到了$\mathcal{Q}$点集的中心。接下来将上式带入优化方程，得：</p><script type="math/tex; mode=display">\begin{aligned}\sum_{i=1}^{n} w_{i}\left\|\left(R \mathbf{p}_{i}+\mathbf{t}\right)-\mathbf{q}_{i}\right\|^{2} &= \sum_{i=1}^{n} w_{i}\left\| R \mathbf{p}_{i}+ \overline{\mathbf{q}} - R\overline{\mathbf{p}} -\mathbf{q}_{i}\right\|^{2}  \\ &= \sum_{i=1}^{n} w_{i}\left\|R (\mathbf{p}_{i} -\overline{\mathbf{p}}) - (\mathbf{q}_{i} - \overline{\mathbf{q}} ) \right\|^{2}\end{aligned} \tag{5}</script><p>由此我们将原问题转换成了无平移量的优化问题，令：</p><script type="math/tex; mode=display">\mathbf{x}_i := \mathbf{p}_{i} -\overline{\mathbf{p}}，\mathbf{y}_i := \mathbf{q}_{i} -\overline{\mathbf{q}}，\tag{6}</script><p>我们把问题简写成如下形式：</p><script type="math/tex; mode=display">R = \underset{R \in S O(d)}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2} \tag{7}</script><h2 id="计算旋转量"><a href="#计算旋转量" class="headerlink" title="计算旋转量"></a>计算旋转量</h2><p>简化上式：</p><script type="math/tex; mode=display">\begin{aligned}\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2} &= \left( R\mathbf{x}_i - \mathbf{y}_i\right)^T\left( R\mathbf{x}_i - \mathbf{y}_i\right)  = \left( \mathbf{x}_i^TR^T - \mathbf{y}_i^T\right)\left( R\mathbf{x}_i - \mathbf{y}_i\right)  \\&= \mathbf{x}_i^TR^TR\mathbf{x}_i - \mathbf{x}_i^TR^T\mathbf{y}_i - \mathbf{y}_i^TR\mathbf{x}_i +\mathbf{y}_i^T\mathbf{y}_i \end{aligned}\tag{8}</script><p>又因为旋转矩阵的正交性：$R^TR=I$；另外$ \mathbf{x}_i^TR^T\mathbf{y}_i$是标量：$\mathbf{x}_i$维度为$1 \times d$，$R^T$维度为$d \times d$，$\mathbf{y}_i$维度为$d \times 1$。于是有下式：</p><script type="math/tex; mode=display"> \mathbf{x}_i^TR^T\mathbf{y}_i = (\mathbf{x}_i^TR^T\mathbf{y}_i)^T = \mathbf{y}_i^TR\mathbf{x}_i \tag{9}</script><p>得：</p><script type="math/tex; mode=display">\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2} = \mathbf{x}_i^T\mathbf{x}_i -  2\mathbf{y}_i^TR\mathbf{x}_i +\mathbf{y}_i^T\mathbf{y}_i  \tag{10}</script><p>将整理好的上式带入简化后的$R$优化问题，得：</p><script type="math/tex; mode=display">\begin{aligned} & \underset{R \in S O(d)}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2}=\underset{R \in S O(d)}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left(\mathbf{x}_{i}^{\top} \mathbf{x}_{i}-2 \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}+\mathbf{y}_{i}^{\top} \mathbf{y}_{i}\right)=\\=& \underset{R \in S O(d)}{\operatorname{argmin}}\left(\sum_{i=1}^{n} w_{i} \mathbf{x}_{i}^{\top} \mathbf{x}_{i}-2 \sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}+\sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} \mathbf{y}_{i}\right)=\\=& \operatorname{argmin}_{R \in S O(d)}\left(-2 \sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}\right) \end{aligned}\tag{11}</script><p>接下来将要利用到如下关于迹的技巧:</p><script type="math/tex; mode=display">\begin{aligned}\left[\begin{array}{cccc}{w_1} \\ {} & {w_1} & {} &{} \\ {} & {} & {\ddots} & {}\\ {} & {} & {} & {w_n}\end{array}\right]\left[\begin{array}{ccc}{—}& {\mathbf{y}_1^T}&{—}   \\ {—}& {\mathbf{y}_2^T}&{—}   \\ {—}  & {\vdots} & {—}\\ {—}& {\mathbf{y}_n^T}&{—}\end{array}\right]\left[\begin{array}{ccc}{}& {} &{}   \\ {}& {R} &{}   \\ {}& {} &{}   \\ \end{array}\right]\left[\begin{array}{cccc}{|}& {|} &{|} &{|}  \\ {\mathbf{x}_1}& {\mathbf{x}_2} &{\dots} &{\mathbf{x}_n}  \\ {|}& {|} &{|} &{|}  \\ \end{array}\right]  \\=\left[\begin{array}{ccc}{—}& {w_1\mathbf{y}_1^T}&{—}   \\ {—}& {w_2\mathbf{y}_2^T}&{—}   \\ {—}  & {\vdots} & {—}\\ {—}& {w_n\mathbf{y}_n^T}&{—}\end{array}\right]\left[\begin{array}{cccc}{|}& {|} &{|} &{|}  \\ {R\mathbf{x}_1}& {R\mathbf{x}_2} &{\dots} &{R\mathbf{x}_n}  \\ {|}& {|} &{|} &{|}  \\ \end{array}\right] \\=\left[\begin{array}{cccc}{w_1\mathbf{y}_1^TR\mathbf{x}_1}& {} &{} &{*}  \\ {}& {w_2\mathbf{y}_2^TR\mathbf{x}_2} &{} &{}  \\ {}& {} &{\ddots} &{}  \\ {*}& {} &{} &{w_n\mathbf{y}_n^TR\mathbf{x}_n} \\ \end{array}\right]\end{aligned}</script><p>上式就是对<script type="math/tex">\sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i} =  \operatorname{tr}\left( WY^TRX\right)</script>的完美解释。</p><p>利用上式，式$(11)$可以整理得：</p><script type="math/tex; mode=display">\begin{aligned}\underset{R \in S O(d)}{\operatorname{argmin}}\left(-2 \sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}\right) &= \underset{R \in S O(d)}{\operatorname{argmax}}\left(\sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}\right) \\&= \underset{R \in S O(d)}{\operatorname{argmax}} \operatorname{tr}\left( WY^TRX\right)\end{aligned}\tag{12}</script><p>这里说明一下维度：$W = diag(w_1,w_2,…,w_n)$维度为$n \times n$，$Y^T$维度为$n \times d$，$R$维度为$d \times d$，$X$维度为$d \times n$。</p><p>接下来回顾一下迹的性质：$\operatorname{tr}(AB) = \operatorname{tr}(BA)$，因此有下式：</p><script type="math/tex; mode=display">\operatorname{tr}\left( WY^TRX\right) = \operatorname{tr}\left( (WY^T)(RX)\right) =\operatorname{tr}\left( RXWY^T\right) \tag{13}</script><p>令$d\times d$的“covariance”矩阵$S = XWY^T$，求$S$的<code>SVD</code>分解：</p><script type="math/tex; mode=display">S= U\Sigma V^T.\tag{14}</script><p>于是式$(13)$变为：</p><script type="math/tex; mode=display">\operatorname{tr}\left( WY^TRX\right) =\operatorname{tr}\left( RS\right) =\operatorname{tr}\left( RU\Sigma V^T\right)=\operatorname{tr}\left( \Sigma V^TRU\right) \tag{15}</script><p>由于$V,T,R$均为正交矩阵，因此$M = V^TRU$也是正交阵，也就是说$M$的列向量$\mathbf{m}_j$是互相正交的单位向量，即$\mathbf{m}_j^T\mathbf{m}_j=1$，于是：</p><script type="math/tex; mode=display">1=\mathbf{m}_{j}^{\top} \mathbf{m}_{j}=\sum_{i=1}^{d} m_{i j}^{2} \Rightarrow m_{i j}^{2} \leq 1 \Rightarrow\left|m_{i j}\right| \leq 1 \tag{16}</script><p>由于<code>SVD</code>分解的性质可知$\sigma$的元素均为非负数：${\sigma}_1,{\sigma}_2,{\sigma}_d \geq 0$，于是式$(18)$变为如下形式：</p><script type="math/tex; mode=display">\operatorname{tr}(\Sigma M)=\left(\begin{array}{ccccc}{\sigma_{1}} & {} & {} & {} & {} \\ {} & {\sigma_{2}} & {} & {} & {} \\ {} & {} & {\ddots} & {} & {} \\ {} & {} & {} & {} & {\sigma_{d}}\end{array}\right)\left(\begin{array}{cccc}{m_{11}} & {m_{12}} & {\dots} & {m_{1 d}} \\ {m_{21}} & {m_{22}} & {\dots} & {m_{2 d}} \\ {\vdots} & {\vdots} & {\vdots} & {\vdots} \\ {m_{d 1}} & {m_{d 2}} & {\dots} & {m_{d d}}\end{array}\right)=\sum_{i=1}^{d} \sigma_{i} m_{i i} \leq \sum_{i=1}^{d} \sigma_{i} \tag{17}</script><p>可见，当迹最大时$m_{ii} = 1 $，又由于$M$是正交阵，这使得$M$为单位阵！</p><script type="math/tex; mode=display">I = M = V^TRU \Rightarrow R = VU^T \tag{18}</script><p>看到没，R的解析解竟然如此简单，并且与<code>SVD</code>分解产生了联系，让人感觉到了数学的美妙。不过到这里还没完，后面作者进行了一步方向矫正，大意是这样的：利用公式$(18)$得到的矩阵并不一定是一个旋转矩阵，也可能为<code>反射矩阵</code>，此时可以通过验证$VU^T$的行列式来判断到底是旋转（行列式 = 1）还是反射（行列式 = -1）。但我们要求的是旋转矩阵，这时需要对公式$(18)$进行一步处理。</p><p>假设$\operatorname{det}(VU^T) = -1$，则限制$R$为旋转就意味着$M = V^TRU $为<code>反射矩阵</code>， 于是我们试图找到一个<code>反射矩阵</code>$M$最大化下式：</p><script type="math/tex; mode=display">\operatorname{tr}(\Sigma M) = {\sigma}_1 m_{11} + {\sigma}_2 m_{22} +...+ {\sigma}_d m_{dd} := f(m_{11},m_{11},...,m_{dd})  \tag{19}</script><p>即$f$是以<script type="math/tex">m_{11},m_{11},...,m_{dd}</script>为变量的线性函数，由于<script type="math/tex">m_{ii} \in \left[ -1,1\right]</script>，其极大值肯定在其定义域的边界处。于是当<script type="math/tex">{\forall} i, m_{ii} = 1</script>时，$f$取得极大值，但是此时的$R$为<code>反射矩阵</code>，所以并不能这样取值。然后我们看第二个极大值点$(1,1,…,-1)$，有：</p><script type="math/tex; mode=display">f = \operatorname{tr}(\Sigma M) = {\sigma}_1 + {\sigma}_2+...+ {\sigma}_{d-1} -  {\sigma}_d \tag{20}</script><p>这个值大于任何其它的自变量取值$(\pm 1,\pm 1,…,\pm 1)$的组合（除了$( 1, 1,…, 1)$），因为奇异值是经过排序的，${\sigma}_d$是最小的一个奇异值。</p><p>综上，为了将解转换为旋转矩阵要进行如下处理：</p><script type="math/tex; mode=display">R=V\left(\begin{array}{cccc}{1} \\ {} & {\ddots} & {} &{} \\ {} & {} & 1 & {}\\ {} & {} & {} & {\operatorname{det}\left(V U^{\top}\right)}\end{array}\right) U^{\top} \tag{21}</script><h2 id="可以总结的套路"><a href="#可以总结的套路" class="headerlink" title="可以总结的套路"></a>可以总结的套路</h2><p>为了得到<code>ICP</code>问题的最优解，我们可以采取如下套路：</p><p><strong>step1</strong>. 计算两组匹配点的加权中心：</p><script type="math/tex; mode=display">\overline{\mathbf{p}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{p}_{i}}{\sum_{i=1}^{n} w_{i}},  \overline{\mathbf{q}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{q}_{i}}{\sum_{i=1}^{n} w_{i}}</script><p><strong>step2</strong>. 得到去中心化的点集：</p><script type="math/tex; mode=display">\mathbf{x}_i := \mathbf{p}_{i} -\overline{\mathbf{p}}，\mathbf{y}_i := \mathbf{q}_{i} -\overline{\mathbf{q}}, i = 1,2...n</script><p><strong>step3</strong>. 计算$d \times d$的covariance矩阵：</p><script type="math/tex; mode=display">S = XWY^T</script><p>其中，$X,Y$为$d \times n$的矩阵，$\mathbf{x}_i,\mathbf{y}_i$分别是它们的列元素，另外$W = diag(w_1,w_2,…,w_n)$。</p><p><strong>step4</strong>. 对$S$进行<code>SVD</code>分解$S = U\Sigma V^T$，得到旋转矩阵：</p><script type="math/tex; mode=display">R=V\left(\begin{array}{cccc}{1} \\ {} & {\ddots} & {} &{} \\ {} & {} & 1 & {}\\ {} & {} & {} & {\operatorname{det}\left(V U^{\top}\right)}\end{array}\right) U^{\top}</script><p><strong>step5</strong>. 计算平移量：</p><script type="math/tex; mode=display">\mathbf{t} = \overline{\mathbf{q}} - R\overline{\mathbf{p}}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天讲一篇关于利用&lt;code&gt;SVD&lt;/code&gt;方法求解&lt;code&gt;ICP&lt;/code&gt;问题的文献&lt;a href=&quot;https://vincentqin.gitee.io/blogresource-3/slam-common-issues-ICP/svd_rot.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《Least-Squares Rigid Motion Using SVD》&lt;/a&gt;，这篇文章非常精彩地推导出将$3D$点对齐问题的解析解，同时总结了求解该问题的统一范式。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="位姿" scheme="https://www.vincentqin.tech/tags/%E4%BD%8D%E5%A7%BF/"/>
    
      <category term="ICP" scheme="https://www.vincentqin.tech/tags/ICP/"/>
    
  </entry>
  
  <entry>
    <title>SLAM常见问题(三)：PNP</title>
    <link href="https://www.vincentqin.tech/posts/slam-common-issues-PNP/"/>
    <id>https://www.vincentqin.tech/posts/slam-common-issues-PNP/</id>
    <published>2019-08-11T13:29:24.000Z</published>
    <updated>2020-03-31T15:31:47.349Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><code>PNP</code>即“Perspective-N-Points”，是求解 3D 到 2D 点对运动的方法。它描述了当我们知道n个3D空间点以及它们在图像上的位置时，如何估计相机所在的位姿。PnP 问题有很多种求解方法，例如用三对点估计位姿的 <code>P3P</code>（通常需要额外一个点进行验证结果），直接线性变换（<code>DLT</code>），<code>EPnP</code>（Efficient PnP，已知内参时用），<code>UPnP</code>（内参未知时用） 等等）。此外，还能用非线性优化的方式，构建最小二乘问题并迭代求解，也就是万金油式的 <code>Bundle Adjustment</code>。</p><a id="more"></a><h2 id="P3P"><a href="#P3P" class="headerlink" title="P3P"></a>P3P</h2><p>已知：$3D-2D$匹配点，$3D$点的<strong>世界坐标</strong>记为$A, B, C$，图像上的2D点记为$a, b, c$。</p><p>未知：<strong>相机系下3D点的坐标是未知的</strong>，即$OA,OB,OC$，一旦$ 3D$ 点在相机坐标系下的坐标能够算出，我们就得到了$3D-3D$的对应点，把<code>PnP</code>问题转换为了<code>ICP</code>问题。</p><p>我们的目标就是通过<strong>纯几何的方法</strong>求出上述未知量，过程如下。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-PNP/p3p.png"></p><p>由于余弦定理可知：</p><script type="math/tex; mode=display">\begin{array}{l}{O A^{2}+O B^{2}-2 O A \cdot O B \cdot \cos \langle a, b\rangle= A B^{2}} \\ {O B^{2}+O C^{2}-2 O B \cdot O C \cdot \cos \langle b, c\rangle= B C^{2}} \\ {O A^{2}+O C^{2}-2 O A \cdot O C \cdot \cos \langle a, c\rangle= A C^{2}}\end{array}</script><p>对上面三式全体除以$OC^{2}$，记$x=O A / O C, y=O B / O C$，得：</p><script type="math/tex; mode=display">\begin{array}{l}{x^{2}+y^{2}-2 x y \cos \langle a, b\rangle= A B^{2} / O C^{2}} \\ {y^{2}+1^{2}-2 y \cos \langle b, c\rangle= B C^{2} / O C^{2}} \\ {x^{2}+1^{2}-2 x \cos \langle a, c\rangle= A C^{2} / O C^{2}}\end{array}</script><p>记$v=A B^{2} / O C^{2}, u v=B C^{2} / O C^{2}, w v=A C^{2} / O C^{2}$，得：</p><script type="math/tex; mode=display">\begin{array}{l}{x^{2}+y^{2}-2 x y \cos \langle a, b\rangle- v=0} \\ {y^{2}+1^{2}-2 y \cos \langle b, c\rangle- u v=0} \\ {x^{2}+1^{2}-2 x \cos \langle a, c\rangle- w v=0}\end{array}</script><p>将第一个式子中$v = x^{2}+y^{2}-2 x y \cos \langle a, b\rangle$带入后面两个式子中，得：</p><script type="math/tex; mode=display">\begin{array}{l}{(1-u) y^{2}-u x^{2}-\cos \langle b, c\rangle y+2 u x y \cos \langle a, b\rangle+ 1=0} \\ {(1-w) x^{2}-w y^{2}-\cos \langle a, c\rangle x+2 w x y \cos \langle a, b\rangle+ 1=0}\end{array}</script><p>上式中几个余弦角度$\cos \langle a, b\rangle, \cos \langle b, c\rangle, \cos \langle a, c\rangle$是已知的，$u=B C^{2} / A B^{2}, w=A C^{2} / A B^{2}$也是已知的，所以未知量仅有$x,y$，解析地求解该方程组是一个复杂的过程，需要用<strong><a href="https://zh.wikipedia.org/wiki/%E5%90%B4%E6%B6%88%E5%85%83%E6%B3%95" target="_blank" rel="noopener">吴消元法</a></strong>。这样就可以求得$x,y$，然后带入$v = x^{2}+y^{2}-2 x y \cos \langle a, b\rangle$求解$v$，即可得到$OC$，进而得到$OB,OA$。该方程最多可能得到四个解，但我们可以用第4个验证点来计算最可能的解，得到$ A, B, C$ 在相机坐标系下的$3D$坐标。然后，根据$ 3D-3D $的点对，计算相机的运动 $R,t$，此处可参考文献<a href="https://igl.ethz.ch/projects/ARAP/svd_rot.pdf" target="_blank" rel="noopener">Least-Squares Rigid Motion Using SVD</a></p><h2 id="EPnP"><a href="#EPnP" class="headerlink" title="EPnP"></a>EPnP</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p><code>EPnP</code>即Efficient PnP，参考文献 <a href="https://icwww.epfl.ch/~lepetit/papers/lepetit_ijcv08.pdf" target="_blank" rel="noopener">EPnP: An Accurate O(n) Solution to the PnP Problem</a>。</p><p>问题描述如PnP，更加具体的，我们已知一组特征点，对于每个特征点$i$，我们有如下信息：</p><ul><li><p>特征点 $i$ 在世界坐标系的坐标<script type="math/tex">P_{i}^{w}=\left[\begin{array}{c}{x_{i}^{w}} \\ {y_{i}^{w}} \\ {z_{i}^{w}}\end{array}\right]</script></p></li><li><p>特征点在成像平面上的坐标<script type="math/tex">p_{i}=\left[\begin{array}{l}{u_{i}} \\ {v_{i}}\end{array}\right]</script></p></li><li>已知相机内参$K$</li></ul><p>求：世界坐标系到相机系的变换矩阵<script type="math/tex">T_{c w}=\left[\begin{array}{cc}{R_{c w}} & {t} \\ {0} & {1}\end{array}\right]</script></p><h3 id="算法假设"><a href="#算法假设" class="headerlink" title="算法假设"></a>算法假设</h3><p><code>EPnP</code>的思想是无论世界系还是相机系下的$3D$点都可以由<strong>4个控制点线性组合</strong>，记：</p><ul><li>世界系下4个控制点表示为:$\mathbf{c}_{j}^{w}, j=1, \cdots, 4$</li><li>相机系下4个控制点表示为:$\mathbf{c}_{j}^{c}, j=1, \cdots, 4$</li></ul><p>EPnP算法将参考点的坐标表示为控制点坐标的加权和：</p><script type="math/tex; mode=display">\mathbf{p}_{i}^{w}=\sum_{j=1}^{4} \alpha_{i j} \mathbf{c}_{j}^{w}, \text { with } \sum_{j=1}^{4} \alpha_{i j}=1</script><p>其中<script type="math/tex">\alpha_{i, j}, j=1, \cdots, 4</script>是加权系数，一旦虚拟控制点确定后，且满足4个控制点不共面的前提，<script type="math/tex">\alpha_{i, j}</script>是唯一的。</p><h3 id="控制点的存在性"><a href="#控制点的存在性" class="headerlink" title="控制点的存在性"></a>控制点的存在性</h3><p>现在讨论控制点的存在性，上式可以写成：</p><script type="math/tex; mode=display">\left[\begin{array}{c}{p_{i}^{w}} \\ {1}\end{array}\right]=\left[\begin{array}{cccc}{C_{1}^{w}} & {C_{2}^{w}} & {C_{3}^{w}} & {C_{4}^{w}} \\ {1} & {1} & {1} & {1}\end{array}\right] \alpha_{i} \stackrel{令}{=} C \alpha_{i}</script><p>可见只要$C$非奇异，就一定可以找到满足条件的$\alpha_{i} $，即：</p><script type="math/tex; mode=display">\left[\begin{array}{l}{\alpha_{i 1}} \\ {\alpha_{i 2}} \\ {\alpha_{i 3}} \\ {\alpha_{i 4}}\end{array}\right]=C^{-1}\left[\begin{array}{c}{\mathbf{p}_{i}^{w}} \\ {1}\end{array}\right]</script><p>接下来，我们讨论相机坐标系下，控制点和参考$3D$点之间的关系：</p><script type="math/tex; mode=display">p_{i}^{c}=R_{c w} p_{i}^{w}+t=R_{c w}\left(\sum_{j=1}^{4} \alpha_{i j} c_{i}^{w}\right)+t</script><p>由于<script type="math/tex">\sum_{j=1}^{4} \alpha_{i j}=1$，因此$t=\sum_{j=1}^{4} \alpha_{i j} t</script>,带入上式，得：</p><script type="math/tex; mode=display">p_{i}^{c}=\sum_{j=1}^{4} \alpha_{i j}\left(R_{c w} c_{i}^{w}\right)+t )=\sum_{j=1}^{4} \alpha_{i j} c_{i}^{c}</script><p>可见系数<script type="math/tex">\alpha_{i}</script>具有不变性，如果我们能够求出控制点在相机坐标系中的坐标<script type="math/tex">c_{1}^{c}, c_{2}^{c},c_{3}^{c},c_{4}^{c}</script>，那么对于任意一个3D点k，我们可以求得其在相机系下的坐标：<script type="math/tex">p_{k}^{c}=\sum_{j=1}^{4} \alpha_{k j} c_{i}^{c}</script>，这就变成了如P3P同样的问题了，即求解<code>3D-3D</code>位姿估计问题。</p><h3 id="如何选择控制点"><a href="#如何选择控制点" class="headerlink" title="如何选择控制点"></a>如何选择控制点</h3><p>记世界系下所有3D点集为<script type="math/tex">\left\{\mathbf{p}_{i}^{w}, i=1, \cdots, n\right\}</script>,第一个控制点是所有3D点的重心:</p><script type="math/tex; mode=display">\mathbf{c}_{1}^{w}=\frac{1}{n} \sum_{i=1}^{n} \mathbf{p}_{i}^{w}</script><p>对所有3D点去中心化，这些点罗列成矩阵形式：</p><script type="math/tex; mode=display">A=\left[\begin{array}{c}{\mathbf{p}_{1}^{w^{T}}-\mathbf{c}_{1}^{w^{T}}} \\ {\cdots} \\ {\mathbf{p}_{n}^{w^{T}}-\mathbf{c}_{1}^{w^{T}}}\end{array}\right]</script><p>对$A^TA$进行特征值分解（注意此时并非对A进行<code>SVD</code>分解，是为了减低时间复杂度，<code>SVD</code>分解的复杂度为$SO(3)$），其特征值为<script type="math/tex">\lambda_{c, i}, i=1,2,3</script>，对应的特征向量为<script type="math/tex">\mathbf{v}_{c, i}, i=1,2,3</script>，则剩余的3个控制点表示为如下公式：</p><script type="math/tex; mode=display">\mathbf{c}_{j}^{w}=\mathbf{c}_{1}^{w}+\lambda_{c, j-1}^{\frac{1}{2}} \mathbf{v}_{c, j-1}, j=2,3,4</script><h3 id="求解控制点在相机系下的坐标"><a href="#求解控制点在相机系下的坐标" class="headerlink" title="求解控制点在相机系下的坐标"></a>求解控制点在相机系下的坐标</h3><p>记<script type="math/tex">\left\{\mathbf{u}_{i}\right\}_{i=1, \cdots, n}</script>为相机下$3D$点<script type="math/tex">\left\{\mathbf{p}^c_{i}\right\}_{i=1, \cdots, n}</script>的图像坐标，则：</p><script type="math/tex; mode=display">\forall i, \quad w_{i}\left[\begin{array}{c}{\mathbf{u}_{i}} \\ {1}\end{array}\right]=K \mathbf{p}_{i}^{c}=K \sum_{j=1}^{4} \alpha_{i j} \mathbf{c}_{j}^{c}</script><p>其中<script type="math/tex">w_i</script>是尺度因子，将控制点<script type="math/tex">\mathbf{c}_{j}^{c}=\left[x_{j}^{c}, y_{j}^{c}, z_{j}^{c}\right]^{T}</script>带入上式，得：</p><script type="math/tex; mode=display">\forall i, \quad w_{i}\left[\begin{array}{c}{u_{i}} \\ {v_{i}} \\ {1}\end{array}\right]=\left[\begin{array}{ccc}{f_{u}} & {0} & {u_{c}} \\ {0} & {f_{v}} & {v_{c}} \\ {0} & {0} & {1}\end{array}\right] \sum_{j=1}^{4} \alpha_{i j}\left[\begin{array}{c}{x_{j}^{c}} \\ {y_{j}^{c}} \\ {z_{j}^{c}}\end{array}\right]</script><p>上式可以得到两个线性方程：</p><script type="math/tex; mode=display">\begin{array}{l}{\sum_{j=1}^{4} \alpha_{i j} f_{u} x_{j}^{c}+\alpha_{i j}\left(u_{c}-u_{i}\right) z_{j}^{c}=0} \\ {\sum_{j=1}^{4} \alpha_{i j} f_{v} y_{j}^{c}+\alpha_{i j}\left(v_{c}-v_{j}\right) z_{j}^{c}=0}\end{array}</script><p>把这N个点的约束罗列在一起，我们就可以得到如下矩阵：</p><script type="math/tex; mode=display">M\mathbf{x} = \mathbf{0}</script><p>其中<script type="math/tex">\mathbf{x}=\left[\mathbf{c}_{1}^{c \top}, \mathbf{c}_{2}^{c \top}, \mathbf{c}_{3}^{c \top}, \mathbf{c}_{4}^{c \top}\right]^{\top}</script>为<strong>12</strong>维向量，<script type="math/tex">\mathbf{M}</script>维度<script type="math/tex">2n\times 12</script>，如下形式:</p><p><img width="75%" data-src="https://cdn.mathpix.com/snip/images/XRV8zRc_TojEnL-nnDpP-eSDrWxXmwLRJ0zOt5FmAzg.original.fullsize.png"></p><h3 id="未完待续…"><a href="#未完待续…" class="headerlink" title="未完待续…"></a>未完待续…</h3><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://blog.csdn.net/jessecw79/article/details/82945918" target="_blank" rel="noopener">深入EPnP算法</a></li><li><a href="https://zhuanlan.zhihu.com/p/46695068" target="_blank" rel="noopener">3d-2d位姿估计之EPnP算法</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;PNP&lt;/code&gt;即“Perspective-N-Points”，是求解 3D 到 2D 点对运动的方法。它描述了当我们知道n个3D空间点以及它们在图像上的位置时，如何估计相机所在的位姿。PnP 问题有很多种求解方法，例如用三对点估计位姿的 &lt;code&gt;P3P&lt;/code&gt;（通常需要额外一个点进行验证结果），直接线性变换（&lt;code&gt;DLT&lt;/code&gt;），&lt;code&gt;EPnP&lt;/code&gt;（Efficient PnP，已知内参时用），&lt;code&gt;UPnP&lt;/code&gt;（内参未知时用） 等等）。此外，还能用非线性优化的方式，构建最小二乘问题并迭代求解，也就是万金油式的 &lt;code&gt;Bundle Adjustment&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="位姿" scheme="https://www.vincentqin.tech/tags/%E4%BD%8D%E5%A7%BF/"/>
    
      <category term="PNP" scheme="https://www.vincentqin.tech/tags/PNP/"/>
    
  </entry>
  
  <entry>
    <title>SLAM常见问题(二)：重定位Relocalisation</title>
    <link href="https://www.vincentqin.tech/posts/slam-common-issues-relocalisation/"/>
    <id>https://www.vincentqin.tech/posts/slam-common-issues-relocalisation/</id>
    <published>2019-08-08T14:51:14.000Z</published>
    <updated>2019-09-10T13:50:17.457Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>可以说整个重定位就是一个精心设计的解算当前帧位姿的模块，秉持着不抛弃不放弃的精神，ORB-SLAM的作者简直把特征匹配压榨到了极致，仿佛在说“小伙子你有很多匹配点的，不要放弃，我们优化一下位姿再找找匹配点呗”。</p><a id="more"></a><p>原理如下流程图：</p><p><img alt="重定位" data-src="//www.vincentqin.tech/posts/slam-common-issues-relocalisation/relocalisation.svg"></p><p>代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bool</span> Tracking::Relocalization()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Compute Bag of Words Vector</span></span><br><span class="line">    <span class="comment">// 步骤1：计算当前帧特征点的Bow映射，能够得到当前帧的词袋向量以及featureVector</span></span><br><span class="line">    <span class="comment">// 可用于SearchByBoW寻找匹配特征点</span></span><br><span class="line">    mCurrentFrame.ComputeBoW();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Relocalization is performed when tracking is lost</span></span><br><span class="line">    <span class="comment">// Track Lost: Query KeyFrame Database for keyframe candidates for relocalisation</span></span><br><span class="line">    <span class="comment">// 步骤2：找到与当前帧相似的候选关键帧，</span></span><br><span class="line">    <span class="comment">// 这里会通过查询关键帧数据库进行快速查找与当前帧相似的候选重定位帧vpCandidateKFs</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;KeyFrame*&gt; vpCandidateKFs = mpKeyFrameDB-&gt;DetectRelocalizationCandidates(&amp;mCurrentFrame);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(vpCandidateKFs.empty())</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> nKFs = vpCandidateKFs.size();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We perform first an ORB matching with each candidate</span></span><br><span class="line">    <span class="comment">// If enough matches are found we setup a PnP solver</span></span><br><span class="line">    <span class="function">ORBmatcher <span class="title">matcher</span><span class="params">(<span class="number">0.75</span>,<span class="literal">true</span>)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">vector</span>&lt;PnPsolver*&gt; vpPnPsolvers;</span><br><span class="line">    vpPnPsolvers.resize(nKFs);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;MapPoint*&gt; &gt; vvpMapPointMatches;</span><br><span class="line">    vvpMapPointMatches.resize(nKFs);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; vbDiscarded;</span><br><span class="line">    vbDiscarded.resize(nKFs);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> nCandidates=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nKFs; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        KeyFrame* pKF = vpCandidateKFs[i];</span><br><span class="line">        <span class="keyword">if</span>(pKF-&gt;isBad())</span><br><span class="line">            vbDiscarded[i] = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 步骤3：通过BoW进行匹配</span></span><br><span class="line">            <span class="comment">// 利用SearchByBoW查找当前帧与关键帧的匹配点vvpMapPointMatches</span></span><br><span class="line">            <span class="keyword">int</span> nmatches = matcher.SearchByBoW(pKF,mCurrentFrame,vvpMapPointMatches[i]);</span><br><span class="line">            <span class="comment">// 如果匹配点数小于15个点，跳过</span></span><br><span class="line">            <span class="keyword">if</span>(nmatches&lt;<span class="number">15</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                vbDiscarded[i] = <span class="literal">true</span>;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 如果匹配点数大于15个点，建立当前帧与关键帧之间的PNP求解器；</span></span><br><span class="line">            <span class="comment">// 仅仅如建立这个求解器，还未求解</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 初始化PnPsolver</span></span><br><span class="line">                PnPsolver* pSolver = <span class="keyword">new</span> PnPsolver(mCurrentFrame,vvpMapPointMatches[i]);</span><br><span class="line">                pSolver-&gt;SetRansacParameters(<span class="number">0.99</span>,<span class="number">10</span>,<span class="number">300</span>,<span class="number">4</span>,<span class="number">0.5</span>,<span class="number">5.991</span>);</span><br><span class="line">                vpPnPsolvers[i] = pSolver;</span><br><span class="line">                nCandidates++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Alternatively perform some iterations of P4P RANSAC</span></span><br><span class="line">    <span class="comment">// Until we found a camera pose supported by enough inliers</span></span><br><span class="line">    <span class="keyword">bool</span> bMatch = <span class="literal">false</span>;</span><br><span class="line">    <span class="function">ORBmatcher <span class="title">matcher2</span><span class="params">(<span class="number">0.9</span>,<span class="literal">true</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果候选关键帧数大于0且没有重定位成功</span></span><br><span class="line">    <span class="keyword">while</span>(nCandidates&gt;<span class="number">0</span> &amp;&amp; !bMatch)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nKFs; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 若当前候选关键帧与当前帧匹配数量小于15，跳过</span></span><br><span class="line">            <span class="keyword">if</span>(vbDiscarded[i])</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Perform 5 Ransac Iterations</span></span><br><span class="line">            <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; vbInliers;</span><br><span class="line">            <span class="keyword">int</span> nInliers;</span><br><span class="line">            <span class="keyword">bool</span> bNoMore;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 步骤4：通过EPnP算法估计初始位姿</span></span><br><span class="line">            PnPsolver* pSolver = vpPnPsolvers[i];</span><br><span class="line">            cv::Mat Tcw = pSolver-&gt;iterate(<span class="number">5</span>,bNoMore,vbInliers,nInliers);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// If Ransac reachs max. iterations discard keyframe</span></span><br><span class="line">            <span class="comment">// 若RANSAC失败，当前候选关键帧被提出候选帧</span></span><br><span class="line">            <span class="keyword">if</span>(bNoMore)</span><br><span class="line">            &#123;</span><br><span class="line">                vbDiscarded[i]=<span class="literal">true</span>;</span><br><span class="line">                nCandidates--;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// If a Camera Pose is computed, optimize</span></span><br><span class="line">            <span class="comment">// PNP求解出了一个比较初始的位姿，比较粗糙，需要进一步优化</span></span><br><span class="line">            <span class="keyword">if</span>(!Tcw.empty())</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 把刚刚PNP求解的位姿赋给当前帧位姿</span></span><br><span class="line">                Tcw.copyTo(mCurrentFrame.mTcw);</span><br><span class="line"></span><br><span class="line">                <span class="built_in">set</span>&lt;MapPoint*&gt; sFound;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> np = vbInliers.size();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;np; j++)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">if</span>(vbInliers[j])</span><br><span class="line">                    &#123;</span><br><span class="line">                        mCurrentFrame.mvpMapPoints[j]=vvpMapPointMatches[i][j];</span><br><span class="line">                        sFound.insert(vvpMapPointMatches[i][j]);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                        mCurrentFrame.mvpMapPoints[j]=<span class="literal">NULL</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 步骤5：通过PoseOptimization对姿态进行优化求解</span></span><br><span class="line">                <span class="keyword">int</span> nGood = Optimizer::PoseOptimization(&amp;mCurrentFrame);</span><br><span class="line">                <span class="comment">// 内点小于10，跳过</span></span><br><span class="line">                <span class="keyword">if</span>(nGood&lt;<span class="number">10</span>)</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                <span class="comment">// 刚才的PO优化会滤除一些外点</span></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> io =<span class="number">0</span>; io&lt;mCurrentFrame.N; io++)</span><br><span class="line">                    <span class="keyword">if</span>(mCurrentFrame.mvbOutlier[io])</span><br><span class="line">                        mCurrentFrame.mvpMapPoints[io]=<span class="keyword">static_cast</span>&lt;MapPoint*&gt;(<span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// If few inliers, search by projection in a coarse window and optimize again</span></span><br><span class="line">                <span class="comment">// 步骤6：如果内点较少，则通过投影的方式对之前未匹配的点进行匹配，再进行优化求解</span></span><br><span class="line">                <span class="comment">// 作者认为10&lt;=nGood&lt;50时仍有可能重定位成功，由于PO调整了位姿，</span></span><br><span class="line">                <span class="comment">// 可以通过位姿投影的方式将候选关键帧上的地图点投影在当前帧上进行搜索匹配点，</span></span><br><span class="line">                <span class="comment">// 从而增加匹配，然后再优化以得到足够多的内点</span></span><br><span class="line">                <span class="keyword">if</span>(nGood&lt;<span class="number">50</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">int</span> nadditional =matcher2.SearchByProjection(mCurrentFrame,vpCandidateKFs[i],sFound,<span class="number">10</span>,<span class="number">100</span>);</span><br><span class="line">                    <span class="comment">// 新增的点与之前PO内点之和大于50，我们考虑再进行一遍优化</span></span><br><span class="line">                    <span class="keyword">if</span>(nadditional+nGood&gt;=<span class="number">50</span>)</span><br><span class="line">                    &#123;</span><br><span class="line">                        nGood = Optimizer::PoseOptimization(&amp;mCurrentFrame);</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// If many inliers but still not enough, search by projection again in a narrower window</span></span><br><span class="line">                        <span class="comment">// the camera has been already optimized with many points</span></span><br><span class="line">                        <span class="comment">// 不够多呀，不要放弃，再来一遍</span></span><br><span class="line">                        <span class="keyword">if</span>(nGood&gt;<span class="number">30</span> &amp;&amp; nGood&lt;<span class="number">50</span>)</span><br><span class="line">                        &#123;</span><br><span class="line">                            sFound.clear();</span><br><span class="line">                            <span class="keyword">for</span>(<span class="keyword">int</span> ip =<span class="number">0</span>; ip&lt;mCurrentFrame.N; ip++)</span><br><span class="line">                                <span class="keyword">if</span>(mCurrentFrame.mvpMapPoints[ip])</span><br><span class="line">                                    sFound.insert(mCurrentFrame.mvpMapPoints[ip]);</span><br><span class="line">                            nadditional =matcher2.SearchByProjection(mCurrentFrame,vpCandidateKFs[i],sFound,<span class="number">3</span>,<span class="number">64</span>);</span><br><span class="line"></span><br><span class="line">                            <span class="comment">// Final optimization</span></span><br><span class="line">                            <span class="comment">// 最后一次优化啦~</span></span><br><span class="line">                            <span class="keyword">if</span>(nGood+nadditional&gt;=<span class="number">50</span>)</span><br><span class="line">                            &#123;</span><br><span class="line">                                nGood = Optimizer::PoseOptimization(&amp;mCurrentFrame);</span><br><span class="line"></span><br><span class="line">                                <span class="keyword">for</span>(<span class="keyword">int</span> io =<span class="number">0</span>; io&lt;mCurrentFrame.N; io++)</span><br><span class="line">                                    <span class="keyword">if</span>(mCurrentFrame.mvbOutlier[io])</span><br><span class="line">                                        mCurrentFrame.mvpMapPoints[io]=<span class="literal">NULL</span>;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// If the pose is supported by enough inliers stop ransacs and continue</span></span><br><span class="line">                <span class="comment">// 只要找到一个候选关键帧与当前帧的匹配点数大于50就重定位成功！</span></span><br><span class="line">                <span class="keyword">if</span>(nGood&gt;=<span class="number">50</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    bMatch = <span class="literal">true</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(!bMatch)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        mnLastRelocFrameId = mCurrentFrame.mnId;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;可以说整个重定位就是一个精心设计的解算当前帧位姿的模块，秉持着不抛弃不放弃的精神，ORB-SLAM的作者简直把特征匹配压榨到了极致，仿佛在说“小伙子你有很多匹配点的，不要放弃，我们优化一下位姿再找找匹配点呗”。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="ORB" scheme="https://www.vincentqin.tech/tags/ORB/"/>
    
      <category term="Relocalisation" scheme="https://www.vincentqin.tech/tags/Relocalisation/"/>
    
      <category term="重定位" scheme="https://www.vincentqin.tech/tags/%E9%87%8D%E5%AE%9A%E4%BD%8D/"/>
    
  </entry>
  
  <entry>
    <title>SLAM常见问题(一)：SearchByBoW</title>
    <link href="https://www.vincentqin.tech/posts/slam-common-issues-SearchbyBoW/"/>
    <id>https://www.vincentqin.tech/posts/slam-common-issues-SearchbyBoW/</id>
    <published>2019-08-04T15:18:14.000Z</published>
    <updated>2019-08-04T16:12:16.670Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>ORB-SLAM</strong>中使用了多种特征匹配的奇技淫巧，其中之一就是利用<strong>词袋信息</strong>进行引导匹配<code>SearchByBoW</code>：利用了<code>BOW</code>里的正向引导进行两帧之间的匹配，核心点在于位于同一个节点处的特征才有可能属于同一匹配，相较于暴力匹配匹配速度更快。</p><a id="more"></a><p>注意：每幅图像都可以通过<code>ComputeBoW</code>得到其对应的词袋向量。<code>featureVector</code>存储的是节点的索引值以及对应图像feature对应的索引向量，即<code>map&lt;node_id,vector&lt;featureID&gt;</code>。这样的话就可以根据两帧图像的<code>node_id</code>来初步确定二者共有的特征点，然后根据该<code>id</code>取出<code>vector&lt;featureID&gt;</code>，根据featureID找到图像上的特征点以及描述子，通过比较二者描述子距离来判定该特征点是否为匹配点，若距离小于某一阈值，则二者为匹配对。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief Bag of Words Representation</span></span><br><span class="line"><span class="comment"> * 计算词袋mBowVec和mFeatVec</span></span><br><span class="line"><span class="comment"> * @see CreateInitialMapMonocular() TrackReferenceKeyFrame() Relocalization()</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//同样的，关键帧也有构造函数void KeyFrame::ComputeBoW()</span></span><br><span class="line"><span class="keyword">void</span> Frame::ComputeBoW() </span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(mBowVec.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 将描述子mDescriptors转换为DBOW要求的输入格式</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;cv::Mat&gt; vCurrentDesc = Converter::toDescriptorVector(mDescriptors);</span><br><span class="line">        <span class="comment">// 转换成词袋向量mBowVec以及特征向量mFeatVec</span></span><br><span class="line">        <span class="comment">// mBowVec存储着单词及其对应的权重TF-IDF值</span></span><br><span class="line">        <span class="comment">// mFeatVec存储节点ID以及对应对应图像feature对应的索引向量</span></span><br><span class="line">        mpORBvocabulary-&gt;transform(vCurrentDesc,mBowVec,mFeatVec,<span class="number">4</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该函数在<code>Tracking</code>线程中的<code>TrackReferenceKeyFrame()</code>/<code>Relocalization()</code>进行调用（注意：<code>LoopClosing</code>线程中<code>ComputeSim3()</code>也会调用该函数，与上述二者的区别在于，ComputeSim3()中的SearchByBoW是寻找关键帧之间的匹配，而非关键帧与当前帧之间的匹配）。</p><p>下面给出<code>ORB-SLAM2</code>中用于<strong>关键帧与当前帧</strong>进行词袋引导匹配的源码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief 通过词袋，对关键帧的特征点进行跟踪</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 通过bow对pKF和F中的特征点进行快速匹配（不属于同一node的特征点直接跳过匹配） \n</span></span><br><span class="line"><span class="comment"> * 对属于同一node的特征点通过描述子距离进行匹配 \n</span></span><br><span class="line"><span class="comment"> * 根据匹配，用pKF中特征点对应的MapPoint去更新F中特征点对应的MapPoints \n</span></span><br><span class="line"><span class="comment"> * 每个特征点都对应一个MapPoint，因此pKF中每个特征点的MapPoint也就是F中对应点的MapPoint \n</span></span><br><span class="line"><span class="comment"> * 通过距离阈值、比例阈值和角度投票进行剔除误匹配</span></span><br><span class="line"><span class="comment"> * @param  pKF               KeyFrame</span></span><br><span class="line"><span class="comment"> * @param  F                 Current Frame</span></span><br><span class="line"><span class="comment"> * @param  vpMapPointMatches F中MapPoints对应的匹配，NULL表示未匹配</span></span><br><span class="line"><span class="comment"> * @return                   成功匹配的数量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> <span class="comment">//const int ORBmatcher::TH_HIGH = 100;</span></span><br><span class="line"> <span class="comment">//const int ORBmatcher::TH_LOW = 50;</span></span><br><span class="line"> <span class="comment">//const int ORBmatcher::HISTO_LENGTH = 30;</span></span><br><span class="line"><span class="keyword">int</span> ORBmatcher::SearchByBoW(KeyFrame* pKF,Frame &amp;F, <span class="built_in">vector</span>&lt;MapPoint*&gt; &amp;vpMapPointMatches)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// vpMapPointsKF：获取输入关键帧匹配到的地图点</span></span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;MapPoint*&gt; vpMapPointsKF = pKF-&gt;GetMapPointMatches();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化当前帧MapPoints对应的匹配NULL</span></span><br><span class="line">    vpMapPointMatches = <span class="built_in">vector</span>&lt;MapPoint*&gt;(F.N,<span class="keyword">static_cast</span>&lt;MapPoint*&gt;(<span class="literal">NULL</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// FeatureVector数据类型 map&lt;node_id,vector&lt;featureID&gt;，</span></span><br><span class="line">    <span class="comment">// 可以快速根据node_id找到属于该node的特征点</span></span><br><span class="line">    <span class="keyword">const</span> DBoW2::FeatureVector &amp;vFeatVecKF = pKF-&gt;mFeatVec;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> nmatches=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; rotHist[HISTO_LENGTH];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;HISTO_LENGTH;i++)</span><br><span class="line">        rotHist[i].reserve(<span class="number">500</span>);</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span> factor = HISTO_LENGTH/<span class="number">360.0f</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We perform the matching over ORB that belong to the same vocabulary node </span></span><br><span class="line">    <span class="comment">// (at a certain level)</span></span><br><span class="line">    <span class="comment">// 建立迭代器，将属于同一节点(特定层)的ORB特征进行匹配</span></span><br><span class="line">    DBoW2::FeatureVector::const_iterator KFit = vFeatVecKF.begin();</span><br><span class="line">    DBoW2::FeatureVector::const_iterator Fit = F.mFeatVec.begin();</span><br><span class="line">    DBoW2::FeatureVector::const_iterator KFend = vFeatVecKF.end();</span><br><span class="line">    DBoW2::FeatureVector::const_iterator Fend = F.mFeatVec.end();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(KFit != KFend &amp;&amp; Fit != Fend)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 步骤1：分别取出属于同一node的ORB特征点(只有属于同一node，才有可能是匹配点)</span></span><br><span class="line">        <span class="comment">// first表示node_id，只有node_id相同才表示这些特征点位于同一层</span></span><br><span class="line">        <span class="keyword">if</span>(KFit-&gt;first == Fit-&gt;first) </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// second中记录了这些特征对应图像中的ID</span></span><br><span class="line">            <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt; vIndicesKF = KFit-&gt;second;</span><br><span class="line">            <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt; vIndicesF = Fit-&gt;second;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 步骤2：遍历KF中属于该node的特征点</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">size_t</span> iKF=<span class="number">0</span>; iKF&lt;vIndicesKF.size(); iKF++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 获取关键帧上某一个特征点的ID</span></span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> realIdxKF = vIndicesKF[iKF];</span><br><span class="line">                <span class="comment">// 根据该ID得到该特征对应的MapPoint</span></span><br><span class="line">                MapPoint* pMP = vpMapPointsKF[realIdxKF]; </span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(!pMP) <span class="comment">//不存在</span></span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(pMP-&gt;isBad())<span class="comment">//被标记为坏点</span></span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 根据该ID取出KF中该特征对应的描述子</span></span><br><span class="line">                <span class="keyword">const</span> cv::Mat &amp;dKF= pKF-&gt;mDescriptors.row(realIdxKF); </span><br><span class="line"></span><br><span class="line">                <span class="keyword">int</span> bestDist1=<span class="number">256</span>; <span class="comment">// 最好的距离（最小距离）</span></span><br><span class="line">                <span class="keyword">int</span> bestIdxF =<span class="number">-1</span> ;</span><br><span class="line">                <span class="keyword">int</span> bestDist2=<span class="number">256</span>; <span class="comment">// 倒数第二好距离（倒数第二小距离）</span></span><br><span class="line"></span><br><span class="line">                <span class="comment">// 步骤3：遍历当前帧中属于该node的特征点，找到了最佳匹配点</span></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">size_t</span> iF=<span class="number">0</span>; iF&lt;vIndicesF.size(); iF++)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// 取出当前帧上位于该node上的某一个特征点的ID</span></span><br><span class="line">                    <span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> realIdxF = vIndicesF[iF];</span><br><span class="line">                    <span class="comment">// 表明这个点已经被匹配过了，不再匹配，加快速度</span></span><br><span class="line">                    <span class="keyword">if</span>(vpMapPointMatches[realIdxF])</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    <span class="comment">// 取出F中该特征对应的描述子</span></span><br><span class="line">                    <span class="keyword">const</span> cv::Mat &amp;dF = F.mDescriptors.row(realIdxF); </span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 计算描述子距离，这里是汉明距离，若非二进制描述子可选择用其他距离</span></span><br><span class="line">                    <span class="keyword">const</span> <span class="keyword">int</span> dist =  DescriptorDistance(dKF,dF); </span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 下面的操作就是分别获得最小bestDist1以及次小bestDist2的描述子距离</span></span><br><span class="line">                    <span class="comment">// dist &lt; bestDist1 &lt; bestDist2，更新bestDist1 bestDist2</span></span><br><span class="line">                    <span class="keyword">if</span>(dist&lt;bestDist1)</span><br><span class="line">                    &#123;</span><br><span class="line">                        bestDist2=bestDist1;</span><br><span class="line">                        bestDist1=dist;</span><br><span class="line">                        bestIdxF=realIdxF;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span>(dist&lt;bestDist2)<span class="comment">// bestDist1 &lt; dist &lt; bestDist2，更新bestDist2</span></span><br><span class="line">                    &#123;</span><br><span class="line">                        bestDist2=dist;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 步骤4：根据描述子距离阈值和角度投票剔除误匹配</span></span><br><span class="line">                <span class="comment">// 最小的描述子距离小于一个阈值 </span></span><br><span class="line">                <span class="keyword">if</span>(bestDist1&lt;=TH_LOW) </span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// trick!</span></span><br><span class="line">                    <span class="comment">// 最佳匹配比次佳匹配明显要好，那么最佳匹配才真正靠谱</span></span><br><span class="line">                    <span class="keyword">if</span>(<span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(bestDist1)&lt;</span><br><span class="line">                    mfNNratio*<span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(bestDist2))</span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="comment">// 步骤5：更新当前帧特征点的MapPoint</span></span><br><span class="line">                        <span class="comment">// 记录了特征点ID以及对应的特征点</span></span><br><span class="line">                        vpMapPointMatches[bestIdxF]=pMP;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 获得关键帧上的特征点位置</span></span><br><span class="line">                        <span class="keyword">const</span> cv::KeyPoint &amp;kp = pKF-&gt;mvKeysUn[realIdxKF];</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//</span></span><br><span class="line">                        <span class="keyword">if</span>(mbCheckOrientation)</span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="comment">// trick!</span></span><br><span class="line">                            <span class="comment">// angle：每个特征点在提取描述子时的旋转主方向角度，</span></span><br><span class="line">                            <span class="comment">// 如果图像旋转了，这个角度将发生改变</span></span><br><span class="line">                            <span class="comment">// 所有的特征点的角度变化应该是一致的，</span></span><br><span class="line">                            <span class="comment">// 通过直方图统计得到最准确的角度变化值</span></span><br><span class="line">                            <span class="comment">// 该特征点的角度变化值</span></span><br><span class="line">                            <span class="keyword">float</span> rot = kp.angle-F.mvKeys[bestIdxF].angle;</span><br><span class="line">                            <span class="keyword">if</span>(rot&lt;<span class="number">0.0</span>)</span><br><span class="line">                                rot+=<span class="number">360.0f</span>;</span><br><span class="line">                            <span class="keyword">int</span> bin = round(rot*factor);<span class="comment">// 将rot分配到bin组</span></span><br><span class="line">                            <span class="keyword">if</span>(bin==HISTO_LENGTH)</span><br><span class="line">                                bin=<span class="number">0</span>;</span><br><span class="line">                            assert(bin&gt;=<span class="number">0</span> &amp;&amp; bin&lt;HISTO_LENGTH);</span><br><span class="line">                            rotHist[bin].push_back(bestIdxF);</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="comment">// 匹配点+1</span></span><br><span class="line">                        nmatches++;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            KFit++;</span><br><span class="line">            Fit++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(KFit-&gt;first &lt; Fit-&gt;first)</span><br><span class="line">        &#123;</span><br><span class="line">            KFit = vFeatVecKF.lower_bound(Fit-&gt;first);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            Fit = F.mFeatVec.lower_bound(KFit-&gt;first);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据方向剔除误匹配的点，即删除那些不属于特征点角度变化最多的三个类别的匹配点</span></span><br><span class="line">    <span class="keyword">if</span>(mbCheckOrientation)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> ind1=<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">int</span> ind2=<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">int</span> ind3=<span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 计算rotHist中最大的三个的index</span></span><br><span class="line">        ComputeThreeMaxima(rotHist,HISTO_LENGTH,ind1,ind2,ind3);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;HISTO_LENGTH; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果特征点的旋转角度变化量属于这三个组，则保留</span></span><br><span class="line">            <span class="keyword">if</span>(i==ind1 || i==ind2 || i==ind3)</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 将除了ind1 ind2 ind3以外的匹配点去掉</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">size_t</span> j=<span class="number">0</span>, jend=rotHist[i].size(); j&lt;jend; j++)</span><br><span class="line">            &#123;</span><br><span class="line">                vpMapPointMatches[rotHist[i][j]]=<span class="keyword">static_cast</span>&lt;MapPoint*&gt;(<span class="literal">NULL</span>);</span><br><span class="line">                nmatches--;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nmatches;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外，<code>LoopClosing</code>线程中<code>ComputeSim3()</code>调用的<code>SearchByBoW</code>的函数声明为：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> ORBmatcher::SearchByBoW(KeyFrame *pKF1, KeyFrame *pKF2, <span class="built_in">vector</span>&lt;MapPoint *&gt; &amp;vpMatches12)</span><br></pre></td></tr></table></figure></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://blog.csdn.net/qq_24893115/article/details/52629248" target="_blank" rel="noopener">https://blog.csdn.net/qq_24893115/article/details/52629248</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;ORB-SLAM&lt;/strong&gt;中使用了多种特征匹配的奇技淫巧，其中之一就是利用&lt;strong&gt;词袋信息&lt;/strong&gt;进行引导匹配&lt;code&gt;SearchByBoW&lt;/code&gt;：利用了&lt;code&gt;BOW&lt;/code&gt;里的正向引导进行两帧之间的匹配，核心点在于位于同一个节点处的特征才有可能属于同一匹配，相较于暴力匹配匹配速度更快。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="ORB" scheme="https://www.vincentqin.tech/tags/ORB/"/>
    
      <category term="特征匹配" scheme="https://www.vincentqin.tech/tags/%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D/"/>
    
  </entry>
  
  <entry>
    <title>2019年浙大CADCG暑假SLAM培训部分课件</title>
    <link href="https://www.vincentqin.tech/posts/slam-summer-courses-CADCG-Lab/"/>
    <id>https://www.vincentqin.tech/posts/slam-summer-courses-CADCG-Lab/</id>
    <published>2019-07-20T13:52:15.000Z</published>
    <updated>2020-04-17T13:35:50.309Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>由浙江大学CAD&amp;CG国家重点实验室主办、浙江大学-商汤三维视觉联合实验室协办的“SLAM技术及应用”暑期学校于7月20日如期拉开序幕。<br>今天（2019/07/20）看到直播的时候已经是下午4点半了，只听到刘浩敏讲到末尾的一段，幸好主办方提供了讲座课件，Download下来慢慢看。</p><a id="more"></a><h2 id="2019年课件"><a href="#2019年课件" class="headerlink" title="2019年课件"></a>2019年课件</h2><ul><li><p>2019年7月20日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/Camera-model-and-Projection-Transformation.pdf" target="_blank" rel="noopener">相机模型与投影变换</a>（讲者：章国锋）</p></li><li><p>2019年7月20日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/SfM-GuofengZhang.pdf" target="_blank" rel="noopener">运动恢复结构</a>（讲者：章国锋）</p></li><li><p>2019年7月20日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/BA-haominLiu.pdf" target="_blank" rel="noopener">集束<strong>Bundle Adjustment</strong>调整</a>（讲者：刘浩敏）</p></li><li><p>2019年7月21日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/3D-Tutorial-ShuhanShen.pdf" target="_blank" rel="noopener">三维重建</a>（讲者：申抒含）</p></li><li><p>2019年7月21日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/V-SLAM-GuofengZhang.pdf" target="_blank" rel="noopener">视觉SLAM</a>（讲者：章国锋）</p></li><li><p>2019年7月21日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/RGB-D-SLAM-HanqingJiang.pdf" target="_blank" rel="noopener">RGB-D SLAM</a>（讲者：姜翰青）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/VI-SLAM.pdf" target="_blank" rel="noopener">视觉惯性SLAM</a>（讲者：黄国权）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/3D-recognition-track-XueyingQin.pdf" target="_blank" rel="noopener">三维物体的识别与跟踪</a>（讲者：秦学英）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/R2VR-VR-Fusion-Technology.pdf" target="_blank" rel="noopener">从现实到虚拟现实-虚实融合呈现技术</a>（讲者：王锐）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/Mav-DanpingZou.pdf" target="_blank" rel="noopener">面向SLAM研究的无人机快速入门与平台选择</a>（讲者：邹丹平）</p></li><li><p>2019年7月23日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/MobileVR-System-Design-Application.pdf" target="_blank" rel="noopener">移动增强现实系统的设计与应用案例解析</a>（讲者：章国锋）</p></li><li><p>2019年7月23日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/AR-Applications.pdf" target="_blank" rel="noopener">AR应用开发</a>（讲者：盛崇山）</p></li><li><p><strong><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2019-SLAM-Summer-School-slides.zip" target="_blank" rel="noopener">打包下载</a></strong></p></li></ul><h2 id="2018年课件"><a href="#2018年课件" class="headerlink" title="2018年课件"></a>2018年课件</h2><ul><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E7%9B%B8%E6%9C%BA%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%8A%95%E5%BD%B1%E5%8F%98%E6%8D%A2-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">相机模型与投影变换-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E8%BF%90%E5%8A%A8%E6%81%A2%E5%A4%8D%E7%BB%93%E6%9E%84-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">运动恢复结构-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E9%9B%86%E6%9D%9F%E8%B0%83%E6%95%B4-%E5%88%98%E6%B5%A9%E6%95%8F.pdf" target="_blank" rel="noopener">集束调整-刘浩敏</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E6%B7%B1%E5%BA%A6%E6%81%A2%E5%A4%8D%E4%B8%8E%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">深度恢复与三维重建-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E8%A7%86%E8%A7%89SLAM-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">视觉SLAM-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E8%A7%86%E8%A7%89%E6%83%AF%E5%AF%BCSLAM-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">视觉惯导SLAM-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/Visual-Inertial%20SLAM-%E6%9D%8E%E5%90%8D%E6%9D%A8.pdf" target="_blank" rel="noopener">Visual-Inertial SLAM-李名杨</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/RGB-D%20SLAM-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">RGB-D SLAM-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E5%9C%B0%E9%9D%A2%E6%97%A0%E4%BA%BA%E5%B9%B3%E5%8F%B0%E4%B8%AD%E7%9A%84SLAM%E6%8A%80%E6%9C%AF-%E5%88%98%E5%8B%87.pdf" target="_blank" rel="noopener">地面无人平台中的SLAM技术-刘勇</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E5%9F%BA%E4%BA%8E%E7%8E%AF%E5%A2%83%E7%BB%93%E6%9E%84%E5%8C%96%E7%89%B9%E6%80%A7%E7%9A%84%E8%A7%86%E8%A7%89SLAM%E6%96%B9%E6%B3%95-%E9%82%B9%E4%B8%B9%E5%B9%B3.pdf" target="_blank" rel="noopener">基于环境结构化特性的视觉SLAM方法-邹丹平</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E7%A7%BB%E5%8A%A8%E5%A2%9E%E5%BC%BA%E7%8E%B0%E5%AE%9E%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B%E8%A7%A3%E6%9E%90-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">移动增强现实系统设计与应用案例解析-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/VR-Ruiwang1.pdf" target="_blank" rel="noopener">虚实融合显示与绘制技术-王锐</a></p></li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://mp.weixin.qq.com/s/PV_xLmuE-HpnUgnJ5GyMOA" target="_blank" rel="noopener">第二届“SLAM技术及应用” 暑期学校与研讨会圆满落幕</a></li><li>章国锋主页：<a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank" rel="noopener">地址</a></li><li>CAD&amp;CG实验室主页，<a href="http://www.zjucvg.net/" target="_blank" rel="noopener">地址</a></li><li><a href="https://github.com/zju3dv" target="_blank" rel="noopener">CAD&amp;CG Github</a></li><li><a href="http://www.zjucvg.net/senseslam/" target="_blank" rel="noopener">SenseSLAM</a>,浙大-商汤三维视觉联合实验室</li><li>Shuhan Shen (申抒含)主页，<a href="http://vision.ia.ac.cn/Faculty/shshen/index.htm" target="_blank" rel="noopener">地址</a></li><li>姜翰青，<a href="https://www.linkedin.com/in/%E7%BF%B0%E9%9D%92-%E5%A7%9C-1194b411b/`" target="_blank" rel="noopener">Linkedin</a></li><li>讲座直播地址：<a href="https://www.douyu.com/7275221" target="_blank" rel="noopener">https://www.douyu.com/7275221</a></li><li>商汤泰坦公开课，<a href="https://cloud.xylink.com/live/v/2c9497116bb8b075016c082adec66ea7" target="_blank" rel="noopener">直播地址</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由浙江大学CAD&amp;amp;CG国家重点实验室主办、浙江大学-商汤三维视觉联合实验室协办的“SLAM技术及应用”暑期学校于7月20日如期拉开序幕。&lt;br&gt;今天（2019/07/20）看到直播的时候已经是下午4点半了，只听到刘浩敏讲到末尾的一段，幸好主办方提供了讲座课件，Download下来慢慢看。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>🔨工具：Filebrowser：一款轻量级个人网盘</title>
    <link href="https://www.vincentqin.tech/posts/build-filebrowser/"/>
    <id>https://www.vincentqin.tech/posts/build-filebrowser/</id>
    <published>2019-07-14T13:48:47.000Z</published>
    <updated>2020-08-26T15:14:39.075Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><br><br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/filebrowser-banner.png"><br><br></p><a id="more"></a><h1 id="个人网盘-Filebrowser"><a href="#个人网盘-Filebrowser" class="headerlink" title="个人网盘 Filebrowser"></a>个人网盘 Filebrowser</h1><p>服务器仅仅用于科学上网未免有些浪费了，是时候尝试一下自建个人网盘和图床了。</p><h2 id="如何安装"><a href="#如何安装" class="headerlink" title="如何安装"></a>如何安装</h2><p><a href="https://filebrowser.xyz/installation" target="_blank" rel="noopener">官方</a>给出了一键安装大法，进入服务器输入以下命令就可以了。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://filebrowser.xyz/get.sh | bash</span><br></pre></td></tr></table></figure></p><h2 id="首次配置"><a href="#首次配置" class="headerlink" title="首次配置"></a>首次配置</h2><p>当安装好之后，你并不能立即使用它，需要修改一些配置(以下内容参考[<a href="https://www.mivm.cn/filebrowser/" target="_blank" rel="noopener">米V米</a>]的教程)。</p><ul><li>创建配置数据库：<code>filebrowser -d /etc/filebrowser.db config init</code></li><li>设置监听地址：<code>filebrowser -d /etc/filebrowser.db config set --address 0.0.0.0</code></li><li>设置监听端口：<code>filebrowser -d /etc/filebrowser.db config set --port 8088</code></li><li>设置语言环境(中文)：<code>filebrowser -d /etc/filebrowser.db config set --locale zh-cn</code></li><li>设置日志位置：<code>filebrowser -d /etc/filebrowser.db config set --log /var/log/filebrowser.log</code></li><li>添加一个用户：<code>filebrowser -d /etc/filebrowser.db users add root password --perm.admin</code>，其中的root和password分别是用户名和密码，根据自己的需求更改。</li></ul><p>有关更多配置的选项，可以参考官方文档：<a href="https://docs.filebrowser.xyz/" target="_blank" rel="noopener">https://docs.filebrowser.xyz/</a><br>配置修改好以后，就可以启动FileBrowser了，使用-d参数指定配置数据库路径。示例：<code>filebrowser -d /etc/filebrowser.db</code><br>启动成功就可以使用浏览器访问FileBrowser了，在浏览器输入 <code>服务器IP:端口</code>，示例：<code>http://192.168.1.1:8088</code></p><p>然后会看到 FileBrowser 的登陆界面：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/filebrowser-login.png"></p><p>用刚刚创建的用户登陆，最后就可以放心使用啦~<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/filebrowser-demo.gif"></p><h2 id="后续配置"><a href="#后续配置" class="headerlink" title="后续配置"></a>后续配置</h2><p>完成以上过程之后已经可以正常访问个人网盘了，但是假如服务器重启之后就必须重新输入<code>filebrowser -d /etc/filebrowser.db</code>才能运行，为了省去这一步，我们需要进行设置服务器开机自动启动FileBrowser。<br>这里我们使用的是systemd 大法：<br>首先下载 FileBrowser 的 <code>service</code>文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl https://cdn.mivm.cn/www.mivm.cn/archives/filebrowser/filebrowser.service -o /lib/systemd/system/filebrowser.service</span><br></pre></td></tr></table></figure><p>如果你的运行命令不是<code>/usr/local/bin/filebrowser -d /etc/filebrowser.db</code>，需要对 service 文件进行修改，将文件的 ExecStart 改为你的运行命令，更改完成后需要输入<code>systemctl daemon-reload</code>。</p><p>下面祭出常用的命令：</p><ul><li>运行：<code>systemctl start filebrowser.service</code></li><li>停止运行：<code>systemctl stop filebrowser.service</code></li><li>开机启动：<code>systemctl enable filebrowser.service</code></li><li>取消开机启动：<code>systemctl disable filebrowser.service</code></li><li>查看运行状态：<code>systemctl status filebrowser.service</code></li></ul><p>这里有个<a href="https://youtu.be/sE31MBvOjxk" target="_blank" rel="noopener">视频教程</a>，需要科学上网查看。</p><p><br></p><h1 id="个人图床Chevereto"><a href="#个人图床Chevereto" class="headerlink" title="个人图床Chevereto"></a>个人图床Chevereto</h1><p>先给出安装好的样子~<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/Chevereto.png"></p><p>安装教程这里<a href="https://gist.github.com/biezhi/f90923b48863c7d745481ccdd678ccab" target="_blank" rel="noopener">install_chevereto.md</a>已经写得非常详细了，在此不做详细介绍。这里有个<a href="https://youtu.be/kShgzNkXRak" target="_blank" rel="noopener">视频教程</a>，我主要按照这个教程进行配置的，需要科学上网查看。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://youtu.be/sE31MBvOjxk" target="_blank" rel="noopener">把玩我的 VPS 主机 - 分分钟搭建时尚简洁的在线网盘</a></li><li><a href="https://youtu.be/kShgzNkXRak" target="_blank" rel="noopener">搭建漂亮的私人图床</a></li><li><a href="https://biezhi.me/" target="_blank" rel="noopener">王爵 nice的主页</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;img src=&quot;https://vincentqin.gitee.io/blogresource-1/build-filebrowser/filebrowser-banner.png&quot; alt&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="个人网盘" scheme="https://www.vincentqin.tech/tags/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%9B%98/"/>
    
  </entry>
  
  <entry>
    <title>📝笔记：SuperPoint: Self-Supervised Interest Point Detection and Description 自监督深度学习特征点</title>
    <link href="https://www.vincentqin.tech/posts/superpoint/"/>
    <id>https://www.vincentqin.tech/posts/superpoint/</id>
    <published>2019-06-23T06:02:06.000Z</published>
    <updated>2020-08-26T15:10:19.746Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文出自近几年备受瞩目的创业公司<a href="https://www.magicleap.com/" target="_blank" rel="noopener">MagicLeap</a>，发表在CVPR 2018,一作<a href="http://www.danieldetone.com/" target="_blank" rel="noopener">Daniel DeTone</a>，<strong>[<a href="https://arxiv.org/abs/1712.07629" target="_blank" rel="noopener">paper</a>]</strong>，<strong>[<a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork/blob/master/assets/DL4VSLAM_talk.pdf" target="_blank" rel="noopener">slides</a>]</strong>，<strong>[<a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork" target="_blank" rel="noopener">code</a>]</strong>。</p><p>这篇文章设计了一种自监督网络框架，能够同时提取特征点的位置以及描述子。相比于patch-based方法，本文提出的算法能够在原始图像提取到像素级精度的特征点的位置及其描述子。<br>本文提出了一种单映性适应（<code>Homographic Adaptation</code>）的策略以增强特征点的复检率以及跨域的实用性（这里跨域指的是synthetic-to-real的能力，网络模型在虚拟数据集上训练完成，同样也可以在真实场景下表现优异的能力）。</p><a id="more"></a><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>诸多应用（诸如SLAM/SfM/相机标定/立体匹配）的首要一步就是特征点提取，这里的特征点指的是<strong>能够在不同光照&amp;不同视角下都能够稳定且可重复检测的2D图像点位置</strong>。</p><p>基于CNN的算法几乎在以图像作为输入的所有领域表现出相比于人类特征工程更加优秀的表达能力。目前已经有一些工作做类似的任务，例如人体位姿估计,目标检测以及室内布局估计等。这些算法以通常以大量的人工标注作为GT，这些精心设计的网络用来训练以得到人体上的角点，例如嘴唇的边缘点亦或人体的关节点，但是这里的问题是这里的点实际是ill-defined（我的理解是，这些点有可能是特征点，但仅仅是一个大概的位置，是特征点的子集，并没有真正的把特征点的概念定义清楚）。</p><p>本文采用了非人工监督的方法提取真实场景的特征点。本文设计了一个由特征点检测器监督的具有伪真值数据集，而非是大量的人工标记。为了得到伪真值，本文首先在大量的虚拟数据集上训练了一个全卷积网络（FCNN），这些虚拟数据集由一些基本图形组成，例如有线段、三角形、矩形和立方体等，这些基本图形具有没有争议的特征点位置，文中称这些特征点为<code>MagicPoint</code>，这个pre-trained的检测器就是<code>MagicPoint</code>检测器。这些<code>MagicPoint</code>在虚拟场景的中检测特征点的性能明显优于传统方式，但是在真实的复杂场景中表现不佳，此时作者提出了一种多尺度多变换的方法<code>Homographic Adaptation</code>。对于输入图像而言，<code>Homographic Adaptation</code>通过对图像进行多次不同的尺度/角度变换来帮助网络能够在不同视角不同尺度观测到特征点。<br>综上：<strong>SuperPoint = MagicPoint+Homographic Adaptation</strong></p><h1 id="算法优劣对比"><a href="#算法优劣对比" class="headerlink" title="算法优劣对比"></a>算法优劣对比</h1><p><img alt="fig1_table1" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/tab_1.png"></p><ul><li>基于图像块的算法导致特征点位置精度不够准确；</li><li>特征点与描述子分开进行训练导致运算资源的浪费，网络不够精简，实时性不足；或者仅仅训练特征点或者描述子的一种，不能用同一个网络进行联合训练；</li></ul><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img alt="fig3" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_3.png"></p><p>上图可见特征点检测器以及描述子网络共享一个单一的前向encoder，只是在decoder时采用了不同的结构，根据任务的不同学习不同的网络参数。这也是本框架与其他网络的不同之处：其他网络采用的是先训练好特征点检测网络，然后再去进行对特征点描述网络进行训练。<br>网络共分成以下4个主要部分，在此进行详述：</p><h2 id="1-Shared-Encoder-共享的编码网络"><a href="#1-Shared-Encoder-共享的编码网络" class="headerlink" title="1. Shared Encoder 共享的编码网络"></a>1. Shared Encoder 共享的编码网络</h2><p>从上图可以看到，整体而言，本质上有两个网络，只是前半部分共享了一部分而已。本文利用了VGG-style的encoder以用于降低图像尺寸，encoder包括卷积层，max-pooling层，以及非线性激活层。通过3个max-pooling层将图像的尺寸变成$H_c = H/8$和$H_c = H/8$，经过encoder之后，图像由$I \in \mathcal{R}^{H \times W}$变为张量$\mathcal{B} \in \mathbb{R}^{H_c \times W_c \times F}$</p><h2 id="2-Interest-Point-Decoder"><a href="#2-Interest-Point-Decoder" class="headerlink" title="2. Interest Point Decoder"></a>2. Interest Point Decoder</h2><p><img alt="fig_10_magicPoint1" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_10_magicPoint1.png"></p><p>这里介绍的是特征点的解码端。每个像素的经过该解码器的输出是该像素是特征点的概率（probability of “point-ness”）。<br>通常而言，我们可以通过反卷积得到上采样的图像，但是这种操作会导致计算量的骤增以及会引入一种“checkerboard artifacts”。因此本文设计了一种带有“特定解码器”（这种解码器没有参数）的特征点检测头以减小模型计算量（子像素卷积）。<br>例如：输入张量的维度是$\mathbb{R}^{H_c \times W_c \times 65}$，输出维度$\mathbb{R}^{H \times W}$，即图像的尺寸。这里的65表示原图$8 \times 8$的局部区域，加上一个非特征点<code>dustbin</code>。通过在channel维度上做softmax，非特征点dustbin会被删除，同时会做一步图像的<code>reshape</code>：$\mathbb{R}^{H_c \times W_c \times 64} \Rightarrow \mathbb{R}^{H \times W}$ 。（这就是<strong><a href="https://blog.csdn.net/leviopku/article/details/84975282" target="_blank" rel="noopener">子像素卷积</a></strong>的意思，俗称像素洗牌）</p><h2 id="3-Descriptor-Decoder"><a href="#3-Descriptor-Decoder" class="headerlink" title="3. Descriptor Decoder"></a>3. Descriptor Decoder</h2><p>首先利用类似于UCN的网络得到一个半稠密的描述子（此处参考文献<a href="https://arxiv.org/abs/1606.03558" target="_blank" rel="noopener">UCN</a>），这样可以减少算法训练内存开销同时减少算法运行时间。之后通过双三次多项式插值得到其余描述，然后通过<code>L2-normalizes</code>归一化描述子得到统一的长度描述。特征维度由$\mathcal{D} \in \mathbb{R}^{H_c \times W_c \times D}$变为$\mathbb{R}^{H\times W \times D}$ 。</p><p><img alt="fig_11_des_decoder" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_11_des_decoder.png"></p><p>由特征点得到其描述子的过程文中没有细讲，看了一下<a href="https://github.com/pytorch/pytorch/blob/f064c5aa33483061a48994608d890b968ae53fb5/aten/src/THNN/generic/SpatialGridSamplerBilinear.c" target="_blank" rel="noopener">源代码</a>就明白了。其实该过程主要用了一个函数即<code>grid_sample</code>，画了一个草图作为解释。</p><ul><li>图像尺寸归一化：首先对图像的尺寸进行归一化，(-1,-1)表示原来图像的(0,0)位置，(1,1)表示原来图像的(H-1,W-1)位置，这样一来，特征点的位置也被归一化到了相应的位置。</li><li>构建grid：将归一化后的特征点罗列起来，构成一个尺度为1*1*K*2的张量，其中K表示特征数量，2分别表示xy坐标。</li><li>特征点位置反归一化：根据输入张量的H与W对grid(1,1,0,:)（表示第一个特征点，其余特征点类似）进行反归一化，其实就是按照比例进行缩放+平移，得到反归一化特征点在张量某个slice（通道）上的位置；但是这个位置可能并非为整像素，此时要对其进行双线性插值补齐，然后其余slice按照同样的方式进行双线性插值。注：代码中实际的就是双线性插值，并非文中讲的双三次插值；</li><li>输出维度：1*C*1*K。</li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/grid_sample.png"></p><h2 id="4-误差构建"><a href="#4-误差构建" class="headerlink" title="4. 误差构建"></a>4. 误差构建</h2><script type="math/tex; mode=display">\begin{array}{l}{\mathcal{L}\left(\mathcal{X}, \mathcal{X}^{\prime}, \mathcal{D}, \mathcal{D}^{\prime} ; Y, Y^{\prime}, S\right)=} \\ {\qquad \mathcal{L}_{p}(\mathcal{X}, Y)+\mathcal{L}_{p}\left(\mathcal{X}^{\prime}, Y^{\prime}\right)+\lambda \mathcal{L}_{d}\left(\mathcal{D}, \mathcal{D}^{\prime}, S\right)}\end{array}</script><p>可见损失函数由两项组成，其中一项为特征点检测loss$\mathcal{L}_{p}$ ，另外一项是描述子的loss$\mathcal{L}_{d}$。</p><p>对于检测项loss，此时采用了交叉熵损失函数:</p><script type="math/tex; mode=display">\mathcal{L}_{p}(\mathcal{X}, Y)=\frac{1}{H_{c} W_{c}} \sum_{h=1 \atop w=1}^{H_{c}, W_{c}} l_{p}\left(\mathbf{x}_{h w} ; y_{h w}\right)</script><p>其中：</p><script type="math/tex; mode=display">l_{p}\left(\mathbf{x}_{h w} ; y\right)=-\log \left(\frac{\exp \left(\mathbf{x}_{h w y}\right)}{\sum_{k=1}^{65} \exp \left(\mathbf{x}_{h w k}\right)}\right)</script><p>描述子的损失函数:</p><script type="math/tex; mode=display">\mathcal{L}_{d}\left(\mathcal{D}, \mathcal{D}^{\prime}, S\right)=\frac{1}{\left(H_{c} W_{c}\right)^{2}} \sum_{h=1 \atop w=1}^{H_{c}, W_{c}} \sum_{h^{\prime}=1 \atop w^{\prime}=1}^{H_{c}, W_{c}} l_{d}\left(\mathbf{d}_{h w}, \mathbf{d}_{h^{\prime} w^{\prime}}^{\prime} ; s_{h w h^{\prime} w^{\prime}}\right)</script><p>其中<script type="math/tex">l_{d}</script>为<code>Hinge-loss</code>（合页损失函数，用于SVM，如支持向量的软间隔，可以保证最后解的稀疏性）；</p><script type="math/tex; mode=display">l_{d}\left(\mathbf{d}, \mathbf{d}^{\prime} ; s\right)=\lambda_{d} * s * \max \left(0, m_{p}-\mathbf{d}^{T} \mathbf{d}^{\prime}\right)+(1-s) * \max \left(0, \mathbf{d}^{T} \mathbf{d}^{\prime}-m_{n}\right)</script><p>同时指示函数为<script type="math/tex">s_{h w h^{\prime} w^{\prime}}</script>,$S$表示所有正确匹配对集合:</p><script type="math/tex; mode=display">s_{h w h^{\prime} w^{\prime}}=\left\{\begin{array}{ll}{1,} & {\text { if }\left\|\widehat{\mathcal{H} \mathbf{p}_{h w}}-\mathbf{p}_{h^{\prime} w^{\prime}}\right\| \leq 8} \\ {0,} & {\text { otherwise }}\end{array}\right.</script><h1 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h1><p><img alt="fig2" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_2.png"></p><p>本文一共设计了两个网络，一个是<code>BaseDetector</code>，用于检测角点（注意，此处提取的并不是最终输出的特征点，可以理解为候选的特征点），另一个是<code>SuperPoint</code>网络，输出特征点和描述子。</p><p>网络的训练共分为三个步骤：</p><ol><li>第一步是采用虚拟的三维物体作为数据集，训练网络去提取角点，这里得到的是<code>BaseDetector</code>即，<code>MagicPoint</code>；</li><li>使用真实场景图片，用第一步训练出来的网络<code>MagicPoint</code> +<code>Homographic Adaptation</code>提取角点，这一步称作兴趣点自标注（Interest Point Self-Labeling）</li><li>对第二步使用的图片进行几何变换得到新的图片，这样就有了已知位姿关系的图片对，把这两张图片输入SuperPoint网络，提取特征点和描述子。</li></ol><h2 id="预训练Magic-Point"><a href="#预训练Magic-Point" class="headerlink" title="预训练Magic Point"></a>预训练Magic Point</h2><p>此处参考作者之前发表的一篇论文<strong>[<a href="https://arxiv.org/abs/1707.07410" target="_blank" rel="noopener">Toward Geometric Deep SLAM</a>]</strong>，其实就是<code>MagicPoint</code>，在此不做展开介绍。<br><img alt="fig2" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_10_magicPoint1.png"></p><p><img alt="fig4" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_4.png"></p><h2 id="Homographic-Adaptation"><a href="#Homographic-Adaptation" class="headerlink" title="Homographic Adaptation"></a>Homographic Adaptation</h2><p>算法在虚拟数据集上表现极其优秀，但是在真实场景下表示没有达到预期，此时本文进行了<code>Homographic Adaptation</code>。<br>作者使用的数据集是<code>MS-COCO</code>，为了使网络的泛化能力更强，本文不仅使用原始了原始图片，而且对每张图片进行随机的旋转和缩放形成新的图片，新的图片也被用来进行识别。这一步其实就类似于训练里常用的数据增强。经过一系列的单映变换之后特征点的复检率以及普适性得以增强。值得注意的是，在实际训练时，这里采用了迭代使用单映变换的方式，例如使用优化后的特征点检测器重新进行单映变换进行训练，然后又可以得到更新后的检测器，如此迭代优化，这就是所谓的self-supervisd。<br><img alt="fig5" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_5.png"></p><p><img alt="fig_9_HA" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_9_HA.png"></p><p>最后的关键点检测器，即<script type="math/tex">\hat{F}\left(I ; f_{\theta}\right)</script>，可以表示为再所有随机单映变换/反变换的聚合：</p><script type="math/tex; mode=display">\hat{F}\left(I ; f_{\theta}\right)=\frac{1}{N_{h}} \sum_{i=1}^{N_{h}} \mathcal{H}_{i}^{-1} f_{\theta}\left(\mathcal{H}_{i}(I)\right)</script><p><img alt="fig_6" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_6.png"></p><h2 id="构建残差，迭代优化描述子以及检测器"><a href="#构建残差，迭代优化描述子以及检测器" class="headerlink" title="构建残差，迭代优化描述子以及检测器"></a>构建残差，迭代优化描述子以及检测器</h2><p>利用上面网络得到的关键点位置以及描述子表示构建残差，利用<code>ADAM</code>进行优化。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><img alt="fig_8" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_8.png"></p><p><img alt="tab_3" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/tab_3.png"></p><p><img alt="tab_4" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/tab_4.png"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol><li>it is possible to transfer knowledge from a synthetic dataset onto real-world images</li><li>sparse interest point detection and description can be cast as a single, efficient convolutional neural network</li><li>the resulting system works well for geometric computer vision matching tasks such as Homography Estimation</li></ol><p>未来工作:</p><ol><li>研究Homographic Adaptation能否在语义分割任务或者目标检测任务中有提升作用</li><li>兴趣点提取以及描述这两个任务是如何影响彼此的</li></ol><p>作者最后提到，他相信该网络能够解决SLAM或者SfM领域的数据关联<em>，并且</em><code>learning-based</code>前端可以使得诸如机器人或者AR等应用获得更加鲁棒。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文出自近几年备受瞩目的创业公司&lt;a href=&quot;https://www.magicleap.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MagicLeap&lt;/a&gt;，发表在CVPR 2018,一作&lt;a href=&quot;http://www.danieldetone.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Daniel DeTone&lt;/a&gt;，&lt;strong&gt;[&lt;a href=&quot;https://arxiv.org/abs/1712.07629&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper&lt;/a&gt;]&lt;/strong&gt;，&lt;strong&gt;[&lt;a href=&quot;https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork/blob/master/assets/DL4VSLAM_talk.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;slides&lt;/a&gt;]&lt;/strong&gt;，&lt;strong&gt;[&lt;a href=&quot;https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;code&lt;/a&gt;]&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这篇文章设计了一种自监督网络框架，能够同时提取特征点的位置以及描述子。相比于patch-based方法，本文提出的算法能够在原始图像提取到像素级精度的特征点的位置及其描述子。&lt;br&gt;本文提出了一种单映性适应（&lt;code&gt;Homographic Adaptation&lt;/code&gt;）的策略以增强特征点的复检率以及跨域的实用性（这里跨域指的是synthetic-to-real的能力，网络模型在虚拟数据集上训练完成，同样也可以在真实场景下表现优异的能力）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="CV" scheme="https://www.vincentqin.tech/categories/CV/"/>
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="Deep Learning" scheme="https://www.vincentqin.tech/tags/Deep-Learning/"/>
    
      <category term="特征提取" scheme="https://www.vincentqin.tech/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
      <category term="SuperPoint" scheme="https://www.vincentqin.tech/tags/SuperPoint/"/>
    
      <category term="MagicLeap" scheme="https://www.vincentqin.tech/tags/MagicLeap/"/>
    
      <category term="深度学习" scheme="https://www.vincentqin.tech/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Bygones</title>
    <link href="https://www.vincentqin.tech/posts/bygones/"/>
    <id>https://www.vincentqin.tech/posts/bygones/</id>
    <published>2019-05-28T16:57:39.000Z</published>
    <updated>2020-03-02T15:15:31.025Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><!-- <div id="dplayer1" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer1"),"loop":true,"video":{"url":"https://www.bilibili.com/video/av92924509/"},"danmaku":{"id":"bbe4286bf164ef6a1497f18a7b42ff944e684b821","api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> --><iframe src="//player.bilibili.com/player.html?aid=92924509&cid=158651721&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><a id="more"></a><p><strong><center>致我们逝去的青春！</center></strong></p>]]></content>
    
    <summary type="html">
    
      &lt;!-- &lt;div id=&quot;dplayer1&quot; class=&quot;dplayer hexo-tag-dplayer-mark&quot; style=&quot;margin-bottom: 20px;&quot;&gt;&lt;/div&gt;&lt;script&gt;(function(){var player = new DPlayer({&quot;container&quot;:document.getElementById(&quot;dplayer1&quot;),&quot;loop&quot;:true,&quot;video&quot;:{&quot;url&quot;:&quot;https://www.bilibili.com/video/av92924509/&quot;},&quot;danmaku&quot;:{&quot;id&quot;:&quot;bbe4286bf164ef6a1497f18a7b42ff944e684b821&quot;,&quot;api&quot;:&quot;https://api.prprpr.me/dplayer/&quot;}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()&lt;/script&gt; --&gt;
&lt;iframe src=&quot;//player.bilibili.com/player.html?aid=92924509&amp;cid=158651721&amp;page=1&quot; scrolling=&quot;no&quot; border=&quot;0&quot; frameborder=&quot;no&quot; framespacing=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt; &lt;/iframe&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Black Hole</title>
    <link href="https://www.vincentqin.tech/posts/first-black-hole/"/>
    <id>https://www.vincentqin.tech/posts/first-black-hole/</id>
    <published>2019-04-10T15:55:01.000Z</published>
    <updated>2020-03-31T14:55:18.422Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/first-black-hole/big-blackhole.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>🔥Awesome CV Works</title>
    <link href="https://www.vincentqin.tech/posts/awesome-works/"/>
    <id>https://www.vincentqin.tech/posts/awesome-works/</id>
    <published>2019-03-31T12:15:41.000Z</published>
    <updated>2020-08-30T13:50:14.291Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>The post contains papers-with-code about SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc. </p><p><a href="https://github.com/Vincentqyw/Recent-Stars-2020" target="_blank" rel="noopener"><img alt="ReadMe Card" data-src="https://github-readme-stats.vercel.app/api/pin/?username=Vincentqyw&amp;repo=Recent-Stars-2020&amp;show_owner=false&amp;theme=default"></a> </p><!-- [![GitHub stars](https://img.shields.io/github/stars/Vincentqyw/Recent-Stars-2019.svg?logo=github&label=Stars)](https://github.com/Vincentqyw/Recent-Stars-2019) --><a id="more"></a><!--# Recent Stars 2020--><!-- <p align="center"> <img width="100px" src="https://cdn.jsdelivr.net/gh/Vincentqyw/blog-resources/awesome-works/github-star.png" align="center" alt="" /></p> --><!-- <p align="center">  <a href="https://github.com/Vincentqyw/Recent-Stars-2020">    <img alt="Awesome" src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" />  </a>  <a href="http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019">    <img alt="HitCount" src="http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019.svg" />  </a>  <a href="https://vincentqin.tech">    <img alt="LICENSE" src="https://img.shields.io/badge/license-Anti%20996-blue.svg?style=flat-square" />  </a></p> --><!--[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/Vincentqyw/Recent-Stars-2020)[![HitCount](http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019.svg)](http://hits.dwyl.io/Vincentqyw/Recent-Stars-2019)[![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg?style=flat-square)](https://github.com/Vincentqyw/Recent-Stars-2020)✔ This repo collects some links with papers which I recently starred related on SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, etc.--><h2 id="SLAM-related"><a href="#SLAM-related" class="headerlink" title="SLAM related"></a>SLAM related</h2><ul><li>[<strong>SLAM</strong>]<a href="https://github.com/UZ-SLAMLab/ORB_SLAM3" target="_blank" rel="noopener">ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM</a>, <strong>[<a href="https://arxiv.org/abs/2007.11898" target="_blank" rel="noopener">PDF</a>]</strong></li><li>[<strong>SLAM</strong>]<a href="https://github.com/TixiaoShan/LIO-SAM" target="_blank" rel="noopener">LIO-SAM</a>, 激光雷达IMU紧耦合SLAM</li><li>[<strong>Tool</strong>]<a href="https://github.com/petercorke/robotics-toolbox-python" target="_blank" rel="noopener">Robotics Toolbox for Python</a>,  a Python implementation of the <a href="https://github.com/petercorke/robotics-toolbox-matlab" target="_blank" rel="noopener">Robotics Toolbox for MATLAB®</a></li><li>[<strong>Matching</strong>]<a href="https://github.com/rpautrat/LISRD" target="_blank" rel="noopener">LISRD</a>,ECCV 2020, <strong>[<a href="https://arxiv.org/abs/2007.08988" target="_blank" rel="noopener">PDF</a>]</strong>，在线局部不变特征匹配！重要！</li><li>[<strong>Matching</strong>]<a href="https://github.com/cavalli1234/AdaLAM" target="_blank" rel="noopener">AdaLAM</a>,特征匹配快速滤除外点</li><li>[<strong>Calib</strong>]<a href="https://github.com/3DCVer/fisheye_pinhole_calib_demo" target="_blank" rel="noopener">fisheye_pinhole_calib_demo</a>, 包括鱼眼模型、针孔模型的相机标定，封装了自动编译、库的打包以及外部库的调用测试</li><li>[<strong>Calib</strong>]<a href="https://github.com/FENGChenxi0823/SensorCalibration" target="_blank" rel="noopener">SensorCalibration</a>, IMU雷达标定</li><li>[<strong>VO</strong>]<a href="https://github.com/PyojinKim/LPVO" target="_blank" rel="noopener">Low-Drift Visual Odometry in Structured Environments by Decoupling Rotational and Translational Motion</a>,ICRA 2018, <strong>[<a href="http://pyojinkim.com/download/papers/2018_ICRA.pdf" target="_blank" rel="noopener">PDF</a>]</strong>, 结构化环境中将旋转量与平移量进行分离优化</li><li>[<strong>VIO</strong>]<a href="https://github.com/iamwangyabin/VIO-SLAM" target="_blank" rel="noopener">VIO-SLAM</a>, 从零开始手写VIO课后作业</li><li>[<strong>Matching</strong>]<a href="https://github.com/lzx551402/tfmatch" target="_blank" rel="noopener">TFMatch: Learning-based image matching in TensorFlow</a>,TensorFlow 实现的 GeoDesc,ASLFeat以及ContextDesc</li><li>[<strong>Tutorial</strong>]<a href="https://github.com/yanyan-li/SLAM-BOOK" target="_blank" rel="noopener">SLAM-BOOK</a>, 一本关于SLAM的书稿，清楚的介绍SLAM系统中的使用的几何方法和深度学习方法，持续更新中</li><li>[<strong>Loop Closing</strong>]<a href="https://github.com/PRBonn/OverlapNet" target="_blank" rel="noopener">OverlapNet - Loop Closing for 3D LiDAR-based SLAM</a>, RSS 2020, <strong>[<a href="https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/chen2020rss.pdf" target="_blank" rel="noopener">PDF</a>]</strong>, 3D激光雷达SLAM闭环</li><li>[<strong>SLAM</strong>]<a href="https://github.com/halajun/VDO_SLAM" target="_blank" rel="noopener">VDO_SLAM</a>, RGB-D相机数据作为输入，实现追踪动态物体SLAM的功能, <strong>[<a href="https://arxiv.org/abs/2005.11052" target="_blank" rel="noopener">PDF</a>]</strong></li><li>[<strong>SLAM</strong>]<a href="https://github.com/TUMFTM/orbslam-map-saving-extension" target="_blank" rel="noopener">orbslam-map-saving-extension</a>，在ORB-SLAM的基础上增加保存+加载地图功能</li><li>[<strong>Tutorial</strong>]<a href="https://github.com/NxRLab/ModernRobotics" target="_blank" rel="noopener">Modern Robotics: Mechanics, Planning, and Control Code Library</a>, 现代机器人学, <strong>[<a href="http://hades.mech.northwestern.edu/index.php/Modern_Robotics" target="_blank" rel="noopener">Homepage</a>]</strong></li><li>[<strong>Matching</strong>]<a href="https://github.com/vcg-uvic/image-matching-benchmark-baselines" target="_blank" rel="noopener">image-matching-benchmark-baselines</a>, 图像特征匹配挑战赛主页</li><li>[<strong>Matching</strong>]<a href="https://github.com/mameng1/GraphLineMatching" target="_blank" rel="noopener">GraphLineMatching</a></li><li>[<strong>Matching</strong>]<a href="https://github.com/jiayi-ma/LPM" target="_blank" rel="noopener">Locality Preserving Matching</a>, IJCAI 2017, <strong>[<a href="https://ai.tencent.com/ailab/media/publications/YuanGao_IJCAI2017_LocalityPreservingMatching.pdf" target="_blank" rel="noopener">PDF</a>]</strong></li><li>[<strong>IMU</strong>]<a href="https://github.com/ydsf16/IMUOrientationEstimator" target="_blank" rel="noopener">IMUOrientationEstimator</a></li><li>[<strong>Feature</strong>]<a href="https://github.com/iago-suarez/BEBLID" target="_blank" rel="noopener">BEBLID: Boosted Efficient Binary Local Image Descriptor</a></li><li>[<strong>Relocalization</strong>]<a href="https://github.com/zlthinker/KFNet" target="_blank" rel="noopener">KFNet: Learning Temporal Camera Relocalization using Kalman Filtering</a>,CVPR 2020,<strong>[<a href="https://arxiv.org/abs/2003.10629" target="_blank" rel="noopener">PDF</a>]</strong></li><li>[<strong>Matching</strong>]<a href="https://github.com/vcg-uvic/image-matching-benchmark" target="_blank" rel="noopener">image-matching-benchmark</a></li><li>[<strong>Matching</strong>]<a href="https://github.com/JiawangBian/GMS-Feature-Matcher" target="_blank" rel="noopener">GMS: Grid-based Motion Statistics for Fast, Ultra-robust Feature Correspondence</a>,CVPR 17 &amp; IJCV 19,<strong>[<a href="http://jwbian.net/Papers/GMS_CVPR17.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://jwbian.net/gms" target="_blank" rel="noopener">Project page</a>]</strong></li><li>[<strong>Reloc</strong>]<a href="https://github.com/Artisense-ai/GN-Net-Benchmark" target="_blank" rel="noopener">GN-Net-Benchmark</a>, CVPR 2020,GN-Net: The Gauss-Newton Loss for Multi-Weather Relocalization, <strong>[<a href="https://arxiv.org/abs/1904.11932" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://vision.in.tum.de/gn-net" target="_blank" rel="noopener">Project page</a>]</strong></li><li>[<strong>Matching</strong>]<a href="https://github.com/magicleap/SuperGluePretrainedNetwork" target="_blank" rel="noopener">SuperGluePretrainedNetwork</a>, CVPR 2020, <strong>[<a href="https://arxiv.org/abs/1911.11763" target="_blank" rel="noopener">PDF</a>]</strong>, 划重点！2020年sota超大视角2D特征匹配，<a href="https://www.vincentqin.tech/posts/superglue/">Blog</a></li><li>[<strong>Feature</strong>]<a href="https://github.com/XuyangBai/D3Feat" target="_blank" rel="noopener">D3Feat</a>, CVPR 2020, <strong>[<a href="https://arxiv.org/abs/2003.03164" target="_blank" rel="noopener">PDF</a>]</strong></li><li>[<strong>Feature</strong>]<a href="https://github.com/lzx551402/ASLFeat" target="_blank" rel="noopener">ASLFeat</a>, CVPR 2020, ASLFeat: Learning Local Features of Accurate Shape and Localization, <strong>[<a href="https://arxiv.org/abs/2003.10071" target="_blank" rel="noopener">PDF</a>]</strong></li><li>[<strong>Feature</strong>]<a href="https://github.com/XuyangBai/D3Feat" target="_blank" rel="noopener">GMS-Feature-Matcher</a>, CVPR 2018, GMS: Grid-based Motion Statistics for Fast, Ultra-robust Feature Correspondence, <strong>[<a href="http://jwbian.net/Papers/GMS_CVPR17.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://jwbian.net/gms" target="_blank" rel="noopener">Project page</a>]</strong></li><li>[<strong>Feature</strong>]<a href="https://github.com/XuyangBai/D3Feat" target="_blank" rel="noopener">D3Feat</a>, CVPR 2020, <strong>[<a href="https://arxiv.org/abs/2003.03164" target="_blank" rel="noopener">PDF</a>]</strong></li><li>[<strong>Feature</strong>]<a href="https://github.com/yewzijian/3DFeatNet" target="_blank" rel="noopener">3DFeatNet</a>, ECCV 2018, <strong>[<a href="https://arxiv.org/abs/1807.09413" target="_blank" rel="noopener">PDF</a>]</strong></li><li>[<strong>Tutorial</strong>]<a href="https://github.com/microsoft/AutonomousDrivingCookbook" target="_blank" rel="noopener">AutonomousDrivingCookbook</a>，Scenarios, tutorials and demos for Autonomous Driving</li><li>[<strong>Tutorial</strong>]<a href="https://github.com/PaoPaoRobot/SLAMPaperReading" target="_blank" rel="noopener">SLAMPaperReading</a>，泡泡机器人北京线下SLAM论文分享资料</li><li>[<strong>Tutorial</strong>]<a href="https://github.com/lishuwei0424/VIO_Tutotial_Course" target="_blank" rel="noopener">VIO_Tutotial_Course</a></li><li>[<strong>Tutorial</strong>]<a href="https://github.com/MichaelBeechan/VO-SLAM-Review" target="_blank" rel="noopener">VO-SLAM-Review</a></li><li>[<strong>Tutorial</strong>]<a href="https://github.com/QingSimon/VINS-Mono-code-annotation" target="_blank" rel="noopener">VINS-Mono-code-annotation</a>,VINS-Mono代码注释以及公式推导</li><li>[<strong>Tutorial</strong>]<a href="https://github.com/ManiiXu/VINS-Mono-Learning" target="_blank" rel="noopener">VINS-Mono-Learning</a>,VINS-Mono代码注释</li><li>[<strong>Tutorial</strong>]<a href="https://github.com/HeYijia/VINS-Course" target="_blank" rel="noopener">VINS-Course</a>,VINS-Mono code without Ceres or ROS</li><li>[<strong>Tutorial</strong>]<a href="https://github.com/StevenCui/VIO-Doc" target="_blank" rel="noopener">VIO-Doc</a>,主流VIO论文推导及代码解析</li><li>[<strong>VO</strong>]<a href="https://github.com/muskie82/CNN-DSO" target="_blank" rel="noopener">CNN-DSO</a>, Direct Sparse Odometry with CNN Depth Prediction</li><li>[<strong>VO</strong>]<a href="https://github.com/lsyads/fisheye-ORB-SLAM" target="_blank" rel="noopener">fisheye-ORB-SLAM</a>, A real-time robust monocular visual SLAM system based on ORB-SLAM for fisheye cameras, without rectifying or cropping the input images</li><li>[<strong>VO</strong>]<a href="https://github.com/robotseu/ORB_Line_SLAM" target="_blank" rel="noopener">ORB_Line_SLAM</a>, Real-Time SLAM with BoPLW Pairs for Stereo Cameras, with Loop Detection and Relocalization Capabilities</li><li>[<strong>VO</strong>]<a href="https://github.com/ChiWeiHsiao/DeepVO-pytorch.git" target="_blank" rel="noopener">DeepVO-pytorch</a>, ICRA 2017 <a href="https://ieeexplore.ieee.org/document/7989236/" target="_blank" rel="noopener">DeepVO: Towards end-to-end visual odometry with deep Recurrent Convolutional Neural Networks</a></li><li>[<strong>Calib</strong>]<a href="https://github.com/MegviiRobot/CamOdomCalibraTool" target="_blank" rel="noopener">CamOdomCalibraTool</a>, The tool to calibrate extrinsic param between camera and wheel.</li><li>[<strong>Calib</strong>]<a href="https://github.com/heethesh/lidar_camera_calibration" target="_blank" rel="noopener">lidar_camera_calibration</a>,<a href="https://github.com/ankitdhall/lidar_camera_calibration" target="_blank" rel="noopener">another version</a></li><li>[<strong>Calib</strong>]<a href="https://github.com/MegviiRobot/OdomLaserCalibraTool.git" target="_blank" rel="noopener">OdomLaserCalibraTool</a>，相机与2D雷达标定</li><li>[<strong>Calib</strong>]<a href="https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration" target="_blank" rel="noopener">extrinsic_lidar_camera_calibration</a>, LiDARTag: A Real-Time Fiducial Tag using Point Clouds, arXiv 2019, <strong>[<a href="https://arxiv.org/abs/1908.10349" target="_blank" rel="noopener">PDF</a>]</strong></li><li>[<strong>Calib</strong>]<a href="https://github.com/beltransen/velo2cam_calibration" target="_blank" rel="noopener">velo2cam_calibration</a>, Automatic Calibration algorithm for Lidar-Stereo camera, <strong>[<a href="http://wiki.ros.org/velo2cam_calibration" target="_blank" rel="noopener">Project page</a>]</strong></li><li>[<strong>Dataset</strong>]<a href="https://github.com/HKBU-HPML/IRS.git" target="_blank" rel="noopener">IRS: A Large Synthetic Indoor Robotics Stereo Dataset for Disparity and Surface Normal Estimation</a></li><li>[<strong>Tools</strong>]<a href="https://github.com/christophhagen/averaging-quaternions" target="_blank" rel="noopener">averaging-quaternions</a>,四元数平均</li></ul><hr><p>分割线，以下是2019年的星标项目，上面是2020年新星标的。</p><ul><li><a href="https://github.com/naver/r2d2" target="_blank" rel="noopener">R2D2: Reliable and Repeatable Detector and Descriptor</a>,NeurIPS 2019,<strong>[<a href="https://arxiv.org/abs/1906.06195" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://europe.naverlabs.com/research/publications/r2d2-reliable-and-repeatable-detectors-and-descriptors-for-joint-sparse-local-keypoint-detection-and-feature-extraction/" target="_blank" rel="noopener">Project page</a>]</strong>，深度学习特征点+描述子</li><li><a href="https://github.com/1989Ryan/Semantic_SLAM" target="_blank" rel="noopener">Semantic_SLAM</a>,语义SLAM：ROS + ORB SLAM + PSPNet101</li><li><a href="https://github.com/BAILOOL/PlaceRecognition-LoopDetection" target="_blank" rel="noopener">PlaceRecognition-LoopDetection</a>, Light-weight place recognition and loop detection using road markings</li><li><a href="https://github.com/MISTLab/DOOR-SLAM" target="_blank" rel="noopener">DOOR-SLAM: Distributed, online, and outlier resilient SLAM for robotic teams</a>,<strong>[<a href="https://arxiv.org/abs/1909.12198" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://mistlab.ca/DOOR-SLAM/" target="_blank" rel="noopener">Project page</a>]</strong>，多机器人协作SLAM，增强了场景的适用性</li><li><a href="https://github.com/shamangary/awesome-local-global-descriptor" target="_blank" rel="noopener">awesome-local-global-descriptor</a>, 超详细深度学习特征点描述子集合，需要重点关注一下这个repo</li><li><a href="https://github.com/zju3dv/GIFT" target="_blank" rel="noopener">GIFT: Learning Transformation-Invariant Dense Visual Descriptors via Group CNNs</a>, NeurIPS 2019，<strong>[<a href="https://arxiv.org/abs/1911.05932" target="_blank" rel="noopener">PDF</a>]</strong>, <strong>[<a href="https://zju3dv.github.io/GIFT/" target="_blank" rel="noopener">Project page</a>]</strong>，浙大CAD+商汤联合实验室出品，利用Group CNN来改进superpoint描述子（仅描述，特征点提取可任意选择），可以大幅度增强视角变化时的特征点复检率与匹配点数</li><li><a href="https://github.com/axelBarroso/Key.Net" target="_blank" rel="noopener">Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters</a>,ICCV 2019, <strong>[<a href="https://arxiv.org/abs/1904.00889" target="_blank" rel="noopener">PDF</a>]</strong>, 深度学习特征点</li><li><a href="https://github.com/TRI-ML/KP3D" target="_blank" rel="noopener">Self-Supervised 3D Keypoint Learning for Ego-motion Estimation</a>,<strong>[<a href="https://arxiv.org/abs/1912.03426" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://www.youtube.com/watch?v=4hFhSD8QUPM" target="_blank" rel="noopener">Youtube</a>]</strong>, 深度学习特征点</li><li><a href="https://github.com/Jichao-Peng/VINS-Mono-Optimization" target="_blank" rel="noopener">VINS-Mono-Optimization</a>, 实现点线紧耦合优化的VINS-Mono</li><li><a href="https://github.com/PetWorm/msckf_vio_zhushi" target="_blank" rel="noopener">msckf_vio注释版本</a></li><li><a href="https://github.com/lyakaap/NetVLAD-pytorch" target="_blank" rel="noopener">NetVLAD-pytorch</a>, NetVLAD场景识别的pytorch实现</li><li><a href="http://microgps.cs.princeton.edu/" target="_blank" rel="noopener">High-Precision Localization Using Ground Texture (Micro-GPS)</a>,ECCV 2018,<strong>[<a href="https://arxiv.org/abs/1710.10687" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://microgps.cs.princeton.edu/" target="_blank" rel="noopener">Project page</a>]</strong>,<strong>[<a href="http://microgps.cs.princeton.edu/data/micro-gps-cpp-master.zip" target="_blank" rel="noopener">code</a>]</strong>，地向（摄像机朝向地面）SLAM，获得高精度重定位效果。</li><li><a href="https://github.com/LRMPUT/PlaneSLAM" target="_blank" rel="noopener">PlaneSLAM</a>, Paper: “On the Representation of Planes for Efficient Graph-based SLAM with High-level Features”</li><li><a href="https://github.com/ucla-vision/xivo" target="_blank" rel="noopener">XIVO: X Inertial-aided Visual Odometry and Sparse Mapping</a>, an open-source repository for visual-inertial odometry/mapping. </li><li><a href="https://github.com/lmb-freiburg/deeptam" target="_blank" rel="noopener">DeepTAM</a>,ECCV 2018,<strong>[<a href="https://arxiv.org/pdf/1808.01900.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://lmb.informatik.uni-freiburg.de/people/zhouh/deeptam/" target="_blank" rel="noopener">Project page</a>]</strong>,a learnt system for keyframe-based dense camera tracking and mapping.</li><li><a href="https://github.com/ajparra/iRotAvg" target="_blank" rel="noopener">iRotAvg, Why bundle adjust?</a>,ICRA 2019,<strong>[<a href="https://cs.adelaide.edu.au/~aparra/publication/parra19_icra/" target="_blank" rel="noopener">PDF</a>]</strong></li><li><a href="https://github.com/Kelym/FAST" target="_blank" rel="noopener">Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation</a>,CVPR 2019,<strong>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Ke_Tactical_Rewind_Self-Correction_via_Backtracking_in_Vision-And-Language_Navigation_CVPR_2019_paper.html" target="_blank" rel="noopener">PDF</a>]</strong>，视觉+语言导航</li><li><a href="https://github.com/MISTLab/DOOR-SLAM" target="_blank" rel="noopener">DOOR-SLAM</a></li><li><a href="https://github.com/JiawangBian/FM-Bench" target="_blank" rel="noopener">An Evaluation of Feature Matchers for Fundamental Matrix Estimation</a>,BMVC 2019,<strong>[<a href="https://jwbian.net/Papers/FM_BMVC19.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://jwbian.net/fm-bench" target="_blank" rel="noopener">Project Page</a>]</strong>，特征匹配</li><li><a href="https://github.com/hyye/lio-mapping" target="_blank" rel="noopener">A Tightly Coupled 3D Lidar and Inertial Odometry and Mapping Approach</a>,ICRA 2019,<strong>[<a href="https://arxiv.org/abs/1904.06993" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://sites.google.com/view/lio-mapping" target="_blank" rel="noopener">Project Page</a>]</strong>，紧耦合雷达+IMU SLAM</li><li><a href="https://github.com/LRMPUT/PlaneSLAM" target="_blank" rel="noopener">On the Representation of Planes for Efficient Graph-based SLAM with High-level Features</a>,利用平面信息的SLAM</li><li><a href="https://github.com/Huangying-Zhan/DF-VO" target="_blank" rel="noopener">Visual Odometry Revisited: What Should Be Learnt?</a>,arXiv 2019,<strong>[<a href="https://arxiv.org/abs/1909.09803" target="_blank" rel="noopener">PDF</a>]</strong>, 深度学习深度+光流进行VO</li><li><a href="https://github.com/Xylon-Sean/rfnet" target="_blank" rel="noopener">RF-Net: An End-to-End Image Matching Network based on Receptive Field</a>,CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1906.00604" target="_blank" rel="noopener">PDF</a>]</strong>, 端到端图像匹配</li><li><a href="https://github.com/HKUST-Aerial-Robotics/Fast-Planner" target="_blank" rel="noopener">Fast-Planner</a>,IEEE Robotics and Automation Letters (RA-L), 2019,<strong>[<a href="https://ieeexplore.ieee.org/document/8758904" target="_blank" rel="noopener">PDF</a>]</strong>, 无人机轨迹生成</li><li><a href="https://github.com/dongjing3309/minisam" target="_blank" rel="noopener">A general and flexible factor graph non-linear least square optimization framework</a>,CoRR 2019,<strong>[<a href="http://arxiv.org/abs/1909.00903" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://minisam.readthedocs.io/" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/gao-ouyang/demo_for_kalmanFilter" target="_blank" rel="noopener">Demo for Kalman filter in ranging system</a>,卡尔曼滤波原理演示</li><li><a href="https://github.com/Ahmedest61/CNN-Region-VLAD-VPR" target="_blank" rel="noopener">A Holistic Visual Place Recognition Approach using Lightweight CNNs for Severe ViewPoint and Appearance Changes</a>，场景识别（外观与视角变化时）,<a href="https://github.com/ethz-asl/hierarchical_loc" target="_blank" rel="noopener">训练和部署源码</a></li><li><a href="https://github.com/uzh-rpg/sips2_open" target="_blank" rel="noopener">SIPs: Succinct Interest Points from Unsupervised Inlierness Probability Learning</a>,3D Vision (3DV) 2019,<strong>[<a href="https://arxiv.org/abs/1805.01358" target="_blank" rel="noopener">PDF</a>]</strong>，RPG实验室出品，深度学习特征点（有特征描述子）</li><li><a href="https://github.com/uzh-rpg/imips_open" target="_blank" rel="noopener">Matching Features Without Descriptors: Implicitly Matched Interest Points</a>,BMVC 2019,<strong>[<a href="http://rpg.ifi.uzh.ch/docs/BMVC19_Cieslewski.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,RPG实验室出品，无需特征描述即可进行特征匹配</li><li><a href="https://github.com/cardwing/Codes-for-Lane-Detection" target="_blank" rel="noopener">Learning Lightweight Lane Detection CNNs by Self Attention Distillation (ICCV 2019)</a>,ICCV 2019,<strong>[<a href="https://arxiv.org/abs/1908.00821" target="_blank" rel="noopener">PDF</a>]</strong>，深度学习道路检测</li><li><a href="https://github.com/youngguncho/awesome-slam-datasets" target="_blank" rel="noopener">Awesome SLAM Datasets</a>,史上最全SLAM数据集， <strong><a href="https://mp.weixin.qq.com/s/BzcghUnXTR9RQqA3Pc9MhA" target="_blank" rel="noopener">公众号说明: 最全 SLAM 开源数据集</a></strong></li><li><a href="https://github.com/Aceinna/gnss-ins-sim" target="_blank" rel="noopener">GNSS-INS-SIM</a>,惯导融合模拟器，支持IMU数据，轨迹生成等</li><li><a href="https://github.com/2013fangwentao/Multi-Sensor-Combined-Navigation" target="_blank" rel="noopener">Multi-Sensor Combined Navigation Program(GNSS, IMU, Camera and so on) 多源多传感器融合定位 GPS/INS组合导航</a></li><li><a href="https://github.com/scape-research/SOSNet" target="_blank" rel="noopener">SOSNet: Second Order Similarity Regularization for Local Descriptor Learning</a>,CVPR 2019,<strong><a href="https://research.scape.io/sosnet/" target="_blank" rel="noopener">[Project page]</a></strong> <strong><a href="https://arxiv.org/abs/1904.05019" target="_blank" rel="noopener">[Paper]</a></strong> <strong><a href="imgs/sosnet-poster.pdf">[Poster]</a></strong> <strong><a href="imgs/sosnet-oral.pdf">[Slides]</a></strong>，一种深度学习特征描述子</li><li><a href="https://github.com/oravus/seq2single" target="_blank" rel="noopener">Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation</a>,ICRA 2019,<strong>[<a href="https://arxiv.org/abs/1902.07381" target="_blank" rel="noopener">PDF</a>]</strong>,利用深度图像实现了大视角长时间的场景识别（根据深度图筛选得到不同深度层次的特征点然后与当前帧进行匹配，提高了场景召回率）</li><li><a href="https://github.com/rpng/calc2.0" target="_blank" rel="noopener">CALC2.0</a>,Convolutional Autoencoder for Loop Closure 2.0,用于闭环检测</li><li><a href="https://github.com/ethz-asl/segmap" target="_blank" rel="noopener">SegMap</a>,RSS 2018,<strong>[<a href="http://www.roboticsproceedings.org/rss14/p03.pdf" target="_blank" rel="noopener">PDF</a>]</strong>, 一种基于3D线段的地图表示，可用于场景识别/机器人定位/环境重建等</li><li><a href="https://github.com/cggos/msckf_vio_cg" target="_blank" rel="noopener">MSCKF_VIO</a>, a stereo version of MSCKF，基于MSCKF的双目VIO</li><li><a href="https://github.com/Relja/netvlad" target="_blank" rel="noopener">NetVLAD: CNN architecture for weakly supervised place recognition</a>，CVPR 2016, CNN框架弱监督学习场景识别,<strong>[<a href="https://www.di.ens.fr/willow/research/netvlad/" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/IFL-CAMP/easy_handeye" target="_blank" rel="noopener">easy_handeye</a>,Simple, straighforward ROS library for hand-eye calibration</li><li><a href="https://github.com/KinglittleQ/SuperPoint_SLAM" target="_blank" rel="noopener">SuperPoint-SLAM</a>,利用SuperPoint替换ORB特征点</li><li><a href="https://github.com/facebookresearch/pyrobot" target="_blank" rel="noopener">PyRobot: An Open Source Robotics Research Platform</a></li><li><a href="https://github.com/ethz-asl/hfnet" target="_blank" rel="noopener">From Coarse to Fine: Robust Hierarchical Localization at Large Scale with HF-Net</a>,<strong>[<a href="https://arxiv.org/abs/1812.03506" target="_blank" rel="noopener">PDF</a>]</strong></li><li><a href="https://github.com/mp3guy/ICPCUDA" target="_blank" rel="noopener">Super fast implementation of ICP in CUDA</a></li><li><a href="https://github.com/ethz-asl/volumetric_mapping" target="_blank" rel="noopener"> A generic interface for disparity map and pointcloud insertion</a></li><li><a href="https://github.com/tdsuper/SPHORB" target="_blank" rel="noopener">SPHORB: A Fast and Robust Binary Feature on the Sphere</a>,International Journal of Computer Vision 2015,<strong>[<a href="http://scs.tju.edu.cn/~lwan/paper/SPHORB/pdf/SPHORB-final-small.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://scs.tju.edu.cn/~lwan/paper/SPHORB/SPHORB.html" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/ETH3D/badslam" target="_blank" rel="noopener">BADSLAM: Bundle Adjusted Direct RGB-D SLAM</a>,CVPR 2019,<strong>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Schops_BAD_SLAM_Bundle_Adjusted_Direct_RGB-D_SLAM_CVPR_2019_paper.pdf" target="_blank" rel="noopener">PDF</a>]</strong></li><li><a href="https://github.com/uzh-rpg/rpg_e2vid" target="_blank" rel="noopener">High Speed and High Dynamic Range Video with an Event Camera</a>,arXiv,<strong>[<a href="http://rpg.ifi.uzh.ch/docs/arXiv19_Rebecq.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="http://rpg.ifi.uzh.ch/E2VID.html" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/PaoPaoRobot/Awesome-VIO" target="_blank" rel="noopener">Awesome-VIO</a>,Discuss about VIO in PaoPaoRobot group</li><li><a href="https://github.com/XinLiGH/GyroAllan" target="_blank" rel="noopener">GyroAllan</a>,陀螺仪随机误差的 Allan 方差分析, <a href="https://github.com/rpng/kalibr_allan" target="_blank" rel="noopener">Another version</a></li><li><a href="https://github.com/fangchangma/self-supervised-depth-completion" target="_blank" rel="noopener">Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and Monocular Camera</a>,ICRA 2019,<strong>[<a href="https://arxiv.org/pdf/1807.00275.pdf" target="_blank" rel="noopener">PDF</a>]</strong>, 优化LiDAR以及单目得到的深度图</li><li><a href="https://github.com/NVlabs/planercnn" target="_blank" rel="noopener">PlaneRCNN: 3D Plane Detection and Reconstruction from a Single Image</a>,CVPR 2019,<strong>[<a href="https://arxiv.org/pdf/1812.04072.pdf" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://research.nvidia.com/publication/2019-06_PlaneRCNN" target="_blank" rel="noopener">Project Page</a>]</strong>,通过单幅图像进行3D平面检测以及重建</li><li><a href="https://github.com/kokerf/DBow3" target="_blank" rel="noopener">DBow3</a>,注释版的DBow3代码</li><li><a href="https://github.com/VladyslavUsenko/basalt-mirror" target="_blank" rel="noopener">Visual-Inertial Mapping with Non-Linear Factor Recovery</a>,<strong>[<a href="https://arxiv.org/abs/1904.06504" target="_blank" rel="noopener">PDF</a>]</strong>,<strong>[<a href="https://vision.in.tum.de/research/vslam/basalt" target="_blank" rel="noopener">Project Page</a>]</strong>, 时空联合的VIO优化方案</li><li><a href="https://github.com/PaoPaoRobot/ICRA2019-paper-list" target="_blank" rel="noopener">ICRA2019-paper-list</a>,ICRA 2019论文列表（泡泡机器人出品暂时无链接）</li><li><a href="https://github.com/pedropro/CAPE" target="_blank" rel="noopener">Fast Cylinder and Plane Extraction from Depth Cameras for Visual Odometry</a>, IROS 2018,<strong>[<a href="https://arxiv.org/abs/1803.02380" target="_blank" rel="noopener">PDF</a>]</strong>,利用深度图进行圆柱检测以及平面检测进行VO</li><li><a href="https://github.com/kiran-mohan/SLAM-Algorithms-Octave" target="_blank" rel="noopener">Solutions to assignments of Robot Mapping Course WS 2013/14 by Dr. Cyrill Stachniss at University of Freiburg</a>,SLAM算法学习课后作业答案</li><li><a href="https://github.com/RonaldSun/VI-Stereo-DSO" target="_blank" rel="noopener">Direct sparse odometry combined with stereo cameras and IMU</a>,双目DSO+IMU</li><li><a href="https://github.com/HorizonAD/stereo_dso" target="_blank" rel="noopener">Direct Sparse Odometry with Stereo Cameras</a>,双目DSO</li><li><a href="https://github.com/uoip/g2opy" target="_blank" rel="noopener">Python binding of SLAM graph optimization framework g2o</a>,python版本的g2o实现</li><li><a href="https://github.com/rpautrat/SuperPoint" target="_blank" rel="noopener">SuperPoint: Self-Supervised Interest Point Detection and Description</a>, CVPR 2018, <strong>[<a href="https://arxiv.org/abs/1712.07629" target="_blank" rel="noopener">Paper</a>]</strong>, 深度学习描述子+描述</li><li><a href="https://github.com/lzx551402/contextdesc" target="_blank" rel="noopener">ContextDesc: Local Descriptor Augmentation with Cross-Modality Context</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1904.04084" target="_blank" rel="noopener">Paper</a>]</strong>, 深度学习描述子</li><li><a href="https://github.com/mihaidusmanu/d2-net" target="_blank" rel="noopener">D2-Net: A Trainable CNN for Joint Description and Detection of Local Features</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1905.03561" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://dsmn.ml/publications/d2-net.html" target="_blank" rel="noopener">Project Page</a>]</strong>, 深度学习关键点+描述</li><li><a href="https://github.com/ethz-asl/orb_slam_2_ros" target="_blank" rel="noopener">ROS interface for ORBSLAM2</a>,ROS版本的ORBSLAM2</li><li><a href="https://github.com/yan99033/CNN-SVO" target="_blank" rel="noopener">CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction</a>， <strong>[<a href="https://arxiv.org/pdf/1810.01011.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/ManiiXu/VINS-Mono-Learning" target="_blank" rel="noopener">VINS-Mono-Learning</a>，代码注释版VINS-Mono，初学者学习</li><li><a href="https://github.com/xdspacelab/openvslam" target="_blank" rel="noopener">OpenVSLAM: Versatile Visual SLAM Framework</a>, <strong>[<a href="https://openvslam.readthedocs.io/" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/fabianschenk/RESLAM" target="_blank" rel="noopener">RESLAM: A real-time robust edge-based SLAM system</a>, ICRA 2019, <strong>[<a href="https://github.com/fabianschenk/fabianschenk.github.io/raw/master/files/schenk_icra_2019.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/rubengooj/pl-slam" target="_blank" rel="noopener">PL-SLAM: a Stereo SLAM System through the Combination of Points and Line Segments</a>, <strong>[<a href="https://arxiv.org/abs/1705.09479" target="_blank" rel="noopener">Paper</a>]</strong>，线特征SLAM</li><li><a href="https://github.com/YipuZhao/GF_PL_SLAM" target="_blank" rel="noopener">Good Line Cutting: towards Accurate Pose Tracking of Line-assisted VO/VSLAM</a>, ECCV 2018, <strong>[<a href="https://sites.google.com/site/zhaoyipu/good-feature-visual-slam" target="_blank" rel="noopener">Project Page</a>]</strong>, 改进的PL-SLAM</li><li><a href="https://github.com/leoshine/Spherical_Regression" target="_blank" rel="noopener">Spherical Regression: Learning Viewpoints, Surface Normals and 3D Rotations on n-Spheres</a>, CVPR 2019, <strong>[<a href="http://arxiv.org/abs/1904.05404" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/icsl-Jeon/traj_gen_vis" target="_blank" rel="noopener">svo_edgelet</a>, 在线轨迹生成</li><li><a href="https://github.com/TimboKZ/caltech_samaritan" target="_blank" rel="noopener">Drone SLAM project for Caltech’s ME 134 Autonomy class</a>, <strong>[<a href="https://github.com/TimboKZ/caltech_samaritan/blob/master/CS134_Final_Project_Report.pdf" target="_blank" rel="noopener">PDF</a>]</strong></li><li><a href="https://github.com/icsl-Jeon/traj_gen_vis" target="_blank" rel="noopener">Online Trajectory Generation of a MAV for Chasing a Moving Target in 3D Dense Environments</a>, <strong>[<a href="https://arxiv.org/pdf/1904.03421.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/AtsushiSakai/PythonRobotics" target="_blank" rel="noopener">PythonRobotics</a>,<strong>[<a href="https://arxiv.org/abs/1808.10703" target="_blank" rel="noopener">Paper</a>]</strong>, <a href="https://github.com/onlytailei/CppRobotics" target="_blank" rel="noopener">CppRobotics</a></li><li><a href="https://github.com/izhengfan/ba_demo_ceres" target="_blank" rel="noopener">Bundle adjustment demo using Ceres Solver</a>,  <strong>[<a href="https://fzheng.me/2018/01/23/ba-demo-ceres/" target="_blank" rel="noopener">Blog</a>]</strong>, ceres实现BA</li><li><a href="https://github.com/shichaoy/cube_slam" target="_blank" rel="noopener">CubeSLAM: Monocular 3D Object Detection and SLAM</a>, <strong>[<a href="https://arxiv.org/abs/1806.00557" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/sshaoshuai/PointRCNN" target="_blank" rel="noopener">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.04244" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/nrupatunga/GIST-global-Image-Descripor" target="_blank" rel="noopener">GIST-Global Image Descriptor</a>, GIST描述子</li><li><a href="https://github.com/ethz-asl/mav_voxblox_planning" target="_blank" rel="noopener">mav voxblox planning</a>, MAV planning tools using voxblox as the map representation.</li><li><a href="https://github.com/zziz/kalman-filter" target="_blank" rel="noopener">Python Kalman Filter</a>, 30行实现卡尔曼滤波</li><li><a href="https://github.com/arpg/vicalib" target="_blank" rel="noopener">vicalib</a>, 视觉惯导系统标定工具</li><li><a href="https://github.com/simondlevy/BreezySLAM" target="_blank" rel="noopener">BreezySLAM</a>, 基于雷达的SLAM，支持Python(&amp;Matlab, C++, and Java)</li><li><a href="https://github.com/Yvon-Shong/Probabilistic-Robotics" target="_blank" rel="noopener">Probabilistic-Robotics</a>, 《概率机器人》中文版，书和课后习题</li><li><a href="https://github.com/emmjaykay/stanford_self_driving_car_code" target="_blank" rel="noopener">Stanford Self Driving Car Code</a>, <strong>[<a href="http://robots.stanford.edu/papers/junior08.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, 斯坦福自动驾驶车代码</li><li><a href="https://github.com/ndrplz/self-driving-car" target="_blank" rel="noopener">Udacity Self-Driving Car Engineer Nanodegree projects</a></li><li><a href="https://github.com/TUMFTM/Lecture_AI_in_Automotive_Technology" target="_blank" rel="noopener">Artificial Intelligence in Automotive Technology</a>, TUM自动驾驶技术中的人工智能课程</li><li><a href="https://github.com/hlzz/DeepMatchVO" target="_blank" rel="noopener">DeepMatchVO: Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation</a>,ICRA 2019, <strong>[<a href="https://arxiv.org/abs/1902.09103" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/zdzhaoyong/GSLAM" target="_blank" rel="noopener">GSLAM: A General SLAM Framework and Benchmark</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1902.07995" target="_blank" rel="noopener">Paper</a>]</strong>, 集成了各种传感器输入的SLAM统一框架</li><li><a href="https://github.com/izhengfan/se2lam" target="_blank" rel="noopener">Visual-Odometric Localization and Mapping for Ground Vehicles Using SE(2)-XYZ Constraints</a>，ICRA 2019,基于SE(2)-XYZ约束的VO系统</li><li><a href="https://github.com/nicolov/simple_slam_loop_closure" target="_blank" rel="noopener">Simple bag-of-words loop closure for visual SLAM</a>, <strong>[<a href="https://nicolovaligi.com/bag-of-words-loop-closure-visual-slam.html" target="_blank" rel="noopener">Blog</a>]</strong>, 回环</li><li><a href="https://github.com/rmsalinas/fbow" target="_blank" rel="noopener">FBOW (Fast Bag of Words), an extremmely optimized version of the DBow2/DBow3 libraries</a>,优化版本的DBow2/DBow3</li><li><a href="https://github.com/tomas789/tonav" target="_blank" rel="noopener">Multi-State Constraint Kalman Filter (MSCKF) for Vision-aided Inertial Navigation(master’s thesis)</a></li><li><a href="https://github.com/yuzhou42/MSCKF" target="_blank" rel="noopener">MSCKF</a>, MSCKF中文注释版</li><li><a href="https://github.com/hbtang/calibcamodo" target="_blank" rel="noopener">Calibration algorithm for a camera odometry system</a>, VO系统的标定程序</li><li><a href="https://github.com/cggos/vins_mono_cg" target="_blank" rel="noopener">Modified version of VINS-Mono</a>, 注释版本VINS Mono</li><li><a href="https://github.com/zhenpeiyang/RelativePose" target="_blank" rel="noopener">Extreme Relative Pose Estimation for RGB-D Scans via Scene Completion</a>,<strong>[<a href="https://arxiv.org/abs/1901.00063" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/jessecw/EPnP_Eigen" target="_blank" rel="noopener">Implementation of EPnP algorithm with Eigen</a>,利用Eigen编写的EPnP</li><li><a href="https://github.com/jiexiong2016/GCNv2_SLAM" target="_blank" rel="noopener">Real-time SLAM system with deep features</a>, 深度学习描述子(ORB vs. GCNv2)</li><li><a href="https://github.com/Huangying-Zhan/Depth-VO-Feat" target="_blank" rel="noopener">Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction</a>, CVPR 2018, 无监督单目深度恢复以及VO</li><li><a href="https://github.com/Phylliida/orbslam-windows" target="_blank" rel="noopener">ORB-SLAM-windows</a>, Windows版本的ORB-SLAM</li><li><a href="https://github.com/danping/structvio" target="_blank" rel="noopener">StructVIO : Visual-inertial Odometry with Structural Regularity of Man-made Environments</a>,<strong>[<a href="http://drone.sjtu.edu.cn/dpzou/project/structvio.html" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/irvingzhang/KalmanFiltering" target="_blank" rel="noopener">KalmanFiltering</a>, 各种卡尔曼滤波器的demo</li><li><a href="https://github.com/ZhenghaoFei/visual_odom" target="_blank" rel="noopener">Stereo Odometry based on careful Feature selection and Tracking</a>, <strong>[<a href="https://lamor.fer.hr/images/50020776/Cvisic2017.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, C++ OpenCV实现SOFT</li><li><a href="https://github.com/dzunigan/zSLAM" target="_blank" rel="noopener">Visual SLAM with RGB-D Cameras based on Pose Graph Optimization</a></li><li><a href="https://github.com/drsrinathsridhar/GRANSAC" target="_blank" rel="noopener">Multi-threaded generic RANSAC implemetation</a>, 多线程RANSAC</li><li><a href="https://github.com/PyojinKim/OPVO" target="_blank" rel="noopener">Visual Odometry with Drift-Free Rotation Estimation Using Indoor Scene Regularities</a>, BMVC 2017, <strong>[<a href="http://pyojinkim.me/pub/Visual-Odometry-with-Drift-Free-Rotation-Estimation-Using-Indoor-Scene-Regularities/" target="_blank" rel="noopener">Project Page</a>]</strong>，利用平面正交信息进行VO</li><li><a href="https://github.com/baidu/ICE-BA" target="_blank" rel="noopener">ICE-BA</a>, CVPR 2018, <strong>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_ICE-BA_Incremental_Consistent_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/AIBluefisher/GraphSfM" target="_blank" rel="noopener">GraphSfM: Robust and Efficient Graph-based Structure from Motion</a>, <strong>[<a href="https://aibluefisher.github.io/GraphSfM/" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/cuitaixiang/LOAM_NOTED" target="_blank" rel="noopener">LOAM_NOTED</a>, loam中文注解版</li><li><a href="https://github.com/Ethan-Zhou/MWO" target="_blank" rel="noopener">Divide and Conquer: Effcient Density-Based Tracking of 3D Sensors in Manhattan Worlds</a>,ACCV 2016,<strong>[<a href="http://users.cecs.anu.edu.au/~u5535909/" target="_blank" rel="noopener">Project Page</a>]</strong>,曼哈顿世界利用深度传感器进行旋转量平移量分离优化</li><li><a href="https://github.com/jstraub/rtmf" target="_blank" rel="noopener">Real-time Manhattan World Rotation Estimation in 3D</a>,IROS 2015,实时曼哈顿世界旋转估计</li><li><a href="https://github.com/uzh-rpg/event-based_vision_resources" target="_blank" rel="noopener">Event-based Vision Resources</a>，关于事件相机的资源</li><li><a href="https://github.com/DeepTecher/AutonomousVehiclePaper" target="_blank" rel="noopener">AutonomousVehiclePaper</a>，无人驾驶相关论文速递</li><li><a href="https://github.com/wutianyiRosun/Segmentation.X" target="_blank" rel="noopener">Segmentation.X</a>, Segmentation相关论文&amp;代码</li><li><a href="https://github.com/amusi/CVPR2019-Code" target="_blank" rel="noopener">CVPR-2019</a>, CVPR 2019 论文开源项目合集</li><li><a href="https://github.com/kanster/awesome-slam" target="_blank" rel="noopener">awesome-slam</a>, SLAM合集</li><li><a href="https://github.com/tzutalin/awesome-visual-slam" target="_blank" rel="noopener">awesome-visual-slam</a>, 视觉SLAM合集</li><li><a href="https://github.com/zziz/pwc" target="_blank" rel="noopener">Papers with code</a>, 周更论文with代码</li><li><a href="https://github.com/cbsudux/awesome-human-pose-estimation" target="_blank" rel="noopener">Awesome Human Pose Estimation</a>,<a href="https://github.com/nkalavak/awesome-object-pose" target="_blank" rel="noopener">awesome-object-pose</a>, 位姿估计合集</li><li><a href="https://github.com/Ewenwan/MVision" target="_blank" rel="noopener">MVision</a>, 大礼包：机器人视觉 移动机器人 VS-SLAM ORB-SLAM2 深度学习目标检测 yolov3 行为检测 opencv PCL 机器学习 无人驾驶</li></ul><h2 id="Pose-Object-tracking"><a href="#Pose-Object-tracking" class="headerlink" title="Pose/Object tracking"></a>Pose/Object tracking</h2><ul><li><a href="https://github.com/KovenYu/MAR" target="_blank" rel="noopener">Unsupervised person re-identification by soft multilabel learning</a>,CVPR 2019,  <strong>[<a href="https://kovenyu.com/papers/2019_CVPR_MAR.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/tianzhi0549/FCOS" target="_blank" rel="noopener">FCOS: Fully Convolutional One-Stage Object Detection</a>,ICCV 2019,  <strong>[<a href="https://arxiv.org/abs/1904.01355" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/yangli18/hand_detection" target="_blank" rel="noopener">Hand Detection and Orientation Estimation</a></li><li><a href="https://github.com/Wanggcong/Spatial-Temporal-Re-identification" target="_blank" rel="noopener">Spatial-Temporal Person Re-identification</a>,AAAI 2019,<strong>[<a href="https://arxiv.org/abs/1812.03282" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/layumi/Person_reID_baseline_pytorch" target="_blank" rel="noopener">A tiny, friendly, strong pytorch implement of person re-identification baseline. <strong>Tutorial</strong></a>,CVPR 2019,  <strong>[<a href="https://arxiv.org/abs/1904.07223" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/tengteng95/Pose-Transfer" target="_blank" rel="noopener">Progressive Pose Attention for Person Image Generation</a>,CVPR 2019,<strong>[<a href="http://arxiv.org/abs/1904.03349" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/shamangary/FSA-Net" target="_blank" rel="noopener">FSA-Net: Learning Fine-Grained Structure Aggregation for Head Pose Estimation from a Single Image</a>, CVPR 2019,<strong>[<a href="https://github.com/shamangary/FSA-Net/blob/master/0191.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/yuanyuanli85/Fast_Human_Pose_Estimation_Pytorch" target="_blank" rel="noopener">An unoffical implemention for paper “Fast Human Pose Estimation”</a>, CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1811.05419" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/edvardHua/PoseEstimationForMobile" target="_blank" rel="noopener">Real-time single person pose estimation for Android and iOS</a>,手机端实现人体位姿估计</li><li><a href="https://github.com/cbsudux/Human-Pose-Estimation-101" target="_blank" rel="noopener">Basics of 2D and 3D Human Pose Estimation</a>,人体姿态估计入门</li><li><a href="https://github.com/OceanPang/Libra_R-CNN" target="_blank" rel="noopener">Libra R-CNN: Towards Balanced Learning for Object Detection</a></li><li><a href="https://github.com/HRNet/HRNet-Object-Detection" target="_blank" rel="noopener">High-resolution networks (HRNets) for object detection</a>, <strong>[<a href="https://arxiv.org/pdf/1904.04514.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/xiaolonw/TimeCycle" target="_blank" rel="noopener">Learning Correspondence from the Cycle-Consistency of Time</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1903.07593" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/zju3dv/pvnet" target="_blank" rel="noopener">PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.11788" target="_blank" rel="noopener">Paper</a>], [<a href="https://zju3dv.github.io/pvnet" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/mkocabas/EpipolarPose" target="_blank" rel="noopener">Self-Supervised Learning of 3D Human Pose using Multi-view Geometry</a>, CVPR 2018, <strong>[<a href="https://arxiv.org/abs/1903.02330" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/vita-epfl/openpifpaf" target="_blank" rel="noopener">PifPaf: Composite Fields for Human Pose Estimation</a>, <strong>[<a href="https://arxiv.org/abs/1903.06593" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" target="_blank" rel="noopener">Deep High-Resolution Representation Learning for Human Pose Estimation</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09212.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://jingdongwang2017.github.io/Projects/HRNet/PoseEstimation.html" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/YuliangXiu/PoseFlow" target="_blank" rel="noopener">PoseFlow: Efficient Online Pose Tracking)</a>, BMVC 2018, <strong>[<a href="https://arxiv.org/abs/1802.00977" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/vana77/Bottom-up-Clustering-Person-Re-identification" target="_blank" rel="noopener">A Bottom-Up Clustering Approach to Unsupervised Person Re-identification</a>，AAAI 2019, 重定位</li><li><a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener">Fast Online Object Tracking and Segmentation: A Unifying Approach</a>,CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1812.05050" target="_blank" rel="noopener">Paper</a>] [<a href="https://youtu.be/I_iOVrcpEBw" target="_blank" rel="noopener">Video</a>] [<a href="http://www.robots.ox.ac.uk/~qwang/SiamMask" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/TuSimple/simpledet" target="_blank" rel="noopener">SimpleDet - A Simple and Versatile Framework for Object Detection and Instance Recognition</a>,<strong>[<a href="https://arxiv.org/abs/1903.05831" target="_blank" rel="noopener">Paper</a>]</strong> </li></ul><h2 id="Depth-Disparity-amp-Flow-estimation"><a href="#Depth-Disparity-amp-Flow-estimation" class="headerlink" title="Depth/Disparity &amp; Flow estimation"></a>Depth/Disparity &amp; Flow estimation</h2><ul><li>[<strong>Depth</strong>]<a href="https://github.com/ethan-li-coding/SemiGlobalMatching" target="_blank" rel="noopener">SemiGlobalMatching</a>, SGM双目立体匹配算法完整实现，代码规范，注释丰富且清晰，CSDN同步教学</li><li><a href="https://github.com/callmeray/PointMVSNet" target="_blank" rel="noopener">PointMVSNet: Point-based Multi-view Stereo Network</a>,ICCV 2019,<strong>[<a href="https://arxiv.org/abs/1908.04422" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/JiaxiongQ/DeepLiDAR" target="_blank" rel="noopener">DeepLiDAR</a>,CVPR 2019, <strong>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qiu_DeepLiDAR_Deep_Surface_Normal_Guided_Depth_Prediction_for_Outdoor_Scene_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, 单张RGB图像+稀疏雷达数据进行室外场景深度估计</li><li><a href="https://github.com/atapour/monocularDepth-Inference" target="_blank" rel="noopener">Real-Time Monocular Depth Estimation using Synthetic Data with Domain Adaptation via Image Style Transfer</a>,CVPR 2018, <strong>[<a href="http://breckon.eu/toby/publications/papers/abarghouei18monocular.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/princeton-vl/YouTube3D" target="_blank" rel="noopener">Learning Single-Image Depth from Videos using Quality Assessment Networks</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1806.09573" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="http://www-personal.umich.edu/~wfchen/youtube3d/" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/WERush/SCDA" target="_blank" rel="noopener">SCDA: Adapting Object Detectors via Selective Cross-Domain Alignment</a>,CVPR 2019, <strong>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Adapting_Object_Detectors_via_Selective_Cross-Domain_Alignment_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="http://zhuxinge.me/aboutme.html" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/fabiotosi92/monoResMatch-Tensorflow" target="_blank" rel="noopener">Learning monocular depth estimation infusing traditional stereo knowledge</a>,CVPR 2019,<strong>[<a href="https://vision.disi.unibo.it/~ftosi/papers/monoResMatch.pdf" target="_blank" rel="noopener">PDF</a>]</strong></li><li><a href="https://github.com/laoreja/HPLFlowNet" target="_blank" rel="noopener">HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on Large-scale Point Clouds</a>,CVPR 2019,<strong>[<a href="hhttps://web.cs.ucdavis.edu/~yjlee/projects/cvpr2019-HPLFlowNet.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/feihuzhang/GANet" target="_blank" rel="noopener">GA-Net: Guided Aggregation Net for End-to-end Stereo Matching</a>,CVPR 2019,<strong>[<a href="https://arxiv.org/pdf/1904.06587.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/sunghoonim/DPSNet" target="_blank" rel="noopener">DPSNet: End-to-end Deep Plane Sweep Stereo</a>,ICLR 2019,<strong>[<a href="https://openreview.net/pdf?id=ryeYHi0ctQ" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/muskie82/AR-Depth-cpp" target="_blank" rel="noopener">Fast Depth Densification for Occlusion-aware Augmented Reality</a>, SIGGRAPH-Asia 2018, <strong>[<a href="https://homes.cs.washington.edu/~holynski/publications/occlusion/index.html" target="_blank" rel="noopener">Project Page</a>]</strong>,<a href="https://github.com/facebookresearch/AR-Depth" target="_blank" rel="noopener">another version</a></li><li><a href="https://github.com/CVLAB-Unibo/Learning2AdaptForStereo" target="_blank" rel="noopener">Learning To Adapt For Stereo</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1904.02957" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/JiaRenChang/PSMNet" target="_blank" rel="noopener">Pyramid Stereo Matching Network</a>,<strong>[<a href="https://arxiv.org/abs/1803.08669" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/lelimite4444/BridgeDepthFlow" target="_blank" rel="noopener">Bridging Stereo Matching and Optical Flow via Spatiotemporal Correspondence</a>, <strong>[<a href="https://arxiv.org/abs/1905.09265" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/wvangansbeke/Sparse-Depth-Completion" target="_blank" rel="noopener">Sparse Depth Completion</a>, <strong>[<a href="https://arxiv.org/pdf/1902.05356.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, RGB图像辅助雷达深度估计</li><li><a href="https://github.com/sshan-zhao/GASDA" target="_blank" rel="noopener">GASDA</a>, CVPR 2019, <strong>[<a href="https://sshan-zhao.github.io/papers/gasda.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/xy-guo/MVSNet_pytorch" target="_blank" rel="noopener">MVSNet: Depth Inference for Unstructured Multi-view Stereo</a>, <strong>[<a href="https://arxiv.org/abs/1804.02505" target="_blank" rel="noopener">Paper</a>]</strong>, 非官方实现版本的MVSNet</li><li><a href="https://github.com/HKUST-Aerial-Robotics/Stereo-RCNN" target="_blank" rel="noopener">Stereo R-CNN based 3D Object Detection for Autonomous Driving</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09738.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo" target="_blank" rel="noopener">Real-time self-adaptive deep stereo</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1810.05424" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/ialhashim/DenseDepth" target="_blank" rel="noopener">High Quality Monocular Depth Estimation via Transfer Learning</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.11941" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://ialhashim.github.io/publications/index.html" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/xy-guo/GwcNet" target="_blank" rel="noopener">Group-wise Correlation Stereo Network</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1903.04025" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/phuang17/DeepMVS" target="_blank" rel="noopener">DeepMVS: Learning Multi-View Stereopsis</a>, CVPR 2018,<strong>[<a href="https://phuang17.github.io/DeepMVS/index.html" target="_blank" rel="noopener">Project Page</a>]</strong>,多目深度估计</li><li><a href="https://github.com/sampepose/flownet2-tf" target="_blank" rel="noopener">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</a>, CVPR 2017, 深度学习光流恢复</li><li><a href="https://github.com/DLuensch/StereoVision-ADCensus" target="_blank" rel="noopener">StereoVision-ADCensus</a>,深度恢复代码集合(<strong>ADCensus, SGBM, BM</strong>)</li><li><a href="https://github.com/yangguorun/SegStereo" target="_blank" rel="noopener">SegStereo: Exploiting Semantic Information for Disparity Estimation</a>, 探究语义信息在深度估计中的作用</li><li><a href="https://github.com/kuantingchen04/Light-Field-Depth-Estimation" target="_blank" rel="noopener">Light Filed Depth Estimation using GAN</a>，利用GAN进行光场深度恢复</li><li><a href="https://github.com/daniilidis-group/EV-FlowNet" target="_blank" rel="noopener">EV-FlowNet: Self-Supervised Optical Flow for Event-based Cameras</a>,Proceedings of Robotics 2018,<strong>[<a href="https://arxiv.org/abs/1802.06898" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/vt-vl-lab/DF-Net" target="_blank" rel="noopener">DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency</a>, ECCV 2018, <strong>[<a href="https://arxiv.org/abs/1809.01649" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/yzcjtr/GeoNet" target="_blank" rel="noopener">GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose</a>, CVPR 2018, <strong>[<a href="https://arxiv.org/abs/1803.02276" target="_blank" rel="noopener">Paper</a>]</strong></li></ul><h2 id="3D-amp-Graphic"><a href="#3D-amp-Graphic" class="headerlink" title="3D &amp; Graphic"></a>3D &amp; Graphic</h2><ul><li><a href="https://github.com/WangYueFt/prnet" target="_blank" rel="noopener">PRNet: Self-Supervised Learning for Partial-to-Partial Registration</a>,NeurIPS 2019</li><li><a href="https://github.com/nkolot/SPIN" target="_blank" rel="noopener">Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop</a>,ICCV 2019, <strong>[<a href="https://arxiv.org/pdf/1909.12828.pdf" target="_blank" rel="noopener">Paper</a>]</strong> , <strong>[<a href="https://www.seas.upenn.edu/~nkolot/projects/spin/" target="_blank" rel="noopener">Project Page</a>]</strong> </li><li><a href="https://github.com/microsoft/multiview-human-pose-estimation-pytorch" target="_blank" rel="noopener">Cross View Fusion for 3D Human Pose Estimation</a>,ICCV 2019, <strong>[<a href="https://arxiv.org/abs/1909.01203" target="_blank" rel="noopener">Paper</a>]</strong> ,跨视角3D位姿估计</li><li><a href="https://github.com/Fanziapril/mvfnet" target="_blank" rel="noopener">MVF-Net: Multi-View 3D Face Morphable Model Regression</a>,多视角3D人脸重建, <strong>[<a href="https://arxiv.org/abs/1904.04473" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/saurabheights/KillingFusion" target="_blank" rel="noopener">KillingFusion</a></li><li><a href="https://github.com/PRBonn/refusion" target="_blank" rel="noopener">ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals</a>, <strong>[<a href="https://arxiv.org/pdf/1905.02082.pdf" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/Lotayou/densebody_pytorch" target="_blank" rel="noopener">densebody_pytorch</a>, <strong>[<a href="https://arxiv.org/abs/1903.10153v3" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/svip-lab/PlanarReconstruction" target="_blank" rel="noopener">Single-Image Piece-wise Planar 3D Reconstruction via Associative Embedding</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09777.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, 单目3D重建</li><li><a href="https://github.com/sunset1995/HorizonNet" target="_blank" rel="noopener">HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch Data Augmentation</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1901.03861" target="_blank" rel="noopener">Paper</a>]</strong>, 深度学习全景转3D</li><li><a href="https://github.com/Microsoft/O-CNN" target="_blank" rel="noopener">Adaptive O-CNN: A Patch-based Deep Representation of 3D Shapes</a>,SIGGRAPH Asia 2018, <strong>[<a href="https://wang-ps.github.io/AO-CNN.html" target="_blank" rel="noopener">Project Page</a>]</strong></li></ul><h2 id="Other-Collections"><a href="#Other-Collections" class="headerlink" title="Other Collections"></a>Other Collections</h2><ul><li><a href="https://github.com/timqian/chinese-independent-blogs" target="_blank" rel="noopener">chinese-independent-blogs</a>, 中文独立博客集锦</li><li><a href="https://github.com/RenYurui/StructureFlow" target="_blank" rel="noopener">StructureFlow: Image Inpainting via Structure-aware Appearance Flow</a>,图像inpainting</li><li><a href="https://github.com/ruanyf/free-books" target="_blank" rel="noopener">free-books</a>,互联网上的免费书籍</li><li><a href="https://github.com/academicpages/academicpages.github.io" target="_blank" rel="noopener">AcademicPages</a>,通用的学术主页模版</li><li><a href="https://github.com/microsoft/MMdnn" target="_blank" rel="noopener">MMdnn</a>,实现深度学习模型之间的相互转换</li><li><a href="https://github.com/abner2015/tensorflow2caffemodel" target="_blank" rel="noopener">tensorflow2caffemodel</a>,tensorflow模型转caffemodel</li><li><a href="https://github.com/fengdu78/lihang-code" target="_blank" rel="noopener">lihang-code</a>,《统计学习方法》的代码实现</li><li><a href="https://github.com/DLTcollab/sse2neon" target="_blank" rel="noopener">sse2neon</a>,<a href="https://github.com/jratcliff63367/sse2neon" target="_blank" rel="noopener">sse2neon</a>,SSE转neon，嵌入式移植时可能会用到;</li><li><a href="https://github.com/alirezadir/Production-Level-Deep-Learning" target="_blank" rel="noopener">Production-Level-Deep-Learning</a>,深度学习模型部署流程</li><li><a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch" target="_blank" rel="noopener">动手学深度学习Dive-into-DL-PyTorch</a></li><li><a href="https://github.com/deeplearning-ai/machine-learning-yearning-cn" target="_blank" rel="noopener">machine-learning-yearning-cn</a>，Machine Learning Yearning 中文版 - 《机器学习训练秘籍》 - Andrew Ng 著</li><li><a href="https://github.com/academicpages/academicpages.github.io" target="_blank" rel="noopener">academicpages.github.io</a>，学术主页模板</li><li><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">Coursera-ML-AndrewNg-Notes</a>,吴恩达老师的机器学习课程个人笔记</li><li><a href="https://github.com/roboticcam/machine-learning-notes" target="_blank" rel="noopener">machine-learning-notes</a>,机器学习，概率模型和深度学习的讲义(1500+页)和视频链接</li><li><a href="https://github.com/scutan90/CNN-Visualization" target="_blank" rel="noopener">CNN-Visualization</a>,CNN可视化、理解CNN</li><li><a href="https://github.com/mrgloom/awesome-semantic-segmentation" target="_blank" rel="noopener">Awesome Semantic Segmentation</a>, 语义分割集合</li><li><a href="https://github.com/mengyuest/iros2018-slam-papers" target="_blank" rel="noopener">IROS2018 SLAM Collections</a>, IROS 2018集合</li><li><a href="https://github.com/TerenceCYJ/VP-SLAM-SC-papers" target="_blank" rel="noopener">VP-SLAM-SC-papers</a>,Visual Positioning &amp; SLAM &amp; Spatial Cognition 论文统计与分析</li><li><a href="https://github.com/HuaizhengZhang/Awesome-System-for-Machine-Learning" target="_blank" rel="noopener">Awesome System for Machine Learning</a></li><li><a href="https://github.com/Thinkgamer/Machine-Learning-With-Python" target="_blank" rel="noopener">Machine-Learning-With-Python</a>, 《机器学习实战》python代码实现</li><li><a href="https://github.com/qqfly/how-to-learn-robotics" target="_blank" rel="noopener">How to learn robotics</a>, 开源机器人学学习指南</li><li><a href="https://github.com/kjw0612/awesome-deep-vision" target="_blank" rel="noopener">Awesome Deep Vision</a>,DL在CV领域的应用</li><li><a href="https://github.com/YapengTian/Single-Image-Super-Resolution" target="_blank" rel="noopener">Single-Image-Super-Resolution</a>, 一个有关<strong>图像超分辨</strong>的合集</li><li><a href="https://github.com/wifity/ai-report" target="_blank" rel="noopener">ai report</a>, AI相关的研究报告</li><li><a href="https://paperswithcode.com/sota" target="_blank" rel="noopener">State-of-the-art papers and code</a>,搜集了目前sota的论文以及代码</li><li><a href="https://github.com/extreme-assistant/cvpr2019" target="_blank" rel="noopener">CVPR 2019 (Papers/Codes/Project/Paper reading)</a></li><li><a href="https://github.com/openMVG/awesome_3DReconstruction_list" target="_blank" rel="noopener">A curated list of papers &amp; resources linked to 3D reconstruction from images</a>,有关三维重建的论文汇总</li><li><a href="https://github.com/nebula-beta/SLAM-Jobs" target="_blank" rel="noopener">SLAM-Jobs</a>, SLAM/SFM求职指南</li><li><a href="https://github.com/stevewongv/SPANet" target="_blank" rel="noopener">Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset</a>,CVPR 2019,去雨</li><li><a href="https://github.com/hezhangsprinter/DCPDN" target="_blank" rel="noopener">Densely Connected Pyramid Dehazing Network</a>,CVPR 2018,去雾</li><li><a href="https://github.com/open-mmlab/mmsr" target="_blank" rel="noopener">MMSR</a>，MMLAB推出的超分辨工具箱</li><li><a href="https://github.com/Bartzi/stn-ocr" target="_blank" rel="noopener">深度学习OCR</a></li><li><a href="https://github.com/Vay-keen/Machine-learning-learning-notes" target="_blank" rel="noopener">西瓜书🍉学习笔记</a></li><li><a href="https://github.com/wwxFromTju/awesome-reinforcement-learning-zh" target="_blank" rel="noopener">awesome-reinforcement-learning-zh</a>,强化学习从入门到放弃的资料</li><li><a href="https://github.com/cszn/DPSR" target="_blank" rel="noopener">Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels</a>,CVPR 2019,超分辨</li><li><a href="https://github.com/lzhbrian/Cool-Fashion-Papers" target="_blank" rel="noopener">Cool Fashion Papers</a>, Cool resources about Fashion + AI.</li><li><a href="https://github.com/nbei/Deep-Flow-Guided-Video-Inpainting" target="_blank" rel="noopener">Deep Flow-Guided Video Inpainting</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1806.10447.pdf" target="_blank" rel="noopener">Paper</a>]</strong> ,图像修复</li><li><a href="https://github.com/dbolya/yolact" target="_blank" rel="noopener">YOLACT: Real-time Instance Segmentation</a></li><li><a href="https://github.com/lyl8213/Plate_Recognition-LPRnet" target="_blank" rel="noopener">LPRNet: License Plate Recognition via Deep Neural Networks</a>, <strong>[<a href="https://arxiv.org/pdf/1806.10447.pdf" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/xiaofengShi/CHINESE-OCR" target="_blank" rel="noopener">CHINESE-OCR</a>, 运用tf实现自然场景文字检测</li><li><a href="https://github.com/PerpetualSmile/BeautyCamera" target="_blank" rel="noopener">BeautyCamera</a>, 美颜相机，具有人脸检测、磨皮美白人脸、滤镜、调节图片、摄像功能</li><li><a href="https://github.com/zhengzhugithub/CV-arXiv-Daily" target="_blank" rel="noopener">CV-arXiv-Daily</a>, 分享计算机视觉每天的arXiv文章</li><li><a href="https://github.com/lyndonzheng/Pluralistic-Inpainting" target="_blank" rel="noopener">Pluralistic-Inpainting</a>, <a href="https://arxiv.org/abs/1903.04227" target="_blank" rel="noopener">ArXiv</a> | <a href="http://www.chuanxiaz.com/publication/pluralistic/" target="_blank" rel="noopener">Project Page</a> | <a href="http://www.chuanxiaz.com/project/pluralistic/" target="_blank" rel="noopener">Online Demo</a> | <a href="https://www.youtube.com/watch?v=9V7rNoLVmSs" target="_blank" rel="noopener">Video(demo)</a></li><li><a href="https://github.com/Jezzamonn/fourier" target="_blank" rel="noopener">An Interactive Introduction to Fourier Transforms</a>, 超棒的傅里叶变换图形化解释</li><li><a href="https://github.com/datawhalechina/pumpkin-book" target="_blank" rel="noopener">pumpkin-book</a>, 《机器学习》（西瓜书）公式推导解析</li><li><a href="https://github.com/JuliaLang/julia" target="_blank" rel="noopener">Julia</a></li><li><a href="https://github.com/alan-turing-institute/MLJ.jl" target="_blank" rel="noopener">A Julia machine learning framework</a>，一种基于Julia的机器学习框架</li><li><a href="https://github.com/ZhaoJ9014/face.evoLVe.PyTorch" target="_blank" rel="noopener">High-Performance Face Recognition Library on PyTorch</a>，人脸识别库</li><li><a href="https://github.com/enggen/Deep-Learning-Coursera" target="_blank" rel="noopener">Deep-Learning-Coursera</a>，深度学习教程（deeplearning.ai）</li><li><a href="https://github.com/RemoteML/bestofml" target="_blank" rel="noopener">The best resources around Machine Learning</a></li><li><a href="https://github.com/cydonia999/VGGFace2-pytorch" target="_blank" rel="noopener">VGGFace2: A dataset for recognising faces across pose and age</a></li><li><a href="https://github.com/SmirkCao/Lihang" target="_blank" rel="noopener">Statistical learning methods</a>，统计学习方法</li><li><a href="https://live.bilibili.com/7332534?visit_id=9ytrx9lpsy80" target="_blank" rel="noopener">End-to-end Adversarial Learning for Generative Conversational Agents</a>，2017，介绍了一种端到端的基于GAN的聊天机器人</li><li><a href="https://github.com/yulunzhang/RNAN" target="_blank" rel="noopener">Residual Non-local Attention Networks for Image Restoration</a>,ICLR 2019.</li><li><a href="https://github.com/HelenMao/MSGAN" target="_blank" rel="noopener">MSGAN: Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis</a>, CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1903.05628" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/NVlabs/SPADE" target="_blank" rel="noopener">SPADE: Semantic Image Synthesis with Spatially-Adaptive Normalization</a>,CVPR 2019, <strong>[<a href="https://nvlabs.github.io/SPADE/" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/Oldpan/Faceswap-Deepfake-Pytorch" target="_blank" rel="noopener">Faceswap with Pytorch or DeepFake with Pytorch</a>, 换脸</li><li><a href="https://github.com/iperov/DeepFaceLab" target="_blank" rel="noopener">DeepFaceLab</a>, 换脸</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The post contains papers-with-code about SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc. &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/Vincentqyw/Recent-Stars-2020&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img src=&quot;https://github-readme-stats.vercel.app/api/pin/?username=Vincentqyw&amp;amp;repo=Recent-Stars-2020&amp;amp;show_owner=false&amp;amp;theme=default&quot; alt=&quot;ReadMe Card&quot;&gt;&lt;/a&gt; &lt;/p&gt;
&lt;!-- [![GitHub stars](https://img.shields.io/github/stars/Vincentqyw/Recent-Stars-2019.svg?logo=github&amp;label=Stars)](https://github.com/Vincentqyw/Recent-Stars-2019) --&gt;
    
    </summary>
    
    
    
      <category term="SLAM" scheme="https://www.vincentqin.tech/tags/SLAM/"/>
    
      <category term="disparity" scheme="https://www.vincentqin.tech/tags/disparity/"/>
    
      <category term="pose-tracking" scheme="https://www.vincentqin.tech/tags/pose-tracking/"/>
    
      <category term="object-tracking" scheme="https://www.vincentqin.tech/tags/object-tracking/"/>
    
      <category term="depth-estimation" scheme="https://www.vincentqin.tech/tags/depth-estimation/"/>
    
      <category term="flow-estimation" scheme="https://www.vincentqin.tech/tags/flow-estimation/"/>
    
      <category term="3D-graphics" scheme="https://www.vincentqin.tech/tags/3D-graphics/"/>
    
  </entry>
  
  <entry>
    <title>开启SSR模式</title>
    <link href="https://www.vincentqin.tech/posts/build-ssr-server/"/>
    <id>https://www.vincentqin.tech/posts/build-ssr-server/</id>
    <published>2019-03-31T05:56:15.000Z</published>
    <updated>2020-04-15T14:39:28.955Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>关于科学上网，食用别人调配的酸酸乳总觉味道不对，自食其力心里才会感到踏实。受<a href="https://newdee.cf/" target="_blank" rel="noopener">Newdee老贼</a>指点，鄙人成功在服务器上开启了酸酸乳服务。详细过程Newdee已经在博文“<a href="https://newdee.cf/posts/1420aa47/" target="_blank" rel="noopener">SS服务器搭建</a>”介绍地相当详细。本人记性不好，遂本文将记录几个关键步骤，以备后续不时之需。<br><strong><font color="#FF0000" face="宋体">注意：本文仅供个人学习使用，不可用于商业或者违法行为！</font></strong></p><a id="more"></a><h2 id="购买服务器"><a href="#购买服务器" class="headerlink" title="购买服务器"></a>购买服务器</h2><ul><li>购买服务器(支持alipay &amp; wechat pay)，地址: <a href="https://www.vultr.com/?ref=7996819" target="_blank" rel="noopener">https://www.vultr.com/</a></li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/vultr.png"></p><ul><li>经过几个步骤：1. Server Location, 2. Server Type, 3. Server Size, 4. Additional Features,5,6,7可以忽略，最后点击右下角的<strong>Deploy New</strong>即可部署。</li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/buy-vultr.png"></p><p>后台是这样的：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/vultr-backend.png"></p><p>然后根据IP以及用户名利用SSH在本地进行远程连接，进行以下步骤。</p><h2 id="安装SSR"><a href="#安装SSR" class="headerlink" title="安装SSR"></a>安装SSR</h2><p>如果是单用户使用，进入服务器直接执行下述命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -N --no-check-certificate https://raw.githubusercontent.com/Vincentqyw/doubi/master/ssr.sh &amp;&amp; chmod +x ssr.sh &amp;&amp; bash ssr.sh</span><br></pre></td></tr></table></figure><p>关于加密协议以及混淆的设置参见下图：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/account.png"></p><p>多用户使用的版本（可配置多个账号）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/ssrmu.sh &amp;&amp; chmod +x ssrmu.sh &amp;&amp; bash ssrmu.sh</span><br></pre></td></tr></table></figure><p>设置完毕之后，后续进行管理直接运行<code>bash ssrmu.sh</code>选择不同的功能项即可。</p><h2 id="封禁某些端口-可选"><a href="#封禁某些端口-可选" class="headerlink" title="封禁某些端口(可选)"></a>封禁某些端口(可选)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -N --no-check-certificate https://raw.githubusercontent.com/Vincentqyw/doubi/master/ban_iptables.sh &amp;&amp; chmod +x ban_iptables.sh &amp;&amp; bash ban_iptables.sh</span><br></pre></td></tr></table></figure><p>选择封禁垃圾邮件端口就行。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/ban-mails.png"></p><h2 id="BBR加速-可选"><a href="#BBR加速-可选" class="headerlink" title="BBR加速(可选)"></a>BBR加速(可选)</h2><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/bbr.png"></p><h2 id="安装SSRR-可选"><a href="#安装SSRR-可选" class="headerlink" title="安装SSRR(可选)"></a>安装SSRR(可选)</h2><p>接下来的链接给出了SSRR的安装教程，不再赘述，<a href="https://gist.github.com/biezhi/45fac901f02f7c867e46aecd41076d70#kcp-%E5%AE%A2%E6%88%B7%E7%AB%AF" target="_blank" rel="noopener">Link</a>。</p><h2 id="建立快照"><a href="#建立快照" class="headerlink" title="建立快照"></a>建立快照</h2><p>建立系统快照就是将系统某个状态下的各种数据记录在一个文件里，下一次新建完主机后恢复快照就能够恢复成之前系统的样子。</p><p>若已有了主机，点击下图所示的<code>Snapshots</code>对该系统建立快照。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/snapshots-step1.png"></p><p>随后就会出现下图所示的页面，在<code>Label</code>一栏输入这个快照的标签，方便区分不同的快照。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/snapshots-step2.png"></p><p>若想对新建的系统恢复以前建立的快照，可以点击上图中的类似于<code>循环</code>的标志。</p><h2 id="不可描述"><a href="#不可描述" class="headerlink" title="不可描述"></a>不可描述</h2><ul><li><a href="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/SSR-WIN-ANDROID-IOS.7z" target="_blank" rel="noopener">不可描述</a></li><li>PC终端可自行挑选，鄙人推荐<a href="https://www.termius.com/windows" target="_blank" rel="noopener">termius</a>, <a href="https://git-scm.com/downloads" target="_blank" rel="noopener">Git Bash</a></li><li>参考链接：<a href="https://gist.github.com/biezhi/45fac901f02f7c867e46aecd41076d70" target="_blank" rel="noopener">ShadowsocksR 协议插件文档</a></li></ul><p>手机端以及电脑端输入对应的IP/端口/混淆/加密等信息进行连接即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于科学上网，食用别人调配的酸酸乳总觉味道不对，自食其力心里才会感到踏实。受&lt;a href=&quot;https://newdee.cf/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Newdee老贼&lt;/a&gt;指点，鄙人成功在服务器上开启了酸酸乳服务。详细过程Newdee已经在博文“&lt;a href=&quot;https://newdee.cf/posts/1420aa47/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SS服务器搭建&lt;/a&gt;”介绍地相当详细。本人记性不好，遂本文将记录几个关键步骤，以备后续不时之需。&lt;br&gt;&lt;strong&gt;&lt;font color=&quot;#FF0000&quot; face=&quot;宋体&quot;&gt;注意：本文仅供个人学习使用，不可用于商业或者违法行为！&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="科学上网" scheme="https://www.vincentqin.tech/tags/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/"/>
    
  </entry>
  
</feed>
