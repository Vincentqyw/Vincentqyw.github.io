<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>使用vercel加速Hexo静态博客访问</title>
      <link href="/posts/speedup-gitpage/"/>
      <url>/posts/speedup-gitpage/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">            <p>估计现在有很多同学使用了Hexo博客框架白嫖一套主题并部署在了Github上，这一切看起来很容易上手，于是开开心心地去写博客了，但到后面才发现这才是“苦难”的开始：原本以为是要写博客，但更多的时间是被用来优化网站。因为强迫症患者总是对网站各种不满意，于是自己挖坑又填坑。</p>          </div><p>本文属于日常折腾篇，尝试用<a href="https://vercel.com/docs" target="_blank" rel="noopener">vercel</a>加速博客访问。</p><a id="more"></a><h2 id="网站代码-repo-导入-vercel"><a href="#网站代码-repo-导入-vercel" class="headerlink" title="网站代码 repo 导入 vercel"></a>网站代码 repo 导入 vercel</h2><h3 id="注册账号"><a href="#注册账号" class="headerlink" title="注册账号"></a>注册账号</h3><p>进入登陆页面：<a href="https://vercel.com/login" target="_blank" rel="noopener">https://vercel.com/login</a>，使用 GitHub 账号登陆即可。</p><h3 id="导入项目"><a href="#导入项目" class="headerlink" title="导入项目"></a>导入项目</h3><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/speedup-gitpage/fig1.png"></p><p>点击<code>Continue</code>，进入如下界面。输入项目URL。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/speedup-gitpage/fig2.png"></p><h3 id="部署网站"><a href="#部署网站" class="headerlink" title="部署网站"></a>部署网站</h3><p>导入过程中，选择 other 模板即可，一切选择默认即可，导入完成后自动部署。最后在部署卡片中就可以看到vercel生成的URL。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/speedup-gitpage/fig3.png"></p><p>接下来就可以通过这几个链接来访问你的网站：</p><ul><li><a href="https://realcat-git-master.realcat.vercel.app" target="_blank" rel="noopener">realcat-git-master.realcat.vercel.app</a></li><li><a href="https://realcat.realcat.vercel.app" target="_blank" rel="noopener">realcat.realcat.vercel.app</a></li><li><a href="https://realcat.vercel.app" target="_blank" rel="noopener">realcat.vercel.app</a></li></ul><h2 id="修改DNS"><a href="#修改DNS" class="headerlink" title="修改DNS"></a>修改DNS</h2><p>如果你有自己的域名，需要在域名服务提供商（阿里云、GoDaddy 等）的 DNS 解析中增加一条 CNAME 记录（将你的域名指向另一个域名）：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/speedup-gitpage/fig5.png"></p><h2 id="设置域名"><a href="#设置域名" class="headerlink" title="设置域名"></a>设置域名</h2><p>添加了<code>vincentqin.tech</code>以及<code>www.vincentqin.tech</code>重定向。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/speedup-gitpage/fig4.png"></p><h2 id="尝试一下"><a href="#尝试一下" class="headerlink" title="尝试一下"></a>尝试一下</h2><p>现在网站访问速度快得飞起！</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://www.qtmuniao.com/2020/03/15/hexo-to-zeit-co/" target="_blank" rel="noopener">使用 zeit.co 托管 Hexo 静态博客</a></li><li><a href="https://fwbo.me/2020/05/29/%E5%85%B6%E4%BB%96/20200529-%E8%AF%95%E7%94%A8vercel%E6%89%98%E7%AE%A1%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">试用vercel托管博客</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> vercel </tag>
            
            <tag> hexo </tag>
            
            <tag> blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>笔记：ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM论文阅读</title>
      <link href="/posts/orb-slam3/"/>
      <url>/posts/orb-slam3/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>首先回顾一下历史：ORB-SLAM首次在2015年被提出，它的改进版ORB-SLAM2在2017年被提出，同年提出了ORB-SLAM-VI，时隔3年，ORB-SLAM3横空出世，朋友圈、学术群里到处都在热议这个挂在ARXIV才不到3天的论文。好奇心的驱使下，本人偷瞄了一下论文，就在这里总结一下吧。</p><p><strong><a href="http://xxx.itp.ac.cn/pdf/2007.11898.pdf" target="_blank" rel="noopener">论文</a></strong>,  <strong><a href="https://github.com/UZ-SLAMLab/ORB_SLAM3" target="_blank" rel="noopener">Code Github</a></strong>,  <strong><a href="https://gitee.com/vincentqin/ORB_SLAM3" target="_blank" rel="noopener">Code国内镜像</a></strong>,  <strong><a href="https://github.com/Vincentqyw/Recent-Stars-2020" target="_blank" rel="noopener">SLAM资源站</a></strong></p><a id="more"></a><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>ORB-SLAM3实现了基于视觉惯导紧耦合，同时能够对多地图进行复用；另外支持单目/双目/RGB-D作为输入，支持针孔以及鱼眼相机模型。</p><p>本文第一个创新点是提出了一种<strong>基于特征点法的视觉惯导紧耦合SLAM系统</strong>，这套系统在很大使用了最大后验估计对状态量进行求解，即使是在<strong>系统初始化</strong>阶段亦是如此。这套系统可在大/小/室内/室外各种环境下鲁棒实时运行，相较于之前的算法有2~5倍的精度提升。</p><p>本文第二个主要创新点在于提出了<strong>多地图复用系统</strong>，这套系统使用了一种高召回率的场景识别算法。正是依赖于这一点，ORB-SLAM3能够有效应对长时弱纹理的环境：若系统丢失，它重新开始建图并在当经过之前走过的地点时与原来的地图无缝融合。另外，相比于其它仅用到之前一段时间内信息的VO方案，本文提出的方案用到之前所有时刻的信息。这样可以利用到很久之前或者来自不同地图的信息。</p><p>实验表明，本文提出的算法在学术领域已有的SLAM系统中表现最优（这句话说的很严谨，没有提到工业界，因为没开源）。</p><h1 id="系统总览"><a href="#系统总览" class="headerlink" title="系统总览"></a>系统总览</h1><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-fig-1.png"></p><p>ORB-SLAM3基于ORB-SLAM2以及ORB-SLAM-VI进行改进，上图是其系统框图。这基本上保持了与ORB-SLAM2类似的框架，同时增加了前述的几个创新点，接下来对其中主要步骤进行简要说明。</p><h2 id="Atlas-地图集"><a href="#Atlas-地图集" class="headerlink" title="Atlas(地图集)"></a>Atlas(地图集)</h2><p>所谓Atlas就是一种多地图的表示形式，多地图由不相连的地图组成。可以看到，它由active map以及non-active map组成。其中active map被Tracking thread（追踪线程）用作定位，它被连续优化同时会加入新的关键帧。系统会构建基于DBoW2的关键帧数据库，可以用来做重定位/闭环以及地图融合。</p><h2 id="Tracking-thread-追踪线程"><a href="#Tracking-thread-追踪线程" class="headerlink" title="Tracking thread(追踪线程)"></a>Tracking thread(追踪线程)</h2><p>与ORB-SLAM2类似，该线程用来处理传感器信息，计算当前帧相对于active map的位姿以及最小化匹配到的地图点的重投影误差。该线程决定何时当前帧被判定为关键帧。在VI模式下，机体的速度以及IMU的bias通过优化惯导残差被估计。当系统追踪丢失后，会触发重定位模式，即当前帧在所有的Altas进行重定位；若重定位成功，当前帧恢复追踪状态；否则，经过一段时间（超过5秒），当前的active map会被存储为non-active map，同时开启新的建图过程。</p><h2 id="Local-Mapping-thread-局部建图线程"><a href="#Local-Mapping-thread-局部建图线程" class="headerlink" title="Local Mapping thread(局部建图线程)"></a>Local Mapping thread(局部建图线程)</h2><p>向active map中新增/删减/优化关键帧以及地图点，上述的操作是通过维护一个靠近当前帧的局部窗口的关键帧进行实现。与此同时，IMU的参数被初始化然后被该线程通过本文提出的最大后验估计技术进行求解。</p><h2 id="Loop-and-map-merging-thread-闭环和地图融合线程"><a href="#Loop-and-map-merging-thread-闭环和地图融合线程" class="headerlink" title="Loop and map merging thread(闭环和地图融合线程)"></a>Loop and map merging thread(闭环和地图融合线程)</h2><p>该线程检测active map与整个Atlas是否有共同的区域，若共同的区域同属于active map，此时进行闭环矫正；若共同的区域属于不同的map，那么这些map就会被融合（融合为1个）,并变为active map。BA被一个独立的线程执行（不损害实时性），用来修正地图。</p><h1 id="相机模型"><a href="#相机模型" class="headerlink" title="相机模型"></a>相机模型</h1><h2 id="重定位"><a href="#重定位" class="headerlink" title="重定位"></a>重定位</h2><p>ORB-SLAM假设所有系统组件都符合针孔模型。对于重定位这项任务，ORB-SLAM基于EPnP算法建立PnP求解器进行解算位姿，其中所有的公式都假设相机为标定好的针孔相机。为适配本文的方法，作者使用了一种独立于相机模型的求解器：Maximum Likelihood Perspective-n-Point algorithm (<a href="http://xxx.itp.ac.cn/pdf/1607.08112v1" target="_blank" rel="noopener">MLPnP</a>)（最大后验PnP算法）。该算法实现了相机模型与求解算法的解耦，相机模型只是用来提供投影射线作为输入。</p><h2 id="非矫正双目SLAM"><a href="#非矫正双目SLAM" class="headerlink" title="非矫正双目SLAM"></a>非矫正双目SLAM</h2><p>几乎所有的SLAM系统都假设双目图像是已经被<strong>矫正</strong>的，这里的矫正是指，使用相同的焦距将两个图像转换为针孔投影，且像平面共面，同时与水平对极线对齐，从而可以通过查看图像中的同一行进行特征匹配。然而，这个假设是及其严格的，在很多应用中，这个假设通常难以满足或则不合适使用。例如，矫正（一对基线较大的相机）双目相机或双目鱼眼镜头将需要对其拍摄的图像进行严格的图像裁剪，从而失去了大视场角的优势，所以我们需要更快的环境映射方式以应对遮挡等情况。</p><p>本文设计的系统并不依赖于图像的矫正，而是将双目设备看作两个单目相机，于是有如下约束：</p><ol><li>两个单目相机之间存在一个固有的SE(3)变换（即双目外参）；</li><li>两个单目相机之间存在共视；</li></ol><p>上述两个约束可以用来有效地估计尺度。按照这种思路，ORB-SLAM3可估计6DOF刚体机体位姿，注意，机体位于某一个相机或者IMU上。另外，若两个相机之间存在共视，我们可以在通过三角化恢复路标点的尺度。注意：仅在首次看到该其区域的路标点时进行三角化，其它时刻只使用单目信息。</p><h1 id="VI-SLAM"><a href="#VI-SLAM" class="headerlink" title="VI SLAM"></a>VI SLAM</h1><p>之前的ORB-SLAM-VI受限于针孔相机/初始化时间太长以及无法应对某些挑战场景。本文设计了一种快速准确IMU初始化技术，以及开发了一种单目/双目惯导SLAM，支持针孔以及鱼眼相机图像作为输入。</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>纯视觉SLAM的状态量仅有当前帧的位姿，而在VI-SLAM中，增加了一些需要被估计的状态量。除了机体在世界系下的位姿$\mathbf{T}_{i}=\left[\mathbf{R}_{i}, \mathbf{p}_{i}\right] \in \operatorname{SE}(3)$（这里是指$\mathbf{T}_{WC_i}$）还有速度$\mathbf{v}_{i}$，以及陀螺仪和加速度计的bias: $\mathbf{b}_{i}^{g}, \mathbf{b}_{i}^{a}$。我们将这些状态量合在一起构成了如下状态向量$\eqref{eq1}$：</p><script type="math/tex; mode=display">\begin{equation} \mathcal{S}_{i} \doteq\left\{\mathbf{T}_{i}, \mathbf{v}_{i}, \mathbf{b}_{i}^{g}, \mathbf{b}_{i}^{a}\right\} \label{eq1} \end{equation}</script><p>对于视觉惯导SLAM而言，我们通过对IMU的测量进行预积分，可以获得连续两帧（如第$i$到第$i+1$帧）之间的相对位姿测量：$\Delta \mathbf{p}_{i, i+1}$，$\Delta \mathbf{v}_{i, i+1}$，$\Delta \mathbf{R}_{i, i+1}$以及整个测量向量的信息矩阵$\Sigma_{\mathcal{I}_{i, i+1}}$。于是给定上述预积分项以及状态向量$\mathcal{S}_{i}$以及$\mathcal{S}_{i+1}$，我们就可以得到如下IMU残差$\mathbf{r}_{\mathcal{I}_{i, i+1}}$： </p><script type="math/tex; mode=display">\begin{equation} \begin{aligned}\mathbf{r}_{\mathcal{I}_{i, i+1}} &=\left[\mathbf{r}_{\Delta \mathrm{R}_{i, i+1}}, \mathbf{r}_{\Delta \mathrm{v}_{i, i+1}}, \mathbf{r}_{\Delta \mathrm{p}_{i, i+1}}\right] \\\mathbf{r}_{\Delta \mathrm{R}_{i, i+1}} &=\log \left(\Delta \mathbf{R}_{i, i+1}^{\mathrm{T}} \mathbf{R}_{i}^{\mathrm{T}} \mathbf{R}_{i+1}\right) \\\mathbf{r}_{\Delta \mathrm{v}_{i, i+1}} &=\mathbf{R}_{i}^{\mathrm{T}}\left(\mathbf{v}_{i+1}-\mathbf{v}_{i}-\mathbf{g} \Delta t_{i, i+1}\right)-\Delta \mathbf{v}_{i, i+1} \\\mathbf{r}_{\Delta \mathrm{p}_{i, i+1}} &=\mathbf{R}_{i}^{\mathrm{T}}\left(\mathbf{p}_{j}-\mathbf{p}_{i}-\mathbf{v}_{i} \Delta t-\frac{1}{2} \mathbf{g} \Delta t^{2}\right)-\Delta \mathbf{p}_{i, i+1}\end{aligned}\label{eq2}\end{equation}</script><p>此外，除了IMU残差还有帧$i$与3D点$\mathbf{x}_j$之间的视觉残差，即重投影误差$\mathbf{r}_{i j}$：</p><script type="math/tex; mode=display">\begin{equation} \mathbf{r}_{i j}=\mathbf{u}_{i j}-\Pi\left(\mathbf{T}_{\mathrm{CB}} \mathbf{T}_{i}^{-1} \oplus \mathbf{x}_{j}\right)\label{eq3}\end{equation}</script><p>其中$\mathbf{u}_{i j}$是3D路标点$\mathbf{x}_j$在当前帧上的投影，信息矩阵为$\Sigma_{i, j}$，$\mathbf{T}_{CB} \in \operatorname{SE}(3)$表示机体系到相机的刚体变换。给定$k+1$个关键帧及其状态量$\bar{S}_{k} \doteq\left\{\mathcal{S}_{0} \ldots \mathcal{S}_{k}\right\}$，同时给定$l$个3D点集，它的状态量为$\mathcal{X} \doteq\left\{\mathbf{x}_{0} \ldots \mathbf{x}_{l-1}\right\}$，于是该优化问题可表示为IMU残差以及重投影误差的组合，具体如下形式：</p><script type="math/tex; mode=display">\begin{equation} \min _{\overline{\mathcal{S}}_{k}, \mathcal{X}}\left(\sum_{i=1}^{k}\left\|\mathbf{r}_{\mathcal{I}_{i-1, i}}\right\|_{\Sigma_{\mathcal{I}_{i, i+1}}^{2}}+\sum_{j=0}^{l-1} \sum_{i \in \mathcal{K}^{j}} \rho_{\text {Hub }}\left(\left\|\mathbf{r}_{i j}\right\|_{\Sigma_{i j}}\right)\right)\label{eq4}\end{equation}</script><p>其中$\mathcal{K}^{j}$表示观测到第$j$个3D点的关键帧集合。这个优化可以用因子图进行表示，如图2a。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-fig-2.png"></p><h2 id="IMU初始化"><a href="#IMU初始化" class="headerlink" title="IMU初始化"></a>IMU初始化</h2><p>这一步的目的是获取IMU参数较好的初始值：速度/重力向以及Bias。本文的工作重点是设计<strong>一种快速准确的IMU初始化算法</strong>。文中提到如下3个见解：</p><ol><li>纯单目SLAM可以提供非常准确的初始地图，问题是尺度未知。首先求解vision-only问题可以提升IMU初始化的性能。</li><li><strong>如果将尺度scale显式表示为优化变量，而不是使用BA的隐式表示，则scale收敛得更快</strong>。</li><li>若在IMU初始化过程中忽略传感器不确定性会产生大量不可预测的错误。</li></ol><p>根据以上三点，本文设计了如下基于最大后验估计的IMU初始化算法，主要分为如下三个步骤：</p><h3 id="Step1-Vision-only-MAP-Estimation"><a href="#Step1-Vision-only-MAP-Estimation" class="headerlink" title="Step1: Vision-only MAP Estimation"></a>Step1: Vision-only MAP Estimation</h3><p>进行单目SLAM，按照关键帧速率4Hz持续运行2s，然后我们可以得到按比例缩放的地图，包括$k=10$个关键帧以及上百个地图点，然后通过Visual-Only BA进行优化，因子图如图2b。于是可以得到优化后的轨迹$\overline{\mathbf{T}}_{0: k}=[\mathbf{R}, \overline{\mathbf{p}}]_{0: k}$，其中上划线表示按比例缩放的变量（即尺度未定）。</p><h3 id="Step2-Inertial-only-MAP-Estimation"><a href="#Step2-Inertial-only-MAP-Estimation" class="headerlink" title="Step2: Inertial-only MAP Estimation"></a>Step2: Inertial-only MAP Estimation</h3><p>这一步的目的是获得IMU参数最优估计。利用$\overline{\mathbf{T}}_{0: k}$以及这些关键帧之间的IMU测量，IMU的参数可以放在一起构成状态向量：</p><script type="math/tex; mode=display">\begin{equation} \mathcal{Y}_{k}=\left\{s, \mathbf{R}_{\mathrm{w} g}, \mathbf{b}, \overline{\mathbf{v}}_{0: k}\right\}\label{eq5}\end{equation}</script><p>其中的$s \in \mathbb{R}^{+}$表示尺度因子，$\mathbf{R}_{\mathrm{w} g} \in \mathrm{SO}(3)$表示重力向，由两个角度表示，重力向量在世界系下的表示为$\mathbf{g}=\mathbf{R}_{\mathrm{w} g} \mathbf{g}_{\mathrm{I}}$，其中$\mathbf{g}_{\mathrm{I}}=(0,0, G)^{\mathrm{T}}$，$G$是重力大小模值；$\mathbf{b}=\left(\mathbf{b}^{a}, \mathbf{b}^{g}\right) \in \mathbb{R}^{6}$表示加速度计以及陀螺仪的bias，在初始化阶段可以假设该值为常数；$\overline{\mathbf{v}}_{0: k}$表示从首帧到末尾帧的无尺度机体速度。此时，我们有IMU的测量$\dot{\mathcal{I}}_{0: k} \doteq\left\{\mathcal{I}_{0,1} \ldots \mathcal{I}_{k-1, k}\right\}$，于是MAP估计问题中的需要被最大化的后验分布为：</p><script type="math/tex; mode=display">\begin{equation} p\left(\mathcal{Y}_{k} \mid \mathcal{I}_{0: k}\right) \propto p\left(\mathcal{I}_{0: k} \mid \mathcal{Y}_{k}\right) p\left(\mathcal{Y}_{k}\right)\label{eq6}\end{equation}</script><p>具体地，$p\left(\mathcal{I}_{0: k} \mid \mathcal{Y}_{k}\right)$表示似然，$p\left(\mathcal{Y}_{k}\right)$表示先验，考虑到IMU测量之间iid，于是最大后验概率估计问题可以表示为如下形式：</p><script type="math/tex; mode=display">\begin{equation} \mathcal{Y}_{k}^{*}=\underset{\mathcal{Y}_{k}}{\arg \max }\left(p\left(\mathcal{Y}_{k}\right) \prod_{i=1}^{k} p\left(\mathcal{I}_{i-1, i} \mid s, \mathbf{g}_{d i r}, \mathbf{b}, \overline{\mathbf{v}}_{i-1}, \overline{\mathbf{v}}_{i}\right)\right)\label{eq7}\end{equation}</script><p>对上式取负对数并且假设IMU预积分以及先验分布服从高斯分布，于是最终的优化问题可以表示为：</p><script type="math/tex; mode=display">\begin{equation} \mathcal{Y}_{k}^{*}=\underset{\mathcal{Y}_{k}}{\arg \min }\left(\left\|\mathbf{r}_{\mathbf{p}}\right\|_{\Sigma_{p}}^{2}+\sum_{i=1}^{k}\left\|\mathbf{r}_{\mathcal{I}_{i-1, i}}\right\|_{\Sigma_{\mathcal{I}_{i-1, i}}}^{2}\right)\label{eq8}\end{equation}</script><p>这个优化问题的因子图表示为图2c，可以看到上式中不包含视觉残差，而是多了一个先验的残差项$\mathbf{r}_{p}$用来约束IMU的bias需要<strong>接近于0</strong>。<br>由于上述优化在流形上进行，重力向的在优化过程中的更新需要构建一个”retraction”：</p><script type="math/tex; mode=display">\begin{equation} \mathbf{R}_{\mathrm{wg}}^{\mathrm{new}}=\mathbf{R}_{\mathrm{wg}}^{\mathrm{old}} \operatorname{Exp}\left(\delta \alpha_{\mathrm{g}}, \delta \beta_{\mathrm{g}}, 0\right)\label{eq9}\end{equation}</script><p>其中$\operatorname{Exp}\left(.\right)$表示从$\mathfrak{s o}(3)$到$\mathrm{SO}(3)$指数映射。<br>为了保证优化过程中尺度因子保持正数，尺度因子的更新形式为如下形式：</p><script type="math/tex; mode=display">\begin{equation} s^{\text {new }}=s^{\text {old }} \exp (\delta s)\label{eq10}\end{equation}</script><p>Inertial-only MAP Estimation完成之后，帧位姿/速度以及3D点根据估计的尺度进行调整，同时将$z$轴对齐重力向。</p><h3 id="Step3-Visual-Inertial-MAP-Estimation"><a href="#Step3-Visual-Inertial-MAP-Estimation" class="headerlink" title="Step3: Visual-Inertial MAP Estimation"></a>Step3: Visual-Inertial MAP Estimation</h3><p>一旦视觉以及IMU有了较好的估计后，进行一个VI联合优化进一步对这些参数进行精化，优化因子图见图2a。<br>作者在EuRoC数据集上进行测试发现上述初始化方式非常有效，可达到2秒内仅5%的误差。为了进一步提升初始估计精度，初始化后会进行5~15秒的VI BA优化，这样可以收敛到仅1%的尺度误差。相较于ORB-SLAM-VI需要15秒才获得首个尺度因子更加快速。经过了这些BA操作之后，我们就认为这个map是mature（成熟的），意思就是尺度/IMU参数/重力向已经被较好地估计完成。</p><h2 id="追踪与建图"><a href="#追踪与建图" class="headerlink" title="追踪与建图"></a>追踪与建图</h2><p>追踪过程借鉴了VI-SLAM-VI的思路：追踪过程解决了简化版的VI优化问题，只优化最后两帧位姿，同时保持地图点固定。</p><p>建图过程是为了解决全图优化问题，若图的规模比较大，这个问题会变得很棘手。本文采用了滑动窗口的思想，即维护了关键帧与地图点的滑动窗口，同时包括它们的共视关键帧，只是在优化时需要保持这些关键帧固定状态。</p><p>慢速运动不能为IMU参数的提供好的可观性，会使初始化无法在仅15秒内收敛到精确的结果。为了 应对这种情况，本文基于修改版“Inertial-only MAP Estimation”的提出了一种尺度精化技术：所有插入的关键帧都参与优化，但优化量只有重力向以及尺度因子，优化因子图见图2d，在这种情况下bias为常数的假设就不再成立。我们使用了每帧的估计量并固定它们。上述的这种操作非常高效，在Local Mapping线程每隔10秒执行一次，直到Map中有100个关键帧或者从初始化起已经过了75秒。</p><h2 id="鲁棒性以及追踪丢失"><a href="#鲁棒性以及追踪丢失" class="headerlink" title="鲁棒性以及追踪丢失"></a>鲁棒性以及追踪丢失</h2><p>快速运动/无纹理/遮挡等会导致系统丢失，ORB-SLAM采用了基于词袋的场景识别进行重定位。本文的VI系统在追踪到少于15个特征点时就会视觉丢失（visually lost），鲁棒性体现在如下两点：</p><ol><li>短期丢失：通过IMU读数对当前帧状态进行估计，地图点根据估计的位姿投影到当前帧后设置较大搜索窗口进行匹配。大多数情况下，通过这种方式能够恢复系统位姿。若超过5秒仍未重定位成功，则进入下一个状态；</li><li>长期丢失：新的VI地图会被重新初始化，并成为active map。</li></ol><h1 id="地图融合与闭环"><a href="#地图融合与闭环" class="headerlink" title="地图融合与闭环"></a>地图融合与闭环</h1><p>前文的介绍可知，短期以及中期数据关联可以通过Tracking以及Local Mapping进行完成。而对于长期的数据关联，可通过重定位以及闭环实现。</p><p>ORB-SLAM采用的基于视觉词袋场景识别的重定位，若候选关键帧只有1个，召回率为50~80%。为了应对假阳性的出现，算法使用了时域校验以及几何校验，这两种手段能够使精确率达到100%的同时召回率为30~40%。至关重要的是，时域连续性检测将使场景识别滞后至少3个关键帧，同时召回率较低，这都是目前存在的问题。</p><p>为了应对这个问题，本文提出一种新的场景识别（召回率得到改善）以及多地图数据关联算法。一旦Local Mapping线程创建了关键帧，场景识别算法就会被激活并且寻找该帧在Atlas中的数据关联。若匹配的关键帧在active map中，则进行闭环；否则，则进行多地图间的数据关联，即将active map与匹配的map进行融合。一旦这个新的关键帧与匹配地图间的相对位姿被计算出，就定义一个在局部窗口，这个局部窗口包括匹配的关键帧以及这个关键帧的共视关键帧。在这个局部窗口中，我们会寻找中期数据关联，以提高闭环以及地图融合的精度。这个改进使得ORB-SLAM3比ORB-SLAM2具有更高的精度。</p><h2 id="场景识别"><a href="#场景识别" class="headerlink" title="场景识别"></a>场景识别</h2><p>此处较为简单，与ORB-SLAM2基本类似，只是增加了重力向校验的步骤。具体的，计算出了当前关键帧在matched map（可能是active map或者其它地图）中的位姿$\mathbf{T}_{a m} \in \operatorname{SE}(3)$，检验pitch(俯仰角)和roll(横滚角)是否小于一定阈值来对场景识别结果进行校验。</p><h2 id="视觉地图融合"><a href="#视觉地图融合" class="headerlink" title="视觉地图融合"></a>视觉地图融合</h2><p>当场景识别成功后，位于actimve map $M_a$中的当前关键帧$K_a$与位于active map $M_m$中的当前关键帧$K_m$产生了多地图的数据关联，此时会进行地图融合。在此过程中，必须格外小心，以确保跟踪线程可以迅速重用$M_m$中的信息，以避免地图重复。</p><p>由于$M_a$可能包含许多元素，融合它们可能需要很长时间，因此融合分为两个步骤。首先，在由$K_a$和$K_m$的邻域定义的welding window（焊接窗口）中执行融合，随后在第二阶段，通过位姿图优化将校正传播到融合图的其余部分。具体过程如下：</p><h3 id="step1-Welding-window-assembly"><a href="#step1-Welding-window-assembly" class="headerlink" title="step1: Welding window assembly"></a>step1: Welding window assembly</h3><p>Welding window包括当前关键帧$K_a$及其共视帧，$K_m$以及其共视帧，以及被这些关键帧观测的地图点。在将它们包含在Welding window中之前，属于$M_a$的关键帧和地图点通过$\mathbf{T}_{m a}$进行变换，以使其与$M_m$对齐。</p><h3 id="step2-融合"><a href="#step2-融合" class="headerlink" title="step2: 融合"></a>step2: 融合</h3><p>$M_a$与$M_m$融合为一个新的active map。为了要删除重复的点，$M_m$关键帧主动搜索匹配$M_a$中的点。对于每个匹配点，都会删除$M_a$中的点，并保留$M_m$中的点，并累积删除点的所有观测值，同时更新共视图以及本质图。</p><h3 id="step3-Welding-bundle-adjustment"><a href="#step3-Welding-bundle-adjustment" class="headerlink" title="step3: Welding bundle adjustment"></a>step3: Welding bundle adjustment</h3><p>Welding window范围内的所有关键帧进行局部BA优化（见下图）。为了确定尺度自由度，<strong>观测到$M_m$的那些关键帧需要保持固定</strong>。优化完成后，Welding window区域中包含的所有关键帧都可以用于跟踪，实现地图$M_m$的快速/准确复用。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-fig-3a.png"></p><h3 id="位姿图优化"><a href="#位姿图优化" class="headerlink" title="位姿图优化"></a>位姿图优化</h3><p>位姿图利用本质图在融合后的地图范围进行优化，Welding window范围内的关键帧保持固定。这一步的意义在于分摊误差。</p><h2 id="视觉惯导地图融合"><a href="#视觉惯导地图融合" class="headerlink" title="视觉惯导地图融合"></a>视觉惯导地图融合</h2><p>与上述纯视觉地图融合步骤类似，只是修改了步骤1和3，具体如下：</p><h3 id="step1-VI-welding-window-assembly"><a href="#step1-VI-welding-window-assembly" class="headerlink" title="step1: VI welding window assembly"></a>step1: VI welding window assembly</h3><p>若active map是mature的，与纯视觉类似，将对地图$M_a$进行$\mathbf{T}_{m a} \in \operatorname{SE}(3)$变换，与$M_m$对齐。若active map不是mature的，将对地图进行$\mathbf{T}_{m a} \in \operatorname{Sim}(3)$变换，与$M_m$对齐。</p><h3 id="step2-同上"><a href="#step2-同上" class="headerlink" title="step2: 同上"></a>step2: 同上</h3><h3 id="step3-VI-welding-bundle-adjustment"><a href="#step3-VI-welding-bundle-adjustment" class="headerlink" title="step3: VI welding bundle adjustment"></a>step3: VI welding bundle adjustment</h3><p>active关键帧$K_a$及其前5个时间连续的关键帧的位姿，速度和bias参与优化，对于$M_m$中的$K_m$及其前5个时间连续的关键帧的位姿，速度和bias也参与优化，详细见下图。对于$M_m$，优化中包括但不固定在紧接局部窗口之前的那一关键帧位姿，而对于$M_a$，局部窗口之前的那一关键帧，但其姿势仍然参与优化。所有这些关键帧看到的所有地图点以及观测这些地图点的关键帧姿势也都进行优化。所有关键帧和地图点都通过重投影误差项（作为约束因子，下图中的蓝色小方块）进行关联。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-fig-3b.png"></p><h2 id="闭环"><a href="#闭环" class="headerlink" title="闭环"></a>闭环</h2><p>闭环校正算法类似于地图融合，但是在这种情况下，当前关键帧以及匹配关键帧都同属于active map。welding window是由匹配的关键帧组合而成/重复的3D点被融合/更新共视图以及本质图的连接关系。下一步是姿势图优化（PG），以均分误差。由于闭环增加了中期/长期数据关联，此时进行全局BA。在VI情况下，仅在关键帧的数量低于阈值时才执行全局BA，以避免巨大的计算量。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>实验主要分为如下部分：</p><ul><li>EuRoC单一会话（地图）：11个场景中的每个序列产生一个地图；传感器配置：单目，单目+IMU，双目以及双目+IMU；</li><li>TUM-VI数据：比较单目/双目鱼眼VI配置下的表现；</li><li>多次会话，上述所有数据；</li></ul><!-- <iframe width="2543" height="1128" src="https://www.youtube.com/embed/kZbpIbaBnZ0?list=PLXwmI2zWtHsY0iCQDOuI-PVroFGLCYHpC" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><iframe width="2543" height="1128" src="https://www.youtube.com/embed/E-7fsq7en2g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> --><h2 id="EuRoC单次会话"><a href="#EuRoC单次会话" class="headerlink" title="EuRoC单次会话"></a>EuRoC单次会话</h2><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-tab-2.png"></p><h2 id="VI-SLAM以及TUM-VI数据集"><a href="#VI-SLAM以及TUM-VI数据集" class="headerlink" title="VI SLAM以及TUM-VI数据集"></a>VI SLAM以及TUM-VI数据集</h2><p><img width="80%" data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-tab-3.png"></p><p><img width="70%" data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-fig-4.png"></p><h2 id="多地图"><a href="#多地图" class="headerlink" title="多地图"></a>多地图</h2><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-fig-5.png"></p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/orb-slam3/orb-slam3-fig-6.png"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>基于ORB-SLAM/ORB-SLAM2以及ORB-SLAM-VI，本文提出了ORB-SLAM3，它是一个功能更加完整，性能更加出众的SLAM系统。这使得该系统更加适合长时/大规模SLAM实际应用。</p><p>实验结果表明，ORB-SLAM3是第一个能够有效利用短期，中期，长期和多地图数据关联的视觉和视觉惯性系统，其精度水平已经超出了现有系统。 实验结果还表明，关于精度，使用所有这些类型的数据关联的能力会超过其他选择，如使用直接方法代替特征点法或对局部BA执行关键帧边缘化，而不是像我们那样假设一组外部静态关键帧。</p><p>关于鲁棒性，直接法在低纹理环境中可能更鲁棒，但仅限于短期和中期数据关联。另一方面，匹配特征描述符可以成功解决长期和多地图数据关联问题，但与使用光度信息的Lucas-Kanade相比，跟踪功能似乎不那么可靠。在直接法中使用这四种数据关联方式是一个非常有趣的研究领域，我们目前正在根据这个想法探索从人体内部的内窥镜图像构建地图。</p><p>在四种不同的传感器配置中，双目惯导SLAM提供了最可靠，最准确的解决方案。此外，惯性传感器允许以IMU速率估算姿势，IMU速率比帧速率高几个数量级，这可能也会在某些领域发挥优势（如AR/MR等领域）。对于设备体积/成本受限等应用，可以选择使用单目惯导方案，精度与鲁棒性并不会下降多少。</p><p>在慢速运动或没有旋转和俯仰旋转的应用中，例如平坦区域中的汽车，IMU传感器可能难以初始化。在这些情况下，推荐使用双目SLAM。或者，使用CNN进行单目深度恢复的最新研究成果为单目SLAM恢复尺度提供了良好的前景，但是需要保证在同样的环境中对网络进行了训练（泛化性问题）。</p>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> CV </tag>
            
            <tag> 笔记 </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>解决Github挂图及龟速访问</title>
      <link href="/posts/manage-pc-hosts/"/>
      <url>/posts/manage-pc-hosts/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>对于经常刷Github的同学而言，是否会经常遇到图片加载不出来/GitHub访问慢等情况？反正我是经常遇到！为了解决这个问题，<a href="https://github.com/521xueweihan" target="_blank" rel="noopener">削微寒</a>公布了解决方案：修改本机<code>hosts</code>，无需安装任何程序。下面是详细说明以及使用方法（修改自项目<em>README</em>）。</p><a id="more"></a><h1 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h1><h2 id="复制如下内容"><a href="#复制如下内容" class="headerlink" title="复制如下内容"></a>复制如下内容</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GitHub520 Host Start</span></span><br><span class="line">185.199.108.154                                   github.githubassets.com</span><br><span class="line">199.232.68.133                                    camo.githubusercontent.com</span><br><span class="line">199.232.68.133                                    github.map.fastly.net</span><br><span class="line">199.232.69.194                                    github.global.ssl.fastly.net</span><br><span class="line">140.82.112.3                                      github.com</span><br><span class="line">140.82.112.6                                      api.github.com</span><br><span class="line">52.136.127.111                                    raw.githubusercontent.com</span><br><span class="line">199.232.68.133                                    favicons.githubusercontent.com</span><br><span class="line">199.232.68.133                                    avatars5.githubusercontent.com</span><br><span class="line">199.232.68.133                                    avatars4.githubusercontent.com</span><br><span class="line">199.232.68.133                                    avatars3.githubusercontent.com</span><br><span class="line">199.232.68.133                                    avatars2.githubusercontent.com</span><br><span class="line">199.232.68.133                                    avatars1.githubusercontent.com</span><br><span class="line">199.232.68.133                                    avatars0.githubusercontent.com</span><br><span class="line"><span class="comment"># GitHub520 Host End</span></span><br></pre></td></tr></table></figure><p>上面内容会<strong>自动定时更新</strong>，这里是<a href="https://raw.githubusercontent.com/521xueweihan/GitHub520/master/hosts" target="_blank" rel="noopener">最新地址</a>，保证最新有效。</p><p>以下介绍两种方式：<a href="#手动方式">手动方式</a>和<a href="#自动方式">自动方式</a>，强烈推荐<a href="#自动方式">自动方式</a>。</p><h2 id="手动方式"><a href="#手动方式" class="headerlink" title="手动方式"></a>手动方式</h2><h3 id="修改-hosts-文件"><a href="#修改-hosts-文件" class="headerlink" title="修改 hosts 文件"></a>修改 hosts 文件</h3><p>hosts 文件在每个系统的位置不一，详情如下：</p><ul><li>Windows 系统：<code>C:\Windows\System32\drivers\etc\hosts</code></li><li>Linux 系统：<code>/etc/hosts</code></li><li>Mac（苹果电脑）系统：<code>/etc/hosts</code></li><li>Android（安卓）系统：<code>/system/etc/hosts</code></li><li>iPhone（iOS）系统：<code>/etc/hosts</code></li></ul><p>修改方法，把第一步的内容复制到文本末尾：</p><ul><li>Windows 使用记事本。</li><li>Linux、Mac 使用 Root 权限：<code>sudo vi /etc/hosts</code>。</li><li>iPhone、iPad 须越狱、Android 必须要 root。</li></ul><h3 id="激活生效"><a href="#激活生效" class="headerlink" title="激活生效"></a>激活生效</h3><p>大部分情况下是直接生效，如未生效可尝试下面的办法，刷新 DNS：</p><ul><li><p>Windows：在 CMD 窗口输入：<code>ipconfig /flushdns</code></p></li><li><p>Linux 命令：<code>sudo rcnscd restart</code></p></li><li><p>Mac 命令：<code>sudo killall -HUP mDNSResponder</code></p></li></ul><p><strong>Tips：</strong> 上述方法无效可以尝试重启机器。</p><h2 id="自动方式"><a href="#自动方式" class="headerlink" title="自动方式"></a>自动方式</h2><h3 id="下载Hosts切换工具"><a href="#下载Hosts切换工具" class="headerlink" title="下载Hosts切换工具"></a>下载Hosts切换工具</h3><p><strong>Tip</strong>：推荐 <a href="https://github.com/oldj/SwitchHosts" target="_blank" rel="noopener">SwitchHosts</a> 工具管理 <code>hosts</code>。根据自己的系统选择对应的版本进行下载，<strong>[<a href="https://github.com/oldj/SwitchHosts/releases" target="_blank" rel="noopener">下载页面</a>]</strong>。</p><h3 id="配置工具"><a href="#配置工具" class="headerlink" title="配置工具"></a>配置工具</h3><p>以SwitchHosts为例，看一下怎么使用的，配置参考下面：</p><div class="note default">            <p><strong>Title</strong>: 随意<br><strong>Type</strong>: <code>Remote</code><br><strong>URL</strong>: <code>https://raw.githubusercontent.com/521xueweihan/GitHub520/master/hosts</code><br><strong>Auto Refresh</strong>: 最好选 <code>1 hour</code></p>          </div><p>配置页面如下图：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/manage-pc-hosts/switch-hosts-realcat-1.png"></p><p>配置好的页面是这样的：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/manage-pc-hosts/switch-hosts-realcat-2.png"></p><p>这样每次 <code>hosts</code> 有更新都能及时进行更新，免去手动更新。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://github.com/521xueweihan/GitHub520" target="_blank" rel="noopener">让你“爱”上 GitHub，解决访问时图裂、加载慢的问题</a></li><li><a href="https://github.com/oldj/SwitchHosts" target="_blank" rel="noopener">系统hosts管理器</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Github </tag>
            
            <tag> hosts </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【译】图解卡尔曼滤波</title>
      <link href="/posts/kalman-filter-in-pictures/"/>
      <url>/posts/kalman-filter-in-pictures/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>译者注</strong>：这恐怕是全网有关卡尔曼滤波最简单易懂的解释，如果你认真的读完本文，你将对卡尔曼滤波有一个更加清晰的认识，并且可以手推卡尔曼滤波。原文作者使用了漂亮的图片和颜色来阐明它的原理（读起来并不会因公式多而感到枯燥），所以请勇敢地读下去！</p><p>本人翻译水平有限，如有疑问，请阅读原文；如有错误，请在评论区指出。</p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="关于滤波"><a href="#关于滤波" class="headerlink" title="关于滤波"></a>关于滤波</h3><p>首先援引来自知乎大神的解释。</p><div class="note info">            <p>一位专业课的教授给我们上课的时候，曾谈到：filtering is weighting（滤波即加权）。滤波的作用就是给不同的信号分量不同的权重。最简单的loss pass filter， 就是直接把低频的信号给1权重，而给高频部分0权重。对于更复杂的滤波，比如维纳滤波, 则要根据信号的统计知识来设计权重。</p><p>从统计信号处理的角度，降噪可以看成滤波的一种。降噪的目的在于突出信号本身而抑制噪声影响。从这个角度，降噪就是给信号一个高的权重而给噪声一个低的权重。维纳滤波就是一个典型的降噪滤波器。</p>          </div><h3 id="关于卡尔曼滤波"><a href="#关于卡尔曼滤波" class="headerlink" title="关于卡尔曼滤波"></a>关于卡尔曼滤波</h3><p>Kalman Filter 算法，是一种递推预测滤波算法，算法中涉及到滤波，也涉及到对下一时刻数据的预测。Kalman Filter 由一系列递归数学公式描述。它提供了一种高效可计算的方法来估计过程的状态，并使估计均方误差最小。卡尔曼滤波器应用广泛且功能强大：它可以估计信号的过去和当前状态，甚至能估计将来的状态，即使并不知道模型的确切性质。</p><p>Kalman Filter 也可以被认为是一种数据融合算法（Data fusion algorithm），已有50多年的历史，是当今使用最重要和最常见的数据融合算法之一。Kalman Filter 的巨大成功归功于其小的计算需求，优雅的递归属性以及作为具有高斯误差统计的一维线性系统的最优估计器的状态<sup><a href="#fn_4" id="reffn_4">4</a></sup>。</p><p>Kalman Filter 只能减小均值为0的测量噪声带来的影响。只要噪声期望为0，那么不管方差多大，只要迭代次数足够多，那效果都很好。反之，噪声期望不为0，那么估计值就还是与实际值有偏差。</p><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>上面的描述可能把大家绕晕了，下面来点轻松的。</p><p>我们会有一个疑问：卡尔曼滤波到底是如何解决实际问题的呢？</p><p>我们以机器人为例介绍卡尔曼滤波的原理，我们的任务是要预测机器人的状态$\vec{x}$，包括位置$p$与速度$v$，即可表示为：</p><script type="math/tex; mode=display">\vec{x} = \begin{bmatrix}  p\\  v  \end{bmatrix}</script><p>某个时刻，我们不知道真实的位置与速度到底是多少，二者存在一个所有可能性的组合，大致如下图所示。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_0.png"></p><p><strong>卡尔曼滤波假设状态所有的变量都是随机的且都服从高斯分布，每个变量都有其对应的均值$\mu$以及方差$\sigma^2$（它代表了不确定性）</strong>。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_1.png"></p><p>在上图中，位置和速度是不相关（<strong>uncorrelated</strong>）的，这意味着某个变量的状态不会告诉你其他变量的状态是怎样的。即虽然我们知道现在的速度，但是无法从现在的速度推测出现在的位置。但实际上并非如此，我们知道速度和位置是有关系的（<strong>correlated</strong>），这样一来二者之间的组合关系变成了如下图所示的情况。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_3.png"></p><p>这种情况是很容易发生的，例如，如果速度很快，我们可能会走得更远，所以我们的位置会更大。如果我们走得很慢，我们就不会走得太远。</p><p>这种状态变量之间的关系很重要，因为它可以为我们提供<strong>更多信息</strong>：One measurement tells us something about what the others could be。这就是卡尔曼滤波器的目标，我们希望从不确定的测量中尽可能多地获取信息！</p><p>这种状态量的相关性可以由协方差矩阵表示。简而言之，矩阵$\Sigma_{ij}$的每个元素是第i个状态变量和第j个状态变量之间的相关度。（显然地可以知道协方差矩阵是对称的，这意味着您交换i和j都没关系）。协方差矩阵通常标记为“ $\Sigma$”，因此我们将它们的元素称为“$\Sigma_{ij}$”。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_2.png"></p><h2 id="状态预测"><a href="#状态预测" class="headerlink" title="状态预测"></a>状态预测</h2><h3 id="问题的矩阵形式表示"><a href="#问题的矩阵形式表示" class="headerlink" title="问题的矩阵形式表示"></a>问题的矩阵形式表示</h3><p>我们把状态建模成高斯分布（Gaussian blob，由于二维高斯分布长得像一个个小泡泡，之所以长这个样子，可参考链接<sup><a href="#fn_2" id="reffn_2">2</a></sup>）。我们需要求解/估计在时间$k$时刻的两个信息：1. 最优估计$\mathbf{\hat{x}_k}$以及它的协方差矩阵$\mathbf{P_k}$，我们可以写成下面矩阵形式：</p><script type="math/tex; mode=display">\begin{equation} \label{eq:statevars}  \begin{aligned}  \mathbf{\hat{x}}_k &= \begin{bmatrix}  \text{position}\\  \text{velocity}  \end{bmatrix}\\  \mathbf{P}_k &=  \begin{bmatrix}  \Sigma_{pp} & \Sigma_{pv} \\  \Sigma_{vp} & \Sigma_{vv} \\  \end{bmatrix}  \end{aligned}  \end{equation}</script><p>（当然，这里我们仅使用位置和速度，但是请记住状态可以包含任意数量的变量，并且可以表示所需的任何变量）</p><p>接下来，我们需要某种方式来查看当前状态（$k-1$时刻）并<strong>预测</strong>在时刻$k$处的状态。请记住，我们不知道哪个状态是“真实”状态，但是我们提到的<strong>预测</strong>（prediction）并不在乎这些。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_7.jpg"></p><p>我们可以用一个矩阵$\mathbf{F_k}$来表示这个预测过程：</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_8.jpg"></p><p>这个矩阵$\mathbf{F_k}$将原始估计中的每个点移动到新的预测位置。</p><p>那么问题来了，应该如何使用上述矩阵来预测下一时刻的位置和速度呢？为了阐述这个过程，我们使用了一个非常基础的运动学公式（初中物理中就学过）进行描述：</p><script type="math/tex; mode=display">\begin{split} \color{deeppink}{p_k} &= \color{royalblue}{p_{k-1}} + \Delta t &\color{royalblue}{v_{k-1}} \\ \color{deeppink}{v_k} &= &\color{royalblue}{v_{k-1}} \end{split}</script><p>写成矩阵形式：</p><script type="math/tex; mode=display">\begin{align} \color{deeppink}{\mathbf{\hat{x}}_k} &= \begin{bmatrix} 1 & \Delta t \\ 0 & 1 \end{bmatrix} \color{royalblue}{\mathbf{\hat{x}}_{k-1}} \\ &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} \label{statevars} \end{align}</script><p>现在我们有了一个<strong>预测矩阵</strong>或者叫做<strong>状态转移矩阵</strong>，该矩阵可以帮助我们计算下一个时刻的状态。但我们仍然不知道如何更新状态的协方差矩阵，其实过程也是很简单，如果我们将分布中的每个点乘以矩阵$A$，那么其协方差矩阵$\Sigma$会发生什么？</p><script type="math/tex; mode=display">\begin{equation} \begin{split} Cov(x) &= \Sigma\\ Cov(\color{firebrick}{\mathbf{A}}x) &= \color{firebrick}{\mathbf{A}} \Sigma \color{firebrick}{\mathbf{A}}^T \end{split} \label{covident} \end{equation}</script><p>将公式$\eqref{covident}$带入公式$\eqref{statevars}$，我们可以得到：</p><script type="math/tex; mode=display">\begin{equation} \begin{split} \color{deeppink}{\mathbf{\hat{x}}_k} &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} \\ \color{deeppink}{\mathbf{P}_k} &= \mathbf{F_k} \color{royalblue}{\mathbf{P}_{k-1}} \mathbf{F}_k^T \end{split} \end{equation}</script><h3 id="External-influence"><a href="#External-influence" class="headerlink" title="External influence"></a>External influence</h3><p>不过我们并没有考虑到所有的影响因素。可能有一些与状态本身无关的变化——如外界因素可能正在影响系统。</p><p>例如，我们用状态对列车的运动进行建模，如果列车长加大油门，火车就加速。同样，在我们的机器人示例中，导航系统软件可能会发出使车轮转动或停止的命令。如果我们很明确地知道这些因素，我们可以将其放在一起构成一个向量$\color{darkorange}{\vec{\mathbf{u}_k}}$，对这个量进行处理，然后将其添加到我们的预测中对状态进行更正。</p><p>假设我们知道由于油门设置或控制命令而产生的预期加速度$\color{darkorange}{a}$。根据基本运动学，我们得到下式：</p><script type="math/tex; mode=display">\begin{split} \color{deeppink}{p_k} &= \color{royalblue}{p_{k-1}} + {\Delta t} &\color{royalblue}{v_{k-1}} + &\frac{1}{2} \color{darkorange}{a} {\Delta t}^2 \\ \color{deeppink}{v_k} &= &\color{royalblue}{v_{k-1}} + & \color{darkorange}{a} {\Delta t} \end{split}</script><p>矩阵形式：</p><script type="math/tex; mode=display">\begin{equation} \begin{split} \color{deeppink}{\mathbf{\hat{x}}_k} &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} + \begin{bmatrix} \frac{\Delta t^2}{2} \\ \Delta t \end{bmatrix} \color{darkorange}{a} \\ &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} + \mathbf{B}_k \color{darkorange}{\vec{\mathbf{u}_k}} \end{split} \end{equation}</script><p>其中$\mathbf{B}_k$被称为<strong>控制矩阵</strong>，$\color{darkorange}{\vec{\mathbf{u}_k}}$被称为<strong>控制向量</strong>。（注意：对于没有外部影响的简单系统，可以忽略该控制项）。</p><p>如果我们的预测并不是100％准确模型，这会发生什么呢？</p><h3 id="External-uncertainty"><a href="#External-uncertainty" class="headerlink" title="External uncertainty"></a>External uncertainty</h3><p>如果状态仅仅依赖其自身的属性进行演进，那一切都很好。如果状态受到外部因素进行演进，我们只要知道那些外部因素是什么，那么一切仍然很好。</p><p>但在实际使用中，我们有时不知道的那些外部因素到底是如何被建模的。例如，我们要跟踪四轴飞行器，它可能会随风摇晃；如果我们跟踪的是轮式机器人，则车轮可能会打滑，或者因地面颠簸导致其减速。我们无法跟踪这些外部因素，如果发生任何这些情况，我们的预测可能会出错，因为我们并没有考虑这些因素。</p><p>通过在每个预测步骤之后添加一些新的不确定性，我们可以对与“世界”相关的不确定性进行建模（如我们无法跟踪的事物）：</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_9.jpg"></p><p>这样一来，由于新增的不确定性<strong>原始估计中的每个状态都可能迁移到多个状态</strong>。 因为我们非常喜欢用高斯分布进行建模，此时也不例外。我们可以说$\color{royalblue}{\mathbf{\hat{x}}_{k-1}}$的每个点都移动到具有协方差$\color{mediumaquamarine}{\mathbf{Q}_k}$的高斯分布内的某个位置，如下图所示：</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_10a.jpg"></p><p>这将产生一个新的高斯分布，其协方差不同（但均值相同）：</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_10b.jpg"></p><p>所以呢，我们在状态量的协方差中增加了额外的协方差$\color{mediumaquamarine}{\mathbf{Q}_k}$，所以预测阶段完整的状态转移方程为：</p><script type="math/tex; mode=display">\begin{equation}  \begin{split}  \color{deeppink}{\mathbf{\hat{x}}_k} &= \mathbf{F}_k \color{royalblue}{\mathbf{\hat{x}}_{k-1}} + \mathbf{B}_k \color{darkorange}{\vec{\mathbf{u}_k}} \\  \color{deeppink}{\mathbf{P}_k} &= \mathbf{F_k} \color{royalblue}{\mathbf{P}_{k-1}} \mathbf{F}_k^T + \color{mediumaquamarine}{\mathbf{Q}_k}  \end{split}  \label{kalpredictfull}  \end{equation}</script><p>换句话说：<strong><font color="deeppink">新的最佳估计</font></strong>是根据<strong><font color="royalblue">先前的最佳估计</font></strong>做出的<strong>预测</strong>，再加上对<strong><font color="darkorange">已知外部影响</font></strong>的校正。</p><p><strong><font color="deeppink">新的不确定度</font></strong>是根据<strong><font color="royalblue">先前的不确定度</font></strong>做出的<strong>预测</strong>，再加上<strong><font color="mediumaquamarine">来自环境额外的不确定度</font></strong>。</p><p>上述过程描绘了状态预测过程，那么当我们从传感器中获取一些测量数据时会发生什么呢？</p><h2 id="状态更新"><a href="#状态更新" class="headerlink" title="状态更新"></a>状态更新</h2><h3 id="利用测量进一步修正状态"><a href="#利用测量进一步修正状态" class="headerlink" title="利用测量进一步修正状态"></a>利用测量进一步修正状态</h3><p>假设我们有几个传感器，这些传感器可以向我们提供有关系统状态的信息。就目前而言，测量什么量都无关紧要，也许一个读取位置，另一个读取速度。每个传感器都告诉我们有关状态的一些间接信息（换句话说，传感器在状态下运作并产生一组测量读数）。</p><p><img data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_12.jpg"></p><p>请注意，测量的单位可能与状态量的单位不同。我们使用矩阵$\mathbf{H}_k$对传感器的测量进行建模。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_13.jpg"></p><p>所以我们期望传感器的度数可以被建模成如下形式：</p><script type="math/tex; mode=display">\begin{equation}  \begin{aligned}  \vec{\mu}_{\text{expected}} &= \mathbf{H}_k \color{deeppink}{\mathbf{\hat{x}}_k} \\  \mathbf{\Sigma}_{\text{expected}} &= \mathbf{H}_k \color{deeppink}{\mathbf{P}_k} \mathbf{H}_k^T  \end{aligned}  \end{equation}</script><p>卡尔曼滤波器的伟大之处就在于它能够处理传感器噪声。换句话说，传感器本身的测量是不准确的，且原始估计中的每个状态都可能导致一定范围的传感器读数，而卡尔曼滤波能够在这些不确定性存在的情况下找到最优的状态。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_14.jpg"></p><p>根据传感器的读数，我们会猜测系统正处于某个特定状态。但是由于不确定性的存在，<strong>某些状态比其他状态更可能产生我们看到的读数</strong>：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_11.jpg"></p><p>我们将这种不确定性（如传感器噪声）的<strong>协方差</strong>表示为$\color{mediumaquamarine}{\mathbf{R}_k}$，读数的分布<strong>均值</strong>等于我们观察到传感器的读数，我们将其表示为$\color{yellowgreen}{\vec{\mathbf{z}_k}}$</p><p>这样一来，我们有了两个高斯分布：一个围绕通过状态转移预测的平均值，另一个围绕实际传感器读数。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_4.jpg"></p><p>因此，我们需要将基于预测状态（<strong><font color="deeppink">粉红色</font></strong>）的推测读数与基于实际观察到的传感器读数（<strong><font color="yellowgreen">绿色</font></strong>）进行融合。</p><p>那么融合后<strong>最有可能的新状态</strong>是什么？ 对于任何可能的读数$(z_1,z_2)$，我们都有两个相关的概率：（1）我们的传感器读数$\color{yellowgreen}{\vec{\mathbf{z}_k}}$是$(z_1,z_2)$的（误-）测量值的概率，以及（2）先前估计值的概率认为$(z_1,z_2)$是我们应该看到的读数。</p><p>如果我们有两个概率，并且想知道两个概率都为真的机会，则将它们相乘。因此，我们对两个高斯分布进行了相乘处理：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_6a.png"></p><p>两个概率分布相乘得到的就是上图中的重叠部分。而且重叠部分的概率分布会比我们之前的任何一个估计值/读数都精确得多，这个分布的均值就是两种估计最有可能配置（得到的状态）。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_6.png"></p><p>事实证明，<strong>两个独立的高斯分布相乘之后会得到一个新的具有其均值和协方差矩阵的高斯分布</strong>！下面开始推公式。</p><h3 id="合并两个高斯分布"><a href="#合并两个高斯分布" class="headerlink" title="合并两个高斯分布"></a>合并两个高斯分布</h3><p>首先考虑一维高斯情况：一个均值为$\mu$，方差为$\sigma^2$的高斯分布的形式为：</p><script type="math/tex; mode=display">\begin{equation} \label{gaussformula} \mathcal{N}(x, \mu,\sigma) = \frac{1}{ \sigma \sqrt{ 2\pi } } e^{ -\frac{ (x – \mu)^2 }{ 2\sigma^2 } } \end{equation}</script><p>我们想知道将两个高斯曲线相乘会发生什么。下图中的蓝色曲线表示两个高斯总体的（未归一化）交集：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/gauss_joint.png"></p><script type="math/tex; mode=display">\begin{equation} \label{gaussequiv} \mathcal{N}(x, \color{fuchsia}{\mu_0}, \color{deeppink}{\sigma_0}) \cdot \mathcal{N}(x, \color{yellowgreen}{\mu_1}, \color{mediumaquamarine}{\sigma_1}) \stackrel{?}{=} \mathcal{N}(x, \color{royalblue}{\mu’}, \color{mediumblue}{\sigma’}) \end{equation}</script><p>将公式$\eqref{gaussformula}$代入公式$\eqref{gaussequiv}$，我们可以得到新的高斯分布的均值和方差如下所示：</p><script type="math/tex; mode=display">\begin{equation} \label{fusionformula} \begin{aligned} \color{royalblue}{\mu’} &= \mu_0 + \frac{\sigma_0^2 (\mu_1 – \mu_0)} {\sigma_0^2 + \sigma_1^2}\\ \color{mediumblue}{\sigma’}^2 &= \sigma_0^2 – \frac{\sigma_0^4} {\sigma_0^2 + \sigma_1^2} \end{aligned} \end{equation}</script><p>我们将其中的一小部分重写为$\color{purple}{\mathbf{k}}$：</p><script type="math/tex; mode=display">\begin{equation} \label{gainformula} \color{purple}{\mathbf{k}} = \frac{\sigma_0^2}{\sigma_0^2 + \sigma_1^2} \end{equation}</script><script type="math/tex; mode=display">\begin{equation} \begin{split} \color{royalblue}{\mu’} &= \mu_0 + &\color{purple}{\mathbf{k}} (\mu_1 – \mu_0)\\ \color{mediumblue}{\sigma’}^2 &= \sigma_0^2 – &\color{purple}{\mathbf{k}} \sigma_0^2 \end{split} \label{update} \end{equation}</script><p>这样一来，公式的形式就简单多了！我们顺势将公式$\eqref{gainformula}$和$\eqref{update}$的矩阵形式写在下面：</p><script type="math/tex; mode=display">\begin{equation} \label{matrixgain} \color{purple}{\mathbf{K}} = \Sigma_0 (\Sigma_0 + \Sigma_1)^{-1} \end{equation}</script><script type="math/tex; mode=display">\begin{equation} \begin{split} \color{royalblue}{\vec{\mu}’} &= \vec{\mu_0} + &\color{purple}{\mathbf{K}} (\vec{\mu_1} – \vec{\mu_0})\\ \color{mediumblue}{\Sigma’} &= \Sigma_0 – &\color{purple}{\mathbf{K}} \Sigma_0 \end{split} \label{matrixupdate} \end{equation}</script><p>其中$\Sigma$表示新高斯分布的协方差矩阵，$\vec{\mu}$是每个维度的均值，$\color{purple}{\mathbf{K}}$就是大名鼎鼎的“<strong>卡尔曼增益</strong>”（<strong>Kalman gain</strong>）。</p><h3 id="公式汇总"><a href="#公式汇总" class="headerlink" title="公式汇总"></a>公式汇总</h3><p>我们有两个高斯分布，一个是我们预测的观测$(\color{fuchsia}{\mu_0}, \color{deeppink}{\Sigma_0}) = (\color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k}, \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T})$，另外一个是实际的观测(传感器读数)$(\color{yellowgreen}{\mu_1}, \color{mediumaquamarine}{\Sigma_1}) = (\color{yellowgreen}{\vec{\mathbf{z}_k}}, \color{mediumaquamarine}{\mathbf{R}_k})$，我们将这两个高斯分布带入公式$\eqref{matrixupdate}$中就可以得到二者的重叠区域：</p><script type="math/tex; mode=display">\begin{equation} \begin{aligned} \mathbf{H}_k \color{royalblue}{\mathbf{\hat{x}}_k’} &= \color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k} & + & \color{purple}{\mathbf{K}} ( \color{yellowgreen}{\vec{\mathbf{z}_k}} – \color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k} ) \\ \mathbf{H}_k \color{royalblue}{\mathbf{P}_k’} \mathbf{H}_k^T &= \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} & – & \color{purple}{\mathbf{K}} \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} \end{aligned} \label {kalunsimplified} \end{equation}</script><p>从公式$\eqref{matrixgain}$我们可以知道，卡尔曼增益是：</p><script type="math/tex; mode=display">\begin{equation} \label{eq:kalgainunsimplified} \color{purple}{\mathbf{K}} = \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} ( \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} + \color{mediumaquamarine}{\mathbf{R}_k})^{-1} \end{equation}</script><p>然后我们将公式$\eqref{kalunsimplified}$与公式$\eqref{eq:kalgainunsimplified}$中的$\mathbf{H}_k$去除，同时将$\color{royalblue}{\mathbf{P}_k’}$后面的$\mathbf{H}_k^T$去除，我们可以得到最终的化简形式的更新方程：</p><script type="math/tex; mode=display">\begin{equation} \begin{split} \color{royalblue}{\mathbf{\hat{x}}_k’} &= \color{fuchsia}{\mathbf{\hat{x}}_k} & + & \color{purple}{\mathbf{K}’} ( \color{yellowgreen}{\vec{\mathbf{z}_k}} – \color{fuchsia}{\mathbf{H}_k \mathbf{\hat{x}}_k} ) \\ \color{royalblue}{\mathbf{P}_k’} &= \color{deeppink}{\mathbf{P}_k} & – & \color{purple}{\mathbf{K}’} \color{deeppink}{\mathbf{H}_k \mathbf{P}_k} \end{split} \label{kalupdatefull} \end{equation}</script><script type="math/tex; mode=display">\begin{equation} \color{purple}{\mathbf{K}’} = \color{deeppink}{\mathbf{P}_k \mathbf{H}_k^T} ( \color{deeppink}{\mathbf{H}_k \mathbf{P}_k \mathbf{H}_k^T} + \color{mediumaquamarine}{\mathbf{R}_k})^{-1} \label{kalgainfull} \end{equation}</script><h2 id="图说"><a href="#图说" class="headerlink" title="图说"></a>图说</h2><p>大功告成，$\color{royalblue}{\mathbf{\hat{x}}_k’}$就是更新后的最优状态！接下来我们可以继续进行预测，然后更新，重复上述过程！下图给出卡尔曼滤波信息流。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-5/kalman-filter-in-pictures/kalflow.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在上述所有数学公式中，你需要实现的只是公式$\eqref{kalpredictfull}, \eqref{kalupdatefull}$和$\eqref{kalgainfull}$。（或者，如果你忘记了这些，可以从等式$\eqref{covident}$和$\eqref{matrixupdate}$重新推导所有内容。）</p><p>这将使你能够准确地对任何线性系统建模。对于非线性系统，我们使用<strong>扩展卡尔曼滤波器</strong>，该滤波器通过简单地线性化预测和测量值的均值进行工作。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><blockquote id="fn_1"><sup>1</sup>. <a href="http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/#mathybits" target="_blank" rel="noopener">How a Kalman filter works, in pictures, 图解卡尔曼滤波是如何工作的</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. <a href="https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/" target="_blank" rel="noopener">A geometric interpretation of the covariance matrix, 协方差矩阵的几何解释</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. <a href="https://sikasjc.github.io/2018/05/08/kalman_filter" target="_blank" rel="noopener">Kalman Filter 卡尔曼滤波</a><a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. R. Faragher, “Understanding the Basis of the Kalman Filter Via a Simple and Intuitive Derivation [Lecture Notes]”, IEEE Signal Processing Magazine, vol. 29, no. 5, pp. 128–132, Sep. 2012.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. G. Welch and G. Bishop, “<a href="http://www.cs.unc.edu/~welch/media/pdf/kalman_intro.pdf" target="_blank" rel="noopener">An Introduction to the Kalman Filter</a>”, p. 16, 2006.<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. Fitzgerald, Robert J. “Divergence of the Kalman filter”, Automatic Control IEEE Transactions on 16.6(1971):736-747.<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>国内加速访问arxiv</title>
      <link href="/posts/redirect-arxiv/"/>
      <url>/posts/redirect-arxiv/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note warning">            <p><strong>注意</strong>：<code>arxiv</code>读音与<code>archive</code>一样，英[ˈɑːkaɪv]，美[ˈɑːrkaɪv]</p>          </div><p>国内访问论文预发布平台<code>arxiv</code>巨慢无比，让人闹心！网上找了一个很好用的方法，按照这个方法配置之后<code>arxiv</code>就秒开了。原理就是将<code>arxiv</code>重定向到<code>xxx.itp.ac.cn</code>（中科院理论物理研究所镜像）。<br>如果此时你找到了一篇文章，地址是<code>arxiv.org/abs/1911.11763</code>，只需要把<code>arxiv.org</code>换成<code>xxx.itp.ac.cn</code>即可。但每次都手动配置就很麻烦，为了贯彻将懒惰进行到底的精神，我们需要将上述过程自动化。配置如下：</p><a id="more"></a><ol><li><p>安装<a href="https://chrome.google.com/webstore/detail/tampermonkey/dhdgffkkebhmkfjojejmpbldmpobfkfo?hl=en" target="_blank" rel="noopener">Tampermonkey</a>油猴插件，自行google。</p></li><li><p>添加如下脚本</p></li></ol><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// ==UserScript==</span></span><br><span class="line"><span class="comment">// @name        redirect_arxiv</span></span><br><span class="line"><span class="comment">// @namespace   redirect_arxiv</span></span><br><span class="line"><span class="comment">// @include     *</span></span><br><span class="line"><span class="comment">// @include     https://*github.io*</span></span><br><span class="line"><span class="comment">// @include     https://*arxiv.org/*</span></span><br><span class="line"><span class="comment">// @include     https://*google.c*</span></span><br><span class="line"><span class="comment">// @include     https://*semanticscholar.org/*</span></span><br><span class="line"><span class="comment">// @include     https://*github.com*</span></span><br><span class="line"><span class="comment">// @include     https://*zhihu.com*</span></span><br><span class="line"><span class="comment">// @include     https://*brainpp.cn*</span></span><br><span class="line"><span class="comment">// @include     https://*outlook.cn*</span></span><br><span class="line"><span class="comment">// @version     1.0</span></span><br><span class="line"><span class="comment">// @grant       none</span></span><br><span class="line"><span class="comment">// ==/UserScript==</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 重定向 arxiv.org 到 xxx.itp.ac.cn（中科院理论物理研究所镜像）</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">findFatherNode</span>(<span class="params">node, nodeName=<span class="string">'A'</span>, maxDeep=<span class="number">1000</span></span>)</span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; maxDeep; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (! node)&#123;<span class="keyword">return</span> node&#125;</span><br><span class="line">        <span class="keyword">if</span> (node.nodeName == nodeName)&#123;</span><br><span class="line">            <span class="keyword">return</span> node</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            node = node.parentElement</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">document</span>.body.addEventListener(<span class="string">'mousedown'</span>, <span class="function"><span class="keyword">function</span>(<span class="params">e</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">var</span> targ = e.target || e.srcElement;</span><br><span class="line">    <span class="keyword">var</span> aTag = findFatherNode(targ, <span class="string">'A'</span>, <span class="number">10</span>);</span><br><span class="line">    <span class="keyword">if</span> (!aTag || !(aTag.href))&#123;<span class="keyword">return</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> headN = <span class="number">17</span>;</span><br><span class="line">    <span class="keyword">var</span> hrefHead = aTag.href.slice(<span class="number">0</span>, headN);</span><br><span class="line">    <span class="keyword">var</span> hrefTail = aTag.href.slice(headN);</span><br><span class="line">    <span class="keyword">if</span> ( (hrefHead.indexOf(<span class="string">'arxiv.org'</span>)==<span class="number">-1</span>))&#123;<span class="keyword">return</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ( hrefHead.match(<span class="regexp">/https?:\/\/arxiv\.org/</span>) ) &#123;</span><br><span class="line">        hrefHead = hrefHead.replace(<span class="regexp">/https?:\/\/arxiv\.org/</span>, <span class="string">'http://xxx.itp.ac.cn'</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    aTag.href = hrefHead + hrefTail</span><br><span class="line">    <span class="comment">// console.log(targ, targ.href);</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><ul><li><a href="https://www.zhihu.com/question/58912862/answer/695125360" target="_blank" rel="noopener">参考来自小磊知乎的回答</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> arxiv </tag>
            
            <tag> redirect </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CVPR2020图像匹配挑战赛，新数据集+新评测方法，SOTA正瑟瑟发抖！</title>
      <link href="/posts/2020-image-matching-cvpr/"/>
      <url>/posts/2020-image-matching-cvpr/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>从一系列的图像中恢复物体的3D结构是计算机视觉研究中一个热门课题，这使得我们可以相隔万里从google map中看到复活节岛的风景。这得益于图像来自于可控的条件，使得最终的重建效果一致性且质量都很高，但是这却限制了采集设备以及视角。畅想一下，假如我们不使用专业设备，而是利用sfm技术根据互联网上大量的图片重建出这个复杂世界。</p><!-- ![A 3D reconstruction generated from over 3000 images, including those from the previous figure](https://1.bp.blogspot.com/-loSqCB3NnM0/XoTiOGP9SYI/AAAAAAAAFlE/rs8iCTq63FYapA7HbljF8iWa7fyHvh3UgCLcBGAsYHQ/s400/image3.gif) --><!-- ![](https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/image_sfm.gif) --><p><img alt data-src="https://vincentqin.gitee.io/posts/2020-image-matching-cvpr/image_sfm.gif"></p><p>为了加快这个领域的研究，更好地利用图像数据有效信息，谷歌联合 <a href="https://www.uvic.ca/" target="_blank" rel="noopener">UVIC</a>, <a href="https://www.cvut.cz/en" target="_blank" rel="noopener">CTU</a>以及EPFL发表了这篇文章 “<a href="https://arxiv.org/abs/2003.01587" target="_blank" rel="noopener">Image Matching across Wide Baselines: From Paper to Practice</a>”，[<strong><a href="http://xxx.itp.ac.cn/pdf/2003.01587v2" target="_blank" rel="noopener">PDF</a></strong>]，旨在公布一种新的衡量用于3D重建方法的标准模块+数据集，这里主要是指2D图像间的匹配。这个评价模块可以很方便地集成并评估现有流行的特征匹配算法，包括传统方法或者基于机器学习的方法。</p><p>谷歌公布2020图像匹配挑战的数据集：<a href="https://image-matching-workshop.github.io/" target="_blank" rel="noopener">官网</a>，<a href="http://ai.googleblog.com/2020/04/announcing-2020-image-matching.html" target="_blank" rel="noopener">博客</a>，文末有排行榜。</p><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>图像特征匹配是计算机视觉的基础+核心问题之一，包括image retrieval <sup><a href="#fn_48" id="reffn_48">48</a></sup> <sup><a href="#fn_7" id="reffn_7">7</a></sup> <sup><a href="#fn_69" id="reffn_69">69</a></sup> <sup><a href="#fn_91" id="reffn_91">91</a></sup> <sup><a href="#fn_63" id="reffn_63">63</a></sup>, 3D reconstruction<sup><a href="#fn_3" id="reffn_3">3</a></sup> <sup><a href="#fn_43" id="reffn_43">43</a></sup> <sup><a href="#fn_79" id="reffn_79">79</a></sup> <sup><a href="#fn_106" id="reffn_106">106</a></sup>，re-localization <sup><a href="#fn_74" id="reffn_74">74</a></sup> <sup><a href="#fn_75" id="reffn_75">75</a></sup> <sup><a href="#fn_51" id="reffn_51">51</a></sup>以及 SLAM <sup><a href="#fn_61" id="reffn_61">61</a></sup> <sup><a href="#fn_30" id="reffn_30">30</a></sup> <sup><a href="#fn_31" id="reffn_31">31</a></sup>等在内的诸多研究领域都会用到特征匹配。这个问题已经研究了几十年，但仍未被很好地解决。特征匹配面临的问题很多，主要包括以下挑战：视角，尺度，旋转，光照，遮挡以及相机渲染等。</p><p>近些年来，研究者开始将视线转移到端到端的学习方法（图像-&gt;位姿），但是这些方法甚至没有达到传统的方法（图像-&gt;匹配-&gt;BA优化）的性能。我们可以看到，传统的方法将3D重建问题拆分成为2个子问题：特征匹配与位姿解算。解决每个子问题的新方法，诸如特征匹配/位姿解算，都使用了“临时指标”，但是单独地评价单个子问题的性能不足以说明整体性能。例如，一些研究仅在某个数据集上展现了相较于手工特征SIFT的优势，但是这些算法是否能够在真实应用中仍然展现出优势呢？我们通过后续实验说明传统算法经过调整之后也可匹敌现有的标称“sota”的算法（着实打脸）。</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig1.png"></p><p>是时候换一种方式进行评价了，本文不去过多关注在临时指标上的表现，而关注在下游任务上的表现。本文贡献：</p><ol><li>30k图像+深度图+真实位姿（posed image）</li><li>模块化流水线处理流程，结合了数十种经典的和最新的特征提取和匹配以及姿态估计方法，以及多种启发式方法，可以分别交换和调整</li><li>两个下游任务，双目/多视角重建</li><li>全面研究了手工特征以及学习特征数十种方法和技术，以及它们的结合以及超参数选择的过程</li></ol><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="局部特征"><a href="#局部特征" class="headerlink" title="局部特征"></a>局部特征</h3><p>在引入SIFT特征之后，局部特征变成了主流。它的处理流程主要分为几个步骤：特征提取，旋转估计，描述子提取。除了SIFT，手工特征还有SURF <sup><a href="#fn_15" id="reffn_15">15</a></sup>, ORB <sup><a href="#fn_73" id="reffn_73">73</a></sup>, 以及 AKAZE <sup><a href="#fn_4" id="reffn_4">4</a></sup>等。</p><p>现代描述子通常在SIFT关键点（即DoG）的预裁剪图像块上训练深度网络，其中包括：Deepdesc <sup><a href="#fn_82" id="reffn_82">82</a></sup>, TFeat <sup><a href="#fn_11" id="reffn_11">11</a></sup>, L2-Net <sup><a href="#fn_89" id="reffn_89">89</a></sup>, Hardnet <sup><a href="#fn_57" id="reffn_57">57</a></sup>, SOSNet [90]以及 LogPolarDesc <sup><a href="#fn_34" id="reffn_34">34</a></sup>（它们中绝大多数都是在同一个数据集上进行的训练）。</p><p>最近有一些工作利用了其它线索，诸如几何或全局上下文信息进行训练，其中包括GeoDesc [50] and ContextDesc <sup><a href="#fn_49" id="reffn_49">49</a></sup>。</p><p>另外还有一些方法将特征点以及描述子进行单独训练，例如TILDE <sup><a href="#fn_95" id="reffn_95">95</a></sup>, TCDet <sup><a href="#fn_103" id="reffn_103">103</a></sup>, QuadNet <sup><a href="#fn_78" id="reffn_78">78</a></sup>, and Key.Net <sup><a href="#fn_13" id="reffn_13">13</a></sup>。当前还有一些算法将二者联合起来训练，例如LIFT <sup><a href="#fn_99" id="reffn_99">99</a></sup>,DELF <sup><a href="#fn_63" id="reffn_63">63</a></sup>, SuperPoint <sup><a href="#fn_31" id="reffn_31">31</a></sup>, LF-Net <sup><a href="#fn_64" id="reffn_64">64</a></sup>, D2-Net <sup><a href="#fn_33" id="reffn_33">33</a></sup>,R2D2 <sup><a href="#fn_72" id="reffn_72">72</a></sup>。</p><h3 id="鲁棒匹配"><a href="#鲁棒匹配" class="headerlink" title="鲁棒匹配"></a>鲁棒匹配</h3><p>大基线的双目匹配的外点内点率可低至10%，甚至更低。要做匹配的话需要从中选择出能够解算出位姿的算法。常用的方式包括基于随机一致采样RANSAC的5-<sup><a href="#fn_62" id="reffn_62">62</a></sup>，7-<sup><a href="#fn_41" id="reffn_41">41</a></sup>，8-point<sup><a href="#fn_39" id="reffn_39">39</a></sup>算法。它的改进算法包括local optimization <sup><a href="#fn_24" id="reffn_24">24</a></sup>, MLESAC <sup><a href="#fn_92" id="reffn_92">92</a></sup>, PROSAC <sup><a href="#fn_23" id="reffn_23">23</a></sup>, DEGENSAC <sup><a href="#fn_26" id="reffn_26">26</a></sup>, GC-RANSAC <sup><a href="#fn_12" id="reffn_12">12</a></sup>,  MAGSAC <sup><a href="#fn_29" id="reffn_29">29</a></sup>，CNe (Context Networks) <sup><a href="#fn_100" id="reffn_100">100</a></sup>+RANSAC，同样还有<sup><a href="#fn_70" id="reffn_70">70</a></sup> <sup><a href="#fn_104" id="reffn_104">104</a></sup> <sup><a href="#fn_85" id="reffn_85">85</a></sup> <sup><a href="#fn_102" id="reffn_102">102</a></sup>。作者最后加了一句“Despite their promise, it remains unclear how well they perform in real settings”（质疑中，哈哈）。</p><h3 id="运动恢复结构（SfM）"><a href="#运动恢复结构（SfM）" class="headerlink" title="运动恢复结构（SfM）"></a>运动恢复结构（SfM）</h3><p>方法 <sup><a href="#fn_3" id="reffn_3">3</a></sup> <sup><a href="#fn_43" id="reffn_43">43</a></sup> <sup><a href="#fn_27" id="reffn_27">27</a></sup> <sup><a href="#fn_37" id="reffn_37">37</a></sup> <sup><a href="#fn_106" id="reffn_106">106</a></sup>，最流行的包括VisualSFM <sup><a href="#fn_98" id="reffn_98">98</a></sup>以及COLMAP <sup><a href="#fn_79" id="reffn_79">79</a></sup>（作为真值）。</p><h3 id="数据集和标准"><a href="#数据集和标准" class="headerlink" title="数据集和标准"></a>数据集和标准</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig2.jpg"></p><p>以前的特征匹配数据集如下：</p><ul><li>Oxford dataset <sup><a href="#fn_54" id="reffn_54">54</a></sup>, 48张图像+真值单应矩阵</li><li>HPatches <sup><a href="#fn_9" id="reffn_9">9</a></sup>, 696张光照以及视角变化，无遮挡平面图像</li><li>DTU <sup><a href="#fn_1" id="reffn_1">1</a></sup>, Edge Foci <sup><a href="#fn_107" id="reffn_107">107</a></sup>, Webcam <sup><a href="#fn_95" id="reffn_95">95</a></sup>, AMOS <sup><a href="#fn_67" id="reffn_67">67</a></sup>, 以及 Strecha’s <sup><a href="#fn_83" id="reffn_83">83</a></sup></li></ul><p>上述数据集都有其限制：窄基线，真值噪声大，图像数量少。基于学习的描述子通常在<sup><a href="#fn_21" id="reffn_21">21</a></sup>上进行训练，它们之所以比SIFT好的原因可能在于过拟合了（作者看到会不会脸红）。<br>另外，用于导航/重定位以及slam的数据集包括Kitti <sup><a href="#fn_38" id="reffn_38">38</a></sup>, Aachen <sup><a href="#fn_76" id="reffn_76">76</a></sup>, Robotcar <sup><a href="#fn_52" id="reffn_52">52</a></sup>以及CMU seasons <sup><a href="#fn_75" id="reffn_75">75</a></sup> <sup><a href="#fn_8" id="reffn_8">8</a></sup>，但并不包含Phototourism数据中的多种变换。</p><h2 id="Phototourism-数据集"><a href="#Phototourism-数据集" class="headerlink" title="Phototourism 数据集"></a>Phototourism 数据集</h2><p>上述数据集这么“烂”，于是作者搞出了他们心目中最好的公开数据集——Phototourism 数据集。作者从<sup><a href="#fn_43" id="reffn_43">43</a></sup> <sup><a href="#fn_88" id="reffn_88">88</a></sup>中选择的25个受欢迎的地标集合（共30k）为基础，每个地标都有成百上千的图像。论文中，作者从中选择出11个场景，其中9个测试集和2个验证集做实验。将它们缩减为最大尺寸为1024像素，并使用COLMAP <sup><a href="#fn_79" id="reffn_79">79</a></sup>对其进行求解位姿以及点云和深度，通过建立好的模型去除遮挡物。</p><p>具体地，如下2个表格所示：</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab1.png"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab2.png"></p><h2 id="处理流程图Pipeline"><a href="#处理流程图Pipeline" class="headerlink" title="处理流程图Pipeline"></a>处理流程图Pipeline</h2><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig7.png"></p><p>流程如上图，蓝色框就是要进行的几个处理，分别介绍一下。</p><h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>作者选择了3大类特征：</p><ol><li>完全手工特征:<br>SIFT <sup><a href="#fn_48" id="reffn_48">48</a></sup> (以及RootSIFT <sup><a href="#fn_6" id="reffn_6">6</a></sup>), SURF <sup><a href="#fn_15" id="reffn_15">15</a></sup>, ORB <sup><a href="#fn_73" id="reffn_73">73</a></sup>, AKAZE <sup><a href="#fn_4" id="reffn_4">4</a></sup>，FREAK <sup><a href="#fn_107" id="reffn_107">107</a></sup>描述子+BRISK <sup><a href="#fn_108" id="reffn_108">108</a></sup>特征点，使用OpenCV的实现，除了ORB特征，降低特征提取阈值以多提取一些特征；<br>除此之外，也考虑VLFeat<sup><a href="#fn_94" id="reffn_94">94</a></sup>中DoG的一些变种：(VL-)DoG, Hessian <sup><a href="#fn_16" id="reffn_16">16</a></sup>, Hessian-Laplace <sup><a href="#fn_55" id="reffn_55">55</a></sup>, Harris-Laplace <sup><a href="#fn_55" id="reffn_55">55</a></sup>, MSER <sup><a href="#fn_53" id="reffn_53">53</a></sup>; 以及它们的仿射变种: DoG-Affine, Hessian-Affine <sup><a href="#fn_55" id="reffn_55">55</a></sup> <sup><a href="#fn_14" id="reffn_14">14</a></sup>, DoG-AffNet <sup><a href="#fn_59" id="reffn_59">59</a></sup>, Hessian-AffNet <sup><a href="#fn_59" id="reffn_59">59</a></sup></li><li>描述子从DoG特征学习得到的特征：<br>L2-Net <sup><a href="#fn_89" id="reffn_89">89</a></sup>, Hardnet <sup><a href="#fn_57" id="reffn_57">57</a></sup>,Geodesc <sup><a href="#fn_50" id="reffn_50">50</a></sup>, SOSNet <sup><a href="#fn_90" id="reffn_90">90</a></sup>, ContextDesc <sup><a href="#fn_49" id="reffn_49">49</a></sup>, LogPolarDesc <sup><a href="#fn_34" id="reffn_34">34</a></sup></li><li>端到端学习来的特征：<br>Superpoint <sup><a href="#fn_31" id="reffn_31">31</a></sup>, LF-Net <sup><a href="#fn_64" id="reffn_64">64</a></sup>, and D2-Net <sup><a href="#fn_33" id="reffn_33">33</a></sup>以及它们的多尺度变种：single- (SS) 以及 multi-scale (MS)</li></ol><h3 id="特征匹配"><a href="#特征匹配" class="headerlink" title="特征匹配"></a>特征匹配</h3><p>此处用的是最近邻。</p><h3 id="外点滤除"><a href="#外点滤除" class="headerlink" title="外点滤除"></a>外点滤除</h3><p>Context Networks <sup><a href="#fn_100" id="reffn_100">100</a></sup>+RANSAC<sup><a href="#fn_100" id="reffn_100">100</a></sup> <sup><a href="#fn_85" id="reffn_85">85</a></sup>，简称CNe，效果如下：</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig3.png"></p><h3 id="Stereo-task"><a href="#Stereo-task" class="headerlink" title="Stereo task"></a>Stereo task</h3><p>给定图像$\mathbf{I}_i$以及$\mathbf{I}_j$，解算基础矩阵 $\mathbf{F}_{i,j}$，除了现有的OpenCV<sup><a href="#fn_19" id="reffn_19">19</a></sup>以及sklearn<sup><a href="#fn_65" id="reffn_65">65</a></sup>中实现的RANSAC <sup><a href="#fn_36" id="reffn_36">36</a></sup> <sup><a href="#fn_25" id="reffn_25">25</a></sup>，作者也用到了DEGENSAC <sup><a href="#fn_26" id="reffn_26">26</a></sup>, GC-RANSAC <sup><a href="#fn_12" id="reffn_12">12</a></sup> and MAGSAC <sup><a href="#fn_29" id="reffn_29">29</a></sup>。最后通过OpenCV的<code>recoverPose</code>函数解算位姿。</p><h3 id="Multi-view-task"><a href="#Multi-view-task" class="headerlink" title="Multi-view task"></a>Multi-view task</h3><p>由于是评价<strong>特征的好坏</strong>而不是SfM算法，作者从几个大场景中<strong>随机选择</strong>出图片构成几个小的数据集，称为”bags”。其中包含3/5张图像的各有100bags，10张图像的各有50bags，25张图像的各有25bags，总共275个bags。将外点滤除后的结果送入COLMAP <sup><a href="#fn_79" id="reffn_79">79</a></sup>作为输入进行SfM重建。</p><h3 id="误差指标"><a href="#误差指标" class="headerlink" title="误差指标"></a>误差指标</h3><ol><li>mAA(mean Average Accuracy): Stereo task/Multi-view task</li><li>ATE(Absolute Trajectory Error): Multi-view task</li></ol><h2 id="实验开始——配置细节很重要"><a href="#实验开始——配置细节很重要" class="headerlink" title="实验开始——配置细节很重要"></a>实验开始——配置细节很重要</h2><p>首先比较了RANSAC在不同参数配置（置信度，极线对齐误差阈值以及最大迭代次数）下的表现：<br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig8.png"><br>总体来说，MAGSAC表现最好，DEGENSAC表现次之。另外，作者提到“default settings can be woefully inadequate. For example, OpenCV sets τ = 0.99 and η = 3 pixels, which results in a mAP at 10o of 0.5292 on the validation set – a performance drop of 23.9% relative.” 所以在日常使用OpenCV的RANSAC函数时需要自己调整下参数。</p><p>作者认为RANSAC的内点阈值对于每种局部特征也是不同的，作者做了如下实验。<br><img alt="Figure 5. RANSAC – Inlier threshold $\eta$" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig9.png"><br>上图可以直观看到从DOG学习的特征都聚集在了一起，其它特征比较分散，这也是太难选择了，于是作者使用了其他论文作者推荐的配置参数或者一些合理的参数作为内点阈值。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>作者列出了很多结果以及结论，我们仅去关注几个感兴趣的。</p><h3 id="8k特征"><a href="#8k特征" class="headerlink" title="8k特征"></a>8k特征</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab5.png"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab6.png"></p><p>大家期待已久的真的sota到底是谁呢？作者在以上特征的超参调整到最优后进行了实验，测试结果如下：</p><ol><li>mAA指标上DoG特征点占据了Top的位置，其中SOSNet排名#1，紧随其后的是HardNet。</li><li>‘HardNetAmos+’ <sup><a href="#fn_56" id="reffn_56">56</a></sup>,它在更多的数据(Brown <sup><a href="#fn_20" id="reffn_20">20</a></sup>, HPatches <sup><a href="#fn_9" id="reffn_9">9</a></sup>, AMOS <sup><a href="#fn_67" id="reffn_67">67</a></sup>)上进行了训练，但是效果却比不上在Brown的‘Liberty’上训练模型的效果。</li><li>multi-view任务中，DoG+HardNet表现属于top水平，略优于ContextDesc, SOSNet，LogpolarDesc；</li><li>R2D2是表现最好的端到端方法，同样在multi-view任务中表现较好（#6），但是在stereo任务中不如SIFT；</li><li>D2-net表现并不太好，可能由于图像下采样造成了较差的定位误差；</li><li>适当调整参数后的SIFT尤其是RootSIFT能够在stereo任务中排名#9，multi-view任务中排名#9，与所谓sota相差13.1%以及4.9%.（真为咱传统特征争气！）</li></ol><h3 id="2k特征"><a href="#2k特征" class="headerlink" title="2k特征"></a>2k特征</h3><p>这样做的理由是能够与LF-Net与Superpoint进行比较，结果如下图：</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab7.png"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/tab8.png"></p><p>结论：</p><ol><li>Key.Net+HardNet获得最好的表现，第二名是LogPolarDesc；</li><li>R2D2在stereo任务中排名#2，multi-view任务中排名#7</li></ol><h3 id="8k-vs-2k"><a href="#8k-vs-2k" class="headerlink" title="8k vs. 2k"></a>8k vs. 2k</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig16.png"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig17.png"></p><p>结论：</p><ol><li><strong>基于DoG的方法容易受益于多个特征，而学习的方法收益于重新训练</strong>（该结论来自于Key.Net+Hardnet的组合，作者进行了重新训练，表现优异）</li><li>整体来说基于学习的特征KeyNet, SuperPoint, R2D2, LF-Net在multi-view任务配置下比stereo任务配置下表现更好；(作者的假设是它们的鲁棒性好，但定位精度低)</li></ol><h3 id="光照变化"><a href="#光照变化" class="headerlink" title="光照变化"></a>光照变化</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig26.png"></p><p>作者用了直方图均衡化（CLAHE<sup><a href="#fn_66" id="reffn_66">66</a></sup>）去调整图像光度，结果如上图，可以看到几乎所有的基于学习的方法的测试效果都下降了，这可能由于没有专门地在这种场景中进行训练。而SIFT也没有得到明显提升，可能在于SIFT描述子是在某些假设条件下最佳表现。</p><h3 id="新指标-vs-传统指标"><a href="#新指标-vs-传统指标" class="headerlink" title="新指标 vs. 传统指标"></a>新指标 vs. 传统指标</h3><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/fig18.png"></p><p>这里要说明的是传统的评价方式与本文提出方式的关系。</p><ol><li>matching score的选择还是比较明智的，它似乎与mAA相关，但也很难保证高的匹配得分就一定有助于提升mAA，例如RootSIFT vs ContextDesc；</li><li>repeatability则比较难去诠释它对最后位姿解算的效果。AKAZE的repeatability最好但是matching score和pose mAA都非常差，作者的原话(arxiv版本1)就是<strong>descriptor may hurt its performance</strong>。</li><li>Key.Net获得最好的repeatability，但是在mAA指标上弱于DoG的方法，即使使用了相同的描述子HardNet;</li></ol><p><strong>注意</strong>，以上结果都是论文发布在arxiv平台时给出的结果，最新结果参考这个官网<a href="https://vision.uvic.ca/image-matching-challenge/leaderboard/" target="_blank" rel="noopener">排行榜</a>。</p><p>由于目前正在使用superpoint特征（SuperPoint (2k features, NMS=4), DEGENSAC），所以比较关注它的表现。感觉在2k特征阵营，它的表现并不好（屈居#35,目前共52个算法），然而SuperPoint + SuperGlue + DEGENSAC以及SuperPoint+GIFT+Graph Motion Coherence Network+DEGENSAC分别位列#1以及#2，这也是结果很让人欣慰！</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/leadboard_superglue.png"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/2020-image-matching-cvpr/leadboard_superpoint.png"></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><blockquote id="fn_1"><sup>1</sup>. H. Aanaes, A. L. Dahl, and K. Steenstrup Pedersen. Interesting Interest Points. IJCV, 97:18–35, 2012. 2<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. H. Aanaes and F. Kahl. Estimation of Deformable Structure and Motion. In Vision and Modelling of Dynamic Scenes Workshop, 2002. 6<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. S. Agarwal, N. Snavely, I. Simon, S.M. Seitz, and R. Szeliski. Building Rome in One Day. In ICCV, 2009. 1, 2<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. P. F. Alcantarilla, J. Nuevo, and A. Bartoli. Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces. In BMVC, 2013. 2, 3<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. Anonymous. DeepSFM: Structure From Motion Via Deep Bundle Adjustment. In Submission to ICLR, 2020. 2<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. Relja Arandjelovic. Three things everyone should know to improve object retrieval. In CVPR, 2012. 3<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote><blockquote id="fn_7"><sup>7</sup>. Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. NetVLAD: CNN Architecture for Weakly Supervised Place Recognition. In CVPR, 2016. 1<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a></blockquote><blockquote id="fn_8"><sup>8</sup>. Hernan Badino, Daniel Huber, and Takeo Kanade. The CMU Visual Localization Data Set. <a href="http://3dvis" target="_blank" rel="noopener">http://3dvis</a>. ri.cmu.edu/data-sets/localization, 2011. 2<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a></blockquote><blockquote id="fn_9"><sup>9</sup>. V. Balntas, K. Lenc, A. Vedaldi, and K. Mikolajczyk. HPatches: A Benchmark and Evaluation of Handcrafted and Learned Local Descriptors. In CVPR, 2017. 2, 7<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a></blockquote><blockquote id="fn_10"><sup>10</sup>. Vassileios Balntas, Shuda Li, and Victor Prisacariu. RelocNet: Continuous Metric Learning Relocalisation using Neural Nets. In The European Conference on Computer Vision (ECCV), September 2018. 1<a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a></blockquote><blockquote id="fn_11"><sup>11</sup>. V. Balntas, E. Riba, D. Ponsa, and K. Mikolajczyk. Learning Local Feature Descriptors with Triplets and Shallow Convolutional Neural Networks. In BMVC, 2016. 2<a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a></blockquote><blockquote id="fn_12"><sup>12</sup>. Daniel Barath and Ji Matas. Graph-cut ransac. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 2, 4<a href="#reffn_12" title="Jump back to footnote [12] in the text."> &#8617;</a></blockquote><blockquote id="fn_13"><sup>13</sup>. Axel Barroso-Laguna, Edgar Riba, Daniel Ponsa, and Krystian Mikolajczyk. Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters. In Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision, 2019. 2, 3<a href="#reffn_13" title="Jump back to footnote [13] in the text."> &#8617;</a></blockquote><blockquote id="fn_14"><sup>14</sup>. A. Baumberg. Reliable Feature Matching Across Widely Separated Views. In CVPR, pages 774–781, 2000. 3, 6<a href="#reffn_14" title="Jump back to footnote [14] in the text."> &#8617;</a></blockquote><blockquote id="fn_15"><sup>15</sup>. H. Bay, T. Tuytelaars, and L. Van Gool. SURF: Speeded Up Robust Features. In ECCV, 2006. 2, 3<a href="#reffn_15" title="Jump back to footnote [15] in the text."> &#8617;</a></blockquote><blockquote id="fn_16"><sup>16</sup>. P. R. Beaudet. Rotationally invariant image operators. In Proceedings of the 4th International Joint Conference on Pattern Recognition, pages 579–583, Kyoto, Japan, Nov. 1978. 3, 6<a href="#reffn_16" title="Jump back to footnote [16] in the text."> &#8617;</a></blockquote><blockquote id="fn_17"><sup>17</sup>. Jia-Wang Bian, Yu-Huan Wu, Ji Zhao, Yun Liu, Le Zhang, Ming-Ming Cheng, and Ian Reid. An Evaluation of Feature Matchers for Fundamental Matrix Estimation. In BMVC, 2019. 2<a href="#reffn_17" title="Jump back to footnote [17] in the text."> &#8617;</a></blockquote><blockquote id="fn_18"><sup>18</sup>. Eric Brachmann and Carsten Rother. Neural- Guided RANSAC: Learning Where to Sample Model Hypotheses. In ICCV, 2019. 2<a href="#reffn_18" title="Jump back to footnote [18] in the text."> &#8617;</a></blockquote><blockquote id="fn_19"><sup>19</sup>. G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000. 4<a href="#reffn_19" title="Jump back to footnote [19] in the text."> &#8617;</a></blockquote><blockquote id="fn_20"><sup>20</sup>. M. Brown, G. Hua, and S. Winder. Discriminative Learning of Local Image Descriptors. PAMI, 2011. 1, 2, 7<a href="#reffn_20" title="Jump back to footnote [20] in the text."> &#8617;</a></blockquote><blockquote id="fn_21"><sup>21</sup>. M. Brown and D. Lowe. Automatic Panoramic Image Stitching Using Invariant Features. IJCV, 74:59–73, 2007. 2<a href="#reffn_21" title="Jump back to footnote [21] in the text."> &#8617;</a></blockquote><blockquote id="fn_22"><sup>22</sup>. Mai Bui, Christoph Baur, Nassir Navab, Slobodan Ilic, and Shadi Albarqouni. Adversarial Networks for Camera Pose Regression and Reﬁnement. In The IEEE International Conference on Computer Vision (ICCV) Workshops, Oct 2019. 1<a href="#reffn_22" title="Jump back to footnote [22] in the text."> &#8617;</a></blockquote><blockquote id="fn_23"><sup>23</sup>. Ondˇrej Chum and Jiˇr´ı Matas. Matching with PROSAC Progressive Sample Consensus. In CVPR, pages 220–226, June 2005. 2<a href="#reffn_23" title="Jump back to footnote [23] in the text."> &#8617;</a></blockquote><blockquote id="fn_24"><sup>24</sup>. Ondˇrej Chum, Jiˇr´ı Matas, and Josef Kittler. Locally Optimized RANSAC. In PR, 2003. 2<a href="#reffn_24" title="Jump back to footnote [24] in the text."> &#8617;</a></blockquote><blockquote id="fn_25"><sup>25</sup>. Ondˇrej Chum, Jiˇr´ı Matas, and Josef Kittler. Locally optimized ransac. In Pattern Recognition, 2003. 4<a href="#reffn_25" title="Jump back to footnote [25] in the text."> &#8617;</a></blockquote><blockquote id="fn_26"><sup>26</sup>. Ondrej Chum, Tomas Werner, and Jiri Matas. Two-View Geometry Estimation Unaffected by a Dominant Plane. In CVPR, 2005. 2, 4<a href="#reffn_26" title="Jump back to footnote [26] in the text."> &#8617;</a></blockquote><blockquote id="fn_27"><sup>27</sup>. Hainan Cui, Xiang Gao, Shuhan Shen, and Zhanyi Hu. Hsfm: Hybrid structure-from-motion. In CVPR, July 2017. 2<a href="#reffn_27" title="Jump back to footnote [27] in the text."> &#8617;</a></blockquote><blockquote id="fn_28"><sup>28</sup>. Zheng Dang, Kwang Moo Yi, Yinlin Hu, Fei Wang, Pascal Fua, and Mathieu Salzmann. Eigendecomposition-Free Training of Deep Networks with Zero Eigenvalue-Based Losses. In ECCV, 2018. 4<a href="#reffn_28" title="Jump back to footnote [28] in the text."> &#8617;</a></blockquote><blockquote id="fn_29"><sup>29</sup>. Jana Noskova Daniel Barath, Jiri Matas. MAGSAC: marginalizing sample consensus. In CVPR, 2019. 1, 2, 4<a href="#reffn_29" title="Jump back to footnote [29] in the text."> &#8617;</a></blockquote><blockquote id="fn_30"><sup>30</sup>. D. Detone, T. Malisiewicz, and A. Rabinovich. Toward Geometric Deep SLAM. arXiv preprint arXiv:1707.07410, 2017. 1<a href="#reffn_30" title="Jump back to footnote [30] in the text."> &#8617;</a></blockquote><blockquote id="fn_31"><sup>31</sup>. D. Detone, T. Malisiewicz, and A. Rabinovich. Superpoint: Self-Supervised Interest Point Detection and Description. CVPR Workshop on Deep Learning for Visual SLAM, 2018. 1, 2, 3, 8<a href="#reffn_31" title="Jump back to footnote [31] in the text."> &#8617;</a></blockquote><blockquote id="fn_32"><sup>32</sup>. J. Dong and S. Soatto. Domain-Size Pooling in Local Descriptors: DSP-SIFT. In CVPR, 2015. 6<a href="#reffn_32" title="Jump back to footnote [32] in the text."> &#8617;</a></blockquote><blockquote id="fn_33"><sup>33</sup>. M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, and T. Sattler. D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. In CVPR, 2019. 1, 2, 3, 8<a href="#reffn_33" title="Jump back to footnote [33] in the text."> &#8617;</a></blockquote><blockquote id="fn_34"><sup>34</sup>. Patrick Ebel, Anastasiia Mishchuk, Kwang Moo Yi, Pascal Fua, and Eduard Trulls. Beyond Cartesian Representations for Local Descriptors. In ICCV, 2019. 2, 3, 6<a href="#reffn_34" title="Jump back to footnote [34] in the text."> &#8617;</a></blockquote><blockquote id="fn_35"><sup>35</sup>. Vassileios Balntas et.al. SILDa: A Multi-Task Dataset for Evaluating Visual Localization. <a href="https://github" target="_blank" rel="noopener">https://github</a>. com/scape-research/silda, 2018. 2<a href="#reffn_35" title="Jump back to footnote [35] in the text."> &#8617;</a></blockquote><blockquote id="fn_36"><sup>36</sup>. M.A Fischler and R.C. Bolles. Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography. Communications ACM, 24(6):381–395, 1981. 1, 2, 4<a href="#reffn_36" title="Jump back to footnote [36] in the text."> &#8617;</a></blockquote><blockquote id="fn_37"><sup>37</sup>. P. Gay, V. Bansal, C. Rubino, and A. D. Bue. Probabilistic Structure from Motion with Objects (PSfMO). In ICCV, 2017. 2<a href="#reffn_37" title="Jump back to footnote [37] in the text."> &#8617;</a></blockquote><blockquote id="fn_38"><sup>38</sup>. Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite. In CVPR, 2012. 2<a href="#reffn_38" title="Jump back to footnote [38] in the text."> &#8617;</a></blockquote><blockquote id="fn_39"><sup>39</sup>. R.I. Hartley. In Defense of the Eight-Point Algorithm. PAMI, 19(6):580–593, June 1997. 2<a href="#reffn_39" title="Jump back to footnote [39] in the text."> &#8617;</a></blockquote><blockquote id="fn_40"><sup>40</sup>. R. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, 2000. 1<a href="#reffn_40" title="Jump back to footnote [40] in the text."> &#8617;</a></blockquote><blockquote id="fn_41"><sup>41</sup>. R. I. Hartley. Projective reconstruction and invariants from multiple images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(10):1036–1041, Oct 1994. 1, 2<a href="#reffn_41" title="Jump back to footnote [41] in the text."> &#8617;</a></blockquote><blockquote id="fn_42"><sup>42</sup>. K. He, Y. Lu, and S. Sclaroff. Local Descriptors Optimized for Average Precision. In CVPR, 2018. 1<a href="#reffn_42" title="Jump back to footnote [42] in the text."> &#8617;</a></blockquote><blockquote id="fn_43"><sup>43</sup>. J. Heinly, J.L. Schoenberger, E. Dunn, and J-M. Frahm. Reconstructing the World in Six Days. In CVPR, 2015. 1, 2, 3<a href="#reffn_43" title="Jump back to footnote [43] in the text."> &#8617;</a></blockquote><blockquote id="fn_44"><sup>44</sup>. Karel Lenc and Varun Gulshan and Andrea Vedaldi. VLBenchmarks. <a href="http://www.vlfeat.org/" target="_blank" rel="noopener">http://www.vlfeat.org/</a> benchmarks/, 2011. 2<a href="#reffn_44" title="Jump back to footnote [44] in the text."> &#8617;</a></blockquote><blockquote id="fn_45"><sup>45</sup>. A. Kendall, M. Grimes, and R. Cipolla. Posenet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization. In ICCV, pages 2938–2946, 2015. 1<a href="#reffn_45" title="Jump back to footnote [45] in the text."> &#8617;</a></blockquote><blockquote id="fn_46"><sup>46</sup>. J. Krishna Murthy, Ganesh Iyer, and Liam Paull. gradSLAM: Dense SLAM meets Automatic Differentiation. arXiv, 2019. 2<a href="#reffn_46" title="Jump back to footnote [46] in the text."> &#8617;</a></blockquote><blockquote id="fn_47"><sup>47</sup>. Zhengqi Li and Noah Snavely. MegaDepth: Learning Single-View Depth Prediction from Internet Photos. In CVPR, 2018. 2<a href="#reffn_47" title="Jump back to footnote [47] in the text."> &#8617;</a></blockquote><blockquote id="fn_48"><sup>48</sup>. David G. Lowe. Distinctive Image Features from ScaleInvariant Keypoints. IJCV, 20(2):91–110, November 2004. 1, 2, 3, 4, 6, 8, 15<a href="#reffn_48" title="Jump back to footnote [48] in the text."> &#8617;</a></blockquote><blockquote id="fn_49"><sup>49</sup>. Zixin Luo, Tianwei Shen, Lei Zhou, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan. ContextDesc: Local Descriptor Augmentation with Cross-Modality Context. In CVPR, 2019. 2, 3<a href="#reffn_49" title="Jump back to footnote [49] in the text."> &#8617;</a></blockquote><blockquote id="fn_50"><sup>50</sup>. Z. Luo, T. Shen, L. Zhou, S. Zhu, R. Zhang, Y. Yao, T. Fang, and L. Quan. Geodesc: Learning Local Descriptors by Integrating Geometry Constraints. In ECCV, 2018. 2, 3<a href="#reffn_50" title="Jump back to footnote [50] in the text."> &#8617;</a></blockquote><blockquote id="fn_51"><sup>51</sup>. Simon Lynen, Bernhard Zeisl, Dror Aiger, Michael Bosse, Joel Hesch, Marc Pollefeys, Roland Siegwart, and Torsten Sattler. Large-scale, real-time visual-inertial localization revisited. arXiv Preprint, 2019. 1<a href="#reffn_51" title="Jump back to footnote [51] in the text."> &#8617;</a></blockquote><blockquote id="fn_52"><sup>52</sup>. Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year, 1000 km: The Oxford RobotCar dataset. IJRR, 36(1):3–15, 2017. 2<a href="#reffn_52" title="Jump back to footnote [52] in the text."> &#8617;</a></blockquote><blockquote id="fn_53"><sup>53</sup>. J. Matas, O. Chum, M. Urban, and T. Pajdla. Robust WideBaseline Stereo from Maximally Stable Extremal Regions. IVC, 22(10):761–767, 2004. 3, 6<a href="#reffn_53" title="Jump back to footnote [53] in the text."> &#8617;</a></blockquote><blockquote id="fn_54"><sup>54</sup>. K. Mikolajczyk and C. Schmid. A Performance Evaluation of Local Descriptors. PAMI, 27(10):1615–1630, 2004. 2<a href="#reffn_54" title="Jump back to footnote [54] in the text."> &#8617;</a></blockquote><blockquote id="fn_55"><sup>55</sup>. K. Mikolajczyk, C. Schmid, and A. Zisserman. Human Detection Based on a Probabilistic Assembly of Robust Part Detectors. In ECCV, pages 69–82, 2004. 3, 6<a href="#reffn_55" title="Jump back to footnote [55] in the text."> &#8617;</a></blockquote><blockquote id="fn_56"><sup>56</sup>. Jiri Matas Milan Pultar, Dmytro Mishkin. Leveraging Outdoor Webcams for Local Descriptor Learning. In Proceedings of CVWW 2019, 2019. 7<a href="#reffn_56" title="Jump back to footnote [56] in the text."> &#8617;</a></blockquote><blockquote id="fn_57"><sup>57</sup>. A. Mishchuk, D. Mishkin, F. Radenovic, and J. Matas. Working Hard to Know Your Neighbor’s Margins: Local Descriptor Learning Loss. In NeurIPS, 2017. 2, 3, 6<a href="#reffn_57" title="Jump back to footnote [57] in the text."> &#8617;</a></blockquote><blockquote id="fn_58"><sup>58</sup>. Dmytro Mishkin, Jiri Matas, and Michal Perdoch. MODS: PAMI, 19(6):580–593, June 1997. 2 Fast and robust method for two-view matching. CVIU, 2015. 6, 15<a href="#reffn_58" title="Jump back to footnote [58] in the text."> &#8617;</a></blockquote><blockquote id="fn_59"><sup>59</sup>. D. Mishkin, F. Radenovic, and J. Matas. Repeatability is Not Enough: Learning Affine Regions via Discriminability. In ECCV, 2018. 3, 6<a href="#reffn_59" title="Jump back to footnote [59] in the text."> &#8617;</a></blockquote><blockquote id="fn_60"><sup>60</sup>. Arun Mukundan, Giorgos Tolias, and Ondrej Chum. Explicit Spatial Encoding for Deep Local Descriptors. In CVPR, 2019. 1<a href="#reffn_60" title="Jump back to footnote [60] in the text."> &#8617;</a></blockquote><blockquote id="fn_61"><sup>61</sup>. R. Mur-Artal, J. Montiel, and J. Tardos. Orb-Slam: A Versatile and Accurate Monocular Slam System. IEEE Transactions on Robotics, 31(5):1147–1163, 2015. 1<a href="#reffn_61" title="Jump back to footnote [61] in the text."> &#8617;</a></blockquote><blockquote id="fn_62"><sup>62</sup>. D. Nister. An Efficient Solution to the Five-Point Relative Pose Problem. In CVPR, June 2003. 2<a href="#reffn_62" title="Jump back to footnote [62] in the text."> &#8617;</a></blockquote><blockquote id="fn_63"><sup>63</sup>. Hyeonwoo Noh, Andre Araujo, Jack Sim, and Tobias Weyanda nd Bohyung Han. Large-Scale Image Retrieval with Attentive Deep Local Features. In ICCV, 2017. 1, 2<a href="#reffn_63" title="Jump back to footnote [63] in the text."> &#8617;</a></blockquote><blockquote id="fn_64"><sup>64</sup>. Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi. LF-Net: Learning Local Features from Images. In NeurIPS, 2018. 2, 3<a href="#reffn_64" title="Jump back to footnote [64] in the text."> &#8617;</a></blockquote><blockquote id="fn_65"><sup>65</sup>. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. 4<a href="#reffn_65" title="Jump back to footnote [65] in the text."> &#8617;</a></blockquote><blockquote id="fn_66"><sup>66</sup>. Stephen M. Pizer, E. Philip Amburn, John D. Austin, Robert Cromartie, Ari Geselowitz, Trey Greer, Bart ter Haar Romeny, John B. Zimmerman, and Karel Zuiderveld. Adaptive histogram equalization and its variations. Computer vision, graphics, and image processing, 1987. 15<a href="#reffn_66" title="Jump back to footnote [66] in the text."> &#8617;</a></blockquote><blockquote id="fn_67"><sup>67</sup>. M. Pultar, D. Mishkin, and J. Matas. Leveraging Outdoor Webcams for Local Descriptor Learning. In Computer Vision Winter Workshop, 2019. 2, 7<a href="#reffn_67" title="Jump back to footnote [67] in the text."> &#8617;</a></blockquote><blockquote id="fn_68"><sup>68</sup>. C.R. Qi, H. Su, K. Mo, and L.J. Guibas. Pointnet: Deep Learning on Point Sets for 3D Classiﬁcation and Segmentation. In CVPR, 2017. 4<a href="#reffn_68" title="Jump back to footnote [68] in the text."> &#8617;</a></blockquote><blockquote id="fn_69"><sup>69</sup>. Filip Radenovic, Georgios Tolias, and Ondra Chum. CNN image retrieval learns from BoW: Unsupervised ﬁne-tuning with hard examples. In ECCV, 2016. 1<a href="#reffn_69" title="Jump back to footnote [69] in the text."> &#8617;</a></blockquote><blockquote id="fn_70"><sup>70</sup>. R. Ranftl and V. Koltun. Deep Fundamental Matrix Estimation. In ECCV, 2018. 2, 4<a href="#reffn_70" title="Jump back to footnote [70] in the text."> &#8617;</a></blockquote><blockquote id="fn_71"><sup>71</sup>. J. Revaud, P. Weinzaepfel, C. De Souza, N. Pion, G. Csurka, Y. Cabon, and M. Humenberger. R2D2: Repeatable and Reliable Detector and Descriptor. In arXiv Preprint, 2019. 8<a href="#reffn_71" title="Jump back to footnote [71] in the text."> &#8617;</a></blockquote><blockquote id="fn_72"><sup>72</sup>. J´erˆome Revaud, Philippe Weinzaepfel, C´esar Roberto de Souza, Noe Pion, Gabriela Csurka, Yohann Cabon, and Martin Humenberger. R2D2: Repeatable and Reliable Detector and Descriptor. In NeurIPS, 2019. 2<a href="#reffn_72" title="Jump back to footnote [72] in the text."> &#8617;</a></blockquote><blockquote id="fn_73"><sup>73</sup>. E. Rublee, V. Rabaud, K. Konolidge, and G. Bradski. ORB: An Efﬁcient Alternative to SIFT or SURF. In ICCV, 2011. 2, 3, 6<a href="#reffn_73" title="Jump back to footnote [73] in the text."> &#8617;</a></blockquote><blockquote id="fn_74"><sup>74</sup>. Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Improving Image-Based Localization by Active Correspondence Search. In ECCV, 2012. 1<a href="#reffn_74" title="Jump back to footnote [74] in the text."> &#8617;</a></blockquote><blockquote id="fn_75"><sup>75</sup>. T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg, D. Safari, M. Okutomi, M. Pollefeys, J. Sivic, F. Kahl, and T. Pajdla. Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions. In CVPR, 2018. 1, 2<a href="#reffn_75" title="Jump back to footnote [75] in the text."> &#8617;</a></blockquote><blockquote id="fn_76"><sup>76</sup>. Torsten Sattler, Tobias Weyand, Bastian Leibe, and Leif Kobbelt. Image Retrieval for Image-Based Localization Revisited. In BMVC, 2012. 2<a href="#reffn_76" title="Jump back to footnote [76] in the text."> &#8617;</a></blockquote><blockquote id="fn_77"><sup>77</sup>. Torsten Sattler, Qunjie Zhou, Marc Pollefeys, and Laura Leal-Taixe. Understanding the Limitations of CNN-based Absolute Camera Pose Regression. In CVPR, 2019. 1<a href="#reffn_77" title="Jump back to footnote [77] in the text."> &#8617;</a></blockquote><blockquote id="fn_78"><sup>78</sup>. N. Savinov, A. Seki, L. Ladicky, T. Sattler, and M. Pollefeys. Quad-Networks: Unsupervised Learning to Rank for Interest Point Detection. CVPR, 2017. 2<a href="#reffn_78" title="Jump back to footnote [78] in the text."> &#8617;</a></blockquote><blockquote id="fn_79"><sup>79</sup>. J.L. Sch¨onberger and J.M. Frahm. Structure-From-Motion Revisited. In CVPR, 2016. 1, 2, 3, 4, 6<a href="#reffn_79" title="Jump back to footnote [79] in the text."> &#8617;</a></blockquote><blockquote id="fn_80"><sup>80</sup>. J.L. Sch¨onberger, H. Hardmeier, T. Sattler, and M. Pollefeys. Comparative Evaluation of Hand-Crafted and Learned Local Features. In CVPR, 2017. 2<a href="#reffn_80" title="Jump back to footnote [80] in the text."> &#8617;</a></blockquote><blockquote id="fn_81"><sup>81</sup>. Yunxiao Shi, Jing Zhu, Yi Fang, Kuochin Lien, and Junli Gu. Self-Supervised Learning of Depth and Ego-motion with Differentiable Bundle Adjustment. arXiv Preprint, 2019. 2<a href="#reffn_81" title="Jump back to footnote [81] in the text."> &#8617;</a></blockquote><blockquote id="fn_82"><sup>82</sup>. E. Simo-serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. Moreno-Noguer. Discriminative Learning of Deep Convolutional Feature Point Descriptors. In ICCV, 2015. 2<a href="#reffn_82" title="Jump back to footnote [82] in the text."> &#8617;</a></blockquote><blockquote id="fn_83"><sup>83</sup>. C. Strecha, W.V. Hansen, L. Van Gool, P. Fua, and U. Thoennessen. On Benchmarking Camera Calibration and Multi-View Stereo for High Resolution Imagery. In CVPR, 2008. 2<a href="#reffn_83" title="Jump back to footnote [83] in the text."> &#8617;</a></blockquote><blockquote id="fn_84"><sup>84</sup>. J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. A Benchmark for the Evaluation of RGB-D SLAM Systems. In IROS, 2012. 4<a href="#reffn_84" title="Jump back to footnote [84] in the text."> &#8617;</a></blockquote><blockquote id="fn_85"><sup>85</sup>. Weiwei Sun, Wei Jiang, Eduard Trulls, Andrea Tagliasacchi, and Kwang Moo Yi. Attentive Context Normalization for Robust Permutation-Equivariant Learning. In arXiv Preprint, 2019. 2, 4, 8<a href="#reffn_85" title="Jump back to footnote [85] in the text."> &#8617;</a></blockquote><blockquote id="fn_86"><sup>86</sup>. Chengzhou Tang and Ping Tan. Ba-Net: Dense Bundle Adjustment Network. In ICLR, 2019. 2<a href="#reffn_86" title="Jump back to footnote [86] in the text."> &#8617;</a></blockquote><blockquote id="fn_87"><sup>87</sup>. Keisuke Tateno, Federico Tombari, Iro Laina, and Nassir Navab. Cnn-slam: Real-time dense monocular slam with learned depth prediction. In CVPR, July 2017. 2<a href="#reffn_87" title="Jump back to footnote [87] in the text."> &#8617;</a></blockquote><blockquote id="fn_88"><sup>88</sup>. B. Thomee, D.A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L. Li. YFCC100M: the New Data in Multimedia Research. In CACM, 2016. 3<a href="#reffn_88" title="Jump back to footnote [88] in the text."> &#8617;</a></blockquote><blockquote id="fn_89"><sup>89</sup>. Y. Tian, B. Fan, and F. Wu. L2-Net: Deep Learning of Discriminative Patch Descriptor in Euclidean Space. In CVPR, 2017. 2, 3<a href="#reffn_89" title="Jump back to footnote [89] in the text."> &#8617;</a></blockquote><blockquote id="fn_90"><sup>90</sup>. Yurun Tian, Xin Yu, Bin Fan, Fuchao Wu, Huub Heijnen, and Vassileios Balntas. SOSNet: Second Order Similarity Regularization for Local Descriptor Learning. In CVPR, 2019. 1, 2, 3<a href="#reffn_90" title="Jump back to footnote [90] in the text."> &#8617;</a></blockquote><blockquote id="fn_91"><sup>91</sup>. Giorgos Tolias, Yannis Avrithis, and Herv´e J´egou. Image Search with Selective Match Kernels: Aggregation Across Single and Multiple Images. IJCV, 116(3):247–261, Feb 2016. 1<a href="#reffn_91" title="Jump back to footnote [91] in the text."> &#8617;</a></blockquote><blockquote id="fn_92"><sup>92</sup>. P.H.S. Torr and A. Zisserman. MLESAC: A New Robust Estimator with Application to Estimating Image Geometry. CVIU, 78:138–156, 2000. 2<a href="#reffn_92" title="Jump back to footnote [92] in the text."> &#8617;</a></blockquote><blockquote id="fn_93"><sup>93</sup>. B. Triggs, P. Mclauchlan, R. Hartley, and A. Fitzgibbon. Bundle Adjustment – A Modern Synthesis. In Vision Algorithms: Theory and Practice, pages 298–372, 2000. 1<a href="#reffn_93" title="Jump back to footnote [93] in the text."> &#8617;</a></blockquote><blockquote id="fn_94"><sup>94</sup>. Andrea Vedaldi and Brian Fulkerson. Vlfeat: An open and portable library of computer vision algorithms. In Proceedings of the 18th ACM International Conference on Multimedia, MM ’10, pages 1469–1472, 2010. 3<a href="#reffn_94" title="Jump back to footnote [94] in the text."> &#8617;</a></blockquote><blockquote id="fn_95"><sup>95</sup>. Y. Verdie, K. M. Yi, P. Fua, and V. Lepetit. TILDE: A Temporally Invariant Learned DEtector. In CVPR, 2015. 2<a href="#reffn_95" title="Jump back to footnote [95] in the text."> &#8617;</a></blockquote><blockquote id="fn_96"><sup>96</sup>. S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, and K. Fragkiadaki. Sfm-Net: Learning of Structure and Motion from Video. arXiv Preprint, 2017. 2<a href="#reffn_96" title="Jump back to footnote [96] in the text."> &#8617;</a></blockquote><blockquote id="fn_97"><sup>97</sup>. X. Wei, Y. Zhang, Y. Gong, and N. Zheng. Kernelized Subspace Pooling for Deep Local Descriptors. In CVPR, 2018. 1<a href="#reffn_97" title="Jump back to footnote [97] in the text."> &#8617;</a></blockquote><blockquote id="fn_98"><sup>98</sup>. Changchang Wu. Towards Linear-Time Incremental Structure from Motion. In 3DV, 2013. 2, 6<a href="#reffn_98" title="Jump back to footnote [98] in the text."> &#8617;</a></blockquote><blockquote id="fn_99"><sup>99</sup>. Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal Fua. LIFT: Learned Invariant Feature Transform. In ECCV, 2016. 2<a href="#reffn_99" title="Jump back to footnote [99] in the text."> &#8617;</a></blockquote><blockquote id="fn_100"><sup>100</sup>. K. M. Yi, E. Trulls, Y. Ono, V. Lepetit, M. Salzmann, and P. Fua. Learning to Find Good Correspondences. In CVPR, 2018. 2, 3, 4, 7, 13, 17<a href="#reffn_100" title="Jump back to footnote [100] in the text."> &#8617;</a></blockquote><blockquote id="fn_101"><sup>101</sup>. S. Zagoruyko and N. Komodakis. Learning to Compare Image Patches via Convolutional Neural Networks. In CVPR, 2015. 6<a href="#reffn_101" title="Jump back to footnote [101] in the text."> &#8617;</a></blockquote><blockquote id="fn_102"><sup>102</sup>. Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou, Tianwei Shen, Yurong Chen, Long Quan, and Hongen Liao. Learning Two-View Correspondences and Geometry Using Order-Aware Network. ICCV, 2019. 2, 3, 4<a href="#reffn_102" title="Jump back to footnote [102] in the text."> &#8617;</a></blockquote><blockquote id="fn_103"><sup>103</sup>. Xu Zhang, Felix X. Yu, Svebor Karaman, and Shih-Fu Chang. Learning Discriminative and Transformation Covariant Local Feature Detectors. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. 2<a href="#reffn_103" title="Jump back to footnote [103] in the text."> &#8617;</a></blockquote><blockquote id="fn_104"><sup>104</sup>. Chen Zhao, Zhiguo Cao, Chi Li, Xin Li, and Jiaqi Yang. NM-Net: Mining Reliable Neighbors for Robust Feature Correspondences. In CVPR, 2019. 2, 4<a href="#reffn_104" title="Jump back to footnote [104] in the text."> &#8617;</a></blockquote><blockquote id="fn_105"><sup>105</sup>. Qunjie Zhou, Torsten Sattler, Marc Pollefeys, and Laura Leal-Taixe. To learn or not to learn: Visual localization from essential matrices. arXiv Preprint, 2019. 1<a href="#reffn_105" title="Jump back to footnote [105] in the text."> &#8617;</a></blockquote><blockquote id="fn_106"><sup>106</sup>. Siyu Zhu, Runze Zhang, Lei Zhou, Tianwei Shen, Tian Fang, Ping Tan, and Long Quan. Very Large-Scale Global SfM by Distributed Motion Averaging. In CVPR, June 2018. 1, 2<a href="#reffn_106" title="Jump back to footnote [106] in the text."> &#8617;</a></blockquote><blockquote id="fn_107"><sup>107</sup>. C.L. Zitnick and K. Ramnath. Edge Foci Interest Points. In ICCV, 2011. 2<a href="#reffn_107" title="Jump back to footnote [107] in the text."> &#8617;</a></blockquote><blockquote id="fn_108"><sup>108</sup>. A. Alahi, R. Ortiz, and P. Vandergheynst. FREAK: Fast Retina Keypoint. In CVPR, 2012. 7, 11<a href="#reffn_108" title="Jump back to footnote [108] in the text."> &#8617;</a></blockquote><blockquote id="fn_109"><sup>109</sup>. S. Leutenegger, M. Chli, and R. Y. Siegwart. Brisk: Binary robust invariant scalable keypoints. In ICCV, pages 2548–2555, 2011.7<a href="#reffn_109" title="Jump back to footnote [109] in the text."> &#8617;</a></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> ORB </tag>
            
            <tag> 特征匹配 </tag>
            
            <tag> SuperPoint </tag>
            
            <tag> SIFT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>笔记：SuperGlue:Learning Feature Matching with Graph Neural Networks论文阅读</title>
      <link href="/posts/superglue/"/>
      <url>/posts/superglue/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>ETHZ ASL与Magicleap联名之作，CVPR 2020 Oral（论文见文末），一作是来自ETHZ的实习生，二作是当年CVPR2018 SuperPoint的作者Daniel DeTone。<br><!-- ![](/posts/superglue/freiburg_matches.gif) --></p><p><img alt data-src="https://vincentqin.gitee.io/posts/superglue/freiburg_matches.gif"></p><a id="more"></a><p>注：</p><ol><li>SuperPoint参见另外一篇文章<a href="https://www.vincentqin.tech/posts/superpoint/">《SuperPoint: Self-Supervised Interest Point Detection and Description》</a>，<a href="https://vincentqin.gitee.io/posts/superpoint/" target="_blank" rel="noopener">备用链接</a>。</li><li>后文中反复提到的self-attention/cross-attention，我暂时翻译成自我注意力/交叉注意力。</li><li>本人知识水平有限，如有错误请在评论区指出。当然，没有问题也可刷刷评论。</li></ol><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文提出了一种能够同时进行特征匹配以及滤除外点的网络。其中特征匹配是通过求解可微分最优化转移问题（ optimal transport problem）来解决，损失函数由GNN来构建；本文基于注意力机制提出了一种灵活的内容聚合机制，这使得SuperGlue能够同时感知潜在的3D场景以及进行特征匹配。该算法与传统的，手工设计的特征相比，能够在室内外环境中位姿估计任务中取得最好的结果，该网络能够在GPU上达到实时，预期能够集成到sfm以及slam算法中。</p><p><img alt="superglue_front" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_front.png"></p><p>SuperGlue是一种特征匹配网络，它的输入是2张图像中特征点以及描述子（手工特征或者深度学习特征均可），输出是图像特征之间的匹配关系。</p><p>作者认为学习特征匹配可以被视为找到两簇点的局部分配关系。作者受到了Transformer的启发，同时将self-和cross-attention利用特征点位置以及其视觉外观进行匹配。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="局部特征匹配"><a href="#局部特征匹配" class="headerlink" title="局部特征匹配"></a>局部特征匹配</h3><p>传统的特征可分5步走：1)提取特征点；2)计算描述子；3)最近邻匹配；4)滤除外点；5)求解几何约束；其中滤除外点一步包括点方法有：计算最优次优比，RANSAC，交叉验证以及neighborhood consensus。</p><p>最近的一些工作主要集中在设计特异性更好的稀疏特征上，而它们的匹配算法仍然依赖于NN等策略：在做匹配时并没有考虑特征的结构相似性以及外观相似性。</p><h3 id="图匹配"><a href="#图匹配" class="headerlink" title="图匹配"></a>图匹配</h3><p>这类方法将特征的匹配问题描述成“quadratic assignment problems”，这是一个NP-hard问题，求解这类问题需要复杂不切实际的算子。后来的研究者将这个问题化简成“linear assignment problems”，但仅仅用了一个浅层模型，相比之下SuperGlue利用深度神经网络构建了一种合适的代价进行求解。此处需要说明的是图匹配问题可以认为是一种“<em>optimal transport</em>”问题，<strong>它是一种有效但简单的近似解的广义线性分配，即Sinkhorn算法</strong>。</p><h3 id="深度点云匹配"><a href="#深度点云匹配" class="headerlink" title="深度点云匹配"></a>深度点云匹配</h3><p>点云匹配的目的是通过在元素之间聚集信息来设计置换等价或不变函数。一些算法同等的对待这些元素，还有一些算法主要关注于元素的局部坐标或者特征空间。注意力机制可以通过关注特定的元素和属性来实现全局以及依赖于数据的局部聚合，因而更加全面和灵活。SuperGlue借鉴了这种注意力机制。</p><h2 id="框架以及原理"><a href="#框架以及原理" class="headerlink" title="框架以及原理"></a>框架以及原理</h2><p>特征匹配必须满足的硬性要求是：i)至多有1个匹配点；ii)有些点由于遮挡等原因并没有匹配点。一个成熟的特征匹配模型应该做到：既能够找到特征之间的正确匹配，又可以鉴别错误匹配。</p><p><img alt="superglue_arch" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_arch.png"></p><p>整个框架由两个主要模块组成：注意力GNN以及最优匹配层。其中注意力GNN将特征点以及描述子编码成为一个向量（该向量可以理解为特征匹配向量），随后利用自我注意力以及交叉注意力来回增强（重复$L$次）这个向量$\mathbf{f}$的特征匹配性能；随后进入最优匹配层，通过计算特征匹配向量的内积得到匹配度得分矩阵，然后通过Sinkhorn算法（迭代$T$次）解算出最优特征分配矩阵。 </p><h3 id="公式化"><a href="#公式化" class="headerlink" title="公式化"></a>公式化</h3><p>该部分对特征匹配问题建模。给定两张图片$A,B$，每张图片上都有特征点位置$\mathbf{p}$以及对应的描述子$\mathbf{d}$，所以我们经常用$(\mathbf{p},\mathbf{d})$来表示图像特征。第$i$个特征可以表示为$\mathbf{p}_i:=(x,y,c)$，其中$c$表示特征点提取置信度，$(x,y)$表示特征坐标；描述子可以表示为$\mathbf{d}_i \in \mathbb{R}^{D}$，其中$D$表示特征维度，这里的特征可以是CNN特征，如SuperPoint，或者是传统特征SIFT。假设图像$A,B$分别有$M,N$个特征，可以表示为$\mathcal{A}:=\{1, \ldots, M\}$以及$\mathcal{B}:=\{1, \ldots, N\}$。</p><p><strong>部分分配矩阵</strong>：约束i）和ii）意味着对应关系来自两组关键点之间的部分分配。我们给出一个软分配矩阵$\mathbf{P} \in[0,1]^{M \times N}$，根据上述约束，我们有如下关系：</p><script type="math/tex; mode=display">\mathbf{P} \mathbf{1}_{N} \leq \mathbf{1}_{M} \quad \text { and } \quad \mathbf{P}^{\top} \mathbf{1}_{M} \leq \mathbf{1}_{N}</script><p>那我们设计网络的目标就是解算这个分配矩阵$\mathbf{P}$。</p><h3 id="注意力GNN"><a href="#注意力GNN" class="headerlink" title="注意力GNN"></a>注意力GNN</h3><p>这里有个有意思的说法：特征点的位置以及视觉外观能够提高其特异性。另外一个具有启发性的观点是人类在寻找匹配点过程是具有参考价值的。想一下人类是怎样进行特征匹配的，人类通过来回浏览两个图像试探性筛选匹配关键点，并进行来回检查（如果不是匹配的特征，观察一下周围有没有匹配的更好的点，直到找到匹配点/或没有匹配）。上述过程人们通过主动寻找上下文来增加特征点特异性，这样可以排除一些具有奇异性的匹配。本文的核心就是利用基于注意力机制的GNN实现上述过程，即模拟了人类进行特征匹配。</p><h4 id="特征点Encode"><a href="#特征点Encode" class="headerlink" title="特征点Encode"></a>特征点Encode</h4><p>首先根据上述说法，特征点位置+描述会获得更强的特征匹配特异性，所以这里将特征点的位置以及描述子合并成每个特征点$i$的初始表示$^{(0)} \mathbf{x}_{i}$，</p><script type="math/tex; mode=display">^{(0)} \mathbf{x}_{i}=\mathbf{d}_{i}+\mathbf{M L P}_{\mathrm{enc}}\left(\mathbf{p}_{i}\right)</script><p>其中MLP表示多层感知机（Multilayer Perceptron ，MLP）此处用于对低维特征升维，上式实际上是将视觉外观以及特征点位置进行了耦合，正因如此，这使得该Encode形式使得后续的注意力机制能够充分考虑到特征的外观以及位置相似度。</p><h4 id="多层GNN"><a href="#多层GNN" class="headerlink" title="多层GNN"></a>多层GNN</h4><p>考虑一个单一的完全图，它的节点是图像中每个特征点，这个图包括两种不同的无向边：一种是“Intra-image edges”（self edge）$\mathcal{E}_{\text {self }}$，它连接了来自图像内部特征点；另外一种是“Inter-image edges”（cross edge）$\mathcal{E}_{\text {cross }}$，它连接本图特征点$i$与另外一张图所有特征点（构成了该边）。</p><p>令$^{(\ell)} \mathbf{x}_{i}^{A}$表示为图像$A$上第$i$个元素在第$\ell$层的中间表达形式。信息（message）$\mathbf{m}_{\mathcal{E} \rightarrow i}$是聚合了所有特征点$\{j:(i, j) \in \mathcal{E}\}$之后点结果（它的具体形式后面的Attentional Aggregation会介绍，一句话来说就是将自我注意力以及交叉注意力进行聚合），其中$\mathcal{E} \in \{\mathcal{E}_{\text {self }},\mathcal{E}_{\text {self }}\}$，所以图像$A$中所有特征$i$传递更新的残差信息（residual message？）是：</p><script type="math/tex; mode=display">^{(\ell+1)} \mathbf{x}_{i}^{A}=^{(\ell)} \mathbf{x}_{i}^{A}+\operatorname{MLP}\left(\left[^{(\ell)} \mathbf{x}_{i}^{A} \| \mathbf{m}_{\mathcal{E} \rightarrow i}\right]\right)</script><p>其中$[\cdot | \cdot]$表示串联操作。同样的，图像$B$上所有特征有类似的更新形式。可以看到self 以及cross edges绑在一起并交替进行更新，先self后cross，作者提到共有固定数量的$L$层。</p><p>需要说明的是，这里的self-/cross-attention实际上就是模拟了人类来回浏览匹配的过程，其中self-attention是为了使得特征更加具有匹配特异性，而cross-attention是为了用这些具有特异性的点做图像间特征的相似度比较。</p><h4 id="Attentional-Aggregation"><a href="#Attentional-Aggregation" class="headerlink" title="Attentional Aggregation"></a>Attentional Aggregation</h4><p>文章的亮点之一就是将注意力机制用于特征匹配，这到底是如何实现的呢？作者提到，注意力机制将self以及cross信息聚合得到$\mathbf{m}_{\mathcal{E} \rightarrow i}$。其中self edge利用了self-attention[58]，cross edge利用了cross-attention。类似于数据库检索，我们想要查询$\mathbf{q}_i$基于元素的属性即键$\mathbf{k}_i$，检索到了某些元素的值$\mathbf{v}_j$。</p><script type="math/tex; mode=display">\mathbf{m}_{\mathcal{E} \rightarrow i}=\sum_{j:(i, j) \in \mathcal{E}} \alpha_{i j} \mathbf{v}_{j}</script><p>其中注意力权重${\alpha}_{ij}$是查询与检索到对象键值相似度的$\operatorname{Softmax}$即，$\alpha_{i j}=\operatorname{Softmax}_{j}\left(\mathbf{q}_{i}^{\top} \mathbf{k}_{j}\right)$。</p><p>这里需要解释一下键（key），query以及值（value）。令待查询点特征点$i$位于查询图像$Q$上，所有的源特征点位于图像$S$上，其中$(Q, S) \in\{A, B\}^{2}$，于是我们可以将key，query以及value写成下述形式：</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{q}_{i} &=\mathbf{W}_{1}^{(\ell)} \mathbf{x}_{i}^{Q}+\mathbf{b}_{1} \\\left[\begin{array}{l}\mathbf{k}_{j} \\ \mathbf{v}_{j}\end{array}\right] &=\left[\begin{array}{l}\mathbf{W}_{2} \\ \mathbf{W}_{3}\end{array}\right](\ell) \mathbf{x}_{i}^{S}+\left[\begin{array}{l}\mathbf{b}_{2} \\ \mathbf{b}_{3}\end{array}\right] \end{aligned}</script><p>每一层$\ell$都有其对应的一套投影参数，这些参数被所有的特征点共享。理解一下：此处的$\mathbf{q}_i$对应于待查询图像上某个特征点$i$的一种表示（self-attention映射），$\mathbf{k}_j$以及$\mathbf{v}_j$都是来自于召回的图像特征点$j$的一种表示（映射）；$\alpha_{i j}$表示这两个特征相似度，它是由$\mathbf{q}_i$以及$\mathbf{k}_j$计算得到（在这里体现了cross-attention的思想？），越大就表示这两个特征越相似，然后利用该相似度对$\mathbf{v}_j$加权求和得到$\mathbf{m}_{\mathcal{E} \rightarrow i}$，这就是所谓的<strong>特征聚合</strong>。</p><p>上面提到的这些概念有些难以理解，作者特意对上述过程进行了可视化，self-attention就是一张图像内部的边相连进行聚合，它能够更加关注具有特异性的所有点，且并不仅局限于其邻域位置特征（心心相依，何惧千里，逃…）；cross-attention做的就是匹配那些外观相似的两张图像见的特征。</p><p><img alt="superglue_fig_4" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_4.png"></p><p><img alt="superglue_fig_7" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_7.jpg"></p><p>下图展示了每层self-attention以及across-attention中权重${\alpha_{i j}}$的结果。按照匹配从难到易，文中画出了3个不同的特征点作为演示，绿色特征点（容易），蓝色特征点（中等）以及红色特征点（困难）。对于self-attention，初始时它（某个特征）关联了图像上所有的点（首行），然后逐渐地关注在与该特征相邻近的特征点（尾行）。同样地，cross-attention主要关注去匹配可能的特征点，随着层的增加，它逐渐减少匹配点集直到收敛。绿色特征点在第9层就已经趋近收敛，而红色特征直到最后才能趋紧收敛（匹配）。可以看到无论是self还是cross，它们关注的区域都会随着网络层深度的增加而逐渐缩小。</p><p><img alt="superglue_fig_15" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_15_1.jpg"></p><p>经过了$L$次self/cross-attention后就可以得到注意力GNN的输出，对于图像$A$我们有：</p><script type="math/tex; mode=display">\mathbf{f}_{i}^{A}=\mathbf{W} \cdot^{(L)} \mathbf{x}_{i}^{A}+\mathbf{b}, \quad \forall i \in \mathcal{A}</script><p>我们可以把$\mathbf{f}_{i}^{A}$理解为<strong>匹配描述子</strong>（类比特征描述子），专门为特征匹配服务，对于图像$B$具有类似的形式。</p><h3 id="匹配层（Optimal-matching-layer）"><a href="#匹配层（Optimal-matching-layer）" class="headerlink" title="匹配层（Optimal matching layer）"></a>匹配层（Optimal matching layer）</h3><p>接下来的任务就是去构建软分配矩阵$\mathbf{P}$。对于一般的图匹配流程，这个分配矩阵可以通过计算一个得分矩阵$\mathbf{S} \in \mathbb{R}^{M \times N}$（用来表示一些潜在的匹配）来实现。具体而言，通过最大化总体得分$\sum_{i, j} \mathbf{S}_{i, j} \mathbf{P}_{i, j}$即可得到这个分配矩阵$\mathbf{P}$，其中要注意的是$\mathbf{P}$是有约束的。</p><h4 id="匹配得分预测"><a href="#匹配得分预测" class="headerlink" title="匹配得分预测"></a>匹配得分预测</h4><p>去计算$M\times N$个潜在匹配得分是不可取的，于是作者就用GNN聚合得到的$\mathbf{f}_{i}^{A}$以及$\mathbf{f}_{i}^{B}$计算内积得到得分：</p><script type="math/tex; mode=display">\mathbf{S}_{i, j}=<\mathbf{f}_{i}^{A}, \mathbf{f}_{j}^{B}>, \forall(i, j) \in \mathcal{A} \times \mathcal{B}</script><h4 id="遮挡以及可见性"><a href="#遮挡以及可见性" class="headerlink" title="遮挡以及可见性"></a>遮挡以及可见性</h4><p>类似于SuperPoint在提取特征点时增加了一层dustbin通道，专门为了应对图像中没有特征点情况。本文借鉴了该思想，在得分矩阵$\mathbf{S}$的最后一列/行设置为dustbins可以得到$\overline{\mathbf{S}}$，这样做的作用在于可以滤出错误的匹配点。</p><script type="math/tex; mode=display">\overline{\mathbf{S}}_{i, N+1}=\overline{\mathbf{S}}_{M+1, j}=\overline{\mathbf{S}}_{M+1, N+1}=z \in \mathbb{R}</script><p>图像$A$上的特征点被分配到图像$B$上某个特征匹配或者被分配到dustbin，这就意味着每个dustbin有$N,M$个匹配，因此软分配矩阵有如下约束：</p><script type="math/tex; mode=display">\overline{\mathbf{P}} \mathbf{1}_{N+1}=\mathbf{a} \quad\text {  and } \quad \overline{\mathbf{P}}^{\top} \mathbf{1}_{M+1}=\mathbf{b}</script><p>其中$\mathbf{a}=\left[\begin{array}{ll}\mathbf{1}_{M}^{\top} &amp; N\end{array}\right]^{\top}$，$\mathbf{b}=\left[\begin{array}{ll}\mathbf{1}_{N}^{\top} &amp; M\end{array}\right]^{\top}$。（<font color="red">此处不太理解，最后一维为何为N？</font>）</p><h4 id="Sinkhorn-Algorithm"><a href="#Sinkhorn-Algorithm" class="headerlink" title="Sinkhorn Algorithm"></a>Sinkhorn Algorithm</h4><p>求解最大化总体得分可由“Sinkhorn Algorithm”[52,12]进行求解，此处并不作为重点讲解。</p><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>GNN网络以及最优匹配层都是可微的，这使得反向传播训练成为可能。网络训练使用了一种监督学习的方式，即有了匹配的真值$\mathcal{M}=\{(i, j)\} \subset \mathcal{A} \times \mathcal{B}$（如，由真值相对位姿变换得到的匹配关系），当然也可以获得一些没有匹配的特征点$\mathcal{I} \subseteq \mathcal{A}$以及$ \mathcal{J} \subseteq \mathcal{B}$。当给定真值标签，就可以去最小化分配矩阵$\overline{\mathbf{P}}$ 负对数似然函数：</p><script type="math/tex; mode=display">\begin{aligned} \operatorname{Loss}=&-\sum_{(i, j) \in \mathcal{M}} \log \overline{\mathbf{P}}_{i, j} \\ &-\sum_{i \in \mathcal{I}} \log \overline{\mathbf{P}}_{i, N+1}-\sum_{j \in \mathcal{J}} \log \overline{\mathbf{P}}_{M+1, j} \end{aligned}</script><p>这个监督学习的目标是同时最大化精度以及匹配的召回率，接下来的训练过程略过，直接开始实验阶段的介绍。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>特征匹配的目的是为了解算出两帧之间的相对位姿，所以实验对比的一个指标就是<strong>单应矩阵</strong>估计，另外还有室内外的位姿估计。只能说SuperGlue的效果太好了，直接放结果吧（本来论文7页就写完了，作者放了10页附录大招）。</p><h3 id="单应矩阵估计"><a href="#单应矩阵估计" class="headerlink" title="单应矩阵估计"></a>单应矩阵估计</h3><p>能够获得非常高的匹配召回率（98.3%）同时获得超高的精度，比传统的暴力匹配都好了一大截。</p><p><img alt="superglue_tb_1" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_tb_1.png"></p><h3 id="室内外位姿估计"><a href="#室内外位姿估计" class="headerlink" title="室内外位姿估计"></a>室内外位姿估计</h3><p>下表看来，大基线室内位姿估计也是相当棒，完胜传统算法。</p><p><img alt="superglue_tb_2" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_tb_2.png"></p><p><img alt="superglue_tb_3" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_tb_3.png"></p><h3 id="网络耗时"><a href="#网络耗时" class="headerlink" title="网络耗时"></a>网络耗时</h3><p>接下来放出大家比较关心的网络耗时，下图是在NVIDIA GeForce GTX 1080 GPU跑了500次的结果，512个点69ms（14.5fps），1024个点87ms（11.5fps）。</p><p><img alt="superglue_tb_3" data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_fig_11.png"></p><h3 id="更多匹配结果"><a href="#更多匹配结果" class="headerlink" title="更多匹配结果"></a>更多匹配结果</h3><p>第一列是SuperPoint+暴力匹配结果，第二列是SuperPoint+OAnet（ICCV 2019）结果，第三列是SuperPoint+SuperGlue结果。能看到SuperGlue惊人的特征匹配能力，尤其是在大视角变化时优势明显（红线表示错误匹配，绿线表示正确匹配）。</p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_res_3.jpg"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_res_1.jpg"></p><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_res_2.jpg"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文展示了基于注意力的图神经网络对局部特征匹配的强大功能。 SuperGlue的框架使用两种注意力：（i）自我注意力，可以增强局部描述符的接受力；以及（ii）交叉注意力，可以实现跨图像交流，并受到人类来回观察方式的启发进行匹配图像。文中方法通过解决<strong>最优运输问题</strong>，优雅地处理了特征分配问题以及遮挡点。实验表明，SuperGlue与现有方法相比有了显着改进，可以在极宽的基线室内和室外图像对上进行高精度的相对姿势估计。此外，SuperGlue可以实时运行，并且可以同时使用经典和深度学习特征。</p><p>总而言之，论文提出的可学习的中后端（middle-end）算法以功能强大的神经网络模型替代了手工启发式技术，该模型同时在单个统一体系结构中执行上下文聚合，匹配和过滤外点。作者最后提到：若与深度学习前端结合使用，SuperGlue是迈向端到端深度学习SLAM的重要里程碑。（when combined with a deep front-end, SuperGlue is a major milestone towards end-to-end deep SLAM）</p><p>这真是鼓舞SLAM研究人员的士气！</p><h2 id="附件"><a href="#附件" class="headerlink" title="附件"></a>附件</h2><ul><li><a href="https://github.com/magicleap/SuperGluePretrainedNetwork" target="_blank" rel="noopener">SuperGlue Github地址</a></li><li><a href="http://xxx.itp.ac.cn/pdf/1911.11763v2" target="_blank" rel="noopener">SuperGlue Paper</a></li><li>*<a href="https://image-matching-workshop.github.io/" target="_blank" rel="noopener">Image Matching: Local Features &amp; Beyond CVPR 2020 Workshop</a></li></ul><hr><p><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_01.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_02.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_03.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_04.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_05.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_06.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_07.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_08.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_09.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_10.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_11.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_12.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_13.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_14.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_15.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_16.png"><br><img alt data-src="https://gitee.com/vincentqin/BlogResource-5/raw/master/superglue/superglue_paper/SuperGlue.pdf_page_17.png"></p>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> 特征提取 </tag>
            
            <tag> MagicLeap </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> SuperGlue </tag>
            
            <tag> 笔记 </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SLAM常见问题(五)：Singular Value Decomposition（SVD）分解</title>
      <link href="/posts/slam-common-issues-SVD/"/>
      <url>/posts/slam-common-issues-SVD/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>SVD分解就是一种矩阵拆解术，它能够把<strong>任意</strong>矩阵$A \in \mathbb{R}^{m \times n}$拆解成3个矩阵的乘积形式，即：</p><script type="math/tex; mode=display">A = U \Sigma V^T</script><p>其中，$U \in \mathbb{R}^{m \times m}$，$V \in \mathbb{R}^{n \times n}$都是正交矩阵，即列向量是正交的单位向量，$\Sigma \in \mathbb{R}^{m \times n}$的对角阵（奇异值）。搬运了来自MIT OpenCourseWare的在线课程并放在了B站，讲解得很清晰。</p><a id="more"></a><!-- <div id="dplayer2" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer2"),"loop":true,"video":{"url":"http://45.76.197.98:888/api/public/dl/0hkAga3Z/Singular-Value-Decomposition.mp4"},"danmaku":{"id":"bbe4286bf164ef6w1497f18a7b42ff944e6r4b821","api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> --><iframe src="//player.bilibili.com/player.html?aid=93275447&cid=159253510&page=15" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>刚才说了矩阵$U, \Sigma, V$的形式，视频中还提到了这三个矩阵的物理意义，即SVD分解可以理解为：任意矩阵都可以分解为<strong>(rotation)*(Stretch)*(rotation)</strong>的形式。接下来说明一下这三个矩阵是如何来的。</p><h2 id="计算-A-TA"><a href="#计算-A-TA" class="headerlink" title="计算 $A^TA$"></a>计算 $A^TA$</h2><script type="math/tex; mode=display">A^TA = (U \Sigma V^T)^TU \Sigma V^T = V{\Sigma}^TU^TU \Sigma V^T = V{\Sigma}^T \Sigma V^T</script><p>可见，$V$正是矩阵$A^TA$的特征向量，而${\Sigma}^T \Sigma $为矩阵$A^TA$的特征值。</p><h2 id="计算-AA-T"><a href="#计算-AA-T" class="headerlink" title="计算 $AA^T$"></a>计算 $AA^T$</h2><script type="math/tex; mode=display">AA^T = U \Sigma V^T(U \Sigma V^T)^T = U \Sigma V^TV{\Sigma}^TU^TU \Sigma V^T = U{\Sigma}^T \Sigma U^T</script><p>可见，$U$正是矩阵$AA^T$的特征向量，而${\Sigma}^T \Sigma $为矩阵$A^TA$的特征值。</p><p>所以$U, \Sigma, V$都可以通过上述方式来计算。</p><h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><script type="math/tex; mode=display">\Sigma = \left[    \begin{array}    {cccc|cccc}    {\sigma_{1}} & {0} & {\dots} & {0} & {0} & {0} & {\dots} & {0} \\     {0} & {\sigma_{2}} & {\dots} & {0} & {0} & {0} & {\dots} & {0} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} & {0} & {0} & {\dots} & {0} \\    {0} & {0} & {} & {\sigma_{k}} & {0} & {0} & {\dots} & {0} \\    \hline     {0} & {0} & {\dots} & {0} & {0} & {0} & {\dots} & {0} \\    {\vdots} & {\vdots} & {} & {\vdots} & {\vdots} & {\vdots} & {} & {\vdots} \\    {0} & {0} & {\dots} & {0} & {0} & {0} & {\dots} & {0}    \end{array}\right]_{m \times n}\Rightarrow\left[    \begin{array}    {cccc}    {\sigma_{1}} & {0} & {\dots} & {0} \\     {0} & {\sigma_{2}} & {\dots} & {0}  \\     {\vdots} & {\vdots} & {\ddots} & {\vdots}  \\    {0} & {0} & {} & {\sigma_{k}}    \end{array}\right]_{k \times k}</script><p>其中${\sigma}_1 \geq {\sigma}_2 \geq … {\sigma}_k &gt; 0 $，将$\Sigma$中主对角线为0的部分删去，同样的$U,V$对应的部分删去，SVD分解就变成了下图的形式。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-SVD/svd.png"></p><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><h3 id="数字例子"><a href="#数字例子" class="headerlink" title="数字例子"></a>数字例子</h3><p>有矩阵A，对其进行SVD分解，已知：</p><script type="math/tex; mode=display">A = \left[\begin{matrix}​    1 & 4 & 3 & 5 & 6  \cr ​    2 & 3 & {4} & 5 & 0  \cr ​    7 & 4 & 0 & 9 & 1  \cr  \end{matrix}\right]</script><p>计算$A^TA$以及$AA^T$：</p><script type="math/tex; mode=display">A^TA = \left[\begin{matrix}​    {54} & {38} & {11} & {78} & {13}  \cr ​    {38} & {41} & {24} & {71} & {28}  \cr ​    {11} & {24} & {25} & {35} & {18}  \cr ​    {78} & {71} & {35} & {131} & {39}  \cr ​    {13} & {28} & {18} & {39} & {37} \end{matrix}\right]\\A^TA = \left[\begin{matrix}​    {87} & {51} & {74}   \cr ​    {51} & {54} & {71}   \cr ​    {74} & {71} & {147}   \cr \end{matrix}\right]</script><p>对以上两式做特征值分解得到：</p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">V <span class="built_in">=</span></span><br><span class="line">    <span class="number">0.4269</span>    <span class="number">0.5222</span>    <span class="number">0.1760</span>   -<span class="number">0.5292</span>   -<span class="number">0.4839</span></span><br><span class="line">    <span class="number">0.4087</span>   -<span class="number">0.1757</span>   -<span class="number">0.0655</span>   -<span class="number">0.5258</span>    <span class="number">0.7221</span></span><br><span class="line">    <span class="number">0.2100</span>   -<span class="number">0.4474</span>   -<span class="number">0.7536</span>   -<span class="number">0.1512</span>   -<span class="number">0.4062</span></span><br><span class="line">    <span class="number">0.7389</span>    <span class="number">0.1520</span>   -<span class="number">0.0603</span>    <span class="number">0.6481</span>    <span class="number">0.0853</span></span><br><span class="line">    <span class="number">0.2464</span>   -<span class="number">0.6879</span>    <span class="number">0.6271</span>   -<span class="number">0.0258</span>   -<span class="number">0.2687</span></span><br><span class="line"></span><br><span class="line">U <span class="built_in">=</span></span><br><span class="line">    <span class="number">0.5095</span>    <span class="number">0.7999</span>    <span class="number">0.3171</span></span><br><span class="line">    <span class="number">0.4285</span>    <span class="number">0.0838</span>   -<span class="number">0.8997</span></span><br><span class="line">    <span class="number">0.7462</span>   -<span class="number">0.5942</span>    <span class="number">0.3001</span></span><br></pre></td></tr></table></figure><p>奇异值$\Sigma ^T \Sigma = \text{Diag}(238.2878, 37.3715, 12.3407) \Rightarrow \Sigma = \text{Diag}(15.4366, 6.1132, 3.5129)$</p><p>这与直接调用<code>svd(A)</code>结果是一致的（可能差个正负号）。</p><h3 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h3><p>祭上亲爱的Battle Angel Alita。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-SVD/alita_origin.jpg"></p><p>原始图像尺寸$1440\times 2560 $，我们可以对该图像做SVD分解，然后仅保留奇异值的前10，50，100重构图像，比较重构图像与原始图像的质量差异。可见仅仅保留其前10个奇异值时，图像质量遭到了极大破坏（此时仅保留原始图像信息的58.864%），随着奇异值数量的增多，图像质量也会逐渐提升，可以看到当奇异值个数为100时，基本上已经看不出与原图的差异（此时仅保留原始图像信息的87.37%）。由此，我们实现了图像压缩。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-SVD/alita_svd1.jpg"></p><p>下图是保留的奇异值数量与图像质量的关系图，保留的奇异值越多，图像质量越高，图像压缩效果越不明显；反之，奇异值越少，图像质量越差，图像压缩效果越明显。这只是一种非常简单的图像压缩算法，仅作原理验证使用，在实际中用到的概率不是很大。</p><p><img alt data-src="//www.vincentqin.tech/posts/slam-common-issues-SVD/alita_svd_quality.svg"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% simple test of using SVD decomposistion</span></span><br><span class="line">clear <span class="built_in">all</span>;</span><br><span class="line">close <span class="built_in">all</span>;</span><br><span class="line">clc;</span><br><span class="line"></span><br><span class="line"><span class="comment">% A = [1 4 3 5 6;</span></span><br><span class="line"><span class="comment">%          2 3 4 5 0;</span></span><br><span class="line"><span class="comment">%          7 4 0 9 1]</span></span><br><span class="line"><span class="comment">% A'*A;</span></span><br><span class="line"><span class="comment">% A*A';</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% [V,Dv] = eig(A'*A);</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% lambda = wrev(diag(Dv));</span></span><br><span class="line"><span class="comment">% V = fliplr(V)</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% [U,Du] = eig(A*A');</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% lambda = wrev(diag(Du));</span></span><br><span class="line"><span class="comment">% U = fliplr(U)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% LOADING IMAGE</span></span><br><span class="line">img <span class="built_in">=</span> imread(<span class="string">'alita_origin.png'</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ENERGE <span class="built_in">=</span> <span class="number">0</span>;</span><br><span class="line">for i <span class="built_in">=</span> <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">    [U(:,:,i) D(:,:,i) V(:,:,i)] <span class="built_in">=</span> svd(double(img(:,:,i)))  ;</span><br><span class="line">    ENERGE <span class="built_in">=</span> ENERGE +sum(diag(D(:,:,i)));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"><span class="comment">%% 10</span></span><br><span class="line">DIM <span class="built_in">=</span> <span class="number">10</span>;</span><br><span class="line">ENERGE10 <span class="built_in">=</span> <span class="number">0</span>;</span><br><span class="line">for i <span class="built_in">=</span> <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">    img_recons10(:,:,i) <span class="built_in">=</span> U(:,<span class="number">1</span>:DIM,i)*D(<span class="number">1</span>:DIM,<span class="number">1</span>:DIM,i)*V(:,<span class="number">1</span>:DIM,i)<span class="string">';</span></span><br><span class="line"><span class="string">     ENERGE10 = ENERGE10 +sum(diag(D(1:DIM,1:DIM,i)));</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">% figure;</span></span><br><span class="line"><span class="string">% imshow(mat2gray(img_recons10))</span></span><br><span class="line"><span class="string">% imwrite(mat2gray(img_recons10),'</span>alita_10.png<span class="string">');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">%% 50</span></span><br><span class="line"><span class="string">DIM = 50;</span></span><br><span class="line"><span class="string">ENERGE50 = 0;</span></span><br><span class="line"><span class="string">for i = 1:3</span></span><br><span class="line"><span class="string">    img_recons50(:,:,i) = U(:,1:DIM,i)*D(1:DIM,1:DIM,i)*V(:,1:DIM,i)'</span>;</span><br><span class="line">     ENERGE50 <span class="built_in">=</span> ENERGE50 +sum(diag(D(<span class="number">1</span>:DIM,<span class="number">1</span>:DIM,i)));</span><br><span class="line">end</span><br><span class="line"><span class="comment">% figure;</span></span><br><span class="line"><span class="comment">% imshow(mat2gray(img_recons50))</span></span><br><span class="line"><span class="comment">% imwrite(mat2gray(img_recons50),'alita_50.png');</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 100</span></span><br><span class="line">DIM <span class="built_in">=</span> <span class="number">100</span>;</span><br><span class="line">ENERGE100 <span class="built_in">=</span> <span class="number">0</span>;</span><br><span class="line">for i <span class="built_in">=</span> <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">    img_recons100(:,:,i) <span class="built_in">=</span> U(:,<span class="number">1</span>:DIM,i)*D(<span class="number">1</span>:DIM,<span class="number">1</span>:DIM,i)*V(:,<span class="number">1</span>:DIM,i)<span class="string">';</span></span><br><span class="line"><span class="string">    ENERGE100 = ENERGE100 +sum(diag(D(1:DIM,1:DIM,i)));</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">% figure;</span></span><br><span class="line"><span class="string">% imshow(mat2gray(img_recons100))</span></span><br><span class="line"><span class="string">% imwrite(mat2gray(img_recons100),'</span>alita_100.png<span class="string">');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">figure;</span></span><br><span class="line"><span class="string">set(gcf,'</span>pos<span class="string">',[ 986 414 1274 826])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">FONTSIZE = 15;</span></span><br><span class="line"><span class="string">h(1) = subplot(221);imshow(mat2gray(img)); </span></span><br><span class="line"><span class="string">xlabel('</span>origin Alita<span class="string">');set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">h(2) = subplot(222);imshow(mat2gray(img_recons10));</span></span><br><span class="line"><span class="string">xlabel(['</span>Using <span class="number">10</span> singular values: <span class="string">' num2str(ENERGE10/ENERGE)]);set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">h(3) = subplot(223);imshow(mat2gray(img_recons50));</span></span><br><span class="line"><span class="string">xlabel(['</span>Using <span class="number">50</span> singular values: <span class="string">' num2str(ENERGE50/ENERGE)]);set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">h(4) = subplot(224);imshow(mat2gray(img_recons100));</span></span><br><span class="line"><span class="string">xlabel(['</span>Using <span class="number">100</span> singular values: <span class="string">' num2str(ENERGE100/ENERGE)]);set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string">set(gcf,'</span>color<span class="string">',[1 1 1])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">%% SHOW ENERGY</span></span><br><span class="line"><span class="string">ENERGY_tmp = zeros(size(img,1),1);</span></span><br><span class="line"><span class="string">for DIM_ = 1:size(img,1)</span></span><br><span class="line"><span class="string">   for i = 1:3</span></span><br><span class="line"><span class="string">     ENERGY_tmp(DIM_,1) = ENERGY_tmp(DIM_,1) +sum(diag(D(1:DIM_,1:DIM_,i)));</span></span><br><span class="line"><span class="string">   end</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">figure;</span></span><br><span class="line"><span class="string">FONTSIZE = 30;</span></span><br><span class="line"><span class="string">ratio = ENERGY_tmp/ENERGE;</span></span><br><span class="line"><span class="string">X =   1:size(img,1);</span></span><br><span class="line"><span class="string">plot(X,ratio,'</span>linewidth<span class="string">',5,'</span>color<span class="string">','</span>r<span class="string">');</span></span><br><span class="line"><span class="string">set(gcf,'</span>color<span class="string">',[1 1 1])</span></span><br><span class="line"><span class="string">xlabel('</span>Number of Singular values<span class="string">');</span></span><br><span class="line"><span class="string">ylabel('</span>Image Quality<span class="string">');</span></span><br><span class="line"><span class="string">set(gca,'</span>fontsize<span class="string">',FONTSIZE)</span></span><br><span class="line"><span class="string">set(gcf,'</span>pos<span class="string">',[ 986 414 1274 826])</span></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="http://www-users.math.umn.edu/~lerman/math5467/svd.pdf" target="_blank" rel="noopener">A Singularly Valuable Decomposition The SVD of a Matrix</a></li><li>李宏毅关于SVD的介绍，<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/LA_2018/Lecture/SVD.pdf" target="_blank" rel="noopener">PPT</a>,<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_LA18.html" target="_blank" rel="noopener">课程列表</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> SVD </tag>
            
            <tag> 位姿 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SLAM常见问题(四)：求解ICP，利用SVD分解得到旋转矩阵</title>
      <link href="/posts/slam-common-issues-ICP/"/>
      <url>/posts/slam-common-issues-ICP/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>今天讲一篇关于利用<code>SVD</code>方法求解<code>ICP</code>问题的文献<a href="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-ICP/svd_rot.pdf" target="_blank" rel="noopener">《Least-Squares Rigid Motion Using SVD》</a>，这篇文章非常精彩地推导出将$3D$点对齐问题的解析解，同时总结了求解该问题的统一范式。</p><a id="more"></a><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>已知<script type="math/tex">{\mathcal{P}=\left\{\mathbf{p}_{1}, \mathbf{p}_{2}, \ldots, \mathbf{p}_{n}\right\}}</script>以及<script type="math/tex">{\mathcal{Q}=\left\{\mathbf{q}_{1}, \mathbf{q}_{2}, \ldots, \mathbf{q}_{n}\right\}}</script>是空间中（文中说的更加普适，<script type="math/tex">\mathbf{p}_i , \mathbf{q}_i \in \mathbb{R}^{d}</script>，可以表示$d$维空间）的匹配点集，我们试图找到这样的旋转矩阵$R$和平移向量$\mathbf{t}$最小化如下对齐误差（即<code>ICP</code>问题的形式）：</p><script type="math/tex; mode=display">(R, \mathbf{t})=\underset{R \in S O(d), \mathbf{t} \in \mathbb{R}^{d}}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left\|\left(R \mathbf{p}_{i}+\mathbf{t}\right)-\mathbf{q}_{i}\right\|^{2} \tag{1}</script><p>接下来文章分别推导了平移向量$\mathbf{t}$以及旋转矩阵$R$的解析解。</p><h2 id="计算平移量"><a href="#计算平移量" class="headerlink" title="计算平移量"></a>计算平移量</h2><p>此时假定旋转矩阵$R$是固定的，令<script type="math/tex">F(\mathbf{t}) = \sum_{i=1}^{n} w_{i}\left\|\left(R \mathbf{p}_{i}+\mathbf{t}\right)-\mathbf{q}_{i}\right\|^{2}</script>，我们可以通过$F$对$\mathbf{t}$求导的方式得到平移量的最优解，如下：</p><script type="math/tex; mode=display">\begin{aligned} 0 &=\frac{\partial F}{\partial \mathbf{t}}=\sum_{i=1}^{n} 2 w_{i}\left(R \mathbf{p}_{i}+\mathbf{t}-\mathbf{q}_{i}\right)=\\ &=2 \mathbf{t}\left(\sum_{i=1}^{n} w_{i}\right)+2 R\left(\sum_{i=1}^{n} w_{i} \mathbf{p}_{i}\right)-2 \sum_{i=1}^{n} w_{i} \mathbf{q}_{i}  \end{aligned} \tag{2}</script><p>令：</p><script type="math/tex; mode=display">\overline{\mathbf{p}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{p}_{i}}{\sum_{i=1}^{n} w_{i}},  \overline{\mathbf{q}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{q}_{i}}{\sum_{i=1}^{n} w_{i}} \tag{3}</script><p>于是我们得到$\mathbf{t}$的解：</p><script type="math/tex; mode=display">\mathbf{t} = \overline{\mathbf{q}} - R\overline{\mathbf{p}} \tag{4}</script><p>从上式看出最优的平移量$\mathbf{t}$将$\mathcal{P}$点集的加权中心映射到了$\mathcal{Q}$点集的中心。接下来将上式带入优化方程，得：</p><script type="math/tex; mode=display">\begin{aligned}\sum_{i=1}^{n} w_{i}\left\|\left(R \mathbf{p}_{i}+\mathbf{t}\right)-\mathbf{q}_{i}\right\|^{2} &= \sum_{i=1}^{n} w_{i}\left\| R \mathbf{p}_{i}+ \overline{\mathbf{q}} - R\overline{\mathbf{p}} -\mathbf{q}_{i}\right\|^{2}  \\ &= \sum_{i=1}^{n} w_{i}\left\|R (\mathbf{p}_{i} -\overline{\mathbf{p}}) - (\mathbf{q}_{i} - \overline{\mathbf{q}} ) \right\|^{2}\end{aligned} \tag{5}</script><p>由此我们将原问题转换成了无平移量的优化问题，令：</p><script type="math/tex; mode=display">\mathbf{x}_i := \mathbf{p}_{i} -\overline{\mathbf{p}}，\mathbf{y}_i := \mathbf{q}_{i} -\overline{\mathbf{q}}，\tag{6}</script><p>我们把问题简写成如下形式：</p><script type="math/tex; mode=display">R = \underset{R \in S O(d)}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2} \tag{7}</script><h2 id="计算旋转量"><a href="#计算旋转量" class="headerlink" title="计算旋转量"></a>计算旋转量</h2><p>简化上式：</p><script type="math/tex; mode=display">\begin{aligned}\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2} &= \left( R\mathbf{x}_i - \mathbf{y}_i\right)^T\left( R\mathbf{x}_i - \mathbf{y}_i\right)  = \left( \mathbf{x}_i^TR^T - \mathbf{y}_i^T\right)\left( R\mathbf{x}_i - \mathbf{y}_i\right)  \\&= \mathbf{x}_i^TR^TR\mathbf{x}_i - \mathbf{x}_i^TR^T\mathbf{y}_i - \mathbf{y}_i^TR\mathbf{x}_i +\mathbf{y}_i^T\mathbf{y}_i \end{aligned}\tag{8}</script><p>又因为旋转矩阵的正交性：$R^TR=I$；另外$ \mathbf{x}_i^TR^T\mathbf{y}_i$是标量：$\mathbf{x}_i$维度为$1 \times d$，$R^T$维度为$d \times d$，$\mathbf{y}_i$维度为$d \times 1$。于是有下式：</p><script type="math/tex; mode=display"> \mathbf{x}_i^TR^T\mathbf{y}_i = (\mathbf{x}_i^TR^T\mathbf{y}_i)^T = \mathbf{y}_i^TR\mathbf{x}_i \tag{9}</script><p>得：</p><script type="math/tex; mode=display">\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2} = \mathbf{x}_i^T\mathbf{x}_i -  2\mathbf{y}_i^TR\mathbf{x}_i +\mathbf{y}_i^T\mathbf{y}_i  \tag{10}</script><p>将整理好的上式带入简化后的$R$优化问题，得：</p><script type="math/tex; mode=display">\begin{aligned} & \underset{R \in S O(d)}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left\|R \mathbf{x}_{i}-\mathbf{y}_{i}\right\|^{2}=\underset{R \in S O(d)}{\operatorname{argmin}} \sum_{i=1}^{n} w_{i}\left(\mathbf{x}_{i}^{\top} \mathbf{x}_{i}-2 \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}+\mathbf{y}_{i}^{\top} \mathbf{y}_{i}\right)=\\=& \underset{R \in S O(d)}{\operatorname{argmin}}\left(\sum_{i=1}^{n} w_{i} \mathbf{x}_{i}^{\top} \mathbf{x}_{i}-2 \sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}+\sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} \mathbf{y}_{i}\right)=\\=& \operatorname{argmin}_{R \in S O(d)}\left(-2 \sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}\right) \end{aligned}\tag{11}</script><p>接下来将要利用到如下关于迹的技巧:</p><script type="math/tex; mode=display">\begin{aligned}\left[\begin{array}{cccc}{w_1} \\ {} & {w_1} & {} &{} \\ {} & {} & {\ddots} & {}\\ {} & {} & {} & {w_n}\end{array}\right]\left[\begin{array}{ccc}{—}& {\mathbf{y}_1^T}&{—}   \\ {—}& {\mathbf{y}_2^T}&{—}   \\ {—}  & {\vdots} & {—}\\ {—}& {\mathbf{y}_n^T}&{—}\end{array}\right]\left[\begin{array}{ccc}{}& {} &{}   \\ {}& {R} &{}   \\ {}& {} &{}   \\ \end{array}\right]\left[\begin{array}{cccc}{|}& {|} &{|} &{|}  \\ {\mathbf{x}_1}& {\mathbf{x}_2} &{\dots} &{\mathbf{x}_n}  \\ {|}& {|} &{|} &{|}  \\ \end{array}\right]  \\=\left[\begin{array}{ccc}{—}& {w_1\mathbf{y}_1^T}&{—}   \\ {—}& {w_2\mathbf{y}_2^T}&{—}   \\ {—}  & {\vdots} & {—}\\ {—}& {w_n\mathbf{y}_n^T}&{—}\end{array}\right]\left[\begin{array}{cccc}{|}& {|} &{|} &{|}  \\ {R\mathbf{x}_1}& {R\mathbf{x}_2} &{\dots} &{R\mathbf{x}_n}  \\ {|}& {|} &{|} &{|}  \\ \end{array}\right] \\=\left[\begin{array}{cccc}{w_1\mathbf{y}_1^TR\mathbf{x}_1}& {} &{} &{*}  \\ {}& {w_2\mathbf{y}_2^TR\mathbf{x}_2} &{} &{}  \\ {}& {} &{\ddots} &{}  \\ {*}& {} &{} &{w_n\mathbf{y}_n^TR\mathbf{x}_n} \\ \end{array}\right]\end{aligned}</script><p>上式就是对<script type="math/tex">\sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i} =  \operatorname{tr}\left( WY^TRX\right)</script>的完美解释。</p><p>利用上式，式$(11)$可以整理得：</p><script type="math/tex; mode=display">\begin{aligned}\underset{R \in S O(d)}{\operatorname{argmin}}\left(-2 \sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}\right) &= \underset{R \in S O(d)}{\operatorname{argmax}}\left(\sum_{i=1}^{n} w_{i} \mathbf{y}_{i}^{\top} R \mathbf{x}_{i}\right) \\&= \underset{R \in S O(d)}{\operatorname{argmax}} \operatorname{tr}\left( WY^TRX\right)\end{aligned}\tag{12}</script><p>这里说明一下维度：$W = diag(w_1,w_2,…,w_n)$维度为$n \times n$，$Y^T$维度为$n \times d$，$R$维度为$d \times d$，$X$维度为$d \times n$。</p><p>接下来回顾一下迹的性质：$\operatorname{tr}(AB) = \operatorname{tr}(BA)$，因此有下式：</p><script type="math/tex; mode=display">\operatorname{tr}\left( WY^TRX\right) = \operatorname{tr}\left( (WY^T)(RX)\right) =\operatorname{tr}\left( RXWY^T\right) \tag{13}</script><p>令$d\times d$的“covariance”矩阵$S = XWY^T$，求$S$的<code>SVD</code>分解：</p><script type="math/tex; mode=display">S= U\Sigma V^T.\tag{14}</script><p>于是式$(13)$变为：</p><script type="math/tex; mode=display">\operatorname{tr}\left( WY^TRX\right) =\operatorname{tr}\left( RS\right) =\operatorname{tr}\left( RU\Sigma V^T\right)=\operatorname{tr}\left( \Sigma V^TRU\right) \tag{15}</script><p>由于$V,T,R$均为正交矩阵，因此$M = V^TRU$也是正交阵，也就是说$M$的列向量$\mathbf{m}_j$是互相正交的单位向量，即$\mathbf{m}_j^T\mathbf{m}_j=1$，于是：</p><script type="math/tex; mode=display">1=\mathbf{m}_{j}^{\top} \mathbf{m}_{j}=\sum_{i=1}^{d} m_{i j}^{2} \Rightarrow m_{i j}^{2} \leq 1 \Rightarrow\left|m_{i j}\right| \leq 1 \tag{16}</script><p>由于<code>SVD</code>分解的性质可知$\sigma$的元素均为非负数：${\sigma}_1,{\sigma}_2,{\sigma}_d \geq 0$，于是式$(18)$变为如下形式：</p><script type="math/tex; mode=display">\operatorname{tr}(\Sigma M)=\left(\begin{array}{ccccc}{\sigma_{1}} & {} & {} & {} & {} \\ {} & {\sigma_{2}} & {} & {} & {} \\ {} & {} & {\ddots} & {} & {} \\ {} & {} & {} & {} & {\sigma_{d}}\end{array}\right)\left(\begin{array}{cccc}{m_{11}} & {m_{12}} & {\dots} & {m_{1 d}} \\ {m_{21}} & {m_{22}} & {\dots} & {m_{2 d}} \\ {\vdots} & {\vdots} & {\vdots} & {\vdots} \\ {m_{d 1}} & {m_{d 2}} & {\dots} & {m_{d d}}\end{array}\right)=\sum_{i=1}^{d} \sigma_{i} m_{i i} \leq \sum_{i=1}^{d} \sigma_{i} \tag{17}</script><p>可见，当迹最大时$m_{ii} = 1 $，又由于$M$是正交阵，这使得$M$为单位阵！</p><script type="math/tex; mode=display">I = M = V^TRU \Rightarrow R = VU^T \tag{18}</script><p>看到没，R的解析解竟然如此简单，并且与<code>SVD</code>分解产生了联系，让人感觉到了数学的美妙。不过到这里还没完，后面作者进行了一步方向矫正，大意是这样的：利用公式$(18)$得到的矩阵并不一定是一个旋转矩阵，也可能为<code>反射矩阵</code>，此时可以通过验证$VU^T$的行列式来判断到底是旋转（行列式 = 1）还是反射（行列式 = -1）。但我们要求的是旋转矩阵，这时需要对公式$(18)$进行一步处理。</p><p>假设$\operatorname{det}(VU^T) = -1$，则限制$R$为旋转就意味着$M = V^TRU $为<code>反射矩阵</code>， 于是我们试图找到一个<code>反射矩阵</code>$M$最大化下式：</p><script type="math/tex; mode=display">\operatorname{tr}(\Sigma M) = {\sigma}_1 m_{11} + {\sigma}_2 m_{22} +...+ {\sigma}_d m_{dd} := f(m_{11},m_{11},...,m_{dd})  \tag{19}</script><p>即$f$是以<script type="math/tex">m_{11},m_{11},...,m_{dd}</script>为变量的线性函数，由于<script type="math/tex">m_{ii} \in \left[ -1,1\right]</script>，其极大值肯定在其定义域的边界处。于是当<script type="math/tex">{\forall} i, m_{ii} = 1</script>时，$f$取得极大值，但是此时的$R$为<code>反射矩阵</code>，所以并不能这样取值。然后我们看第二个极大值点$(1,1,…,-1)$，有：</p><script type="math/tex; mode=display">f = \operatorname{tr}(\Sigma M) = {\sigma}_1 + {\sigma}_2+...+ {\sigma}_{d-1} -  {\sigma}_d \tag{20}</script><p>这个值大于任何其它的自变量取值$(\pm 1,\pm 1,…,\pm 1)$的组合（除了$( 1, 1,…, 1)$），因为奇异值是经过排序的，${\sigma}_d$是最小的一个奇异值。</p><p>综上，为了将解转换为旋转矩阵要进行如下处理：</p><script type="math/tex; mode=display">R=V\left(\begin{array}{cccc}{1} \\ {} & {\ddots} & {} &{} \\ {} & {} & 1 & {}\\ {} & {} & {} & {\operatorname{det}\left(V U^{\top}\right)}\end{array}\right) U^{\top} \tag{21}</script><h2 id="可以总结的套路"><a href="#可以总结的套路" class="headerlink" title="可以总结的套路"></a>可以总结的套路</h2><p>为了得到<code>ICP</code>问题的最优解，我们可以采取如下套路：</p><p><strong>step1</strong>. 计算两组匹配点的加权中心：</p><script type="math/tex; mode=display">\overline{\mathbf{p}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{p}_{i}}{\sum_{i=1}^{n} w_{i}},  \overline{\mathbf{q}}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{q}_{i}}{\sum_{i=1}^{n} w_{i}}</script><p><strong>step2</strong>. 得到去中心化的点集：</p><script type="math/tex; mode=display">\mathbf{x}_i := \mathbf{p}_{i} -\overline{\mathbf{p}}，\mathbf{y}_i := \mathbf{q}_{i} -\overline{\mathbf{q}}, i = 1,2...n</script><p><strong>step3</strong>. 计算$d \times d$的covariance矩阵：</p><script type="math/tex; mode=display">S = XWY^T</script><p>其中，$X,Y$为$d \times n$的矩阵，$\mathbf{x}_i,\mathbf{y}_i$分别是它们的列元素，另外$W = diag(w_1,w_2,…,w_n)$。</p><p><strong>step4</strong>. 对$S$进行<code>SVD</code>分解$S = U\Sigma V^T$，得到旋转矩阵：</p><script type="math/tex; mode=display">R=V\left(\begin{array}{cccc}{1} \\ {} & {\ddots} & {} &{} \\ {} & {} & 1 & {}\\ {} & {} & {} & {\operatorname{det}\left(V U^{\top}\right)}\end{array}\right) U^{\top}</script><p><strong>step5</strong>. 计算平移量：</p><script type="math/tex; mode=display">\mathbf{t} = \overline{\mathbf{q}} - R\overline{\mathbf{p}}</script>]]></content>
      
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> 位姿 </tag>
            
            <tag> ICP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SLAM常见问题(三)：PNP</title>
      <link href="/posts/slam-common-issues-PNP/"/>
      <url>/posts/slam-common-issues-PNP/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><code>PNP</code>即“Perspective-N-Points”，是求解 3D 到 2D 点对运动的方法。它描述了当我们知道n个3D空间点以及它们在图像上的位置时，如何估计相机所在的位姿。PnP 问题有很多种求解方法，例如用三对点估计位姿的 <code>P3P</code>（通常需要额外一个点进行验证结果），直接线性变换（<code>DLT</code>），<code>EPnP</code>（Efficient PnP，已知内参时用），<code>UPnP</code>（内参未知时用） 等等）。此外，还能用非线性优化的方式，构建最小二乘问题并迭代求解，也就是万金油式的 <code>Bundle Adjustment</code>。</p><a id="more"></a><h2 id="P3P"><a href="#P3P" class="headerlink" title="P3P"></a>P3P</h2><p>已知：$3D-2D$匹配点，$3D$点的<strong>世界坐标</strong>记为$A, B, C$，图像上的2D点记为$a, b, c$。</p><p>未知：<strong>相机系下3D点的坐标是未知的</strong>，即$OA,OB,OC$，一旦$ 3D$ 点在相机坐标系下的坐标能够算出，我们就得到了$3D-3D$的对应点，把<code>PnP</code>问题转换为了<code>ICP</code>问题。</p><p>我们的目标就是通过<strong>纯几何的方法</strong>求出上述未知量，过程如下。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-3/slam-common-issues-PNP/p3p.png"></p><p>由于余弦定理可知：</p><script type="math/tex; mode=display">\begin{array}{l}{O A^{2}+O B^{2}-2 O A \cdot O B \cdot \cos \langle a, b\rangle= A B^{2}} \\ {O B^{2}+O C^{2}-2 O B \cdot O C \cdot \cos \langle b, c\rangle= B C^{2}} \\ {O A^{2}+O C^{2}-2 O A \cdot O C \cdot \cos \langle a, c\rangle= A C^{2}}\end{array}</script><p>对上面三式全体除以$OC^{2}$，记$x=O A / O C, y=O B / O C$，得：</p><script type="math/tex; mode=display">\begin{array}{l}{x^{2}+y^{2}-2 x y \cos \langle a, b\rangle= A B^{2} / O C^{2}} \\ {y^{2}+1^{2}-2 y \cos \langle b, c\rangle= B C^{2} / O C^{2}} \\ {x^{2}+1^{2}-2 x \cos \langle a, c\rangle= A C^{2} / O C^{2}}\end{array}</script><p>记$v=A B^{2} / O C^{2}, u v=B C^{2} / O C^{2}, w v=A C^{2} / O C^{2}$，得：</p><script type="math/tex; mode=display">\begin{array}{l}{x^{2}+y^{2}-2 x y \cos \langle a, b\rangle- v=0} \\ {y^{2}+1^{2}-2 y \cos \langle b, c\rangle- u v=0} \\ {x^{2}+1^{2}-2 x \cos \langle a, c\rangle- w v=0}\end{array}</script><p>将第一个式子中$v = x^{2}+y^{2}-2 x y \cos \langle a, b\rangle$带入后面两个式子中，得：</p><script type="math/tex; mode=display">\begin{array}{l}{(1-u) y^{2}-u x^{2}-\cos \langle b, c\rangle y+2 u x y \cos \langle a, b\rangle+ 1=0} \\ {(1-w) x^{2}-w y^{2}-\cos \langle a, c\rangle x+2 w x y \cos \langle a, b\rangle+ 1=0}\end{array}</script><p>上式中几个余弦角度$\cos \langle a, b\rangle, \cos \langle b, c\rangle, \cos \langle a, c\rangle$是已知的，$u=B C^{2} / A B^{2}, w=A C^{2} / A B^{2}$也是已知的，所以未知量仅有$x,y$，解析地求解该方程组是一个复杂的过程，需要用<strong><a href="https://zh.wikipedia.org/wiki/%E5%90%B4%E6%B6%88%E5%85%83%E6%B3%95" target="_blank" rel="noopener">吴消元法</a></strong>。这样就可以求得$x,y$，然后带入$v = x^{2}+y^{2}-2 x y \cos \langle a, b\rangle$求解$v$，即可得到$OC$，进而得到$OB,OA$。该方程最多可能得到四个解，但我们可以用第4个验证点来计算最可能的解，得到$ A, B, C$ 在相机坐标系下的$3D$坐标。然后，根据$ 3D-3D $的点对，计算相机的运动 $R,t$，此处可参考文献<a href="https://igl.ethz.ch/projects/ARAP/svd_rot.pdf" target="_blank" rel="noopener">Least-Squares Rigid Motion Using SVD</a></p><h2 id="EPnP"><a href="#EPnP" class="headerlink" title="EPnP"></a>EPnP</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p><code>EPnP</code>即Efficient PnP，参考文献 <a href="https://icwww.epfl.ch/~lepetit/papers/lepetit_ijcv08.pdf" target="_blank" rel="noopener">EPnP: An Accurate O(n) Solution to the PnP Problem</a>。</p><p>问题描述如PnP，更加具体的，我们已知一组特征点，对于每个特征点$i$，我们有如下信息：</p><ul><li><p>特征点 $i$ 在世界坐标系的坐标<script type="math/tex">P_{i}^{w}=\left[\begin{array}{c}{x_{i}^{w}} \\ {y_{i}^{w}} \\ {z_{i}^{w}}\end{array}\right]</script></p></li><li><p>特征点在成像平面上的坐标<script type="math/tex">p_{i}=\left[\begin{array}{l}{u_{i}} \\ {v_{i}}\end{array}\right]</script></p></li><li>已知相机内参$K$</li></ul><p>求：世界坐标系到相机系的变换矩阵<script type="math/tex">T_{c w}=\left[\begin{array}{cc}{R_{c w}} & {t} \\ {0} & {1}\end{array}\right]</script></p><h3 id="算法假设"><a href="#算法假设" class="headerlink" title="算法假设"></a>算法假设</h3><p><code>EPnP</code>的思想是无论世界系还是相机系下的$3D$点都可以由<strong>4个控制点线性组合</strong>，记：</p><ul><li>世界系下4个控制点表示为:$\mathbf{c}_{j}^{w}, j=1, \cdots, 4$</li><li>相机系下4个控制点表示为:$\mathbf{c}_{j}^{c}, j=1, \cdots, 4$</li></ul><p>EPnP算法将参考点的坐标表示为控制点坐标的加权和：</p><script type="math/tex; mode=display">\mathbf{p}_{i}^{w}=\sum_{j=1}^{4} \alpha_{i j} \mathbf{c}_{j}^{w}, \text { with } \sum_{j=1}^{4} \alpha_{i j}=1</script><p>其中<script type="math/tex">\alpha_{i, j}, j=1, \cdots, 4</script>是加权系数，一旦虚拟控制点确定后，且满足4个控制点不共面的前提，<script type="math/tex">\alpha_{i, j}</script>是唯一的。</p><h3 id="控制点的存在性"><a href="#控制点的存在性" class="headerlink" title="控制点的存在性"></a>控制点的存在性</h3><p>现在讨论控制点的存在性，上式可以写成：</p><script type="math/tex; mode=display">\left[\begin{array}{c}{p_{i}^{w}} \\ {1}\end{array}\right]=\left[\begin{array}{cccc}{C_{1}^{w}} & {C_{2}^{w}} & {C_{3}^{w}} & {C_{4}^{w}} \\ {1} & {1} & {1} & {1}\end{array}\right] \alpha_{i} \stackrel{令}{=} C \alpha_{i}</script><p>可见只要$C$非奇异，就一定可以找到满足条件的$\alpha_{i} $，即：</p><script type="math/tex; mode=display">\left[\begin{array}{l}{\alpha_{i 1}} \\ {\alpha_{i 2}} \\ {\alpha_{i 3}} \\ {\alpha_{i 4}}\end{array}\right]=C^{-1}\left[\begin{array}{c}{\mathbf{p}_{i}^{w}} \\ {1}\end{array}\right]</script><p>接下来，我们讨论相机坐标系下，控制点和参考$3D$点之间的关系：</p><script type="math/tex; mode=display">p_{i}^{c}=R_{c w} p_{i}^{w}+t=R_{c w}\left(\sum_{j=1}^{4} \alpha_{i j} c_{i}^{w}\right)+t</script><p>由于<script type="math/tex">\sum_{j=1}^{4} \alpha_{i j}=1$，因此$t=\sum_{j=1}^{4} \alpha_{i j} t</script>,带入上式，得：</p><script type="math/tex; mode=display">p_{i}^{c}=\sum_{j=1}^{4} \alpha_{i j}\left(R_{c w} c_{i}^{w}\right)+t )=\sum_{j=1}^{4} \alpha_{i j} c_{i}^{c}</script><p>可见系数<script type="math/tex">\alpha_{i}</script>具有不变性，如果我们能够求出控制点在相机坐标系中的坐标<script type="math/tex">c_{1}^{c}, c_{2}^{c},c_{3}^{c},c_{4}^{c}</script>，那么对于任意一个3D点k，我们可以求得其在相机系下的坐标：<script type="math/tex">p_{k}^{c}=\sum_{j=1}^{4} \alpha_{k j} c_{i}^{c}</script>，这就变成了如P3P同样的问题了，即求解<code>3D-3D</code>位姿估计问题。</p><h3 id="如何选择控制点"><a href="#如何选择控制点" class="headerlink" title="如何选择控制点"></a>如何选择控制点</h3><p>记世界系下所有3D点集为<script type="math/tex">\left\{\mathbf{p}_{i}^{w}, i=1, \cdots, n\right\}</script>,第一个控制点是所有3D点的重心:</p><script type="math/tex; mode=display">\mathbf{c}_{1}^{w}=\frac{1}{n} \sum_{i=1}^{n} \mathbf{p}_{i}^{w}</script><p>对所有3D点去中心化，这些点罗列成矩阵形式：</p><script type="math/tex; mode=display">A=\left[\begin{array}{c}{\mathbf{p}_{1}^{w^{T}}-\mathbf{c}_{1}^{w^{T}}} \\ {\cdots} \\ {\mathbf{p}_{n}^{w^{T}}-\mathbf{c}_{1}^{w^{T}}}\end{array}\right]</script><p>对$A^TA$进行特征值分解（注意此时并非对A进行<code>SVD</code>分解，是为了减低时间复杂度，<code>SVD</code>分解的复杂度为$SO(3)$），其特征值为<script type="math/tex">\lambda_{c, i}, i=1,2,3</script>，对应的特征向量为<script type="math/tex">\mathbf{v}_{c, i}, i=1,2,3</script>，则剩余的3个控制点表示为如下公式：</p><script type="math/tex; mode=display">\mathbf{c}_{j}^{w}=\mathbf{c}_{1}^{w}+\lambda_{c, j-1}^{\frac{1}{2}} \mathbf{v}_{c, j-1}, j=2,3,4</script><h3 id="求解控制点在相机系下的坐标"><a href="#求解控制点在相机系下的坐标" class="headerlink" title="求解控制点在相机系下的坐标"></a>求解控制点在相机系下的坐标</h3><p>记<script type="math/tex">\left\{\mathbf{u}_{i}\right\}_{i=1, \cdots, n}</script>为相机下$3D$点<script type="math/tex">\left\{\mathbf{p}^c_{i}\right\}_{i=1, \cdots, n}</script>的图像坐标，则：</p><script type="math/tex; mode=display">\forall i, \quad w_{i}\left[\begin{array}{c}{\mathbf{u}_{i}} \\ {1}\end{array}\right]=K \mathbf{p}_{i}^{c}=K \sum_{j=1}^{4} \alpha_{i j} \mathbf{c}_{j}^{c}</script><p>其中<script type="math/tex">w_i</script>是尺度因子，将控制点<script type="math/tex">\mathbf{c}_{j}^{c}=\left[x_{j}^{c}, y_{j}^{c}, z_{j}^{c}\right]^{T}</script>带入上式，得：</p><script type="math/tex; mode=display">\forall i, \quad w_{i}\left[\begin{array}{c}{u_{i}} \\ {v_{i}} \\ {1}\end{array}\right]=\left[\begin{array}{ccc}{f_{u}} & {0} & {u_{c}} \\ {0} & {f_{v}} & {v_{c}} \\ {0} & {0} & {1}\end{array}\right] \sum_{j=1}^{4} \alpha_{i j}\left[\begin{array}{c}{x_{j}^{c}} \\ {y_{j}^{c}} \\ {z_{j}^{c}}\end{array}\right]</script><p>上式可以得到两个线性方程：</p><script type="math/tex; mode=display">\begin{array}{l}{\sum_{j=1}^{4} \alpha_{i j} f_{u} x_{j}^{c}+\alpha_{i j}\left(u_{c}-u_{i}\right) z_{j}^{c}=0} \\ {\sum_{j=1}^{4} \alpha_{i j} f_{v} y_{j}^{c}+\alpha_{i j}\left(v_{c}-v_{j}\right) z_{j}^{c}=0}\end{array}</script><p>把这N个点的约束罗列在一起，我们就可以得到如下矩阵：</p><script type="math/tex; mode=display">M\mathbf{x} = \mathbf{0}</script><p>其中<script type="math/tex">\mathbf{x}=\left[\mathbf{c}_{1}^{c \top}, \mathbf{c}_{2}^{c \top}, \mathbf{c}_{3}^{c \top}, \mathbf{c}_{4}^{c \top}\right]^{\top}</script>为<strong>12</strong>维向量，<script type="math/tex">\mathbf{M}</script>维度<script type="math/tex">2n\times 12</script>，如下形式:</p><p><img width="75%" data-src="https://cdn.mathpix.com/snip/images/XRV8zRc_TojEnL-nnDpP-eSDrWxXmwLRJ0zOt5FmAzg.original.fullsize.png"></p><h3 id="未完待续…"><a href="#未完待续…" class="headerlink" title="未完待续…"></a>未完待续…</h3><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://blog.csdn.net/jessecw79/article/details/82945918" target="_blank" rel="noopener">深入EPnP算法</a></li><li><a href="https://zhuanlan.zhihu.com/p/46695068" target="_blank" rel="noopener">3d-2d位姿估计之EPnP算法</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> 位姿 </tag>
            
            <tag> PNP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SLAM常见问题(二)：重定位Relocalisation</title>
      <link href="/posts/slam-common-issues-relocalisation/"/>
      <url>/posts/slam-common-issues-relocalisation/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>可以说整个重定位就是一个精心设计的解算当前帧位姿的模块，秉持着不抛弃不放弃的精神，ORB-SLAM的作者简直把特征匹配压榨到了极致，仿佛在说“小伙子你有很多匹配点的，不要放弃，我们优化一下位姿再找找匹配点呗”。</p><a id="more"></a><p>原理如下流程图：</p><p><img alt="重定位" data-src="//www.vincentqin.tech/posts/slam-common-issues-relocalisation/relocalisation.svg"></p><p>代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bool</span> Tracking::Relocalization()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Compute Bag of Words Vector</span></span><br><span class="line">    <span class="comment">// 步骤1：计算当前帧特征点的Bow映射，能够得到当前帧的词袋向量以及featureVector</span></span><br><span class="line">    <span class="comment">// 可用于SearchByBoW寻找匹配特征点</span></span><br><span class="line">    mCurrentFrame.ComputeBoW();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Relocalization is performed when tracking is lost</span></span><br><span class="line">    <span class="comment">// Track Lost: Query KeyFrame Database for keyframe candidates for relocalisation</span></span><br><span class="line">    <span class="comment">// 步骤2：找到与当前帧相似的候选关键帧，</span></span><br><span class="line">    <span class="comment">// 这里会通过查询关键帧数据库进行快速查找与当前帧相似的候选重定位帧vpCandidateKFs</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;KeyFrame*&gt; vpCandidateKFs = mpKeyFrameDB-&gt;DetectRelocalizationCandidates(&amp;mCurrentFrame);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(vpCandidateKFs.empty())</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> nKFs = vpCandidateKFs.size();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We perform first an ORB matching with each candidate</span></span><br><span class="line">    <span class="comment">// If enough matches are found we setup a PnP solver</span></span><br><span class="line">    <span class="function">ORBmatcher <span class="title">matcher</span><span class="params">(<span class="number">0.75</span>,<span class="literal">true</span>)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">vector</span>&lt;PnPsolver*&gt; vpPnPsolvers;</span><br><span class="line">    vpPnPsolvers.resize(nKFs);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;MapPoint*&gt; &gt; vvpMapPointMatches;</span><br><span class="line">    vvpMapPointMatches.resize(nKFs);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; vbDiscarded;</span><br><span class="line">    vbDiscarded.resize(nKFs);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> nCandidates=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nKFs; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        KeyFrame* pKF = vpCandidateKFs[i];</span><br><span class="line">        <span class="keyword">if</span>(pKF-&gt;isBad())</span><br><span class="line">            vbDiscarded[i] = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 步骤3：通过BoW进行匹配</span></span><br><span class="line">            <span class="comment">// 利用SearchByBoW查找当前帧与关键帧的匹配点vvpMapPointMatches</span></span><br><span class="line">            <span class="keyword">int</span> nmatches = matcher.SearchByBoW(pKF,mCurrentFrame,vvpMapPointMatches[i]);</span><br><span class="line">            <span class="comment">// 如果匹配点数小于15个点，跳过</span></span><br><span class="line">            <span class="keyword">if</span>(nmatches&lt;<span class="number">15</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                vbDiscarded[i] = <span class="literal">true</span>;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 如果匹配点数大于15个点，建立当前帧与关键帧之间的PNP求解器；</span></span><br><span class="line">            <span class="comment">// 仅仅如建立这个求解器，还未求解</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 初始化PnPsolver</span></span><br><span class="line">                PnPsolver* pSolver = <span class="keyword">new</span> PnPsolver(mCurrentFrame,vvpMapPointMatches[i]);</span><br><span class="line">                pSolver-&gt;SetRansacParameters(<span class="number">0.99</span>,<span class="number">10</span>,<span class="number">300</span>,<span class="number">4</span>,<span class="number">0.5</span>,<span class="number">5.991</span>);</span><br><span class="line">                vpPnPsolvers[i] = pSolver;</span><br><span class="line">                nCandidates++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Alternatively perform some iterations of P4P RANSAC</span></span><br><span class="line">    <span class="comment">// Until we found a camera pose supported by enough inliers</span></span><br><span class="line">    <span class="keyword">bool</span> bMatch = <span class="literal">false</span>;</span><br><span class="line">    <span class="function">ORBmatcher <span class="title">matcher2</span><span class="params">(<span class="number">0.9</span>,<span class="literal">true</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果候选关键帧数大于0且没有重定位成功</span></span><br><span class="line">    <span class="keyword">while</span>(nCandidates&gt;<span class="number">0</span> &amp;&amp; !bMatch)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nKFs; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 若当前候选关键帧与当前帧匹配数量小于15，跳过</span></span><br><span class="line">            <span class="keyword">if</span>(vbDiscarded[i])</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Perform 5 Ransac Iterations</span></span><br><span class="line">            <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; vbInliers;</span><br><span class="line">            <span class="keyword">int</span> nInliers;</span><br><span class="line">            <span class="keyword">bool</span> bNoMore;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 步骤4：通过EPnP算法估计初始位姿</span></span><br><span class="line">            PnPsolver* pSolver = vpPnPsolvers[i];</span><br><span class="line">            cv::Mat Tcw = pSolver-&gt;iterate(<span class="number">5</span>,bNoMore,vbInliers,nInliers);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// If Ransac reachs max. iterations discard keyframe</span></span><br><span class="line">            <span class="comment">// 若RANSAC失败，当前候选关键帧被提出候选帧</span></span><br><span class="line">            <span class="keyword">if</span>(bNoMore)</span><br><span class="line">            &#123;</span><br><span class="line">                vbDiscarded[i]=<span class="literal">true</span>;</span><br><span class="line">                nCandidates--;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// If a Camera Pose is computed, optimize</span></span><br><span class="line">            <span class="comment">// PNP求解出了一个比较初始的位姿，比较粗糙，需要进一步优化</span></span><br><span class="line">            <span class="keyword">if</span>(!Tcw.empty())</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 把刚刚PNP求解的位姿赋给当前帧位姿</span></span><br><span class="line">                Tcw.copyTo(mCurrentFrame.mTcw);</span><br><span class="line"></span><br><span class="line">                <span class="built_in">set</span>&lt;MapPoint*&gt; sFound;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> np = vbInliers.size();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;np; j++)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">if</span>(vbInliers[j])</span><br><span class="line">                    &#123;</span><br><span class="line">                        mCurrentFrame.mvpMapPoints[j]=vvpMapPointMatches[i][j];</span><br><span class="line">                        sFound.insert(vvpMapPointMatches[i][j]);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                        mCurrentFrame.mvpMapPoints[j]=<span class="literal">NULL</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 步骤5：通过PoseOptimization对姿态进行优化求解</span></span><br><span class="line">                <span class="keyword">int</span> nGood = Optimizer::PoseOptimization(&amp;mCurrentFrame);</span><br><span class="line">                <span class="comment">// 内点小于10，跳过</span></span><br><span class="line">                <span class="keyword">if</span>(nGood&lt;<span class="number">10</span>)</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                <span class="comment">// 刚才的PO优化会滤除一些外点</span></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> io =<span class="number">0</span>; io&lt;mCurrentFrame.N; io++)</span><br><span class="line">                    <span class="keyword">if</span>(mCurrentFrame.mvbOutlier[io])</span><br><span class="line">                        mCurrentFrame.mvpMapPoints[io]=<span class="keyword">static_cast</span>&lt;MapPoint*&gt;(<span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// If few inliers, search by projection in a coarse window and optimize again</span></span><br><span class="line">                <span class="comment">// 步骤6：如果内点较少，则通过投影的方式对之前未匹配的点进行匹配，再进行优化求解</span></span><br><span class="line">                <span class="comment">// 作者认为10&lt;=nGood&lt;50时仍有可能重定位成功，由于PO调整了位姿，</span></span><br><span class="line">                <span class="comment">// 可以通过位姿投影的方式将候选关键帧上的地图点投影在当前帧上进行搜索匹配点，</span></span><br><span class="line">                <span class="comment">// 从而增加匹配，然后再优化以得到足够多的内点</span></span><br><span class="line">                <span class="keyword">if</span>(nGood&lt;<span class="number">50</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">int</span> nadditional =matcher2.SearchByProjection(mCurrentFrame,vpCandidateKFs[i],sFound,<span class="number">10</span>,<span class="number">100</span>);</span><br><span class="line">                    <span class="comment">// 新增的点与之前PO内点之和大于50，我们考虑再进行一遍优化</span></span><br><span class="line">                    <span class="keyword">if</span>(nadditional+nGood&gt;=<span class="number">50</span>)</span><br><span class="line">                    &#123;</span><br><span class="line">                        nGood = Optimizer::PoseOptimization(&amp;mCurrentFrame);</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// If many inliers but still not enough, search by projection again in a narrower window</span></span><br><span class="line">                        <span class="comment">// the camera has been already optimized with many points</span></span><br><span class="line">                        <span class="comment">// 不够多呀，不要放弃，再来一遍</span></span><br><span class="line">                        <span class="keyword">if</span>(nGood&gt;<span class="number">30</span> &amp;&amp; nGood&lt;<span class="number">50</span>)</span><br><span class="line">                        &#123;</span><br><span class="line">                            sFound.clear();</span><br><span class="line">                            <span class="keyword">for</span>(<span class="keyword">int</span> ip =<span class="number">0</span>; ip&lt;mCurrentFrame.N; ip++)</span><br><span class="line">                                <span class="keyword">if</span>(mCurrentFrame.mvpMapPoints[ip])</span><br><span class="line">                                    sFound.insert(mCurrentFrame.mvpMapPoints[ip]);</span><br><span class="line">                            nadditional =matcher2.SearchByProjection(mCurrentFrame,vpCandidateKFs[i],sFound,<span class="number">3</span>,<span class="number">64</span>);</span><br><span class="line"></span><br><span class="line">                            <span class="comment">// Final optimization</span></span><br><span class="line">                            <span class="comment">// 最后一次优化啦~</span></span><br><span class="line">                            <span class="keyword">if</span>(nGood+nadditional&gt;=<span class="number">50</span>)</span><br><span class="line">                            &#123;</span><br><span class="line">                                nGood = Optimizer::PoseOptimization(&amp;mCurrentFrame);</span><br><span class="line"></span><br><span class="line">                                <span class="keyword">for</span>(<span class="keyword">int</span> io =<span class="number">0</span>; io&lt;mCurrentFrame.N; io++)</span><br><span class="line">                                    <span class="keyword">if</span>(mCurrentFrame.mvbOutlier[io])</span><br><span class="line">                                        mCurrentFrame.mvpMapPoints[io]=<span class="literal">NULL</span>;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// If the pose is supported by enough inliers stop ransacs and continue</span></span><br><span class="line">                <span class="comment">// 只要找到一个候选关键帧与当前帧的匹配点数大于50就重定位成功！</span></span><br><span class="line">                <span class="keyword">if</span>(nGood&gt;=<span class="number">50</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    bMatch = <span class="literal">true</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(!bMatch)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        mnLastRelocFrameId = mCurrentFrame.mnId;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> ORB </tag>
            
            <tag> Relocalisation </tag>
            
            <tag> 重定位 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SLAM常见问题(一)：SearchByBoW</title>
      <link href="/posts/slam-common-issues-SearchbyBoW/"/>
      <url>/posts/slam-common-issues-SearchbyBoW/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>ORB-SLAM</strong>中使用了多种特征匹配的奇技淫巧，其中之一就是利用<strong>词袋信息</strong>进行引导匹配<code>SearchByBoW</code>：利用了<code>BOW</code>里的正向引导进行两帧之间的匹配，核心点在于位于同一个节点处的特征才有可能属于同一匹配，相较于暴力匹配匹配速度更快。</p><a id="more"></a><p>注意：每幅图像都可以通过<code>ComputeBoW</code>得到其对应的词袋向量。<code>featureVector</code>存储的是节点的索引值以及对应图像feature对应的索引向量，即<code>map&lt;node_id,vector&lt;featureID&gt;</code>。这样的话就可以根据两帧图像的<code>node_id</code>来初步确定二者共有的特征点，然后根据该<code>id</code>取出<code>vector&lt;featureID&gt;</code>，根据featureID找到图像上的特征点以及描述子，通过比较二者描述子距离来判定该特征点是否为匹配点，若距离小于某一阈值，则二者为匹配对。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief Bag of Words Representation</span></span><br><span class="line"><span class="comment"> * 计算词袋mBowVec和mFeatVec</span></span><br><span class="line"><span class="comment"> * @see CreateInitialMapMonocular() TrackReferenceKeyFrame() Relocalization()</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//同样的，关键帧也有构造函数void KeyFrame::ComputeBoW()</span></span><br><span class="line"><span class="keyword">void</span> Frame::ComputeBoW() </span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(mBowVec.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 将描述子mDescriptors转换为DBOW要求的输入格式</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;cv::Mat&gt; vCurrentDesc = Converter::toDescriptorVector(mDescriptors);</span><br><span class="line">        <span class="comment">// 转换成词袋向量mBowVec以及特征向量mFeatVec</span></span><br><span class="line">        <span class="comment">// mBowVec存储着单词及其对应的权重TF-IDF值</span></span><br><span class="line">        <span class="comment">// mFeatVec存储节点ID以及对应对应图像feature对应的索引向量</span></span><br><span class="line">        mpORBvocabulary-&gt;transform(vCurrentDesc,mBowVec,mFeatVec,<span class="number">4</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该函数在<code>Tracking</code>线程中的<code>TrackReferenceKeyFrame()</code>/<code>Relocalization()</code>进行调用（注意：<code>LoopClosing</code>线程中<code>ComputeSim3()</code>也会调用该函数，与上述二者的区别在于，ComputeSim3()中的SearchByBoW是寻找关键帧之间的匹配，而非关键帧与当前帧之间的匹配）。</p><p>下面给出<code>ORB-SLAM2</code>中用于<strong>关键帧与当前帧</strong>进行词袋引导匹配的源码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief 通过词袋，对关键帧的特征点进行跟踪</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 通过bow对pKF和F中的特征点进行快速匹配（不属于同一node的特征点直接跳过匹配） \n</span></span><br><span class="line"><span class="comment"> * 对属于同一node的特征点通过描述子距离进行匹配 \n</span></span><br><span class="line"><span class="comment"> * 根据匹配，用pKF中特征点对应的MapPoint去更新F中特征点对应的MapPoints \n</span></span><br><span class="line"><span class="comment"> * 每个特征点都对应一个MapPoint，因此pKF中每个特征点的MapPoint也就是F中对应点的MapPoint \n</span></span><br><span class="line"><span class="comment"> * 通过距离阈值、比例阈值和角度投票进行剔除误匹配</span></span><br><span class="line"><span class="comment"> * @param  pKF               KeyFrame</span></span><br><span class="line"><span class="comment"> * @param  F                 Current Frame</span></span><br><span class="line"><span class="comment"> * @param  vpMapPointMatches F中MapPoints对应的匹配，NULL表示未匹配</span></span><br><span class="line"><span class="comment"> * @return                   成功匹配的数量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> <span class="comment">//const int ORBmatcher::TH_HIGH = 100;</span></span><br><span class="line"> <span class="comment">//const int ORBmatcher::TH_LOW = 50;</span></span><br><span class="line"> <span class="comment">//const int ORBmatcher::HISTO_LENGTH = 30;</span></span><br><span class="line"><span class="keyword">int</span> ORBmatcher::SearchByBoW(KeyFrame* pKF,Frame &amp;F, <span class="built_in">vector</span>&lt;MapPoint*&gt; &amp;vpMapPointMatches)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// vpMapPointsKF：获取输入关键帧匹配到的地图点</span></span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;MapPoint*&gt; vpMapPointsKF = pKF-&gt;GetMapPointMatches();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化当前帧MapPoints对应的匹配NULL</span></span><br><span class="line">    vpMapPointMatches = <span class="built_in">vector</span>&lt;MapPoint*&gt;(F.N,<span class="keyword">static_cast</span>&lt;MapPoint*&gt;(<span class="literal">NULL</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// FeatureVector数据类型 map&lt;node_id,vector&lt;featureID&gt;，</span></span><br><span class="line">    <span class="comment">// 可以快速根据node_id找到属于该node的特征点</span></span><br><span class="line">    <span class="keyword">const</span> DBoW2::FeatureVector &amp;vFeatVecKF = pKF-&gt;mFeatVec;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> nmatches=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; rotHist[HISTO_LENGTH];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;HISTO_LENGTH;i++)</span><br><span class="line">        rotHist[i].reserve(<span class="number">500</span>);</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span> factor = HISTO_LENGTH/<span class="number">360.0f</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We perform the matching over ORB that belong to the same vocabulary node </span></span><br><span class="line">    <span class="comment">// (at a certain level)</span></span><br><span class="line">    <span class="comment">// 建立迭代器，将属于同一节点(特定层)的ORB特征进行匹配</span></span><br><span class="line">    DBoW2::FeatureVector::const_iterator KFit = vFeatVecKF.begin();</span><br><span class="line">    DBoW2::FeatureVector::const_iterator Fit = F.mFeatVec.begin();</span><br><span class="line">    DBoW2::FeatureVector::const_iterator KFend = vFeatVecKF.end();</span><br><span class="line">    DBoW2::FeatureVector::const_iterator Fend = F.mFeatVec.end();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(KFit != KFend &amp;&amp; Fit != Fend)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 步骤1：分别取出属于同一node的ORB特征点(只有属于同一node，才有可能是匹配点)</span></span><br><span class="line">        <span class="comment">// first表示node_id，只有node_id相同才表示这些特征点位于同一层</span></span><br><span class="line">        <span class="keyword">if</span>(KFit-&gt;first == Fit-&gt;first) </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// second中记录了这些特征对应图像中的ID</span></span><br><span class="line">            <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt; vIndicesKF = KFit-&gt;second;</span><br><span class="line">            <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt; vIndicesF = Fit-&gt;second;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 步骤2：遍历KF中属于该node的特征点</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">size_t</span> iKF=<span class="number">0</span>; iKF&lt;vIndicesKF.size(); iKF++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 获取关键帧上某一个特征点的ID</span></span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> realIdxKF = vIndicesKF[iKF];</span><br><span class="line">                <span class="comment">// 根据该ID得到该特征对应的MapPoint</span></span><br><span class="line">                MapPoint* pMP = vpMapPointsKF[realIdxKF]; </span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(!pMP) <span class="comment">//不存在</span></span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(pMP-&gt;isBad())<span class="comment">//被标记为坏点</span></span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 根据该ID取出KF中该特征对应的描述子</span></span><br><span class="line">                <span class="keyword">const</span> cv::Mat &amp;dKF= pKF-&gt;mDescriptors.row(realIdxKF); </span><br><span class="line"></span><br><span class="line">                <span class="keyword">int</span> bestDist1=<span class="number">256</span>; <span class="comment">// 最好的距离（最小距离）</span></span><br><span class="line">                <span class="keyword">int</span> bestIdxF =<span class="number">-1</span> ;</span><br><span class="line">                <span class="keyword">int</span> bestDist2=<span class="number">256</span>; <span class="comment">// 倒数第二好距离（倒数第二小距离）</span></span><br><span class="line"></span><br><span class="line">                <span class="comment">// 步骤3：遍历当前帧中属于该node的特征点，找到了最佳匹配点</span></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">size_t</span> iF=<span class="number">0</span>; iF&lt;vIndicesF.size(); iF++)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// 取出当前帧上位于该node上的某一个特征点的ID</span></span><br><span class="line">                    <span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> realIdxF = vIndicesF[iF];</span><br><span class="line">                    <span class="comment">// 表明这个点已经被匹配过了，不再匹配，加快速度</span></span><br><span class="line">                    <span class="keyword">if</span>(vpMapPointMatches[realIdxF])</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    <span class="comment">// 取出F中该特征对应的描述子</span></span><br><span class="line">                    <span class="keyword">const</span> cv::Mat &amp;dF = F.mDescriptors.row(realIdxF); </span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 计算描述子距离，这里是汉明距离，若非二进制描述子可选择用其他距离</span></span><br><span class="line">                    <span class="keyword">const</span> <span class="keyword">int</span> dist =  DescriptorDistance(dKF,dF); </span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 下面的操作就是分别获得最小bestDist1以及次小bestDist2的描述子距离</span></span><br><span class="line">                    <span class="comment">// dist &lt; bestDist1 &lt; bestDist2，更新bestDist1 bestDist2</span></span><br><span class="line">                    <span class="keyword">if</span>(dist&lt;bestDist1)</span><br><span class="line">                    &#123;</span><br><span class="line">                        bestDist2=bestDist1;</span><br><span class="line">                        bestDist1=dist;</span><br><span class="line">                        bestIdxF=realIdxF;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span>(dist&lt;bestDist2)<span class="comment">// bestDist1 &lt; dist &lt; bestDist2，更新bestDist2</span></span><br><span class="line">                    &#123;</span><br><span class="line">                        bestDist2=dist;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 步骤4：根据描述子距离阈值和角度投票剔除误匹配</span></span><br><span class="line">                <span class="comment">// 最小的描述子距离小于一个阈值 </span></span><br><span class="line">                <span class="keyword">if</span>(bestDist1&lt;=TH_LOW) </span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// trick!</span></span><br><span class="line">                    <span class="comment">// 最佳匹配比次佳匹配明显要好，那么最佳匹配才真正靠谱</span></span><br><span class="line">                    <span class="keyword">if</span>(<span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(bestDist1)&lt;</span><br><span class="line">                    mfNNratio*<span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(bestDist2))</span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="comment">// 步骤5：更新当前帧特征点的MapPoint</span></span><br><span class="line">                        <span class="comment">// 记录了特征点ID以及对应的特征点</span></span><br><span class="line">                        vpMapPointMatches[bestIdxF]=pMP;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 获得关键帧上的特征点位置</span></span><br><span class="line">                        <span class="keyword">const</span> cv::KeyPoint &amp;kp = pKF-&gt;mvKeysUn[realIdxKF];</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//</span></span><br><span class="line">                        <span class="keyword">if</span>(mbCheckOrientation)</span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="comment">// trick!</span></span><br><span class="line">                            <span class="comment">// angle：每个特征点在提取描述子时的旋转主方向角度，</span></span><br><span class="line">                            <span class="comment">// 如果图像旋转了，这个角度将发生改变</span></span><br><span class="line">                            <span class="comment">// 所有的特征点的角度变化应该是一致的，</span></span><br><span class="line">                            <span class="comment">// 通过直方图统计得到最准确的角度变化值</span></span><br><span class="line">                            <span class="comment">// 该特征点的角度变化值</span></span><br><span class="line">                            <span class="keyword">float</span> rot = kp.angle-F.mvKeys[bestIdxF].angle;</span><br><span class="line">                            <span class="keyword">if</span>(rot&lt;<span class="number">0.0</span>)</span><br><span class="line">                                rot+=<span class="number">360.0f</span>;</span><br><span class="line">                            <span class="keyword">int</span> bin = round(rot*factor);<span class="comment">// 将rot分配到bin组</span></span><br><span class="line">                            <span class="keyword">if</span>(bin==HISTO_LENGTH)</span><br><span class="line">                                bin=<span class="number">0</span>;</span><br><span class="line">                            assert(bin&gt;=<span class="number">0</span> &amp;&amp; bin&lt;HISTO_LENGTH);</span><br><span class="line">                            rotHist[bin].push_back(bestIdxF);</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="comment">// 匹配点+1</span></span><br><span class="line">                        nmatches++;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            KFit++;</span><br><span class="line">            Fit++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(KFit-&gt;first &lt; Fit-&gt;first)</span><br><span class="line">        &#123;</span><br><span class="line">            KFit = vFeatVecKF.lower_bound(Fit-&gt;first);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            Fit = F.mFeatVec.lower_bound(KFit-&gt;first);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据方向剔除误匹配的点，即删除那些不属于特征点角度变化最多的三个类别的匹配点</span></span><br><span class="line">    <span class="keyword">if</span>(mbCheckOrientation)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> ind1=<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">int</span> ind2=<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">int</span> ind3=<span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 计算rotHist中最大的三个的index</span></span><br><span class="line">        ComputeThreeMaxima(rotHist,HISTO_LENGTH,ind1,ind2,ind3);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;HISTO_LENGTH; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果特征点的旋转角度变化量属于这三个组，则保留</span></span><br><span class="line">            <span class="keyword">if</span>(i==ind1 || i==ind2 || i==ind3)</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 将除了ind1 ind2 ind3以外的匹配点去掉</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">size_t</span> j=<span class="number">0</span>, jend=rotHist[i].size(); j&lt;jend; j++)</span><br><span class="line">            &#123;</span><br><span class="line">                vpMapPointMatches[rotHist[i][j]]=<span class="keyword">static_cast</span>&lt;MapPoint*&gt;(<span class="literal">NULL</span>);</span><br><span class="line">                nmatches--;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nmatches;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外，<code>LoopClosing</code>线程中<code>ComputeSim3()</code>调用的<code>SearchByBoW</code>的函数声明为：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> ORBmatcher::SearchByBoW(KeyFrame *pKF1, KeyFrame *pKF2, <span class="built_in">vector</span>&lt;MapPoint *&gt; &amp;vpMatches12)</span><br></pre></td></tr></table></figure></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://blog.csdn.net/qq_24893115/article/details/52629248" target="_blank" rel="noopener">https://blog.csdn.net/qq_24893115/article/details/52629248</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> ORB </tag>
            
            <tag> 特征匹配 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019年浙大CADCG暑假SLAM培训部分课件</title>
      <link href="/posts/slam-summer-courses-CADCG-Lab/"/>
      <url>/posts/slam-summer-courses-CADCG-Lab/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>由浙江大学CAD&amp;CG国家重点实验室主办、浙江大学-商汤三维视觉联合实验室协办的“SLAM技术及应用”暑期学校于7月20日如期拉开序幕。<br>今天（2019/07/20）看到直播的时候已经是下午4点半了，只听到刘浩敏讲到末尾的一段，幸好主办方提供了讲座课件，Download下来慢慢看。</p><a id="more"></a><h2 id="2019年课件"><a href="#2019年课件" class="headerlink" title="2019年课件"></a>2019年课件</h2><ul><li><p>2019年7月20日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/Camera-model-and-Projection-Transformation.pdf" target="_blank" rel="noopener">相机模型与投影变换</a>（讲者：章国锋）</p></li><li><p>2019年7月20日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/SfM-GuofengZhang.pdf" target="_blank" rel="noopener">运动恢复结构</a>（讲者：章国锋）</p></li><li><p>2019年7月20日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/BA-haominLiu.pdf" target="_blank" rel="noopener">集束<strong>Bundle Adjustment</strong>调整</a>（讲者：刘浩敏）</p></li><li><p>2019年7月21日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/3D-Tutorial-ShuhanShen.pdf" target="_blank" rel="noopener">三维重建</a>（讲者：申抒含）</p></li><li><p>2019年7月21日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/V-SLAM-GuofengZhang.pdf" target="_blank" rel="noopener">视觉SLAM</a>（讲者：章国锋）</p></li><li><p>2019年7月21日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/RGB-D-SLAM-HanqingJiang.pdf" target="_blank" rel="noopener">RGB-D SLAM</a>（讲者：姜翰青）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/VI-SLAM.pdf" target="_blank" rel="noopener">视觉惯性SLAM</a>（讲者：黄国权）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/3D-recognition-track-XueyingQin.pdf" target="_blank" rel="noopener">三维物体的识别与跟踪</a>（讲者：秦学英）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/R2VR-VR-Fusion-Technology.pdf" target="_blank" rel="noopener">从现实到虚拟现实-虚实融合呈现技术</a>（讲者：王锐）</p></li><li><p>2019年7月22日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/Mav-DanpingZou.pdf" target="_blank" rel="noopener">面向SLAM研究的无人机快速入门与平台选择</a>（讲者：邹丹平）</p></li><li><p>2019年7月23日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/MobileVR-System-Design-Application.pdf" target="_blank" rel="noopener">移动增强现实系统的设计与应用案例解析</a>（讲者：章国锋）</p></li><li><p>2019年7月23日，<a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/AR-Applications.pdf" target="_blank" rel="noopener">AR应用开发</a>（讲者：盛崇山）</p></li><li><p><strong><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2019-SLAM-Summer-School-slides.zip" target="_blank" rel="noopener">打包下载</a></strong></p></li></ul><h2 id="2018年课件"><a href="#2018年课件" class="headerlink" title="2018年课件"></a>2018年课件</h2><ul><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E7%9B%B8%E6%9C%BA%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%8A%95%E5%BD%B1%E5%8F%98%E6%8D%A2-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">相机模型与投影变换-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E8%BF%90%E5%8A%A8%E6%81%A2%E5%A4%8D%E7%BB%93%E6%9E%84-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">运动恢复结构-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E9%9B%86%E6%9D%9F%E8%B0%83%E6%95%B4-%E5%88%98%E6%B5%A9%E6%95%8F.pdf" target="_blank" rel="noopener">集束调整-刘浩敏</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E6%B7%B1%E5%BA%A6%E6%81%A2%E5%A4%8D%E4%B8%8E%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">深度恢复与三维重建-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E8%A7%86%E8%A7%89SLAM-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">视觉SLAM-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E8%A7%86%E8%A7%89%E6%83%AF%E5%AF%BCSLAM-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">视觉惯导SLAM-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/Visual-Inertial%20SLAM-%E6%9D%8E%E5%90%8D%E6%9D%A8.pdf" target="_blank" rel="noopener">Visual-Inertial SLAM-李名杨</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/RGB-D%20SLAM-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">RGB-D SLAM-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E5%9C%B0%E9%9D%A2%E6%97%A0%E4%BA%BA%E5%B9%B3%E5%8F%B0%E4%B8%AD%E7%9A%84SLAM%E6%8A%80%E6%9C%AF-%E5%88%98%E5%8B%87.pdf" target="_blank" rel="noopener">地面无人平台中的SLAM技术-刘勇</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E5%9F%BA%E4%BA%8E%E7%8E%AF%E5%A2%83%E7%BB%93%E6%9E%84%E5%8C%96%E7%89%B9%E6%80%A7%E7%9A%84%E8%A7%86%E8%A7%89SLAM%E6%96%B9%E6%B3%95-%E9%82%B9%E4%B8%B9%E5%B9%B3.pdf" target="_blank" rel="noopener">基于环境结构化特性的视觉SLAM方法-邹丹平</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/%E7%A7%BB%E5%8A%A8%E5%A2%9E%E5%BC%BA%E7%8E%B0%E5%AE%9E%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B%E8%A7%A3%E6%9E%90-%E7%AB%A0%E5%9B%BD%E9%94%8B.pdf" target="_blank" rel="noopener">移动增强现实系统设计与应用案例解析-章国锋</a></p></li><li><p><a href="https://vincentqin.gitee.io/blogresource-3/slam-summer-courses-CADCG-Lab/2018/VR-Ruiwang1.pdf" target="_blank" rel="noopener">虚实融合显示与绘制技术-王锐</a></p></li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://mp.weixin.qq.com/s/PV_xLmuE-HpnUgnJ5GyMOA" target="_blank" rel="noopener">第二届“SLAM技术及应用” 暑期学校与研讨会圆满落幕</a></li><li>章国锋主页：<a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank" rel="noopener">地址</a></li><li>CAD&amp;CG实验室主页，<a href="http://www.zjucvg.net/" target="_blank" rel="noopener">地址</a></li><li><a href="https://github.com/zju3dv" target="_blank" rel="noopener">CAD&amp;CG Github</a></li><li><a href="http://www.zjucvg.net/senseslam/" target="_blank" rel="noopener">SenseSLAM</a>,浙大-商汤三维视觉联合实验室</li><li>Shuhan Shen (申抒含)主页，<a href="http://vision.ia.ac.cn/Faculty/shshen/index.htm" target="_blank" rel="noopener">地址</a></li><li>姜翰青，<a href="https://www.linkedin.com/in/%E7%BF%B0%E9%9D%92-%E5%A7%9C-1194b411b/`" target="_blank" rel="noopener">Linkedin</a></li><li>讲座直播地址：<a href="https://www.douyu.com/7275221" target="_blank" rel="noopener">https://www.douyu.com/7275221</a></li><li>商汤泰坦公开课，<a href="https://cloud.xylink.com/live/v/2c9497116bb8b075016c082adec66ea7" target="_blank" rel="noopener">直播地址</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> SLAM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Filebrowser：一款轻量级个人网盘</title>
      <link href="/posts/build-filebrowser/"/>
      <url>/posts/build-filebrowser/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><br><br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/filebrowser-banner.png"><br><br></p><a id="more"></a><h1 id="个人网盘-Filebrowser"><a href="#个人网盘-Filebrowser" class="headerlink" title="个人网盘 Filebrowser"></a>个人网盘 Filebrowser</h1><p>服务器仅仅用于科学上网未免有些浪费了，是时候尝试一下自建个人网盘和图床了。</p><h2 id="如何安装"><a href="#如何安装" class="headerlink" title="如何安装"></a>如何安装</h2><p><a href="https://filebrowser.xyz/installation" target="_blank" rel="noopener">官方</a>给出了一键安装大法，进入服务器输入以下命令就可以了。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://filebrowser.xyz/get.sh | bash</span><br></pre></td></tr></table></figure></p><h2 id="首次配置"><a href="#首次配置" class="headerlink" title="首次配置"></a>首次配置</h2><p>当安装好之后，你并不能立即使用它，需要修改一些配置(以下内容参考[<a href="https://www.mivm.cn/filebrowser/" target="_blank" rel="noopener">米V米</a>]的教程)。</p><ul><li>创建配置数据库：<code>filebrowser -d /etc/filebrowser.db config init</code></li><li>设置监听地址：<code>filebrowser -d /etc/filebrowser.db config set --address 0.0.0.0</code></li><li>设置监听端口：<code>filebrowser -d /etc/filebrowser.db config set --port 8088</code></li><li>设置语言环境(中文)：<code>filebrowser -d /etc/filebrowser.db config set --locale zh-cn</code></li><li>设置日志位置：<code>filebrowser -d /etc/filebrowser.db config set --log /var/log/filebrowser.log</code></li><li>添加一个用户：<code>filebrowser -d /etc/filebrowser.db users add root password --perm.admin</code>，其中的root和password分别是用户名和密码，根据自己的需求更改。</li></ul><p>有关更多配置的选项，可以参考官方文档：<a href="https://docs.filebrowser.xyz/" target="_blank" rel="noopener">https://docs.filebrowser.xyz/</a><br>配置修改好以后，就可以启动FileBrowser了，使用-d参数指定配置数据库路径。示例：<code>filebrowser -d /etc/filebrowser.db</code><br>启动成功就可以使用浏览器访问FileBrowser了，在浏览器输入 <code>服务器IP:端口</code>，示例：<code>http://192.168.1.1:8088</code></p><p>然后会看到 FileBrowser 的登陆界面：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/filebrowser-login.png"></p><p>用刚刚创建的用户登陆，最后就可以放心使用啦~<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/filebrowser-demo.gif"></p><h2 id="后续配置"><a href="#后续配置" class="headerlink" title="后续配置"></a>后续配置</h2><p>完成以上过程之后已经可以正常访问个人网盘了，但是假如服务器重启之后就必须重新输入<code>filebrowser -d /etc/filebrowser.db</code>才能运行，为了省去这一步，我们需要进行设置服务器开机自动启动FileBrowser。<br>这里我们使用的是systemd 大法：<br>首先下载 FileBrowser 的 <code>service</code>文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl https://cdn.mivm.cn/www.mivm.cn/archives/filebrowser/filebrowser.service -o /lib/systemd/system/filebrowser.service</span><br></pre></td></tr></table></figure><p>如果你的运行命令不是<code>/usr/local/bin/filebrowser -d /etc/filebrowser.db</code>，需要对 service 文件进行修改，将文件的 ExecStart 改为你的运行命令，更改完成后需要输入<code>systemctl daemon-reload</code>。</p><p>下面祭出常用的命令：</p><ul><li>运行：<code>systemctl start filebrowser.service</code></li><li>停止运行：<code>systemctl stop filebrowser.service</code></li><li>开机启动：<code>systemctl enable filebrowser.service</code></li><li>取消开机启动：<code>systemctl disable filebrowser.service</code></li><li>查看运行状态：<code>systemctl status filebrowser.service</code></li></ul><p>这里有个<a href="https://youtu.be/sE31MBvOjxk" target="_blank" rel="noopener">视频教程</a>，需要科学上网查看。</p><p><br></p><h1 id="个人图床Chevereto"><a href="#个人图床Chevereto" class="headerlink" title="个人图床Chevereto"></a>个人图床Chevereto</h1><p>先给出安装好的样子~<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/build-filebrowser/Chevereto.png"></p><p>安装教程这里<a href="https://gist.github.com/biezhi/f90923b48863c7d745481ccdd678ccab" target="_blank" rel="noopener">install_chevereto.md</a>已经写得非常详细了，在此不做详细介绍。这里有个<a href="https://youtu.be/kShgzNkXRak" target="_blank" rel="noopener">视频教程</a>，我主要按照这个教程进行配置的，需要科学上网查看。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://youtu.be/sE31MBvOjxk" target="_blank" rel="noopener">把玩我的 VPS 主机 - 分分钟搭建时尚简洁的在线网盘</a></li><li><a href="https://youtu.be/kShgzNkXRak" target="_blank" rel="noopener">搭建漂亮的私人图床</a></li><li><a href="https://biezhi.me/" target="_blank" rel="noopener">王爵 nice的主页</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 个人网盘 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SuperPoint: Self-Supervised Interest Point Detection and Description</title>
      <link href="/posts/superpoint/"/>
      <url>/posts/superpoint/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文出自近几年备受瞩目的创业公司<a href="https://www.magicleap.com/" target="_blank" rel="noopener">MagicLeap</a>，发表在CVPR 2018,一作<a href="http://www.danieldetone.com/" target="_blank" rel="noopener">Daniel DeTone</a>，<strong>[<a href="https://arxiv.org/abs/1712.07629" target="_blank" rel="noopener">paper</a>]</strong>，<strong>[<a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork/blob/master/assets/DL4VSLAM_talk.pdf" target="_blank" rel="noopener">slides</a>]</strong>，<strong>[<a href="https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork" target="_blank" rel="noopener">code</a>]</strong>。</p><p>这篇文章设计了一种自监督网络框架，能够同时提取特征点的位置以及描述子。相比于patch-based方法，本文提出的算法能够在原始图像提取到像素级精度的特征点的位置及其描述子。<br>本文提出了一种单映性适应（<code>Homographic Adaptation</code>）的策略以增强特征点的复检率以及跨域的实用性（这里跨域指的是synthetic-to-real的能力，网络模型在虚拟数据集上训练完成，同样也可以在真实场景下表现优异的能力）。</p><a id="more"></a><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>诸多应用（诸如SLAM/SfM/相机标定/立体匹配）的首要一步就是特征点提取，这里的特征点指的是<strong>能够在不同光照&amp;不同视角下都能够稳定且可重复检测的2D图像点位置</strong>。</p><p>基于CNN的算法几乎在以图像作为输入的所有领域表现出相比于人类特征工程更加优秀的表达能力。目前已经有一些工作做类似的任务，例如人体位姿估计,目标检测以及室内布局估计等。这些算法以通常以大量的人工标注作为GT，这些精心设计的网络用来训练以得到人体上的角点，例如嘴唇的边缘点亦或人体的关节点，但是这里的问题是这里的点实际是ill-defined（我的理解是，这些点有可能是特征点，但仅仅是一个大概的位置，是特征点的子集，并没有真正的把特征点的概念定义清楚）。</p><p>本文采用了非人工监督的方法提取真实场景的特征点。本文设计了一个由特征点检测器监督的具有伪真值数据集，而非是大量的人工标记。为了得到伪真值，本文首先在大量的虚拟数据集上训练了一个全卷积网络（FCNN），这些虚拟数据集由一些基本图形组成，例如有线段、三角形、矩形和立方体等，这些基本图形具有没有争议的特征点位置，文中称这些特征点为<code>MagicPoint</code>，这个pre-trained的检测器就是<code>MagicPoint</code>检测器。这些<code>MagicPoint</code>在虚拟场景的中检测特征点的性能明显优于传统方式，但是在真实的复杂场景中表现不佳，此时作者提出了一种多尺度多变换的方法<code>Homographic Adaptation</code>。对于输入图像而言，<code>Homographic Adaptation</code>通过对图像进行多次不同的尺度/角度变换来帮助网络能够在不同视角不同尺度观测到特征点。<br>综上：<strong>SuperPoint = MagicPoint+Homographic Adaptation</strong></p><h1 id="算法优劣对比"><a href="#算法优劣对比" class="headerlink" title="算法优劣对比"></a>算法优劣对比</h1><p><img alt="fig1_table1" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/tab_1.png"></p><ul><li>基于图像块的算法导致特征点位置精度不够准确；</li><li>特征点与描述子分开进行训练导致运算资源的浪费，网络不够精简，实时性不足；或者仅仅训练特征点或者描述子的一种，不能用同一个网络进行联合训练；</li></ul><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img alt="fig3" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_3.png"></p><p>上图可见特征点检测器以及描述子网络共享一个单一的前向encoder，只是在decoder时采用了不同的结构，根据任务的不同学习不同的网络参数。这也是本框架与其他网络的不同之处：其他网络采用的是先训练好特征点检测网络，然后再去进行对特征点描述网络进行训练。<br>网络共分成以下4个主要部分，在此进行详述：</p><h2 id="1-Shared-Encoder-共享的编码网络"><a href="#1-Shared-Encoder-共享的编码网络" class="headerlink" title="1. Shared Encoder 共享的编码网络"></a>1. Shared Encoder 共享的编码网络</h2><p>从上图可以看到，整体而言，本质上有两个网络，只是前半部分共享了一部分而已。本文利用了VGG-style的encoder以用于降低图像尺寸，encoder包括卷积层，max-pooling层，以及非线性激活层。通过3个max-pooling层将图像的尺寸变成$H_c = H/8$和$H_c = H/8$，经过encoder之后，图像由$I \in \mathcal{R}^{H \times W}$变为张量$\mathcal{B} \in \mathbb{R}^{H_c \times W_c \times F}$</p><h2 id="2-Interest-Point-Decoder"><a href="#2-Interest-Point-Decoder" class="headerlink" title="2. Interest Point Decoder"></a>2. Interest Point Decoder</h2><p><img alt="fig_10_magicPoint1" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_10_magicPoint1.png"></p><p>这里介绍的是特征点的解码端。每个像素的经过该解码器的输出是该像素是特征点的概率（probability of “point-ness”）。<br>通常而言，我们可以通过反卷积得到上采样的图像，但是这种操作会导致计算量的骤增以及会引入一种“checkerboard artifacts”。因此本文设计了一种带有“特定解码器”（这种解码器没有参数）的特征点检测头以减小模型计算量（子像素卷积）。<br>例如：输入张量的维度是$\mathbb{R}^{H_c \times W_c \times 65}$，输出维度$\mathbb{R}^{H \times W}$，即图像的尺寸。这里的65表示原图$8 \times 8$的局部区域，加上一个非特征点<code>dustbin</code>。通过在channel维度上做softmax，非特征点dustbin会被删除，同时会做一步图像的<code>reshape</code>：$\mathbb{R}^{H_c \times W_c \times 64} \Rightarrow \mathbb{R}^{H \times W}$ 。（这就是<strong><a href="https://blog.csdn.net/leviopku/article/details/84975282" target="_blank" rel="noopener">子像素卷积</a></strong>的意思，俗称像素洗牌）</p><h2 id="3-Descriptor-Decoder"><a href="#3-Descriptor-Decoder" class="headerlink" title="3. Descriptor Decoder"></a>3. Descriptor Decoder</h2><p>首先利用类似于UCN的网络得到一个半稠密的描述子（此处参考文献<a href="https://arxiv.org/abs/1606.03558" target="_blank" rel="noopener">UCN</a>），这样可以减少算法训练内存开销同时减少算法运行时间。之后通过双三次多项式插值得到其余描述，然后通过<code>L2-normalizes</code>归一化描述子得到统一的长度描述。特征维度由$\mathcal{D} \in \mathbb{R}^{H_c \times W_c \times D}$变为$\mathbb{R}^{H\times W \times D}$ 。</p><p><img alt="fig_11_des_decoder" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_11_des_decoder.png"></p><p>由特征点得到其描述子的过程文中没有细讲，看了一下<a href="https://github.com/pytorch/pytorch/blob/f064c5aa33483061a48994608d890b968ae53fb5/aten/src/THNN/generic/SpatialGridSamplerBilinear.c" target="_blank" rel="noopener">源代码</a>就明白了。其实该过程主要用了一个函数即<code>grid_sample</code>，画了一个草图作为解释。</p><ul><li>图像尺寸归一化：首先对图像的尺寸进行归一化，(-1,-1)表示原来图像的(0,0)位置，(1,1)表示原来图像的(H-1,W-1)位置，这样一来，特征点的位置也被归一化到了相应的位置。</li><li>构建grid：将归一化后的特征点罗列起来，构成一个尺度为1*1*K*2的张量，其中K表示特征数量，2分别表示xy坐标。</li><li>特征点位置反归一化：根据输入张量的H与W对grid(1,1,0,:)（表示第一个特征点，其余特征点类似）进行反归一化，其实就是按照比例进行缩放+平移，得到反归一化特征点在张量某个slice（通道）上的位置；但是这个位置可能并非为整像素，此时要对其进行双线性插值补齐，然后其余slice按照同样的方式进行双线性插值。注：代码中实际的就是双线性插值，并非文中讲的双三次插值；</li><li>输出维度：1*C*1*K。</li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/grid_sample.png"></p><h2 id="4-误差构建"><a href="#4-误差构建" class="headerlink" title="4. 误差构建"></a>4. 误差构建</h2><script type="math/tex; mode=display">\begin{array}{l}{\mathcal{L}\left(\mathcal{X}, \mathcal{X}^{\prime}, \mathcal{D}, \mathcal{D}^{\prime} ; Y, Y^{\prime}, S\right)=} \\ {\qquad \mathcal{L}_{p}(\mathcal{X}, Y)+\mathcal{L}_{p}\left(\mathcal{X}^{\prime}, Y^{\prime}\right)+\lambda \mathcal{L}_{d}\left(\mathcal{D}, \mathcal{D}^{\prime}, S\right)}\end{array}</script><p>可见损失函数由两项组成，其中一项为特征点检测loss$\mathcal{L}_{p}$ ，另外一项是描述子的loss$\mathcal{L}_{d}$。</p><p>对于检测项loss，此时采用了交叉熵损失函数:</p><script type="math/tex; mode=display">\mathcal{L}_{p}(\mathcal{X}, Y)=\frac{1}{H_{c} W_{c}} \sum_{h=1 \atop w=1}^{H_{c}, W_{c}} l_{p}\left(\mathbf{x}_{h w} ; y_{h w}\right)</script><p>其中：</p><script type="math/tex; mode=display">l_{p}\left(\mathbf{x}_{h w} ; y\right)=-\log \left(\frac{\exp \left(\mathbf{x}_{h w y}\right)}{\sum_{k=1}^{65} \exp \left(\mathbf{x}_{h w k}\right)}\right)</script><p>描述子的损失函数:</p><script type="math/tex; mode=display">\mathcal{L}_{d}\left(\mathcal{D}, \mathcal{D}^{\prime}, S\right)=\frac{1}{\left(H_{c} W_{c}\right)^{2}} \sum_{h=1 \atop w=1}^{H_{c}, W_{c}} \sum_{h^{\prime}=1 \atop w^{\prime}=1}^{H_{c}, W_{c}} l_{d}\left(\mathbf{d}_{h w}, \mathbf{d}_{h^{\prime} w^{\prime}}^{\prime} ; s_{h w h^{\prime} w^{\prime}}\right)</script><p>其中<script type="math/tex">l_{d}</script>为<code>Hinge-loss</code>（合页损失函数，用于SVM，如支持向量的软间隔，可以保证最后解的稀疏性）；</p><script type="math/tex; mode=display">l_{d}\left(\mathbf{d}, \mathbf{d}^{\prime} ; s\right)=\lambda_{d} * s * \max \left(0, m_{p}-\mathbf{d}^{T} \mathbf{d}^{\prime}\right)+(1-s) * \max \left(0, \mathbf{d}^{T} \mathbf{d}^{\prime}-m_{n}\right)</script><p>同时指示函数为<script type="math/tex">s_{h w h^{\prime} w^{\prime}}</script>,$S$表示所有正确匹配对集合:</p><script type="math/tex; mode=display">s_{h w h^{\prime} w^{\prime}}=\left\{\begin{array}{ll}{1,} & {\text { if }\left\|\widehat{\mathcal{H} \mathbf{p}_{h w}}-\mathbf{p}_{h^{\prime} w^{\prime}}\right\| \leq 8} \\ {0,} & {\text { otherwise }}\end{array}\right.</script><h1 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h1><p><img alt="fig2" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_2.png"></p><p>本文一共设计了两个网络，一个是<code>BaseDetector</code>，用于检测角点（注意，此处提取的并不是最终输出的特征点，可以理解为候选的特征点），另一个是<code>SuperPoint</code>网络，输出特征点和描述子。</p><p>网络的训练共分为三个步骤：</p><ol><li>第一步是采用虚拟的三维物体作为数据集，训练网络去提取角点，这里得到的是<code>BaseDetector</code>即，<code>MagicPoint</code>；</li><li>使用真实场景图片，用第一步训练出来的网络<code>MagicPoint</code> +<code>Homographic Adaptation</code>提取角点，这一步称作兴趣点自标注（Interest Point Self-Labeling）</li><li>对第二步使用的图片进行几何变换得到新的图片，这样就有了已知位姿关系的图片对，把这两张图片输入SuperPoint网络，提取特征点和描述子。</li></ol><h2 id="预训练Magic-Point"><a href="#预训练Magic-Point" class="headerlink" title="预训练Magic Point"></a>预训练Magic Point</h2><p>此处参考作者之前发表的一篇论文<strong>[<a href="https://arxiv.org/abs/1707.07410" target="_blank" rel="noopener">Toward Geometric Deep SLAM</a>]</strong>，其实就是<code>MagicPoint</code>，在此不做展开介绍。<br><img alt="fig2" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_10_magicPoint1.png"></p><p><img alt="fig4" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_4.png"></p><h2 id="Homographic-Adaptation"><a href="#Homographic-Adaptation" class="headerlink" title="Homographic Adaptation"></a>Homographic Adaptation</h2><p>算法在虚拟数据集上表现极其优秀，但是在真实场景下表示没有达到预期，此时本文进行了<code>Homographic Adaptation</code>。<br>作者使用的数据集是<code>MS-COCO</code>，为了使网络的泛化能力更强，本文不仅使用原始了原始图片，而且对每张图片进行随机的旋转和缩放形成新的图片，新的图片也被用来进行识别。这一步其实就类似于训练里常用的数据增强。经过一系列的单映变换之后特征点的复检率以及普适性得以增强。值得注意的是，在实际训练时，这里采用了迭代使用单映变换的方式，例如使用优化后的特征点检测器重新进行单映变换进行训练，然后又可以得到更新后的检测器，如此迭代优化，这就是所谓的self-supervisd。<br><img alt="fig5" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_5.png"></p><p><img alt="fig_9_HA" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_9_HA.png"></p><p>最后的关键点检测器，即<script type="math/tex">\hat{F}\left(I ; f_{\theta}\right)</script>，可以表示为再所有随机单映变换/反变换的聚合：</p><script type="math/tex; mode=display">\hat{F}\left(I ; f_{\theta}\right)=\frac{1}{N_{h}} \sum_{i=1}^{N_{h}} \mathcal{H}_{i}^{-1} f_{\theta}\left(\mathcal{H}_{i}(I)\right)</script><p><img alt="fig_6" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_6.png"></p><h2 id="构建残差，迭代优化描述子以及检测器"><a href="#构建残差，迭代优化描述子以及检测器" class="headerlink" title="构建残差，迭代优化描述子以及检测器"></a>构建残差，迭代优化描述子以及检测器</h2><p>利用上面网络得到的关键点位置以及描述子表示构建残差，利用<code>ADAM</code>进行优化。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><img alt="fig_8" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/fig_8.png"></p><p><img alt="tab_3" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/tab_3.png"></p><p><img alt="tab_4" data-src="https://vincentqin.gitee.io/blogresource-1/superpoint/tab_4.png"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol><li>it is possible to transfer knowledge from a synthetic dataset onto real-world images</li><li>sparse interest point detection and description can be cast as a single, efficient convolutional neural network</li><li>the resulting system works well for geometric computer vision matching tasks such as Homography Estimation</li></ol><p>未来工作:</p><ol><li>研究Homographic Adaptation能否在语义分割任务或者目标检测任务中有提升作用</li><li>兴趣点提取以及描述这两个任务是如何影响彼此的</li></ol><p>作者最后提到，他相信该网络能够解决SLAM或者SfM领域的数据关联<em>，并且</em><code>learning-based</code>前端可以使得诸如机器人或者AR等应用获得更加鲁棒。</p>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> 特征提取 </tag>
            
            <tag> SuperPoint </tag>
            
            <tag> MagicLeap </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bygones</title>
      <link href="/posts/bygones/"/>
      <url>/posts/bygones/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><!-- <div id="dplayer1" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer1"),"loop":true,"video":{"url":"https://www.bilibili.com/video/av92924509/"},"danmaku":{"id":"bbe4286bf164ef6a1497f18a7b42ff944e684b821","api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script> --><iframe src="//player.bilibili.com/player.html?aid=92924509&cid=158651721&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><a id="more"></a><p><strong><center>致我们逝去的青春！</center></strong></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Black Hole</title>
      <link href="/posts/first-black-hole/"/>
      <url>/posts/first-black-hole/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/first-black-hole/big-blackhole.png"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Awesome CV Works</title>
      <link href="/posts/awesome-works/"/>
      <url>/posts/awesome-works/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>The post contains papers-with-code about SLAM, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, Machine Learning, Deep Learning etc. <a href="https://github.com/Vincentqyw/Recent-Stars-2019" target="_blank" rel="noopener"><img alt="GitHub stars" data-src="https://img.shields.io/github/stars/Vincentqyw/Recent-Stars-2019.svg?logo=github&amp;label=Stars"></a></p><a id="more"></a><h2 id="SLAM-related"><a href="#SLAM-related" class="headerlink" title="SLAM related"></a>SLAM related</h2><ul><li><a href="https://github.com/kiran-mohan/SLAM-Algorithms-Octave" target="_blank" rel="noopener">Solutions to assignments of Robot Mapping Course WS 2013/14 by Dr. Cyrill Stachniss at University of Freiburg</a>,SLAM算法学习课后作业答案</li><li><a href="https://github.com/RonaldSun/VI-Stereo-DSO" target="_blank" rel="noopener">Direct sparse odometry combined with stereo cameras and IMU</a>,双目DSO+IMU</li><li><a href="https://github.com/HorizonAD/stereo_dso" target="_blank" rel="noopener">Direct Sparse Odometry with Stereo Cameras</a>,双目DSO</li><li><a href="https://github.com/uoip/g2opy" target="_blank" rel="noopener">Python binding of SLAM graph optimization framework g2o</a>,python版本的g2o实现</li><li><a href="https://github.com/mihaidusmanu/d2-net" target="_blank" rel="noopener">D2-Net: A Trainable CNN for Joint Description and Detection of Local Features</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1905.03561" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://dsmn.ml/publications/d2-net.html" target="_blank" rel="noopener">Project Page</a>]</strong>, 深度学习描述子</li><li><a href="https://github.com/ethz-asl/orb_slam_2_ros" target="_blank" rel="noopener">ROS interface for ORBSLAM2</a>,ROS版本的ORBSLAM2</li><li><a href="https://github.com/yan99033/CNN-SVO" target="_blank" rel="noopener">CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction</a>， <strong>[<a href="https://arxiv.org/pdf/1810.01011.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/ManiiXu/VINS-Mono-Learning" target="_blank" rel="noopener">VINS-Mono-Learning</a>，代码注释版VINS-Mono，初学者学习</li><li><a href="https://github.com/xdspacelab/openvslam" target="_blank" rel="noopener">OpenVSLAM: Versatile Visual SLAM Framework</a>,  <strong>[<a href="https://openvslam.readthedocs.io/" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/fabianschenk/RESLAM" target="_blank" rel="noopener">RESLAM: A real-time robust edge-based SLAM system</a>, <strong>[<a href="https://github.com/fabianschenk/fabianschenk.github.io/raw/master/files/schenk_icra_2019.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/rubengooj/pl-slam" target="_blank" rel="noopener">PL-SLAM: a Stereo SLAM System through the Combination of Points and Line Segments</a>, <strong>[<a href="https://arxiv.org/abs/1705.09479" target="_blank" rel="noopener">Paper</a>]</strong>，线特征SLAM</li><li><a href="https://github.com/YipuZhao/GF_PL_SLAM" target="_blank" rel="noopener">Good Line Cutting: towards Accurate Pose Tracking of Line-assisted VO/VSLAM</a>, ECCV 2018, <strong>[<a href="https://sites.google.com/site/zhaoyipu/good-feature-visual-slam" target="_blank" rel="noopener">Project Page</a>]</strong>, 改进的PL-SLAM</li><li><a href="https://github.com/leoshine/Spherical_Regression" target="_blank" rel="noopener">Spherical Regression: Learning Viewpoints, Surface Normals and 3D Rotations on n-Spheres</a>, CVPR 2019, <strong>[<a href="http://arxiv.org/abs/1904.05404" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/icsl-Jeon/traj_gen_vis" target="_blank" rel="noopener">svo_edgelet</a>, 在线轨迹生成</li><li><a href="https://github.com/TimboKZ/caltech_samaritan" target="_blank" rel="noopener">Drone SLAM project for Caltech’s ME 134 Autonomy class</a>, <strong>[<a href="https://github.com/TimboKZ/caltech_samaritan/blob/master/CS134_Final_Project_Report.pdf" target="_blank" rel="noopener">PDF</a>]</strong></li><li><a href="https://github.com/icsl-Jeon/traj_gen_vis" target="_blank" rel="noopener">Online Trajectory Generation of a MAV for Chasing a Moving Target in 3D Dense Environments</a>, <strong>[<a href="https://arxiv.org/pdf/1904.03421.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/AtsushiSakai/PythonRobotics" target="_blank" rel="noopener">PythonRobotics</a>,<strong>[<a href="https://arxiv.org/abs/1808.10703" target="_blank" rel="noopener">Paper</a>]</strong>, <a href="https://github.com/onlytailei/CppRobotics" target="_blank" rel="noopener">CppRobotics</a></li><li><a href="https://github.com/izhengfan/ba_demo_ceres" target="_blank" rel="noopener">Bundle adjustment demo using Ceres Solver</a>,  <strong>[<a href="https://fzheng.me/2018/01/23/ba-demo-ceres/" target="_blank" rel="noopener">Blog</a>]</strong>, ceres实现BA</li><li><a href="https://github.com/shichaoy/cube_slam" target="_blank" rel="noopener">CubeSLAM: Monocular 3D Object Detection and SLAM</a>, <strong>[<a href="https://arxiv.org/abs/1806.00557" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/sshaoshuai/PointRCNN" target="_blank" rel="noopener">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.04244" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/nrupatunga/GIST-global-Image-Descripor" target="_blank" rel="noopener">GIST-Global Image Descriptor</a>, GIST描述子</li><li><a href="https://github.com/ethz-asl/mav_voxblox_planning" target="_blank" rel="noopener">mav voxblox planning</a>, MAV planning tools using voxblox as the map representation.</li><li><a href="https://github.com/zziz/kalman-filter" target="_blank" rel="noopener">Python Kalman Filter</a>, 30行实现卡尔曼滤波</li><li><a href="https://github.com/arpg/vicalib" target="_blank" rel="noopener">vicalib</a>, 视觉惯导系统标定工具</li><li><a href="https://github.com/simondlevy/BreezySLAM" target="_blank" rel="noopener">BreezySLAM</a>, 基于雷达的SLAM，支持Python(&amp;Matlab, C++, and Java)</li><li><a href="https://github.com/Yvon-Shong/Probabilistic-Robotics" target="_blank" rel="noopener">Probabilistic-Robotics</a>, 《概率机器人》中文版，书和课后习题</li><li><a href="https://github.com/emmjaykay/stanford_self_driving_car_code" target="_blank" rel="noopener">Stanford Self Driving Car Code</a>, <strong>[<a href="http://robots.stanford.edu/papers/junior08.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, 斯坦福自动驾驶车代码</li><li><a href="https://github.com/ndrplz/self-driving-car" target="_blank" rel="noopener">Udacity Self-Driving Car Engineer Nanodegree projects</a></li><li><a href="https://github.com/TUMFTM/Lecture_AI_in_Automotive_Technology" target="_blank" rel="noopener">Artificial Intelligence in Automotive Technology</a>, TUM自动驾驶技术中的人工智能课程</li><li><a href="https://github.com/hlzz/DeepMatchVO" target="_blank" rel="noopener">DeepMatchVO: Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation</a>,ICRA 2019, <strong>[<a href="https://arxiv.org/abs/1902.09103" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/zdzhaoyong/GSLAM" target="_blank" rel="noopener">GSLAM: A General SLAM Framework and Benchmark</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1902.07995" target="_blank" rel="noopener">Paper</a>]</strong>, 集成了各种传感器输入的SLAM统一框架</li><li><a href="https://github.com/izhengfan/se2lam" target="_blank" rel="noopener">Visual-Odometric Localization and Mapping for Ground Vehicles Using SE(2)-XYZ Constraints</a>，ICRA 2019,基于SE(2)-XYZ约束的VO系统</li><li><a href="https://github.com/nicolov/simple_slam_loop_closure" target="_blank" rel="noopener">Simple bag-of-words loop closure for visual SLAM</a>, <strong>[<a href="https://nicolovaligi.com/bag-of-words-loop-closure-visual-slam.html" target="_blank" rel="noopener">Blog</a>]</strong>, 回环</li><li><a href="https://github.com/rmsalinas/fbow" target="_blank" rel="noopener">FBOW (Fast Bag of Words), an extremmely optimized version of the DBow2/DBow3 libraries</a>,优化版本的DBow2/DBow3</li><li><a href="https://github.com/tomas789/tonav" target="_blank" rel="noopener">Multi-State Constraint Kalman Filter (MSCKF) for Vision-aided Inertial Navigation(master’s thesis)</a></li><li><a href="https://github.com/yuzhou42/MSCKF" target="_blank" rel="noopener">MSCKF</a>, MSCKF中文注释版</li><li><a href="https://github.com/hbtang/calibcamodo" target="_blank" rel="noopener">Calibration algorithm for a camera odometry system</a>, VO系统的标定程序</li><li><a href="https://github.com/cggos/vins_mono_cg" target="_blank" rel="noopener">Modified version of VINS-Mono</a>, 注释版本VINS Mono</li><li><a href="https://github.com/zhenpeiyang/RelativePose" target="_blank" rel="noopener">Extreme Relative Pose Estimation for RGB-D Scans via Scene Completion</a>,<strong>[<a href="https://arxiv.org/abs/1901.00063" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/jessecw/EPnP_Eigen" target="_blank" rel="noopener">Implementation of EPnP algorithm with Eigen</a>,利用Eigen编写的EPnP</li><li><a href="https://github.com/jiexiong2016/GCNv2_SLAM" target="_blank" rel="noopener">Real-time SLAM system with deep features</a>, 深度学习描述子(ORB vs. GCNv2)</li><li><a href="https://github.com/Huangying-Zhan/Depth-VO-Feat" target="_blank" rel="noopener">Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction</a>, CVPR 2018, 无监督单目深度恢复以及VO</li><li><a href="https://github.com/Phylliida/orbslam-windows" target="_blank" rel="noopener">ORB-SLAM-windows</a>, Windows版本的ORB-SLAM</li><li><a href="https://github.com/danping/structvio" target="_blank" rel="noopener">StructVIO : Visual-inertial Odometry with Structural Regularity of Man-made Environments</a>,<strong>[<a href="http://drone.sjtu.edu.cn/dpzou/project/structvio.html" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/irvingzhang/KalmanFiltering" target="_blank" rel="noopener">KalmanFiltering</a>, 各种卡尔曼滤波器的demo</li><li><a href="https://github.com/ZhenghaoFei/visual_odom" target="_blank" rel="noopener">Stereo Odometry based on careful Feature selection and Tracking</a>, <strong>[<a href="https://lamor.fer.hr/images/50020776/Cvisic2017.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, C++ OpenCV实现SOFT</li><li><a href="https://github.com/dzunigan/zSLAM" target="_blank" rel="noopener">Visual SLAM with RGB-D Cameras based on Pose Graph Optimization</a></li><li><a href="https://github.com/drsrinathsridhar/GRANSAC" target="_blank" rel="noopener">Multi-threaded generic RANSAC implemetation</a>, 多线程RANSAC</li><li><a href="https://github.com/PyojinKim/OPVO" target="_blank" rel="noopener">Visual Odometry with Drift-Free Rotation Estimation Using Indoor Scene Regularities</a>, BMVC 2017, <strong>[<a href="http://pyojinkim.me/pub/Visual-Odometry-with-Drift-Free-Rotation-Estimation-Using-Indoor-Scene-Regularities/" target="_blank" rel="noopener">Project Page</a>]</strong>，利用平面正交信息进行VO</li><li><a href="https://github.com/baidu/ICE-BA" target="_blank" rel="noopener">ICE-BA</a>, CVPR 2018, <strong>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_ICE-BA_Incremental_Consistent_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/AIBluefisher/GraphSfM" target="_blank" rel="noopener">GraphSfM: Robust and Efficient Graph-based Structure from Motion</a>, <strong>[<a href="https://aibluefisher.github.io/GraphSfM/" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/cuitaixiang/LOAM_NOTED" target="_blank" rel="noopener">LOAM_NOTED</a>, loam中文注解版</li><li><a href="https://github.com/Ethan-Zhou/MWO" target="_blank" rel="noopener">Divide and Conquer: Effcient Density-Based Tracking of 3D Sensors in Manhattan Worlds</a>,ACCV 2016,<strong>[<a href="http://users.cecs.anu.edu.au/~u5535909/" target="_blank" rel="noopener">Project Page</a>]</strong>,曼哈顿世界利用深度传感器进行旋转量平移量分离优化</li><li><a href="https://github.com/jstraub/rtmf" target="_blank" rel="noopener">Real-time Manhattan World Rotation Estimation in 3D</a>,IROS 2015,实时曼哈顿世界旋转估计</li></ul><h2 id="Pose-Object-tracking"><a href="#Pose-Object-tracking" class="headerlink" title="Pose/Object tracking"></a>Pose/Object tracking</h2><ul><li><a href="https://github.com/cbsudux/Human-Pose-Estimation-101" target="_blank" rel="noopener">Basics of 2D and 3D Human Pose Estimation</a>,人体姿态估计入门</li><li><a href="https://github.com/OceanPang/Libra_R-CNN" target="_blank" rel="noopener">Libra R-CNN: Towards Balanced Learning for Object Detection</a></li><li><a href="https://github.com/HRNet/HRNet-Object-Detection" target="_blank" rel="noopener">High-resolution networks (HRNets) for object detection</a>, <strong>[<a href="https://arxiv.org/pdf/1904.04514.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/xiaolonw/TimeCycle" target="_blank" rel="noopener">Learning Correspondence from the Cycle-Consistency of Time</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1903.07593" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/zju3dv/pvnet" target="_blank" rel="noopener">PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.11788" target="_blank" rel="noopener">Paper</a>], [<a href="https://zju3dv.github.io/pvnet" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/mkocabas/EpipolarPose" target="_blank" rel="noopener">Self-Supervised Learning of 3D Human Pose using Multi-view Geometry</a>, CVPR 2018, <strong>[<a href="https://arxiv.org/abs/1903.02330" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/vita-epfl/openpifpaf" target="_blank" rel="noopener">PifPaf: Composite Fields for Human Pose Estimation</a>, <strong>[<a href="https://arxiv.org/abs/1903.06593" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" target="_blank" rel="noopener">Deep High-Resolution Representation Learning for Human Pose Estimation</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09212.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://jingdongwang2017.github.io/Projects/HRNet/PoseEstimation.html" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/YuliangXiu/PoseFlow" target="_blank" rel="noopener">PoseFlow: Efficient Online Pose Tracking)</a>, BMVC 2018, <strong>[<a href="https://arxiv.org/abs/1802.00977" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/vana77/Bottom-up-Clustering-Person-Re-identification" target="_blank" rel="noopener">A Bottom-Up Clustering Approach to Unsupervised Person Re-identification</a>，AAAI 2019, 重定位</li><li><a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener">Fast Online Object Tracking and Segmentation: A Unifying Approach</a>,CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1812.05050" target="_blank" rel="noopener">Paper</a>] [<a href="https://youtu.be/I_iOVrcpEBw" target="_blank" rel="noopener">Video</a>] [<a href="http://www.robots.ox.ac.uk/~qwang/SiamMask" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/TuSimple/simpledet" target="_blank" rel="noopener">SimpleDet - A Simple and Versatile Framework for Object Detection and Instance Recognition</a>,<strong>[<a href="https://arxiv.org/abs/1903.05831" target="_blank" rel="noopener">Paper</a>]</strong> </li></ul><h2 id="Depth-Disparity-amp-Flow-estimation"><a href="#Depth-Disparity-amp-Flow-estimation" class="headerlink" title="Depth/Disparity &amp; Flow estimation"></a>Depth/Disparity &amp; Flow estimation</h2><ul><li><a href="https://github.com/muskie82/AR-Depth-cpp" target="_blank" rel="noopener">Fast Depth Densification for Occlusion-aware Augmented Reality</a>, SIGGRAPH-Asia 2018, <strong>[<a href="https://homes.cs.washington.edu/~holynski/publications/occlusion/index.html" target="_blank" rel="noopener">Project Page</a>]</strong>,<a href="https://github.com/facebookresearch/AR-Depth" target="_blank" rel="noopener">another version</a></li><li><a href="https://github.com/CVLAB-Unibo/Learning2AdaptForStereo" target="_blank" rel="noopener">Learning To Adapt For Stereo</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1904.02957" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/JiaRenChang/PSMNet" target="_blank" rel="noopener">Pyramid Stereo Matching Network</a>,<strong>[<a href="https://arxiv.org/abs/1803.08669" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/lelimite4444/BridgeDepthFlow" target="_blank" rel="noopener">Bridging Stereo Matching and Optical Flow via Spatiotemporal Correspondence</a>, <strong>[<a href="https://arxiv.org/abs/1905.09265" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/wvangansbeke/Sparse-Depth-Completion" target="_blank" rel="noopener">Sparse Depth Completion</a>, <strong>[<a href="https://arxiv.org/pdf/1902.05356.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, RGB图像辅助雷达深度估计</li><li><a href="https://github.com/sshan-zhao/GASDA" target="_blank" rel="noopener">GASDA</a>, CVPR 2019, <strong>[<a href="https://sshan-zhao.github.io/papers/gasda.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/xy-guo/MVSNet_pytorch" target="_blank" rel="noopener">MVSNet: Depth Inference for Unstructured Multi-view Stereo</a>, <strong>[<a href="https://arxiv.org/abs/1804.02505" target="_blank" rel="noopener">Paper</a>]</strong>, 非官方实现版本的MVSNet</li><li><a href="https://github.com/HKUST-Aerial-Robotics/Stereo-RCNN" target="_blank" rel="noopener">Stereo R-CNN based 3D Object Detection for Autonomous Driving</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09738.pdf" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo" target="_blank" rel="noopener">Real-time self-adaptive deep stereo</a>, CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1810.05424" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/ialhashim/DenseDepth" target="_blank" rel="noopener">High Quality Monocular Depth Estimation via Transfer Learning</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1812.11941" target="_blank" rel="noopener">Paper</a>]</strong>, <strong>[<a href="https://ialhashim.github.io/publications/index.html" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/xy-guo/GwcNet" target="_blank" rel="noopener">Group-wise Correlation Stereo Network</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1903.04025" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/phuang17/DeepMVS" target="_blank" rel="noopener">DeepMVS: Learning Multi-View Stereopsis</a>, CVPR 2018,<strong>[<a href="https://phuang17.github.io/DeepMVS/index.html" target="_blank" rel="noopener">Project Page</a>]</strong>,多目深度估计</li><li><a href="https://github.com/sampepose/flownet2-tf" target="_blank" rel="noopener">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</a>, CVPR 2017, 深度学习光流恢复</li><li><a href="https://github.com/DLuensch/StereoVision-ADCensus" target="_blank" rel="noopener">StereoVision-ADCensus</a>,深度恢复代码集合(<strong>ADCensus, SGBM, BM</strong>)</li><li><a href="https://github.com/yangguorun/SegStereo" target="_blank" rel="noopener">SegStereo: Exploiting Semantic Information for Disparity Estimation</a>, 探究语义信息在深度估计中的作用</li><li><a href="https://github.com/kuantingchen04/Light-Field-Depth-Estimation" target="_blank" rel="noopener">Light Filed Depth Estimation using GAN</a>，利用GAN进行光场深度恢复</li><li><a href="https://github.com/daniilidis-group/EV-FlowNet" target="_blank" rel="noopener">EV-FlowNet: Self-Supervised Optical Flow for Event-based Cameras</a>,Proceedings of Robotics 2018,<strong>[<a href="https://arxiv.org/abs/1802.06898" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/vt-vl-lab/DF-Net" target="_blank" rel="noopener">DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency</a>, ECCV 2018, <strong>[<a href="https://arxiv.org/abs/1809.01649" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/yzcjtr/GeoNet" target="_blank" rel="noopener">GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose</a>, CVPR 2018, <strong>[<a href="https://arxiv.org/abs/1803.02276" target="_blank" rel="noopener">Paper</a>]</strong> </li></ul><h2 id="3D-amp-Graphic"><a href="#3D-amp-Graphic" class="headerlink" title="3D &amp; Graphic"></a>3D &amp; Graphic</h2><ul><li><a href="https://github.com/PRBonn/refusion" target="_blank" rel="noopener">ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals</a>, <strong>[<a href="https://arxiv.org/pdf/1905.02082.pdf" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/Lotayou/densebody_pytorch" target="_blank" rel="noopener">densebody_pytorch</a>, <strong>[<a href="https://arxiv.org/abs/1903.10153v3" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/svip-lab/PlanarReconstruction" target="_blank" rel="noopener">Single-Image Piece-wise Planar 3D Reconstruction via Associative Embedding</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1902.09777.pdf" target="_blank" rel="noopener">Paper</a>]</strong>, 单目3D重建</li><li><a href="https://github.com/sunset1995/HorizonNet" target="_blank" rel="noopener">HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch Data Augmentation</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/abs/1901.03861" target="_blank" rel="noopener">Paper</a>]</strong>, 深度学习全景转3D</li><li><a href="https://github.com/Microsoft/O-CNN" target="_blank" rel="noopener">Adaptive O-CNN: A Patch-based Deep Representation of 3D Shapes</a>,SIGGRAPH Asia 2018, <strong>[<a href="https://wang-ps.github.io/AO-CNN.html" target="_blank" rel="noopener">Project Page</a>]</strong></li></ul><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><ul><li><a href="https://live.bilibili.com/7332534?visit_id=9ytrx9lpsy80" target="_blank" rel="noopener">End-to-end Adversarial Learning for Generative Conversational Agents</a>，2017，介绍了一种端到端的基于GAN的聊天机器人</li><li><a href="https://github.com/yulunzhang/RNAN" target="_blank" rel="noopener">Residual Non-local Attention Networks for Image Restoration</a>,ICLR 2019.</li><li><a href="https://github.com/HelenMao/MSGAN" target="_blank" rel="noopener">MSGAN: Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis</a>, CVPR 2019,<strong>[<a href="https://arxiv.org/abs/1903.05628" target="_blank" rel="noopener">Paper</a>]</strong></li><li><a href="https://github.com/NVlabs/SPADE" target="_blank" rel="noopener">SPADE: Semantic Image Synthesis with Spatially-Adaptive Normalization</a>,CVPR 2019, <strong>[<a href="https://nvlabs.github.io/SPADE/" target="_blank" rel="noopener">Project Page</a>]</strong></li><li><a href="https://github.com/Oldpan/Faceswap-Deepfake-Pytorch" target="_blank" rel="noopener">Faceswap with Pytorch or DeepFake with Pytorch</a>, 假脸</li></ul><h2 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h2><ul><li><a href="https://github.com/RemoteML/bestofml" target="_blank" rel="noopener">The best resources around Machine Learning</a></li><li><a href="https://github.com/cydonia999/VGGFace2-pytorch" target="_blank" rel="noopener">VGGFace2: A dataset for recognising faces across pose and age</a></li><li><a href="https://github.com/SmirkCao/Lihang" target="_blank" rel="noopener">Statistical learning methods</a></li></ul><h2 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h2><ul><li><a href="https://github.com/ZhaoJ9014/face.evoLVe.PyTorch" target="_blank" rel="noopener">High-Performance Face Recognition Library on PyTorch</a>，人脸识别库</li><li><a href="https://github.com/enggen/Deep-Learning-Coursera" target="_blank" rel="noopener">Deep-Learning-Coursera</a>，深度学习教程（deeplearning.ai）</li></ul><h2 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h2><ul><li><a href="https://github.com/JuliaLang/julia" target="_blank" rel="noopener">Julia</a></li><li><a href="https://github.com/alan-turing-institute/MLJ.jl" target="_blank" rel="noopener">A Julia machine learning framework</a>，一种基于Julia的机器学习框架</li></ul><p><img alt data-src="https://github.com/alan-turing-institute/MLJ.jl/blob/master/doc/two_model_stack.png"></p><h2 id="Collections"><a href="#Collections" class="headerlink" title="Collections"></a>Collections</h2><ul><li><a href="https://github.com/wwxFromTju/awesome-reinforcement-learning-zh" target="_blank" rel="noopener">awesome-reinforcement-learning-zh</a>,强化学习从入门到放弃的资料</li><li><a href="https://github.com/uzh-rpg/event-based_vision_resources" target="_blank" rel="noopener">Event-based Vision Resources</a>，关于事件相机的资源</li><li><a href="https://github.com/DeepTecher/AutonomousVehiclePaper" target="_blank" rel="noopener">AutonomousVehiclePaper</a>，无人驾驶相关论文速递</li><li><a href="https://github.com/wutianyiRosun/Segmentation.X" target="_blank" rel="noopener">Segmentation.X</a>, Segmentation相关论文&amp;代码</li><li><a href="https://github.com/amusi/CVPR2019-Code" target="_blank" rel="noopener">CVPR-2019</a>, CVPR 2019 论文开源项目合集</li><li><a href="https://github.com/kanster/awesome-slam" target="_blank" rel="noopener">awesome-slam</a>, SLAM合集</li><li><a href="https://github.com/tzutalin/awesome-visual-slam" target="_blank" rel="noopener">awesome-visual-slam</a>, 视觉SLAM合集</li><li><a href="https://github.com/zziz/pwc" target="_blank" rel="noopener">Papers with code</a>, 周更论文with代码</li><li><a href="https://github.com/cbsudux/awesome-human-pose-estimation" target="_blank" rel="noopener">Awesome Human Pose Estimation</a>,<a href="https://github.com/nkalavak/awesome-object-pose" target="_blank" rel="noopener">awesome-object-pose</a>, 位姿估计合集</li><li><a href="https://github.com/mrgloom/awesome-semantic-segmentation" target="_blank" rel="noopener">Awesome Semantic Segmentation</a>, 语义分割集合</li><li><a href="https://github.com/mengyuest/iros2018-slam-papers" target="_blank" rel="noopener">IROS2018 SLAM Collections</a>, IROS 2018集合</li><li><a href="https://github.com/TerenceCYJ/VP-SLAM-SC-papers" target="_blank" rel="noopener">VP-SLAM-SC-papers</a>,Visual Positioning &amp; SLAM &amp; Spatial Cognition 论文统计与分析</li><li><a href="https://github.com/HuaizhengZhang/Awesome-System-for-Machine-Learning" target="_blank" rel="noopener">Awesome System for Machine Learning</a></li><li><a href="https://github.com/Thinkgamer/Machine-Learning-With-Python" target="_blank" rel="noopener">Machine-Learning-With-Python</a>, 《机器学习实战》python代码实现</li><li><a href="https://github.com/qqfly/how-to-learn-robotics" target="_blank" rel="noopener">How to learn robotics</a>, 开源机器人学学习指南</li><li><a href="https://github.com/kjw0612/awesome-deep-vision" target="_blank" rel="noopener">Awesome Deep Vision</a>,DL在CV领域的应用</li><li><a href="https://github.com/YapengTian/Single-Image-Super-Resolution" target="_blank" rel="noopener">Single-Image-Super-Resolution</a>, 一个有关<strong>图像超分辨</strong>的合集</li><li><a href="https://github.com/wifity/ai-report" target="_blank" rel="noopener">ai report</a>, AI相关的研究报告</li><li><a href="https://paperswithcode.com/sota" target="_blank" rel="noopener">State-of-the-art papers and code</a>,搜集了目前sota的论文以及代码</li><li><a href="https://github.com/extreme-assistant/cvpr2019" target="_blank" rel="noopener">CVPR 2019 (Papers/Codes/Project/Paper reading)</a></li><li><a href="https://github.com/openMVG/awesome_3DReconstruction_list" target="_blank" rel="noopener">A curated list of papers &amp; resources linked to 3D reconstruction from images</a>,有关三维重建的论文汇总</li><li><a href="https://github.com/nebula-beta/SLAM-Jobs" target="_blank" rel="noopener">SLAM-Jobs</a>, SLAM/SFM求职指南</li></ul><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><ul><li><a href="https://github.com/cszn/DPSR" target="_blank" rel="noopener">Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels</a>,CVPR 2019,超分辨</li><li><a href="https://github.com/lzhbrian/Cool-Fashion-Papers" target="_blank" rel="noopener">Cool Fashion Papers</a>, Cool resources about Fashion + AI.</li><li><a href="https://github.com/nbei/Deep-Flow-Guided-Video-Inpainting" target="_blank" rel="noopener">Deep Flow-Guided Video Inpainting</a>,CVPR 2019, <strong>[<a href="https://arxiv.org/pdf/1806.10447.pdf" target="_blank" rel="noopener">Paper</a>]</strong> ,图像修复</li><li><a href="https://github.com/dbolya/yolact" target="_blank" rel="noopener">YOLACT: Real-time Instance Segmentation</a></li><li><a href="https://github.com/lyl8213/Plate_Recognition-LPRnet" target="_blank" rel="noopener">LPRNet: License Plate Recognition via Deep Neural Networks</a>, <strong>[<a href="https://arxiv.org/pdf/1806.10447.pdf" target="_blank" rel="noopener">Paper</a>]</strong> </li><li><a href="https://github.com/xiaofengShi/CHINESE-OCR" target="_blank" rel="noopener">CHINESE-OCR</a>, 运用tf实现自然场景文字检测</li><li><a href="https://github.com/PerpetualSmile/BeautyCamera" target="_blank" rel="noopener">BeautyCamera</a>, 美颜相机，具有人脸检测、磨皮美白人脸、滤镜、调节图片、摄像功能</li><li><a href="https://github.com/zhengzhugithub/CV-arXiv-Daily" target="_blank" rel="noopener">CV-arXiv-Daily</a>, 分享计算机视觉每天的arXiv文章</li><li>Pluralistic-Inpainting, <a href="https://arxiv.org/abs/1903.04227" target="_blank" rel="noopener">ArXiv</a> | <a href="http://www.chuanxiaz.com/publication/pluralistic/" target="_blank" rel="noopener">Project Page</a> | <a href="http://www.chuanxiaz.com/project/pluralistic/" target="_blank" rel="noopener">Online Demo</a> | <a href="https://www.youtube.com/watch?v=9V7rNoLVmSs" target="_blank" rel="noopener">Video(demo)</a></li><li><a href="https://github.com/Jezzamonn/fourier" target="_blank" rel="noopener">An Interactive Introduction to Fourier Transforms</a>, 超棒的傅里叶变换图形化解释</li><li><a href="https://github.com/datawhalechina/pumpkin-book" target="_blank" rel="noopener">pumpkin-book</a>, 《机器学习》（西瓜书）公式推导解析</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> disparity </tag>
            
            <tag> pose-tracking </tag>
            
            <tag> object-tracking </tag>
            
            <tag> depth-estimation </tag>
            
            <tag> flow-estimation </tag>
            
            <tag> 3D-graphics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开启SSR模式</title>
      <link href="/posts/build-ssr-server/"/>
      <url>/posts/build-ssr-server/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>关于科学上网，食用别人调配的酸酸乳总觉味道不对，自食其力心里才会感到踏实。受<a href="https://newdee.cf/" target="_blank" rel="noopener">Newdee老贼</a>指点，鄙人成功在服务器上开启了酸酸乳服务。详细过程Newdee已经在博文“<a href="https://newdee.cf/posts/1420aa47/" target="_blank" rel="noopener">SS服务器搭建</a>”介绍地相当详细。本人记性不好，遂本文将记录几个关键步骤，以备后续不时之需。<br><strong><font color="#FF0000" face="宋体">注意：本文仅供个人学习使用，不可用于商业或者违法行为！</font></strong></p><a id="more"></a><h2 id="购买服务器"><a href="#购买服务器" class="headerlink" title="购买服务器"></a>购买服务器</h2><ul><li>购买服务器(支持alipay &amp; wechat pay)，地址: <a href="https://www.vultr.com/?ref=7996819" target="_blank" rel="noopener">https://www.vultr.com/</a></li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/vultr.png"></p><ul><li>经过几个步骤：1. Server Location, 2. Server Type, 3. Server Size, 4. Additional Features,5,6,7可以忽略，最后点击右下角的<strong>Deploy New</strong>即可部署。</li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/buy-vultr.png"></p><p>后台是这样的：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/vultr-backend.png"></p><p>然后根据IP以及用户名利用SSH在本地进行远程连接，进行以下步骤。</p><h2 id="安装SSR"><a href="#安装SSR" class="headerlink" title="安装SSR"></a>安装SSR</h2><p>如果是单用户使用，进入服务器直接执行下述命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -N --no-check-certificate https://raw.githubusercontent.com/Vincentqyw/doubi/master/ssr.sh &amp;&amp; chmod +x ssr.sh &amp;&amp; bash ssr.sh</span><br></pre></td></tr></table></figure><p>关于加密协议以及混淆的设置参见下图：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/account.png"></p><p>多用户使用的版本（可配置多个账号）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/ssrmu.sh &amp;&amp; chmod +x ssrmu.sh &amp;&amp; bash ssrmu.sh</span><br></pre></td></tr></table></figure><p>设置完毕之后，后续进行管理直接运行<code>bash ssrmu.sh</code>选择不同的功能项即可。</p><h2 id="封禁某些端口-可选"><a href="#封禁某些端口-可选" class="headerlink" title="封禁某些端口(可选)"></a>封禁某些端口(可选)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -N --no-check-certificate https://raw.githubusercontent.com/Vincentqyw/doubi/master/ban_iptables.sh &amp;&amp; chmod +x ban_iptables.sh &amp;&amp; bash ban_iptables.sh</span><br></pre></td></tr></table></figure><p>选择封禁垃圾邮件端口就行。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/ban-mails.png"></p><h2 id="BBR加速-可选"><a href="#BBR加速-可选" class="headerlink" title="BBR加速(可选)"></a>BBR加速(可选)</h2><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/bbr.png"></p><h2 id="安装SSRR-可选"><a href="#安装SSRR-可选" class="headerlink" title="安装SSRR(可选)"></a>安装SSRR(可选)</h2><p>接下来的链接给出了SSRR的安装教程，不再赘述，<a href="https://gist.github.com/biezhi/45fac901f02f7c867e46aecd41076d70#kcp-%E5%AE%A2%E6%88%B7%E7%AB%AF" target="_blank" rel="noopener">Link</a>。</p><h2 id="建立快照"><a href="#建立快照" class="headerlink" title="建立快照"></a>建立快照</h2><p>建立系统快照就是将系统某个状态下的各种数据记录在一个文件里，下一次新建完主机后恢复快照就能够恢复成之前系统的样子。</p><p>若已有了主机，点击下图所示的<code>Snapshots</code>对该系统建立快照。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/snapshots-step1.png"></p><p>随后就会出现下图所示的页面，在<code>Label</code>一栏输入这个快照的标签，方便区分不同的快照。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/snapshots-step2.png"></p><p>若想对新建的系统恢复以前建立的快照，可以点击上图中的类似于<code>循环</code>的标志。</p><h2 id="不可描述"><a href="#不可描述" class="headerlink" title="不可描述"></a>不可描述</h2><ul><li><a href="https://vincentqin.gitee.io/blogresource-2/build-ssr-server/SSR-WIN-ANDROID-IOS.7z" target="_blank" rel="noopener">不可描述</a></li><li>PC终端可自行挑选，鄙人推荐<a href="https://www.termius.com/windows" target="_blank" rel="noopener">termius</a>, <a href="https://git-scm.com/downloads" target="_blank" rel="noopener">Git Bash</a></li><li>参考链接：<a href="https://gist.github.com/biezhi/45fac901f02f7c867e46aecd41076d70" target="_blank" rel="noopener">ShadowsocksR 协议插件文档</a></li></ul><p>手机端以及电脑端输入对应的IP/端口/混淆/加密等信息进行连接即可。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 科学上网 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>虚实:「未麻的部屋」</title>
      <link href="/posts/recent-status/"/>
      <url>/posts/recent-status/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/recent-status/Perfect-Blue-cover.png"></p><a id="more"></a><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/recent-status/post-2.jpg"></p><p>过去，到底是哪一条支线造就了现在的自己。虚实之间，到底是谁在支配？</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>笔记：李群与李代数求导</title>
      <link href="/posts/LieAlgebra/"/>
      <url>/posts/LieAlgebra/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note primary">            <p>最近一段时间在推$Jacobian$，会用到一些关于李代数求导的知识。参考高博《视觉slam十四讲》一书，在此总结一些常用的关于李群与李代数相关的知识点。</p>          </div><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在SLAM中位姿是未知的，而我们需要解决什么样的相机位姿最符合当前观测数据这样的问题。一种典型的方式是把它构建成一个优化问题，求解最优的$R$,$t$，使得误差最小化。<br>旋转矩阵自身是带有约束的（正交且行列式为 1）。它们作为优化变量时，会引入额外的约束，使优化变得困难。通过李群——李代数间的转换关系，我们希望把位姿估计变成无约束的优化问题，简化求解方式。群（Group）是一种集合加上一种运算的代数结构，李群是指具有连续（光滑）性质的群，李群在相机姿态估计时具有重要意义，接下来主要讨论特殊正交群$SO(n)$与特殊欧式群$SE(n)$。</p><h2 id="特殊正交群与特殊欧式群"><a href="#特殊正交群与特殊欧式群" class="headerlink" title="特殊正交群与特殊欧式群"></a>特殊正交群与特殊欧式群</h2><script type="math/tex; mode=display">SO(n) = \{ \mathbf{R} \in \mathbb{R}^{n \times n} | \mathbf{R R}^T = \mathbf{I}, det(\mathbf{R})=1 \}</script><script type="math/tex; mode=display">SE(3) = \left\{ \mathbf{T} = \left[ {\begin{array}{*{20}{c}} \mathbf{R} & \mathbf{t} \\ \mathbf{0}^T & 1 \end{array}} \right]  \in \mathbb{R}^{4 \times 4} | \mathbf{R} \in SO(3), \mathbf{t} \in \mathbb{R}^3\right\}</script><p>在李群中，我们使用矩阵来表达一个旋转和平移，这存在冗余的自由度。三维空间的旋转只有三自由度，旋转+平移有六自由度。因此，我们希望寻找一个没有冗余自由度（但是相应的存在奇异性）的表示，也就是李代数$\mathfrak{so}(3)$和$\mathfrak{se}(3)$。且无论是旋转还是变换矩阵，它们都是对加法不封闭的，但是对乘法是封闭的。</p><h2 id="李代数的引出"><a href="#李代数的引出" class="headerlink" title="李代数的引出"></a>李代数的引出</h2><p>对于任意旋转矩阵$R$，它必定满足：</p><script type="math/tex; mode=display">\mathbf{R} \mathbf{R}^T＝\mathbf{I}.</script><p>考虑它随时间发生变化，即从$\mathbf{R}$变为$\mathbf{R(t)}$，它仍然满足如下如下等式：</p><script type="math/tex; mode=display">\mathbf{R}(t) \mathbf{R}(t) ^T = \mathbf{I}</script><p>对两侧同时对时间求导数得：</p><script type="math/tex; mode=display">\mathbf{\dot{R}} (t) \mathbf{R} {(t)^T} + \mathbf{R} (t) \mathbf{\dot{R}} {(t)^T} = 0</script><p>则有：</p><script type="math/tex; mode=display">\mathbf{\dot{R}} (t) \mathbf{R} {(t)^T} = - \left(  \mathbf{\dot{R}} (t) \mathbf{R} {(t)^T} \right)^T</script><p>可见$\mathbf{\dot{R}} (t) \mathbf{R} {(t)^T}$是一个反对称矩阵，将其记作$\mathbf{A}$，于是$\mathbf{A}^T=-\mathbf{A}$,所以它主对角线元素必为，而非对角线元素则只有三个自由度。我们一定可以找到一个这样的向量$\mathbf{a}=[a_1, a_2, a_3]^T$使得：</p><script type="math/tex; mode=display">{\mathbf{a}^ \wedge } = \mathbf{A} = \left[ {\begin{array}{*{20}{c}} 0& -a_3 & a_2\\ {a_3}&0& - {a_1}\\  - {a_2}&{a_1}&0 \end{array}} \right]</script><p>其中$^{\wedge}$符号表示由向量转换为矩阵，反之我们也可以用$^{\vee}$符号定义由矩阵转换为向量的方式:</p><script type="math/tex; mode=display">{ \mathbf{A}^ \vee } = \mathbf{a}</script><p>现在，由于$\mathbf{\dot{R}} (t) \mathbf{R} {(t)^T}$是一个反对称矩阵，所以我们一定可以找到一个三维向量$\mathbf{\phi} (t) \in \mathbb{R}^3$与之对应。于是我们有：</p><script type="math/tex; mode=display">\mathbf{ \dot{R} } (t) \mathbf{R}(t)^T = \mathbf{\phi} (t) ^ {\wedge}</script><p>左右各右乘$\mathbf{R}(t)$，由于其正交性，有：</p><script type="math/tex; mode=display">\mathbf{ \dot{R} } (t)  = \mathbf{\phi} (t)^{\wedge} \mathbf{R}(t) =   \left[ {\begin{array}{*{20}{c}} 0&- {\phi _3}&{\phi _2}\\ {\phi _3}&0& - {\phi _1}\\ { - \phi _2}&{\phi _1}&0 \end{array}} \right] \mathbf{R} (t)</script><p>可以看到，每对旋转矩阵求一次导数，只需左乘一个矩阵$\mathbf{\phi} (t)^{\wedge}$即可。由于$\mathbf{\phi} (t)^{\wedge}$反映了的导数性质，故称它在的正切空间(tangent space)上。同时在$t_0$附近，设$\mathbf{\phi}$保持为常数$\mathbf{\phi}(t_0)=\mathbf{\phi}_0$，我们有：</p><script type="math/tex; mode=display">\mathbf{ \dot{R} } (t)  = \mathbf{\phi} (t_0)^{\wedge} \mathbf{R}(t)= \mathbf{\phi}_0^{\wedge} \mathbf{R}(t)</script><p>又因为初始值$\mathbf{ R } (0) = \mathbf{I} $对上式进行求解可得：</p><script type="math/tex; mode=display">\label{eq:so3ode} \mathbf{R}(t) = \exp \left( \mathbf{\phi}_0^{\wedge} t\right) .</script><p>上式描述$\mathbf{R}$在局部的导数关系。</p><h2 id="李代数-mathfrak-so-3"><a href="#李代数-mathfrak-so-3" class="headerlink" title="李代数 $\mathfrak{so}(3)$"></a>李代数 $\mathfrak{so}(3)$</h2><p>上文提及的$\mathbf{\phi}$是一种李代数，$SO(3)$对应的李代数是定义在$\mathbb{R}^3$上的向量，我们记作$\mathbf{\phi}$，它 对应与一个反对称矩阵：</p><script type="math/tex; mode=display">\label{eq:phi} \mathbf{\Phi} = \mathbf{\phi}^{\wedge} = \left[ {\begin{array}{*{20}{c}}     0&{ - \phi _3}&{\phi _2}\\     {\phi _3}&0&{ - \phi _1}\\     { - \phi _2}&{\phi _1}&0     \end{array}} \right] \in \mathbb{R}^{3 \times 3}</script><p>由于它与反对称矩阵关系很紧密，在不引起歧义的情况下，就说的元素是3维向量或者3维反对称矩阵，不加区别：</p><script type="math/tex; mode=display">\bbox[5px,border:2px solid red]{\mathfrak{so}(3) = \left\{ \Phi = \mathbf{\phi^\wedge} \in \mathbb{R}^{3 \times 3} | \mathbf{\phi} \in \mathbb{R}^3 \right\}}</script><h2 id="李代数-mathfrak-se-3"><a href="#李代数-mathfrak-se-3" class="headerlink" title="李代数 $\mathfrak{se}(3)$"></a>李代数 $\mathfrak{se}(3)$</h2><p>$SE(3)$对应的李代数为$\mathfrak{se}(3)$，$\mathfrak{se}(3)$定义在$\mathbb{R}^{6}$空间，其具体形式如下：</p><script type="math/tex; mode=display">\bbox[5px,border:2px solid red]{\mathfrak{se}(3) = \left\{ \mathbf{ \xi } = \left[ \begin{array}{l}     \mathbf{\rho} \\     \mathbf{\phi}      \end{array} \right] \in \mathbb{R}^{6}, \mathbf{\rho} \in \mathbb{R}^{3},\mathbf{\phi} \in \mathfrak{so}(3),\mathbf{\xi}^\wedge  = \left[ {\begin{array}{*{20}{c}}     \mathbf{\phi} ^ \wedge &\mathbf{\rho} \\ \mathbf{0}^T&0 \end{array}} \right] \in \mathbb{R}^{4 \times 4} \right\}}</script><p>$\mathfrak{se}(3)$是一个这样的六维向量，前三维表示平移，记作$\mathbf{\rho}$；后三维表示旋转，记作$\mathbf{\phi}$（有时候这两个参数会反过来，可也可以的）。</p><h2 id="指数映射"><a href="#指数映射" class="headerlink" title="指数映射"></a>指数映射</h2><p>$\mathfrak{so}(3)$以及$\mathfrak{se}(3)$的指数映射分别对应于$SO(3)$以及$SE(3)$，它们之间的转换关系可以由下图表示：</p><p><img alt="lieGroup" data-src="https://vincentqin.gitee.io/blogresource-3/LieAlgebra/lieGroup.png"></p><h2 id="李代数求导"><a href="#李代数求导" class="headerlink" title="李代数求导"></a>李代数求导</h2><h3 id="对旋转矩阵李代数求导"><a href="#对旋转矩阵李代数求导" class="headerlink" title="对旋转矩阵李代数求导"></a>对旋转矩阵李代数求导</h3><p>对$\mathbf{R}$进行一次扰动$\Delta \mathbf{R}$，假设左扰动$\Delta \mathbf{R}$对应的李代数为$ {\boldsymbol \varphi}$，对$ {\boldsymbol \varphi}$求导，得到：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial ({\boldsymbol Rp})}{\partial {\boldsymbol \varphi}}&= \lim_{\boldsymbol \varphi \to 0}\frac{ \overbrace{ \exp ({\boldsymbol \varphi}^{\land}) }^{\color{Red}{可作泰勒展开}} \exp ({\boldsymbol \phi}^{\land}) {\boldsymbol p} - \exp ({\boldsymbol \phi}^{\land}) {\boldsymbol p}}{ {\boldsymbol \varphi} }\\&\approx \lim_{\boldsymbol \varphi \to 0}\frac{({\boldsymbol I} + {\boldsymbol \varphi}^{\land}) \exp ({\boldsymbol \phi}^{\land}) {\boldsymbol p} - \exp ({\boldsymbol \phi}^{\land}) {\boldsymbol p}}{ {\boldsymbol \varphi} }  \\&= \lim_{\boldsymbol \varphi \to 0}\frac{ {\boldsymbol \varphi}^{\land} {\boldsymbol {Rp}} }{ {\boldsymbol \varphi} }  \\&= \lim_{\boldsymbol \varphi \to 0}\frac{ -({\boldsymbol {Rp}})^{\land} {\boldsymbol \varphi} }{ {\boldsymbol \varphi} } \\&= -({\boldsymbol {Rp}})^{\land}\end{aligned}</script><h3 id="变换转矩阵李代数求导"><a href="#变换转矩阵李代数求导" class="headerlink" title="变换转矩阵李代数求导"></a>变换转矩阵李代数求导</h3><p>假设空间点${\boldsymbol p}$经过一次变换${\boldsymbol T}$（对应的李代数为${\boldsymbol \xi}$）后变为 ${\boldsymbol Tp}$ 。当给${\boldsymbol T}$左乘一个扰动$\Delta {\boldsymbol T} = \exp (\delta {\boldsymbol \xi}^{\land})$，设扰动项的李代数为$\delta {\boldsymbol \xi} = [\delta {\boldsymbol \rho}, \delta {\boldsymbol \phi}]^{T}$，有：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial ({\boldsymbol {Tp}})}{\partial \delta{\boldsymbol \xi}}&= \lim_{\delta{\boldsymbol \xi} \to 0}\frac{ \overbrace{ \exp (\delta {\boldsymbol \xi}^{\land}) }^{\color{Red}{可作泰勒展开}}  \exp ({\boldsymbol \xi}^{\land}) {\boldsymbol p} - \exp ({\boldsymbol \xi}^{\land}) {\boldsymbol p}}{ \delta {\boldsymbol \xi} } \\&\approx \lim_{\delta{\boldsymbol \xi} \to 0}\frac{ ({\boldsymbol I} + \delta {\boldsymbol \xi}^{\land}) \exp ({\boldsymbol \xi}^{\land}) {\boldsymbol p} - \exp ({\boldsymbol \xi}^{\land}) {\boldsymbol p} }{ \delta {\boldsymbol \xi} } \\&= \lim_{\delta{\boldsymbol \xi} \to 0}\frac{  \delta {\boldsymbol \xi}^{\land} \exp ({\boldsymbol \xi}^{\land}) {\boldsymbol p}  }{ \delta {\boldsymbol \xi} } \\&= \lim_{\delta{\boldsymbol \xi} \to 0}\frac{\begin{bmatrix} \delta {\boldsymbol \phi}^{\land}  &   \delta {\boldsymbol \rho}     \\     {\boldsymbol 0}^{T}                 &                      1                           \\\end{bmatrix}\begin{bmatrix}   {\boldsymbol {Rp}} +  {\boldsymbol t}     \\                                     1                                \\\end{bmatrix}}{ \delta {\boldsymbol \xi} } \\&= \lim_{\delta{\boldsymbol \xi} \to 0}\frac{\begin{bmatrix}   \delta {\boldsymbol \phi}^{\land} ({\boldsymbol {Rp}} + {\boldsymbol t}) + \delta {\boldsymbol \rho}     \\                                     0                                \\\end{bmatrix}}{ \delta {\boldsymbol \xi} } \\&=\overbrace{\begin{bmatrix} {\boldsymbol I}            &   -({\boldsymbol {Rp}} + {\boldsymbol t})^{\land}    \\ {\boldsymbol 0}^{T}    &     {\boldsymbol 0}^{T}             \\\end{bmatrix}}^{\color{Red}{上式分块求导}}\\&= ({\boldsymbol {Tp}})^{\bigodot}\end{aligned}</script><p>上式中运算符号$\bigodot$的含义：将一个齐次坐标的空间点变换成一个$4 \times 6$的矩阵。</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><h3 id="SE-3-左扰"><a href="#SE-3-左扰" class="headerlink" title="$SE(3)$左扰"></a>$SE(3)$左扰</h3><script type="math/tex; mode=display">\begin{aligned}\rm{exp}\left( {\Delta {\xi}^{\land} } \right){\rm{exp}}\left( {\xi}^{\land}  \right)& \approx \left( {I + {\left[ {\Delta \xi } \right]}_ \times } \right){\rm{exp}}(\xi) \\&= \left( I_{4 \times 4} +\left[\begin{array}{*{20}{c}}{  \left[ \Delta \phi  \right]}_{\times}   &   \Delta \rho    \\0&0\end{array}\right]\right)\left[\begin{array}{*{20}{c}} R   &   t    \\0&1\end{array}\right] \\&=\left[\begin{array}{*{20}{c}}{  \left[ \Delta \phi  \right]}_{\times}+I_{3 \times 3}   &   \Delta \rho    \\0&1\end{array}\right]\left[\begin{array}{*{20}{c}} R   &   t    \\0&1\end{array}\right] \\&=\left[\begin{array}{*{20}{c}} \left( \left[ \Delta \phi  \right]_{\times}+I_{3 \times 3} \right) R  &    \left( \left[ \Delta \phi  \right]_{\times}+I_{3 \times 3} \right) t + \Delta \rho    \\0&1\end{array}\right]\end{aligned}</script><h3 id="SE-3-右扰"><a href="#SE-3-右扰" class="headerlink" title="$SE(3)$右扰"></a>$SE(3)$右扰</h3><script type="math/tex; mode=display">\begin{aligned}\rm{exp}\left( {\xi}^{\land}  \right) \rm{exp}\left( {\Delta {\xi}^{\land} } \right)& \approx {\rm{exp}}({\xi}^{\land}) \left( {I + {\left[ {\Delta \xi } \right]}_ \times } \right)\\&=\left[\begin{array}{*{20}{c}} R   &   t    \\0&1\end{array}\right]\left( I_{4 \times 4} +\left[\begin{array}{*{20}{c}}{  \left[ \Delta \phi  \right]}_{\times}   &   \Delta \rho    \\0&0\end{array}\right]\right)\\&=\left[\begin{array}{*{20}{c}} R   &   t    \\0&1\end{array}\right]\left[\begin{array}{*{20}{c}}{  \left[ \Delta \phi  \right]}_{\times}+I_{3 \times 3}   &   \Delta \rho    \\0&1\end{array}\right] \\&=\left[\begin{array}{*{20}{c}} R \left( \left[ \Delta \phi  \right]_{\times}+I_{3 \times 3} \right)  &   R \Delta \rho + t   \\0&1\end{array}\right]\end{aligned}</script>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> 李代数 </tag>
            
            <tag> computer vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>资料：ORB SLAM2 阅读报告</title>
      <link href="/posts/orb-slam/"/>
      <url>/posts/orb-slam/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>首先，解释下SLAM的概念，借鉴高博《视觉 SLAM 十四讲》中的一句话：SLAM 是 Simultaneous Localization and Mapping 的缩写，中文译作“同时定位与地图构建”。它是指搭载特定传感器的主体，在没有环境先验信息的情况下，于运动过程中建立环境的模型，同时估计自己的运动。如果这里的传感器主要为相机，那就称为“视觉 SLAM”。</p><a id="more"></a><p>先来张图，下图就是利用相机作为传感器在环境中采集一系列的图像，经过SLAM系统建立的点云图以及相机轨迹。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/orb-slam/MH01-traj.gif"></p><p>SLAM自1986年提出之后，一直以来是机器人领域的热点问题。这里总结一些常用开源SLAM方案。</p><style>table th:nth-of-type(1) {    width: 150px;}table th:nth-of-type(2) {    width: 200px;}</style><div class="table-container"><table><thead><tr><th>方案名称</th><th>传感器形式</th><th>地址</th></tr></thead><tbody><tr><td>MonoSLAM</td><td>单目</td><td><a href="https://github.com/hanmekim/SceneLib2" target="_blank" rel="noopener">https://github.com/hanmekim/SceneLib2</a></td></tr><tr><td>PTAM</td><td>单目</td><td><a href="http://www.robots.ox.ac.uk/~gk/PTAM/" target="_blank" rel="noopener">http://www.robots.ox.ac.uk/~gk/PTAM/</a></td></tr><tr><td>ORB-SLAM</td><td>单目</td><td><a href="http://webdiis.unizar.es/~raulmur/orbslam/" target="_blank" rel="noopener">http://webdiis.unizar.es/~raulmur/orbslam/</a></td></tr><tr><td>ORB-SLAM2</td><td>单目/双目/RGB-D</td><td><a href="https://github.com/raulmur/ORB_SLAM2" target="_blank" rel="noopener">https://github.com/raulmur/ORB_SLAM2</a></td></tr><tr><td>LSD-SLAM</td><td>单目为主</td><td><a href="http://vision.in.tum.de/research/vslam/lsdslam" target="_blank" rel="noopener">http://vision.in.tum.de/research/vslam/lsdslam</a></td></tr><tr><td>SVO</td><td>单目</td><td><a href="https://github.com/uzh-rpg/rpg_svo" target="_blank" rel="noopener">https://github.com/uzh-rpg/rpg_svo</a></td></tr><tr><td>DTAM</td><td>RGB-D</td><td><a href="https://github.com/anuranbaka/OpenDTAM" target="_blank" rel="noopener">https://github.com/anuranbaka/OpenDTAM</a></td></tr><tr><td>DVO</td><td>RGB-D</td><td><a href="https://github.com/tum-vision/dvo_slam" target="_blank" rel="noopener">https://github.com/tum-vision/dvo_slam</a></td></tr><tr><td>DSO</td><td>单目</td><td><a href="https://github.com/JakobEngel/dso" target="_blank" rel="noopener">https://github.com/JakobEngel/dso</a></td></tr><tr><td>RTAB-MAP</td><td>双目/RGB-D</td><td><a href="https://github.com/introlab/rtabmap" target="_blank" rel="noopener">https://github.com/introlab/rtabmap</a></td></tr><tr><td>RGBD-SLAM-V2</td><td>RGB-D</td><td><a href="https://github.com/felixendres/rgbdslam_v2" target="_blank" rel="noopener">https://github.com/felixendres/rgbdslam_v2</a></td></tr><tr><td>Elastic Fusion</td><td>RGB-D</td><td><a href="https://github.com/mp3guy/ElasticFusion" target="_blank" rel="noopener">https://github.com/mp3guy/ElasticFusion</a></td></tr><tr><td>Hector SLAM</td><td>激光</td><td><a href="http://wiki.ros.org/hector_slam" target="_blank" rel="noopener">http://wiki.ros.org/hector_slam</a></td></tr><tr><td>GMapping</td><td>激光</td><td><a href="http://wiki.ros.org/gmapping" target="_blank" rel="noopener">http://wiki.ros.org/gmapping</a></td></tr><tr><td>OKVIS</td><td>多目+IMU</td><td><a href="https://github.com/ethz-asl/okvis" target="_blank" rel="noopener">https://github.com/ethz-asl/okvis</a></td></tr><tr><td>ROVIO</td><td>多目+IMU</td><td><a href="https://github.com/ethz-asl/rovio" target="_blank" rel="noopener">https://github.com/ethz-asl/rovio</a></td></tr><tr><td>VINS</td><td>单目+IMU</td><td><a href="https://github.com/HKUST-Aerial-Robotics/VINS-Mono" target="_blank" rel="noopener">https://github.com/HKUST-Aerial-Robotics/VINS-Mono</a></td></tr></tbody></table></div><p>ORB-SLAM应该是SLAM最具有代表性的算法，<a href="https://arxiv.org/abs/1610.06475" target="_blank" rel="noopener">ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras；</a> <a href="https://github.com/raulmur/ORB_SLAM2" target="_blank" rel="noopener"><strong>code；</strong></a> <a href="http://webdiis.unizar.es/~raulmur/orbslam/" target="_blank" rel="noopener"><strong>主页；</strong></a></p><p>ORB-SLAM 是PTAM 的继承者们中非常有名的一位。它提出于 2015 年，是现代 SLAM 系统中做的非常完善，非常易用的系统之一（如果不是最完善和易用的话）。ORB-SLAM 代表着主流的特征点 SLAM 的一个高峰。相比于之前的工作，ORB-SLAM 具有以下几条明显的优势：</p><ul><li>支持单目、双目、RGB-D 三种模式。这使得无论我们拿到了任何一种常见的传感器，都可以先放到 ORB-SLAM 上测试一下，它具有良好的泛用性。</li><li>整个系统围绕 ORB 特征进行计算，包括视觉里程计与回环检测的 ORB 字典。它体现出 ORB 特征是现阶段计算平台的一种优秀的效率与精度之间的折衷方式。ORB不像 SIFT 或 SURF 那样费时，在 CPU 上面即可实时计算；相比 Harris 角点等简单角点特征，又具有良好的旋转和缩放不变性。并且，ORB 提供描述子，使我们在大范围运动时能够进行回环检测和重定位。</li><li>ORB 的回环检测是它的亮点。优秀的回环检测算法保证了 ORB-SLAM 有效地防止累计误差，并且在丢失之后还能迅速找回，这在许多现有的 SLAM 系统中都不够完善。为此，ORB-SLAM 在运行之前必须加载一个很大的 ORB 字典文件。</li><li>ORB-SLAM 创新式地使用了三个线程完成 SLAM：实时跟踪特征点的 Tracking 线程，局部 Bundle Adjustment 的优化线程（Co-visibility Graph，俗称小图），以及全局 Pose Graph 的回环检测与优化线程（Essential Graph 俗称大图）。其中，Tracking线程负责对每张新来的图像提取 ORB 特征点，并与最近的关键帧进行比较，计算特征点的位置并粗略估计相机位姿。小图线程求解一个Bundle Adjustment 问题，它包括局部空间内的特征点与相机位姿。这个线程负责求解更精细的相机位姿与特征点空间位置。不过，仅有前两个线程，只完成了一个比较好的视觉里程计。第三个线程，也就是大图线程，对全局的地图与关键帧进行回环检测，消除累积误差。由于全局地图中的地图点太多，所以这个线程的优化不包括地图点，而只有相机位姿组成的位姿图。继 PTAM 的双线程结构之后，ORB-SLAM 的三线程结构取得了非常好的跟踪和建图效果，能够保证轨迹与地图的全局一致性。这种三线程结构亦将被后续的研究者认同和采用。</li><li>ORB-SLAM围绕特征点进行了不少的优化。例如，在OpenCV的特征提取基础上保证了特征点的均匀分布；在优化位姿时使用了一种循环优化四遍以得到更多正确匹配的方法；比PTAM更为宽松的关键帧选取策略等等。这些细小的改进使得 ORB-SLAM 具有远超其他方案的鲁棒性：即使对于较差的场景，较差的标定内参，ORB-SLAM 都能够顺利地工作。</li></ul><p>整个ORB-SLAM系统包括三个部分组成，分别是跟踪（Tracking）、局部建图（Local Mapping）以及回环检测（Loop Closing）模块，它们分别被三个线程并行地进行处理。接下来对这个系统进行介绍。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/orb-slam/orb-slam2-mainflow.png"></p><h1 id="理论篇"><a href="#理论篇" class="headerlink" title="理论篇"></a>理论篇</h1><h2 id="跟踪模块"><a href="#跟踪模块" class="headerlink" title="跟踪模块"></a>跟踪模块</h2><p>跟踪（Tracking）是在每帧中粗略地定位相机位姿以及决定何时插入新的关键帧。算法设计了运动模型以及跟踪参考帧模型去大致预测出相机的位姿。如相机跟踪失败（由于遮挡、大幅度运动等），就启动重定位模块对相机进行位置查找。如果已经有了初始位姿以及特征匹配，利用关键帧的Covisibility Graph恢复出局部可见图。之后，局部地图点的匹配可利用重投影实现，随后相机的位姿利用BA来优化。最后，Tracking线程决定是否插入新的关键帧。</p><ul><li>初始位姿估计：利用运动模型或者关键帧模型去预测相机位姿。如果运动模型已经跟踪到了当前帧，会利用引导匹配（Guided Search）在上一帧中寻找地图点。如果没有找到足够的匹配（如，运动模型不适用的情况），我们就在上一帧中更大的范围中寻找地图点。如不满足运动模型条件，导致运动模型失败，则采用参考关键帧模型利用参考帧模型对当前帧进行跟踪。通过以上两个模型即可对相机位姿进行初步定位。</li><li>跟踪局部地图：一旦我们已经估计了相机位姿以及我们得到一系列匹配的特征。我们可以将地图点投影到该帧上以搜索更多的匹配地图点。为了减小计算大图的超大复杂度，我们仅将其投影局部小图。局部地图包括，一系列关键帧<script type="math/tex">K_1</script>，这些关键帧与当前帧共享着相同的地图点；还有与<script type="math/tex">K_1</script>有共视关系的关键帧<script type="math/tex">K_2</script>们。局部图还有一个参考帧<script type="math/tex">K_{ref}</script>，这个关键帧与当前帧有最多的匹配点。</li><li>重定位：当运动模型以及跟踪关键帧失败时，可利用重定位来恢复得到相机位姿。应该从历史关键帧中选取和当前帧相似的图片，对当前帧进行位姿估计以及位姿优化。</li></ul><h2 id="局部建图模块"><a href="#局部建图模块" class="headerlink" title="局部建图模块"></a>局部建图模块</h2><p>局部建图（Local Mapping）的主要任务：当跟踪当前帧成功之后，需要利用局部建图更新其运动模型同时更新地图点。等待跟踪过程判断是否应该插入一个新的关键帧，并把关键帧插入到地图中，并对局部地图进行局部BA优化。这个线程能够获得更为精细的相机位姿以及点云。</p><ul><li>处理关键帧：跟踪成功之后，需要对关键帧进行处理以得到地图。具体而言：从关键帧队列中获得一帧，计算出其特征点的BoW映射向量（表示）。关键帧和其对应的地图点进行绑定，更新地图点的平均观测方向以及观测距离范围。更新关键帧之间的连接关系（共视关系），最后将关键帧插入地图中。</li><li>精选地图点：由于跟踪过程引入地图点的策略较为宽松，此时需要检查最近加入的地图点，并将一些冗余的地图点从最近地图点的列表中剔除。</li><li>创建新地图点：由于上一步已经剔除了一些冗余地图点，该模块需要通过当前关键帧及其共视关键帧利用三角化得到更多高质量的3D地图点并添加地图点的属性。</li><li>Local BA：该步骤通过局部BA优化局部地图点以及局部关键帧的位姿。</li><li>精选关键帧：剔除冗余的关键帧，这样不至于增加后期BA的压力，而且可以保证在相同的环境下，关键帧的数目不会无限制的增长，同时减小存储压力。</li></ul><h2 id="回环检测模块"><a href="#回环检测模块" class="headerlink" title="回环检测模块"></a>回环检测模块</h2><p>回环检测（Loop Closing）的主要目标是检测当前关键帧是否经过历史位置。如有经过，则利用回环检测得到的回环帧去修正整个SLAM长期跟踪过程中带来的累积误差、尺度漂移等。如果仅有前两个线程的话，仅仅完成了一个很好的视觉里程计（VO），这个线程会对全局地图以及关键帧进行回环检测，以消除上述累积误差。</p><ul><li>候选关键帧检测：当前关键帧仅有与历史关键帧足够相似才可能成为回环候选帧，该模块通过一定的筛选策略对当前关键帧进行筛选，判断其是否为闭环候选关键帧。由于在实际闭环检测过程中，回环候选帧及其共视关键帧，在一定连续的时间内都可能被观测到。该模块主要通过利用这一条件，对闭环候选关键帧进一步地筛选，通过筛选条件的候选关键帧将进行下一步的判断。</li><li>相似性变换计算：考虑到单目SLAM的尺度漂移，当前帧和回环帧之间的相对位姿应是一个相似变换，并且，二者之间应具有足够多的匹配点。该模块主要是通过循环计算当前帧和上述经过筛选后的候选关键帧之间的相似变换，直到找到一个和当前帧具有足够多匹配点的相似变换，对应的候选关键帧即为最终的回环帧。</li><li>回环修正：受累积误差的影响，时间越久，越接近当前帧的关键帧及相应的地图点，误差将越大。若寻找到的回环帧，当前帧位姿及其对应的地图点会更精确。该模块就是为了修正累积误差，利用回环帧及其共视关键帧，以及对应的地图点，来修正当前帧及其共视关键帧的位姿以及对应的地图点的世界坐标。紧接着进行地图点融合，更新共视图，然后通过本质图优化相机位姿，最后进行全局BA来修正整个SLAM的累积误差（相机位姿以及地图点）。</li></ul><p>牛吹完了，说下缺点。<br>当然，ORB-SLAM 也存在一些不足之处。首先，由于整个 SLAM 系统都采用特征点进行计算，我们必须对每张图像都计算一遍 ORB 特征，这是非常耗时的。ORB-SLAM 的三线程结构也对 CPU 带来了较重的负担，使得它只有在当前 PC 架构的 CPU 上才能实时运算，移植到嵌入式端则有一定困难。其次，ORB-SLAM 的建图为稀疏特征点，目前还没有开放存储和读取地图后重新定位的功能（虽然从实现上来讲并不困难）。根据我们在建图章节的分析，稀疏特征点地图只能满足我们对定位的需求，而无法提供导航、避障、交互等诸多功能。然而，如果我们仅用 ORB-SLAM 处理定位问题，似乎又嫌它有些过于重量级了。相比之下，另外一些方案提供了更为轻量级的定位，使我们能够在低端的处理器上运行 SLAM，或者让 CPU 有余力处理其他的事务。</p><h1 id="实践篇"><a href="#实践篇" class="headerlink" title="实践篇"></a>实践篇</h1><p>在这里下载ORB-SLAM2的源码，然后参考ORB-SLAM2项目的说明文档，安装一些必要的第三方软件：</p><ul><li>pangolin：<a href="http://eigen.tuxfamily.org/index.php?title=Main_Page" target="_blank" rel="noopener">http://eigen.tuxfamily.org/index.php?title=Main_Page</a></li><li>Eigen：<a href="http://eigen.tuxfamily.org/index.php?title=Main_Page" target="_blank" rel="noopener">http://eigen.tuxfamily.org/index.php?title=Main_Page</a></li><li>opencv 3.4.2 ：<a href="https://blog.csdn.net/haoqimao_hard/article/details/82049565" target="_blank" rel="noopener">https://blog.csdn.net/haoqimao_hard/article/details/82049565</a></li><li>ROS：<a href="http://wiki.ros.org/melodic/Installation/Ubuntu#Ubuntu_install_of_ROS_Melodic" target="_blank" rel="noopener">http://wiki.ros.org/melodic/Installation/Ubuntu#Ubuntu_install_of_ROS_Melodic</a></li></ul><p>注意，其中有坑，务必安装正确。安装好之后顺便在Euroc数据集中的MH01上测试，得到下面的轨迹地图。</p><p><img alt data-src="//www.vincentqin.tech/posts/orb-slam/MH01-traj.png"></p><h2 id="EuRoC数据集"><a href="#EuRoC数据集" class="headerlink" title="EuRoC数据集"></a>EuRoC数据集</h2><p>EuRoC数据集包含11个双目序列，这个序列由小型无人机在两个房间（V1/V2, Vicon Room）以及一个大工厂环境(MH, Machine Hall)中拍摄得到。相机的基线长约为11cm，以20Hz速度拍摄图片。序列被分成了三种（根据MAV的速度，光照以及场景纹理）easy , medium, difficult。<br>$ATE$表示绝对轨迹误差，是衡量相机位姿的标准之一。假设有真实位姿序列：<script type="math/tex">P_1,P_2,P_3,...,P_n</script>以及估计的位姿序列：<script type="math/tex">Q_1,Q_2,Q_3,...,Q_n</script> ，它们已经做了包括时间戳对齐等操作。实际场景中，这两个序列可能有不同的采样率、长度亦或数据可能丢失，此时需要进行数据关联和插值。首先得的在第i时刻的轨迹误差：</p><script type="math/tex; mode=display">F_i := Q_i^{-1}SP_i</script><p>其中$S$是从<script type="math/tex">P_{1:n}</script>到<script type="math/tex">Q_{1:n}</script>的最小二乘刚体变换，通过求取以上误差在所有位置时刻的均方根我们得到APE的具体形式：</p><script type="math/tex; mode=display">RMSE(F_{1:n}):=\left(\frac{1}{n}\sum_{i=1}^n||trans(F_i)||^2\right)^{\frac{1}{2}}</script><p>其中的$trans(·)$表示求取该位姿的平移分量算子。</p><p>文中大部分内容来自网络以及高博十四讲。因本人水平有限，如有错误，谢谢指出。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="http://www.slamcn.org/index.php/%E9%A6%96%E9%A1%B5" target="_blank" rel="noopener">http://www.slamcn.org/index.php/%E9%A6%96%E9%A1%B5</a></li><li><a href="https://blog.csdn.net/qinruiyan/article/details/50918504" target="_blank" rel="noopener">https://blog.csdn.net/qinruiyan/article/details/50918504</a></li><li>Sturm J, Engelhard N, Endres F, et al. A benchmark for the evaluation of RGB-D SLAM systems[C]. Ieee International Conference on Intelligent Robots and Systems. IEEE, 2012:573-580.</li><li>Horn B K P. Closed-form solution of absolute orientation using unit quaternions[J]. J.opt.soc.am.a, 1987, 5(7):1127-1135.</li></ul>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>资料：SLAM草稿</title>
      <link href="/posts/slam/"/>
      <url>/posts/slam/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="ECCV-2018-视觉定位综述"><a href="#ECCV-2018-视觉定位综述" class="headerlink" title="ECCV 2018 视觉定位综述"></a>ECCV 2018 视觉定位综述</h2><p>以下是今年ECCV上几位大牛介绍SLAM技术的tutorial pdfs，涉及基于特征点以及基于学习的SLAM算法介绍，并在最后探究了SLAM领域的主要问题以及未来的发展趋势。</p><ul><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/1.Intro.pdf" target="_blank" rel="noopener">Feature-based vs. Learned Approaches</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/2.Sattler-Feature-Based-3D-Localization.pdf" target="_blank" rel="noopener">Current State of Feature-based Localization</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/3.Learning-Based Localization_Upload.pdf" target="_blank" rel="noopener">Learning-based Localization</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/4.Failure_Cases.pdf" target="_blank" rel="noopener">Failure Cases of Feature-based and Learning-based Methods</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/5.Long_Term_Localization.pdf" target="_blank" rel="noopener">Long-term Localization: Towards Higher-level Scene Understanding</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/6.Learning Problems_Upload.pdf" target="_blank" rel="noopener">Open Problems of Learning-based Methods</a></li></ul><p>整理还未完备，先祭出这几本书&amp;博客，方便随时查看。</p><a id="more"></a><h2 id="教程"><a href="#教程" class="headerlink" title="教程"></a>教程</h2><ul><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/slambook14.pdf" target="_blank" rel="noopener">slambook</a></li><li><a href="https://github.com/gaoxiang12/slambook/tree/master/project/0.3" target="_blank" rel="noopener">slambook code</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/OpenCV3book.pdf" target="_blank" rel="noopener">OpenCV3.0</a></li><li><a href="https://blog.csdn.net/OnafioO/article/details/73175835" target="_blank" rel="noopener">SLAM前世今生</a></li><li><a href="https://github.com/RainerKuemmerle/g2o" target="_blank" rel="noopener">g2o Github</a></li><li><a href="https://vincentqin.gitee.io/blogresource-4/slam/g2o-details.pdf" target="_blank" rel="noopener">g2o details</a></li><li><a href="https://blog.csdn.net/u012700322/article/details/52857244" target="_blank" rel="noopener">g2o译文</a></li><li><a href="https://www.cnblogs.com/gaoxiang12/p/5304272.html" target="_blank" rel="noopener">深入理解图优化与g2o：g2o篇</a></li><li><a href="https://github.com/RainerKuemmerle/g2o" target="_blank" rel="noopener">深入理解图优化与g2o：g2o篇 code</a></li><li><a href="https://me.csdn.net/heyijia0327" target="_blank" rel="noopener">白巧克力亦唯心的博客</a></li><li><a href="http://www.360doc.com/content/17/0718/14/44420101_672315705.shtml" target="_blank" rel="noopener">Graph slam学习</a></li></ul><h2 id="g2o漫谈"><a href="#g2o漫谈" class="headerlink" title="g2o漫谈"></a><a href="https://openslam-org.github.io/g2o.html" target="_blank" rel="noopener">g2o</a>漫谈</h2><p>g2o里面有各种各样的求解器，而它的顶点、边的类型多种多样。通过自定义顶点和边，事实上，只要一个优化问题能够表达成图，就可以用g2o去求解它。常见的，比如bundle adjustment，ICP，数据拟合等。g2o是一个C++项目，其中矩阵数据结构多来自Eigen。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-4/slam/g2o.png"></p><p>先看上部分，SparseOptimizer是我们需要维护的东西，是一个Optimizable Graph，也是一个Hyper Graph。一个SparseOptimizer含有很多个顶点（继承与Base Vertex）和多条边（继承自BaseUnaryEdge，BaseBinaryEdge或BaseMultiEdge）。这些Base Vertex和Base Edge都是抽象的基类，而实际用的顶点和边，都是它们的派生类。</p><p>我们用SparseOptimizer.addVertex 和 SparseOptimizer.addEdge 向图中添加顶点和边，然后调用SpaseOptimizer.optimize来优化。</p><p>在优化前，需要指定我们用的求解器和迭代算法。从图下半部分来看，一个SparseOptimization拥有一个Optimization Algorithm,继承自Gusss-Newton，Levernberg-Marquardt，Powell’s dogleg 三者之一，同时拥有一个Solver，含有俩个部分。一个是SparseBlockMatrix，用于计算稀疏的雅克比和海塞；一个用于计算 <script type="math/tex">H\Delta x = -b</script>，需要一个线性方程的求解器。而这个求解器，可以从PCG，CSparse，Choldmod三者选一。<br>则一共三个步骤：</p><ol><li>选择一个线性方程求解器，从 PCG, CSparse, Choldmod中选</li><li>选择一个 BlockSolver</li><li>选择一个迭代策略，从GN, LM, Doglog中选</li></ol><h3 id="BlockSolver，块求解器"><a href="#BlockSolver，块求解器" class="headerlink" title="BlockSolver，块求解器"></a>BlockSolver，块求解器</h3><p>块求解器是包含线性求解器的存在，之所以是包含，是因为块求解器会构建好线性求解器所需要的矩阵块（也就是<script type="math/tex">H</script>和<script type="math/tex">b</script>，<script type="math/tex">H\Delta x = -b</script>），之后给线性求解器让它进行运算，边的jacobian也就是在这个时候发挥了自己的光和热。</p><p>这里再记录下一个比较容易混淆的问题，也就是在初始化块求解器的时候的参数问题。大部分的例程在初始化块求解器的时候都会使用如下的程序代码：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;g2o::BlockSolver_6_3::LinearSolverType&gt; linearSolver = g2o::make_unique&lt;g2o::LinearSolverCholmod&lt;g2o::BlockSolver_6_3::PoseMatrixType&gt;&gt;();</span><br></pre></td></tr></table></figure></p><p>其中的BlockSolver_6_3有两个参数，分别是6和3，在定义的时候可以看到这是一个模板的重命名（模板类的重命名只能用using）<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">int</span> p, <span class="keyword">int</span> l&gt;  </span><br><span class="line"><span class="keyword">using</span> BlockSolverPL = BlockSolver&lt; BlockSolverTraits&lt;p, l&gt; &gt;;</span><br></pre></td></tr></table></figure></p><p>其中<strong>p代表pose的维度，l表示landmark的维度</strong>，且这里都表示的是增量的维度。</p><h3 id="g2o的顶点（Vertex）"><a href="#g2o的顶点（Vertex）" class="headerlink" title="g2o的顶点（Vertex）"></a>g2o的顶点（Vertex）</h3><ul><li><code>g2o::BaseVertex&lt; D, T &gt;</code> 其中 <code>int D, typename T</code></li></ul><p>首先记录一下定义模板的两个参数D和T，两个类型分别是int和typename的类型，D表示的是维度，g2o源码里面是这个注释的:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> Dimension = D; <span class="comment">//&lt; dimension of the estimate (minimal) in the manifold space</span></span><br></pre></td></tr></table></figure><p>可以看到这个D并非是顶点（更确切的说是状态变量）的维度，而是其在流形空间（manifold）的最小表示，这里一定要区别开；之后是T，源码里面也给出了T的作用:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> T EstimateType;</span><br><span class="line">EstimateType _estimate;</span><br></pre></td></tr></table></figure><p>可以看到，这里T就是顶点（状态变量）的类型。在顶点的继承中，这两个参数是直接面向我们的，所以务必要定义妥当。</p><h3 id="g2o的边（Edge）"><a href="#g2o的边（Edge）" class="headerlink" title="g2o的边（Edge）"></a>g2o的边（Edge）</h3><ul><li><code>g2o::BaseBinaryEdge&lt; D, E, VertexXi, VertexXj &gt;</code> 其中 <code>int D, typename E</code></li></ul><p>首先还是介绍这两个参数，还是从源码上来看：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> Dimension = D;</span><br><span class="line"><span class="keyword">typedef</span> E Measurement;</span><br><span class="line"><span class="keyword">typedef</span> Eigen::Matrix&lt;<span class="keyword">number_t</span>, D, <span class="number">1</span>, Eigen::ColMajor&gt; ErrorVector;</span><br></pre></td></tr></table></figure><p>可以看到，D决定了误差的维度，从映射的角度讲，三维情况下就是2维的，二维的情况下是1维的；然后E是measurement的类型，也就是测量值是什么类型的，这里E就是什么类型的（一般都是Eigen::VectorN表示的，N是自然数）。</p><ul><li><code>typename VertexXi</code>, <code>typename VertexXj</code></li></ul><p>这两个参数就是边连接的两个顶点的类型，这里特别注意一下，这两个必须一定是顶点的类型，也就是继承自<code>BaseVertex</code>等基础类的类！不是顶点的数据类！例如必须是<code>VertexSE3Expmap</code>而不是<code>VertexSE3Expmap</code>的数据类型类<code>SE3Quat</code>。原因的话源码里面也很清楚，因为后面会用到一系列顶点的维度等等的属性，这些属性是数据类型类里面没有的。</p><ul><li><code>_jacobianOplusXi</code>，<code>_jacobianOplusXj</code></li></ul><p>在成员函数<code>linearizeOplus()</code>（线性化函数）中维护着这两个量，<code>_jacobianOplusXi</code>和<code>_jacobianOplusXj</code>就是所谓的雅可比矩阵，如果是二元边的话二者都有；若为一元边，只有<code>_jacobianOplusXi</code>。</p><p>这两个变量本质上是Eigen::Matrix类型的，具体定义在这里：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">typename</span> Eigen::Matrix&lt;<span class="keyword">number_t</span>, D, Di, D==<span class="number">1</span>?Eigen::RowMajor:Eigen::ColMajor&gt;::AlignedMapType JacobianXiOplusType;</span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">typename</span> Eigen::Matrix&lt;<span class="keyword">number_t</span>, D, Dj, D==<span class="number">1</span>?Eigen::RowMajor:Eigen::ColMajor&gt;::AlignedMapType JacobianXjOplusType;</span><br><span class="line">JacobianXiOplusType _jacobianOplusXi;</span><br><span class="line">JacobianXjOplusType _jacobianOplusXj;</span><br></pre></td></tr></table></figure></p><hr><p>to be continued…</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li><a href="https://blog.csdn.net/Hansry/article/details/78080807" target="_blank" rel="noopener">RGB-D SLAM——g2o篇（三）</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> cv </tag>
            
            <tag> AR </tag>
            
            <tag> computer vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>资料：Line Segments Detection</title>
      <link href="/posts/line-segments-detection/"/>
      <url>/posts/line-segments-detection/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>线段检测算法汇总，若无时间看正文，直接到<a href="https://github.com/Vincentqyw/LineSegmentsDetection" target="_blank" rel="noopener">链接</a>看线段检测算法代码集合。</p><a id="more"></a><h2 id="HoughLine"><a href="#HoughLine" class="headerlink" title="HoughLine"></a>HoughLine</h2><p>基于<code>hough</code>变换的线段检测算法。<code>opencv</code>提供了2个基于<code>hough</code>变换的函数：<code>cv::HoughLines()</code>以及<code>cv::HoughLinesP()</code>。其中<code>cv::HoughLines()</code>为<a href="https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#ga46b4e588934f6c8dfd509cc6e0e4545a" target="_blank" rel="noopener">标准霍夫变换</a>，此函数通常不会用到，它经常会被<a href="https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#ga8618180a5948286384e3b7ca02f6feeb" target="_blank" rel="noopener">累积概率霍夫变换</a>函数<code>cv::HoughLinesP()</code>代替。概率霍夫变换函数原型：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> cv::HoughLinesP(InputArray image,</span><br><span class="line">OutputArray lines,</span><br><span class="line"><span class="keyword">double</span> rho,</span><br><span class="line"><span class="keyword">double</span> theta,</span><br><span class="line"><span class="keyword">int</span> threshold,</span><br><span class="line"><span class="keyword">double</span> minLineLength = <span class="number">0</span>,</span><br><span class="line"><span class="keyword">double</span> maxLineGap = <span class="number">0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>注意函数返回值<code>line</code>，它是一个四维向量$(x_1,y_1,x_2,y_2)$，其中$(x_1,y_1)$以及$(x_2,y_2)$分别表示线段的端点坐标。以下给出示例代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/imgproc.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Mat src, dst, color_dst;</span><br><span class="line">    <span class="keyword">if</span>( argc != <span class="number">2</span> || !(src=imread(argv[<span class="number">1</span>], <span class="number">0</span>)).data)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="comment">// 边缘检测转换为二值边缘图</span></span><br><span class="line">    Canny( src, dst, <span class="number">50</span>, <span class="number">200</span>, <span class="number">3</span> );</span><br><span class="line">    cvtColor( dst, color_dst, COLOR_GRAY2BGR );</span><br><span class="line">    <span class="built_in">vector</span>&lt;Vec4i&gt; lines;</span><br><span class="line">    HoughLinesP( dst, lines, <span class="number">1</span>, CV_PI/<span class="number">180</span>, <span class="number">80</span>, <span class="number">30</span>, <span class="number">10</span> );</span><br><span class="line">    <span class="keyword">for</span>( <span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; lines.size(); i++ )</span><br><span class="line">    &#123;</span><br><span class="line">        line( color_dst, Point(lines[i][<span class="number">0</span>], lines[i][<span class="number">1</span>]),</span><br><span class="line">        Point( lines[i][<span class="number">2</span>], lines[i][<span class="number">3</span>]), Scalar(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>), <span class="number">3</span>, <span class="number">8</span> );</span><br><span class="line">    &#125;</span><br><span class="line">    namedWindow( <span class="string">"Source"</span>, <span class="number">1</span> );</span><br><span class="line">    imshow( <span class="string">"Source"</span>, src );</span><br><span class="line">    namedWindow( <span class="string">"Detected Lines"</span>, <span class="number">1</span> );</span><br><span class="line">    imshow( <span class="string">"Detected Lines"</span>, color_dst );</span><br><span class="line">    waitKey(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="LSD"><a href="#LSD" class="headerlink" title="LSD"></a><a href="http://www.ipol.im/pub/art/2012/gjmr-lsd/" target="_blank" rel="noopener">LSD</a></h2><ul><li>论文标题：”LSD: a Line Segment Detector”</li><li>项目主页：<a href="http://www.ipol.im/pub/art/2012/gjmr-lsd/" target="_blank" rel="noopener">http://www.ipol.im/pub/art/2012/gjmr-lsd/</a></li><li>论文地址：<a href="http://www.ipol.im/pub/art/2012/gjmr-lsd/article.pdf" target="_blank" rel="noopener">http://www.ipol.im/pub/art/2012/gjmr-lsd/article.pdf</a></li><li>代码地址：<a href="http://www.ipol.im/pub/art/2012/gjmr-lsd/lsd_1.5.zip" target="_blank" rel="noopener">http://www.ipol.im/pub/art/2012/gjmr-lsd/lsd_1.5.zip</a></li></ul><p>该方法是目前性价比（速度精度）最好的算法，现已经集成到<code>opencv</code>中<a href="https://docs.opencv.org/master/d1/dbd/classcv_1_1line__descriptor_1_1LSDDetector.html" target="_blank" rel="noopener"><code>LSDDetector</code></a>。LSD能够在线性时间内检测到亚像素精度的线段。无需调整参数，适用于各种场景。因为每张图有误检，LSD能够控制误检率。PS：论文此处不介绍了，可以参考<a href="https://blog.csdn.net/chishuideyu/article/details/78081643?locationNum=9&amp;fps=1" target="_blank" rel="noopener">这里</a>。</p><h2 id="LSWMS"><a href="#LSWMS" class="headerlink" title="LSWMS"></a>LSWMS</h2><ul><li>论文标题：”Line segment detection using weighted mean shift procedures on a 2D slice sampling strategy”</li><li>论文地址：<a href="https://www.researchgate.net/profile/Marcos_Nieto3/publication/220654859_Line_segment_detection_using_weighted_mean_shift_procedures_on_a_2D_slice_sampling_strategy/links/56a5d56a08aef91c8c16b1ac.pdf?inViewer=0&amp;origin=publication_detail&amp;pdfJsDownload=0" target="_blank" rel="noopener">https://www.researchgate.net/LSWMS.pdf</a></li><li>代码地址：<a href="https://sourceforge.net/projects/lswms/" target="_blank" rel="noopener">https://sourceforge.net/projects/lswms/</a></li></ul><h2 id="EDline（ED-Edge-Drawing）"><a href="#EDline（ED-Edge-Drawing）" class="headerlink" title="EDline（ED: Edge Drawing）"></a>EDline（ED: Edge Drawing）</h2><ul><li>论文标题：”Edge Drawing: A Combined Real-Time Edge and Segment Detector”</li><li>论文地址：<a href="https://sci-hub.tw/10.1016/j.jvcir.2012.05.004" target="_blank" rel="noopener">https://sci-hub.tw/10.1016/j.jvcir.2012.05.004</a></li><li>代码地址：<a href="https://github.com/mtamburrano/LBD_Descriptor" target="_blank" rel="noopener">https://github.com/mtamburrano/LBD_Descriptor</a></li></ul><h2 id="CannyLines"><a href="#CannyLines" class="headerlink" title="CannyLines"></a><a href="http://cvrs.whu.edu.cn/projects/cannyLines/" target="_blank" rel="noopener">CannyLines</a></h2><ul><li>论文标题：”CannyLines: A Parameter-Free Line Segment Detector”</li><li>项目主页：<a href="http://cvrs.whu.edu.cn/projects/cannyLines/" target="_blank" rel="noopener">http://cvrs.whu.edu.cn/projects/cannyLines/</a></li><li>论文地址：<a href="http://cvrs.whu.edu.cn/projects/cannyLines/papers/CannyLines-ICIP2015.pdf" target="_blank" rel="noopener">http://cvrs.whu.edu.cn/projects/cannyLines/papers/CannyLines-ICIP2015.pdf</a></li><li>代码地址：<a href="http://cvrs.whu.edu.cn/projects/cannyLines/codes/CannyLines-v3.rar" target="_blank" rel="noopener">http://cvrs.whu.edu.cn/projects/cannyLines/codes/CannyLines-v3.rar</a></li></ul><p>本文提出了一种鲁棒的线段检测算法来有效地检测来自输入图像的线段。首先，文章提出了一种无参数的Canny算子，称为Canny（PANYPF），通过自适应地设置传统Canny算子的低阈值和高阈值来鲁棒地从输入图像中提取边缘。然后，提出了有效的像素连接和分割技术，直接从边缘图中收集共线点群，用于基于最小二乘拟合方法拟合初始线段。第三，通过有效的扩展和合并，产生更长、更完整的线段。最后，利用亥姆霍兹原理（Helmholtz Principle）对所有的检测线段检测，主要考虑梯度方向和幅度信息。该算法能够在人工场景中能够获得比LSD以及EDline精度更高以及平均长度更高的线段。</p><h2 id="MCMLSD"><a href="#MCMLSD" class="headerlink" title="MCMLSD"></a>MCMLSD</h2><ul><li>论文标题：”MCMLSD: A Dynamic Programming Approach to Line Segment Detection”</li><li>论文地址：<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Almazan_MCMLSD_A_Dynamic_CVPR_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/papers/Almazan_MCMLSD_A_Dynamic_CVPR_2017_paper.pdf</a></li><li>代码地址：<a href="http://www.elderlab.yorku.ca/?smd_process_download=1&amp;download_id=8423" target="_blank" rel="noopener">http://www.elderlab.yorku.ca/?smd_process_download=1&amp;download_id=8423</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cv </tag>
            
            <tag> line segment </tag>
            
            <tag> detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>装机只是为了换种心情</title>
      <link href="/posts/new-pc/"/>
      <url>/posts/new-pc/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>上周配了一台中档主机，具体配置如下。<br><a id="more"></a></p><div class="table-container"><table><thead><tr><th>配件</th><th>品牌型号</th><th>价格（元）</th></tr></thead><tbody><tr><td>CPU+主板</td><td>AMD R5-2600X 六核盒装处理器搭华硕B450台式机电脑主板CPU套装</td><td>2199</td></tr><tr><td>散热器</td><td>九州风神（ DEEPCOOL） 玄冰400幻彩版CPU风冷散热器</td><td>129</td></tr><tr><td>内存</td><td>影驰 （Galaxy）GAMER DDR4 8GB 2400 频率单条台式机四代内存条</td><td>427</td></tr><tr><td>机械硬盘</td><td>WD/西部数据 WD10SPZX 蓝盘1TB</td><td>289</td></tr><tr><td>固态硬盘</td><td>Teclast/台电 240G M.2 幻影 NVME PCI-E笔记本台式机SSD固态硬盘</td><td>339</td></tr><tr><td>主机箱</td><td>AIGO/爱国者炫影2全侧透水冷电竞机箱DIY组装整机电脑游戏主机箱</td><td>219</td></tr><tr><td>电源</td><td>酷冷至尊（CoolerMaster）额定500W MWE450机箱电源(80PLUS铜牌)</td><td>269</td></tr><tr><td>显卡</td><td>影驰 (Galaxy)GTX 1060 大将 6GB大显存台式机独立电脑显卡</td><td>1539</td></tr><tr><td>显示器</td><td>翔野27英寸144hz电竞显示器2K台式液晶电脑显示屏幕</td><td>1288</td></tr><tr><td>键盘</td><td>IKBC C87c104机械键盘吃鸡游戏 cherry樱桃 青轴</td><td>387</td></tr><tr><td>无线网卡</td><td>TP-LINK TL-WN725N免驱版 迷你USB无线网卡mini</td><td>45</td></tr><tr><td>合计</td><td>-</td><td>7130</td></tr></tbody></table></div><p>就是这些无疑了，上一张全家福：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/new-pc/family.jpg"></p><!--more--><p>B450-PLUS大主板</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/new-pc/matherboard.jpg"></p><p>CPU选用了性价比更高的AMD 2600X，对标于Intel的i7-6900K，详情见<a href="http://www.mydrivers.com/zhuanti/tianti/cpu/" target="_blank" rel="noopener">CPU天梯图</a>。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/new-pc/amd.jpg"></p><p>第一次装机，主机箱和主板之间连接的开机控制线接错了一根，没能一次点亮。幸好我火眼金睛，及时纠正。说到最关心的性能，还行吧，鲁大师跑分32W+。夜深了。下面就是闪亮主机箱：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/new-pc/pc-box.jpg"></p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/new-pc/pc-part.jpg"></p><p>po一张壁纸，嘿嘿~</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/new-pc/desktop.png"></p><p>远远地望去就是这样了：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/new-pc/desktop-all.jpg"></p><p>凑个数：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/new-pc/wall-st.jpg"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 装机 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>资料：那些年我们一起调过的Bug</title>
      <link href="/posts/tips-for-fix-errors/"/>
      <url>/posts/tips-for-fix-errors/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><ul><li><a href="http://www.cnblogs.com/gaoxiang12/p/5244828.html" target="_blank" rel="noopener">图优化</a></li><li><a href="http://www.cnblogs.com/yiyezhai/p/3176725.html" target="_blank" rel="noopener">三维旋转</a></li><li><a href="https://blog.csdn.net/u013390476/article/details/50209603" target="_blank" rel="noopener">C++记录时间</a></li><li><a href="https://blog.csdn.net/u010128736/article/details/52850444" target="_blank" rel="noopener">相机内外参解释</a></li><li><a href="https://www.cnblogs.com/gaochsh/p/6901809.html" target="_blank" rel="noopener">shell echo字符串处理</a></li><li><a href="http://www.ceres-solver.org/installation.html#linux" target="_blank" rel="noopener">ubuntu 14.04 安装 ceres</a></li><li><a href="http://wiki.ros.org/cn/indigo/Installation/Ubuntu" target="_blank" rel="noopener">Ubuntu 14.04 安装 ROS indigo</a></li><li><a href="https://blog.csdn.net/u010472607/article/details/76166008" target="_blank" rel="noopener">Ubuntu 14.04 安装 Cmake 3.9.x</a></li><li><a href="https://blog.csdn.net/youngpan1101/article/details/58027049" target="_blank" rel="noopener">ubuntu 14.04 安装 Opencv 3.2.0</a></li><li><a href="https://blog.csdn.net/a874909657/article/details/79161533" target="_blank" rel="noopener">VMware Ubuntu 无法全屏解决方案</a></li><li><a href="https://blog.csdn.net/poem_qianmo/article/details/30974513" target="_blank" rel="noopener">OpenCV重映射 &amp; SURF特征点检测合辑</a></li><li><a href="https://blog.csdn.net/xiaotanyu13/article/details/8210955" target="_blank" rel="noopener">VS2010控制台程序运行一闪而过的完美解决办法</a></li><li><a href="https://blog.csdn.net/iracer/article/details/51339377" target="_blank" rel="noopener">OpenCV数据持久化: <strong>FileStorage</strong>类的数据存取操作与示例</a></li><li><a href="http://answers.opencv.org/question/121651/fata-error-lapacke_h_path-notfound-when-building-opencv-32/" target="_blank" rel="noopener">fata error: LAPACKE_H_PATH-NOTFOUND when building OpenCV 3.2</a></li><li><a href="https://blog.csdn.net/zyxlinux888/article/details/6358615" target="_blank" rel="noopener">E:Could not get lock /var/lib/apt/lists/lock - open (11: Resource temporarily unavailable)</a></li></ul><a id="more"></a><ul><li><p>Ubuntu下上Matlab运行时terminal会提示一堆错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">libGL error: unable to load driver: vmwgfx_dri.so</span><br><span class="line">libGL error: driver pointer missing</span><br><span class="line">libGL error: failed to load driver: vmwgfx</span><br><span class="line">libGL error: unable to load driver: swrast_dri.so</span><br><span class="line">libGL error: failed to load driver: swrast</span><br><span class="line">X Error of failed request:  BadValue (integer parameter out of range for operation)</span><br><span class="line">  Major opcode of failed request:  155 (GLX)</span><br><span class="line">  Minor opcode of failed request:  3 (X_GLXCreateContext)</span><br><span class="line">  Value in failed request:  0x0</span><br><span class="line">  Serial number of failed request:  31</span><br><span class="line">  Current serial number in output stream:  34</span><br></pre></td></tr></table></figure><p>  <a href="https://bbs.archlinux.org/viewtopic.php?id=154775" target="_blank" rel="noopener">解决方法</a>：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -sf /lib/libstdc++.so.6 /usr/local/MATLAB/R2015a/sys/os/glnxa64/libstdc++.so.6</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> 资料 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bugs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>资料：Eigen与Matlab语句之对应关系</title>
      <link href="/posts/eigen/"/>
      <url>/posts/eigen/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>Eigen是一个仅仅由头函数组成的C++库，支持线性代数，矩阵和矢量运算，数值分析及其相关的算法。</strong></p><p>Eigen中无论是矩阵还是数组、向量，无论是静态矩阵还是动态矩阵都提供默认构造函数，也就是定义这些数据结构时都可以不用提供任何参数，其大小均由运行时来确定。矩阵的构造函数中只提供行列数、元素类型的构造参数，而不提供元素值的构造，对于比较小的、固定长度的向量提供初始化元素的定义。矩阵类型：Eigen中的矩阵类型一般都是用类似<strong>MatrixXXX</strong>来表示，可以根据该名字来判断其数据类型，比如<strong>”d”表示double类型，”f”表示float类型，”i”表示整数，”c”表示复数；Matrix2f，表示的是一个2*2维的，其每个元素都是float类型</strong>。</p><p>我对Matlab再熟悉不过了，这里列出Matlab与Eigen语句的对应关系，以便于快速入门。</p><a id="more"></a><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><ol><li><a href="https://www.cnblogs.com/newneul/p/8256803.html" target="_blank" rel="noopener">Ubuntu 16.04安装Eigen</a></li><li><a href="https://blog.csdn.net/u012428169/article/details/71169546" target="_blank" rel="noopener">vs2013配置Eigen库</a></li></ol><h1 id="Eigen-矩阵定义"><a href="#Eigen-矩阵定义" class="headerlink" title="Eigen 矩阵定义"></a>Eigen 矩阵定义</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;Eigen/Dense&gt;</span><br><span class="line"></span><br><span class="line">Matrix&lt;double, 3, 3&gt; A;               // Fixed rows and cols. Same as Matrix3d.</span><br><span class="line">Matrix&lt;double, 3, Dynamic&gt; B;         // Fixed rows, dynamic cols.</span><br><span class="line">Matrix&lt;double, Dynamic, Dynamic&gt; C;   // Full dynamic. Same as MatrixXd.</span><br><span class="line">Matrix&lt;double, 3, 3, RowMajor&gt; E;     // Row major; default is column-major.</span><br><span class="line">Matrix3f P, Q, R;                     // 3x3 float matrix.</span><br><span class="line">Vector3f x, y, z;                     // 3x1 float matrix.</span><br><span class="line">RowVector3f a, b, c;                  // 1x3 float matrix.</span><br><span class="line">VectorXd v;                           // Dynamic column vector of doubles</span><br><span class="line">// Eigen          // Matlab           // comments</span><br><span class="line">x.size()          // length(x)        // vector size</span><br><span class="line">C.rows()          // size(C,1)        // number of rows</span><br><span class="line">C.cols()          // size(C,2)        // number of columns</span><br><span class="line">x(i)              // x(i+1)           // Matlab is 1-based</span><br><span class="line">C(i,j)            // C(i+1,j+1)       //</span><br></pre></td></tr></table></figure><h1 id="Eigen-基础使用"><a href="#Eigen-基础使用" class="headerlink" title="Eigen 基础使用"></a>Eigen 基础使用</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// Basic usage</span><br><span class="line">// Eigen        // Matlab           // comments</span><br><span class="line">x.size()        // length(x)        // vector size</span><br><span class="line">C.rows()        // size(C,1)        // number of rows</span><br><span class="line">C.cols()        // size(C,2)        // number of columns</span><br><span class="line">x(i)            // x(i+1)           // Matlab is 1-based</span><br><span class="line">C(i, j)         // C(i+1,j+1)       //</span><br><span class="line"></span><br><span class="line">A.resize(4, 4);   // Runtime error if assertions are on.</span><br><span class="line">B.resize(4, 9);   // Runtime error if assertions are on.</span><br><span class="line">A.resize(3, 3);   // Ok; size didn&apos;t change.</span><br><span class="line">B.resize(3, 9);   // Ok; only dynamic cols changed.</span><br><span class="line">                  </span><br><span class="line">A &lt;&lt; 1, 2, 3,     // Initialize A. The elements can also be</span><br><span class="line">     4, 5, 6,     // matrices, which are stacked along cols</span><br><span class="line">     7, 8, 9;     // and then the rows are stacked.</span><br><span class="line">B &lt;&lt; A, A, A;     // B is three horizontally stacked A&apos;s.</span><br><span class="line">A.fill(10);       // Fill A with all 10&apos;s.</span><br></pre></td></tr></table></figure><h1 id="Eigen-矩阵分块"><a href="#Eigen-矩阵分块" class="headerlink" title="Eigen 矩阵分块"></a>Eigen 矩阵分块</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">// Matrix slicing and blocks. All expressions listed here are read/write.</span><br><span class="line">// Templated size versions are faster. Note that Matlab is 1-based (a size N</span><br><span class="line">// vector is x(1)...x(N)).</span><br><span class="line">// Eigen                           // Matlab</span><br><span class="line">x.head(n)                          // x(1:n)</span><br><span class="line">x.head&lt;n&gt;()                        // x(1:n)</span><br><span class="line">x.tail(n)                          // x(end - n + 1: end)</span><br><span class="line">x.tail&lt;n&gt;()                        // x(end - n + 1: end)</span><br><span class="line">x.segment(i, n)                    // x(i+1 : i+n)</span><br><span class="line">x.segment&lt;n&gt;(i)                    // x(i+1 : i+n)</span><br><span class="line">P.block(i, j, rows, cols)          // P(i+1 : i+rows, j+1 : j+cols)</span><br><span class="line">P.block&lt;rows, cols&gt;(i, j)          // P(i+1 : i+rows, j+1 : j+cols)</span><br><span class="line">P.row(i)                           // P(i+1, :)</span><br><span class="line">P.col(j)                           // P(:, j+1)</span><br><span class="line">P.leftCols&lt;cols&gt;()                 // P(:, 1:cols)</span><br><span class="line">P.leftCols(cols)                   // P(:, 1:cols)</span><br><span class="line">P.middleCols&lt;cols&gt;(j)              // P(:, j+1:j+cols)</span><br><span class="line">P.middleCols(j, cols)              // P(:, j+1:j+cols)</span><br><span class="line">P.rightCols&lt;cols&gt;()                // P(:, end-cols+1:end)</span><br><span class="line">P.rightCols(cols)                  // P(:, end-cols+1:end)</span><br><span class="line">P.topRows&lt;rows&gt;()                  // P(1:rows, :)</span><br><span class="line">P.topRows(rows)                    // P(1:rows, :)</span><br><span class="line">P.middleRows&lt;rows&gt;(i)              // P(i+1:i+rows, :)</span><br><span class="line">P.middleRows(i, rows)              // P(i+1:i+rows, :)</span><br><span class="line">P.bottomRows&lt;rows&gt;()               // P(end-rows+1:end, :)</span><br><span class="line">P.bottomRows(rows)                 // P(end-rows+1:end, :)</span><br><span class="line">P.topLeftCorner(rows, cols)        // P(1:rows, 1:cols)</span><br><span class="line">P.topRightCorner(rows, cols)       // P(1:rows, end-cols+1:end)</span><br><span class="line">P.bottomLeftCorner(rows, cols)     // P(end-rows+1:end, 1:cols)</span><br><span class="line">P.bottomRightCorner(rows, cols)    // P(end-rows+1:end, end-cols+1:end)</span><br><span class="line">P.topLeftCorner&lt;rows,cols&gt;()       // P(1:rows, 1:cols)</span><br><span class="line">P.topRightCorner&lt;rows,cols&gt;()      // P(1:rows, end-cols+1:end)</span><br><span class="line">P.bottomLeftCorner&lt;rows,cols&gt;()    // P(end-rows+1:end, 1:cols)</span><br><span class="line">P.bottomRightCorner&lt;rows,cols&gt;()   // P(end-rows+1:end, end-cols+1:end)</span><br></pre></td></tr></table></figure><h1 id="Eigen-矩阵元素交换"><a href="#Eigen-矩阵元素交换" class="headerlink" title="Eigen 矩阵元素交换"></a>Eigen 矩阵元素交换</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// Of particular note is Eigen&apos;s swap function which is highly optimized.</span><br><span class="line">// Eigen                           // Matlab</span><br><span class="line">R.row(i) = P.col(j);               // R(i, :) = P(:, i)</span><br><span class="line">R.col(j1).swap(mat1.col(j2));      // R(:, [j1 j2]) = R(:, [j2, j1])</span><br></pre></td></tr></table></figure><h1 id="Eigen-矩阵转置"><a href="#Eigen-矩阵转置" class="headerlink" title="Eigen 矩阵转置"></a>Eigen 矩阵转置</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// Views, transpose, etc; all read-write except for .adjoint().</span><br><span class="line">// Eigen                           // Matlab</span><br><span class="line">R.adjoint()                        // R&apos;</span><br><span class="line">R.transpose()                      // R.&apos; or conj(R&apos;)</span><br><span class="line">R.diagonal()                       // diag(R)</span><br><span class="line">x.asDiagonal()                     // diag(x)</span><br><span class="line">R.transpose().colwise().reverse(); // rot90(R)</span><br><span class="line">R.conjugate()                      // conj(R)</span><br></pre></td></tr></table></figure><h1 id="Eigen-矩阵乘积"><a href="#Eigen-矩阵乘积" class="headerlink" title="Eigen 矩阵乘积"></a>Eigen 矩阵乘积</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// All the same as Matlab, but matlab doesn&apos;t have *= style operators.</span><br><span class="line">// Matrix-vector.  Matrix-matrix.   Matrix-scalar.</span><br><span class="line">y  = M*x;          R  = P*Q;        R  = P*s;</span><br><span class="line">a  = b*M;          R  = P - Q;      R  = s*P;</span><br><span class="line">a *= M;            R  = P + Q;      R  = P/s;</span><br><span class="line">                   R *= Q;          R  = s*P;</span><br><span class="line">                   R += Q;          R *= s;</span><br><span class="line">                   R -= Q;          R /= s;</span><br></pre></td></tr></table></figure><h1 id="Eigen-矩阵单个元素操作"><a href="#Eigen-矩阵单个元素操作" class="headerlink" title="Eigen 矩阵单个元素操作"></a>Eigen 矩阵单个元素操作</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">// Vectorized operations on each element independently</span><br><span class="line">// Eigen                  // Matlab</span><br><span class="line">R = P.cwiseProduct(Q);    // R = P .* Q</span><br><span class="line">R = P.array() * s.array();// R = P .* s</span><br><span class="line">R = P.cwiseQuotient(Q);   // R = P ./ Q</span><br><span class="line">R = P.array() / Q.array();// R = P ./ Q</span><br><span class="line">R = P.array() + s.array();// R = P + s</span><br><span class="line">R = P.array() - s.array();// R = P - s</span><br><span class="line">R.array() += s;           // R = R + s</span><br><span class="line">R.array() -= s;           // R = R - s</span><br><span class="line">R.array() &lt; Q.array();    // R &lt; Q</span><br><span class="line">R.array() &lt;= Q.array();   // R &lt;= Q</span><br><span class="line">R.cwiseInverse();         // 1 ./ P</span><br><span class="line">R.array().inverse();      // 1 ./ P</span><br><span class="line">R.array().sin()           // sin(P)</span><br><span class="line">R.array().cos()           // cos(P)</span><br><span class="line">R.array().pow(s)          // P .^ s</span><br><span class="line">R.array().square()        // P .^ 2</span><br><span class="line">R.array().cube()          // P .^ 3</span><br><span class="line">R.cwiseSqrt()             // sqrt(P)</span><br><span class="line">R.array().sqrt()          // sqrt(P)</span><br><span class="line">R.array().exp()           // exp(P)</span><br><span class="line">R.array().log()           // log(P)</span><br><span class="line">R.cwiseMax(P)             // max(R, P)</span><br><span class="line">R.array().max(P.array())  // max(R, P)</span><br><span class="line">R.cwiseMin(P)             // min(R, P)</span><br><span class="line">R.array().min(P.array())  // min(R, P)</span><br><span class="line">R.cwiseAbs()              // abs(P)</span><br><span class="line">R.array().abs()           // abs(P)</span><br><span class="line">R.cwiseAbs2()             // abs(P.^2)</span><br><span class="line">R.array().abs2()          // abs(P.^2)</span><br><span class="line">(R.array() &lt; s).select(P,Q);  // (R &lt; s ? P : Q)</span><br></pre></td></tr></table></figure><h1 id="Eigen-矩阵化简"><a href="#Eigen-矩阵化简" class="headerlink" title="Eigen 矩阵化简"></a>Eigen 矩阵化简</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// Reductions.</span><br><span class="line">int r, c;</span><br><span class="line">// Eigen                  // Matlab</span><br><span class="line">R.minCoeff()              // min(R(:))</span><br><span class="line">R.maxCoeff()              // max(R(:))</span><br><span class="line">s = R.minCoeff(&amp;r, &amp;c)    // [s, i] = min(R(:)); [r, c] = ind2sub(size(R), i);</span><br><span class="line">s = R.maxCoeff(&amp;r, &amp;c)    // [s, i] = max(R(:)); [r, c] = ind2sub(size(R), i);</span><br><span class="line">R.sum()                   // sum(R(:))</span><br><span class="line">R.colwise().sum()         // sum(R)</span><br><span class="line">R.rowwise().sum()         // sum(R, 2) or sum(R&apos;)&apos;</span><br><span class="line">R.prod()                  // prod(R(:))</span><br><span class="line">R.colwise().prod()        // prod(R)</span><br><span class="line">R.rowwise().prod()        // prod(R, 2) or prod(R&apos;)&apos;</span><br><span class="line">R.trace()                 // trace(R)</span><br><span class="line">R.all()                   // all(R(:))</span><br><span class="line">R.colwise().all()         // all(R)</span><br><span class="line">R.rowwise().all()         // all(R, 2)</span><br><span class="line">R.any()                   // any(R(:))</span><br><span class="line">R.colwise().any()         // any(R)</span><br><span class="line">R.rowwise().any()         // any(R, 2)</span><br></pre></td></tr></table></figure><h1 id="Eigen-特殊矩阵生成"><a href="#Eigen-特殊矩阵生成" class="headerlink" title="Eigen 特殊矩阵生成"></a>Eigen 特殊矩阵生成</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// Eigen                            // Matlab</span><br><span class="line">MatrixXd::Identity(rows,cols)       // eye(rows,cols)</span><br><span class="line">C.setIdentity(rows,cols)            // C = eye(rows,cols)</span><br><span class="line">MatrixXd::Zero(rows,cols)           // zeros(rows,cols)</span><br><span class="line">C.setZero(rows,cols)                // C = ones(rows,cols)</span><br><span class="line">MatrixXd::Ones(rows,cols)           // ones(rows,cols)</span><br><span class="line">C.setOnes(rows,cols)                // C = ones(rows,cols)</span><br><span class="line">MatrixXd::Random(rows,cols)         // rand(rows,cols)*2-1        // MatrixXd::Random returns uniform random numbers in (-1, 1).</span><br><span class="line">C.setRandom(rows,cols)              // C = rand(rows,cols)*2-1</span><br><span class="line">VectorXd::LinSpaced(size,low,high)  // linspace(low,high,size)&apos;</span><br><span class="line">v.setLinSpaced(size,low,high)       // v = linspace(low,high,size)&apos;</span><br></pre></td></tr></table></figure><h1 id="Eigen-矩阵点乘"><a href="#Eigen-矩阵点乘" class="headerlink" title="Eigen 矩阵点乘"></a>Eigen 矩阵点乘</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// Dot products, norms, etc.</span><br><span class="line">// Eigen                  // Matlab</span><br><span class="line">x.norm()                  // norm(x).    Note that norm(R) doesn&apos;t work in Eigen.</span><br><span class="line">x.squaredNorm()           // dot(x, x)   Note the equivalence is not true for complex</span><br><span class="line">x.dot(y)                  // dot(x, y)</span><br><span class="line">x.cross(y)                // cross(x, y) Requires #include &lt;Eigen/Geometry&gt;</span><br></pre></td></tr></table></figure><h1 id="Eigen-矩阵特征值"><a href="#Eigen-矩阵特征值" class="headerlink" title="Eigen 矩阵特征值"></a>Eigen 矩阵特征值</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// Eigenvalue problems</span><br><span class="line">// Eigen                          // Matlab</span><br><span class="line">A.eigenvalues();                  // eig(A);</span><br><span class="line">EigenSolver&lt;Matrix3d&gt; eig(A);     // [vec val] = eig(A)</span><br><span class="line">eig.eigenvalues();                // diag(val)</span><br><span class="line">eig.eigenvectors();               // vec</span><br><span class="line">// For self-adjoint matrices use SelfAdjointEigenSolver&lt;&gt;</span><br></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="http://eigen.tuxfamily.org/dox/" target="_blank" rel="noopener">Eigen Homepage</a></li><li><a href="http://eigen.tuxfamily.org/dox/AsciiQuickReference.txt" target="_blank" rel="noopener">Eigen与Matlab对应关系</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> Eigen </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>笔记：清华-谷歌人工智能研讨会(Tsinghua-Google AI Symposium)</title>
      <link href="/posts/tsinghua-google-ai/"/>
      <url>/posts/tsinghua-google-ai/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>6 月 28 日，在清华大学人工智能研究院成立仪式暨清华-谷歌 AI 学术研讨会开幕式上，清华大学副校长尤政宣布成立清华大学人工智能研究院。张钹院士担任新研究院的院长，与此同时，谷歌 AI 负责人 Jeff Dean 也成为了清华大学计算机学科顾问委员会委员。</p></blockquote><a id="more"></a><p>毕业季申请了一下Tsinghua和google主办的研讨会（Tsinghua-Google AI Symposium），有幸被选上参会。在此，我就此时研讨会进行一下总结。会议共分为两天进行。会上聚集了多个学术界以及产业界的大牛，其中包括Bo Zhang，Jeff Dean，Fei Fei Li，Bill Freeman等大佬。他们分别在其专业领域进行了主题演讲（Keynote）。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/all-people.jpg"></p><h1 id="Bo-Zhang（张钹院士）"><a href="#Bo-Zhang（张钹院士）" class="headerlink" title="Bo Zhang（张钹院士）"></a>Bo Zhang（张钹院士）</h1><p>出任清华谷歌AI研究院院长的张钹是清华大学计算机系教授，中科院院士。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/zhangbo-2.jpg"></p><p>1958 年毕业于清华大学自动控制系，同年留校任教至今。他在 2011 年获汉堡大学授予的自然科学荣誉博士，并现任微软亚洲研究院技术顾问。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/zhangbo_0.jpg"></p><p>（上图）张钹院士进行名为“Towards A Real Artifical Intelligence”的主题报告</p><blockquote><p>目前的AI并不能进行理解，它不是真正意义上的AI。例如，一个目标识别系统就仅仅是个机械的分类器，它与人类的感知能力大相径庭。因为像这样的分类器仅仅能够区分物体，它并不能真正进行理解以及判定目标。因此呢，目前的AI能够在完美的、静态的、具有明确信息的单任务场景中胜任。为了对其进行拓展，我们需要建立真正意义上的AI。</p></blockquote><p>张钹院士主要参与人工智能、人工神经网络、机器学习等理论研究，以及这些理论应用于模式识别、知识工程与机器人等技术研究。在这些领域，张钹院士已发表 200 多篇学术论文和 5 篇（或章节）专著（中英文版），且专著获得国家教委高等学校出版社颁发的优秀学术专著特等奖。</p><p>张钹院士的科研成果分别获得 ICL 欧洲人工智能奖、国家自然科学三等奖、国家科技进步三等奖、国家教委科技进步一、二等奖、电子工业部科技进步一等奖以及国防科工委科技进步一等奖奖励。此外，张钹院士还参与创建智能技术与系统国家重点实验室，于 1990‐1996 年担任该实验室主任。</p><p>在过去 30 多年中，张钹院士提出问题求解的商空间理论，在商空间数学模型的基础上，提出了多粒度空间之间相互转换、综合与推理的方法。此外，张院士还提出了问题分层求解的计算复杂性分析以及降低复杂性的方法。该理论与相应的新算法已经应用于不同领域，如统计启发式搜索、路径规划的拓扑降维法、基于关系矩阵的时间规划以及多粒度信息融合等，这些新算法均能显著降低计算复杂性。在人工神经网络上，张院士提出基于规划和基于点集覆盖的学习算法。这些自顶向下的结构学习方法比传统的自底向上的搜索方法在许多方面具有显著优越性。</p><p>根据 Google Scholar，张院士引用量最高的研究论文主要关注于神经网络的稳定性分析、神经网络的建模方法等，它们都是在深度学习崛起之前做的研究，由此可见张院士是较早研究这种层级表征模型的研究者。</p><h1 id="Jeff-Dean：快点用深度学习解决问题吧！"><a href="#Jeff-Dean：快点用深度学习解决问题吧！" class="headerlink" title="Jeff Dean：快点用深度学习解决问题吧！"></a>Jeff Dean：快点用深度学习解决问题吧！</h1><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/jeff-dean-3.jpg"></p><p>（上图）Jeff Dean进行名为“Deep Learning to Solve Challenging Problems”的主题报告</p><p>Jeff Dean 大神无需过多介绍，想必大家都了解。这次他又多了一个新身份：受聘新成立的<strong>清华AI研究院学科顾问委员会委员</strong>，所以他也罕见的一身正装范儿。</p><blockquote><p>在此次报告中，我将介绍我们团队正在做的工作：建立高性能，大规模机器学习系统。此次报告涉及特定硬件（Tensor Processing Units，张量处理单元）的设计初衷以及细节，同样地也会介绍诸如TensorFlow这样的软件架构如何被设计成为能够允许ML研究者方便地进行算法实现以解决困难问题的利器。同样也会讨论目前涌现的能够解决困难问题的机器学习算法，它能够让我们无须手动调节参数以及设计算法，取而代之的是让机器去学习这些东西，我们可利用该算法建立自适应以及灵活的机器学习系统。</p></blockquote><p>过去6年来，Google Brain团队一直在研究人工智能中的难题，构建用于机器学习研究的大型计算机系统，并与Google的许多团队合作，将其研究和系统应用于众多Google产品当中。他们已经在计算机视觉，语音识别，语言理解，机器翻译，医疗保健，机器人控制等领域取得了重大进展。谷歌在人工智能领域最终目标是三点：利用人工智能和机器学习让谷歌的产品更加实用（Making products more useful）；帮助企业和外部开发者利用人工智能和机器学习进行创新（Helping others innovate）；为研究人员提供更好的工具，解决人类面临的重大挑战。</p><p>演讲从深度学习热潮的兴起讲起：从2010年开始，深度学习的热度稳步上升，如今Arxiv上发表的机器学习论文增长趋势已经超过了摩尔定律。深度学习在图像和语音识别为代表的一系列任务中取得了越来越卓越的成果，这个概念和技术并不是全新的，但为什么在过去的几年当中实现了极大的突破？这一切都得益于计算力的提升，<strong>在有充分计算力的情况下，深度学习解决问题的精度将大幅超越传统方法</strong>。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/trend.jpg"></p><p>在2008年美国工程院列出的14大“21世纪重大工程难题”中，有5项都能用到深度学习和机器学习，甚至用深度学习和机器学习去解决，包括环境问题、城市基础设施，健康医疗，以及人脑的逆向工程。Jeff Dean本人还添加了两项，他认为不受语言限制获取信息和交流，以及构建灵活通用的AI系统也十分重要，而这两点也需要深度学习。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/jeff-dean-4.jpg"></p><p>（上图）2008：21世纪重大工程难题</p><p>接下来，Jeff Dean重点介绍了一些Google Brain团队已经完成的研究和计算机系统工作，着眼于如何使用深度学习来解决具有挑战性的问题，来证明深度学习的有效性：</p><p><strong>提高城市基础设施方面</strong>，<strong>Waymo</strong>的自动驾驶已经离实际应用越来越近。</p><p><strong>在健康信息学方面</strong>，谷歌用深度学习分析糖尿病视网膜图像，算法的准确率已经超越了人类医生；不仅如此，使用深度学习视网膜图像分析来预测心血管疾病突发风险，获得人体解剖学和疾病变化之间的联系，这是人类医生此前完全不知道的诊断和预测方法，不仅能帮助科学家生成更有针对性的假设，还可能代表了科学发现的新方向。此外，谷歌还与顶级医学院合作使用深度学习分析电子病例，预测患者预后等情况，已经取得了不错的初步成果。</p><p><strong>促进跨语言的交流和信息共享</strong>，有谷歌的神经机器翻译（GNMT），GNMT在多个语种的翻译上平均质量提高50%到80%以上，超过了过去十年的进展，而且谷歌还开放了基于TensorFlow的源代码。Jeff Dean特别提到，谷歌的目标是一百多种语言对之间相互翻译，这是一个非常复杂的工程问题，使用同一个基于神经网络的模型去翻译不同的语种，在工程上大大简化了工作量。</p><p><strong>在人脑逆向工程方面</strong>，谷歌和马克思普朗克研究所等机构合作，从理解大脑神经网络的图像入手，重构生物神经网络。目前，使用马克思普朗克研究所的数据，研究人员已经生成了大约6000亿个体素。他们还提出了一种模拟生成神经网络的算法“Flood Filling Networks”，可以使用原始数据，利用此前的预测，自动跟踪神经传导。</p><p>其他还有使用深度学习预测分子性质，制作更好的药物，开发碳封存方法，管理氮循环……这些问题都能够在更好的科学工具的帮助下实现。而这个帮助科学工具开发的工具，就是谷歌深度学习开源框架TensorFlow：<strong>TensorFlow的目标是成为每个人都可以使用的机器学习平台，成为通用的平台，成为最好的平台</strong>，去更好的促进行业交流和创新。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/tensorflow.jpg"><br>TensorFlow是目前全球最受欢迎的深度学习框架，在中国也有强劲的开发者生态。此前一位参与TensorFlow开发的中国开发者声称，他认为谷歌推广TensorFlow不是为了赚钱，而是很纯粹的为了技术。</p><p>“2017年以前，谷歌并没有在中国展开太多活动。尽管谷歌知道中国市场很大，但很多业务无法展开。即使谷歌的云业务服务器能在中国大陆运行，但是由于阿里巴巴等本土竞争对手也在销售便宜的云计算产品，这使得谷歌难以盈利。但是，我们所有的中国开发者都在等待谷歌来中国，推出更多TensorFlow技术和产品。”</p><p>谷歌当然明白这一点，而包括这次研讨会在内的众多高校活动，将进一步把TensorFlow的用户人群拓展到学生里面。最后，这位谷歌AI的总负责人号召大家都使用深度学习：“深度神经网络和机器学习取得的重大突破，正在解决世界上一些最为重大的挑战；如果你还没有考虑使用深度学习，我几乎可以肯定你应该马上这么做！”</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/more_info.jpg"></p><h1 id="Feifei-Li：机器理解人类，提供医疗环境智能"><a href="#Feifei-Li：机器理解人类，提供医疗环境智能" class="headerlink" title="Feifei Li：机器理解人类，提供医疗环境智能"></a>Feifei Li：机器理解人类，提供医疗环境智能</h1><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/feifeili-2.jpg"></p><p>李飞飞进行名为“Illuminating the Dark Space: Towards Ambient Intelligence in AI-assisted Healthcare”的主题讲座</p><blockquote><p>今天给大家分享的实际上是最近五六年以来一次比较新的探索，虽然大家知道我做的很多研究很多都是计算机领域机器学习的基础科学，但是在应用方面，我们一直坚信“以人为本”的AI，需要对人类的福祉有所帮助。最重视的一个应用领域是医疗健康领域，所以，此次讲座我将给大家分享一下，最近两年我们在医疗健康领域的一些探索，还希望听取大家的意见。</p></blockquote><p>接下来就是此次报告的正文。</p><p>在开始之前，我想向众多的合作方、学生、博士后，特别是在过去的五六年时间里，与我们在 AI 和医疗健康领域一起工作的临床医生表示感谢。除了以上与我列举的这部分人员合作之外，我们还与世界各地的医院合作，包括斯坦福大学Lucile Packard儿童医院以及斯坦福大学医学院。此外，我们还与犹他州山间麦凯迪医院、旧金山Unlock高级中心合作。刚刚，我们还与上海交通大学以及瑞金医院开展了一项令人兴奋的研究合作。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/feifeili-4.jpg"></p><p>李飞飞团队成员</p><h2 id="何为Ambient-Intelligence？"><a href="#何为Ambient-Intelligence？" class="headerlink" title="何为Ambient Intelligence？"></a>何为Ambient Intelligence？</h2><p>在中国和美国，医疗健康都是最受关注的问题。不断提高的成本，是全球医疗健康的主要问题之一。虽然医疗成本不断上涨，但质量并不见得会一定提高。那我们又该如何提高医疗质量呢？削减成本是目前主要的研究和提高的方向。但幸运的是，在过去的十年里，推进医疗方面的工作已取得了很大的成就。</p><p>我们已经看到药物和疫苗上的改进。我们看到了医疗影像的改进，医疗设备等方面巨大的进步。正如我的同事 Jeff Dean 在上午的分享中提到的那样，大数据和人工智能正推动医疗健康特别是诊断方面的进一步发展。此外，精密医学、药物发现相关的治疗选择，正基于机器学习、人工智能取得了新的进展。但是，在过去几年里我关注的医疗健康领域里，有一个往往被多数人忽略的领域，即医疗健康服务的物理空间。如果你考虑到了“医疗”这个词，那么“疗”这个词则非常重要。因为，物理空间指的是临床医生、护士、医生为治疗病人的地方。我们需要在一定的物理环境下通过与患者的互动来提升医疗服务的水准。</p><p>因此，在这段时间里我们在斯坦福大学研究的方向是，赋予医疗物理空间“Ambient Intelligence”的属性。让我先来定义下“Ambient Intelligence”的概念。需要说明的是，我们并非第一个想到这个概念，而这个概念也并非特属于医疗领域。一个可接受的定义是：<strong>未来将是一个环境满足需求的世界，多数情况下我们无需思考，智能也会萦绕空间，就像这个房间里的灯光。你感受不到科技的存在，但它就在那里，帮助我们更好地做一些事情，这就是我们所说的“Ambient Intelligence”</strong>。</p><p>(这里有相关视频介绍（需翻墙）：<a href="https://www.youtube.com/watch?reload=9&amp;v=5RTkhfVIW40" target="_blank" rel="noopener">https://www.youtube.com/watch?reload=9&amp;v=5RTkhfVIW40</a>)</p><p>那么，为什么我们需要变得智能？为什么我们需要提高医疗健康的服务质量？这是因为，执行和操作是临床医生在医疗服务中的一大痛点。在医疗领域，我们通过数百年知识的积累，需要在各种程序中完成预期的操作，而实际上，符合预期的操作并不总是发生。当出现小毛病、疏忽或错误时，就会涉及医疗成本。而这种成本，往往关乎人类的生命。事实上，如果与一年内车祸死亡的人数相比的话，医疗事故引发的死亡人数远远高于前者。所以，这对我们而言是一个非常重要的问题。如果 AI 可以用来帮助解决这个问题，那么这会是一个以人为本的应用。在美国，国家医学研究所每隔几年就会针对医疗服务中出现的人为错误进行深度研究。这是我们思考的起点。</p><p>为什么临床医生会在医疗中犯错误呢？这一切都是靠人的主观意识完成的。在一个高度复杂的环境下，治疗到什么样的程度也是非常复杂，中间有很多步骤和程序，也有很多的不确定性和不可预测性。而且，错误或疏忽等都会导致这些问题的发生。所以，当潜在的错误都可以预测时， 便意味着以上医疗问题都能得以解决。例如，病人可能会从床上掉下来，就需要通过行为活动传感器以检查患者是否坠落。再或者检查是否需要进行手部卫生的处理，与之相关的传感器就被发明出来，试图解决这个问题。此外，还有许多不同类型的本地化解决方案试图缩小医疗健康质量与服务之间的差距。这关键就在于高度本地化。 每当出现一个错误或潜在的缺陷，就需要一个新的解决方案，且不具备可扩展性。这些本地化解决方案有很多不同的情景无法预测、监控。</p><p>那么我们能做些什么呢？有另一种方式可以考虑改善医疗健康的质量。大概五年前，我和斯坦福的同事们就开始跟进一种新的技术浪潮——自动驾驶技术，而这种新技术似乎与医疗健康毫无关系。但事实上，它们是高度相关的。先来看看自动驾驶汽车是如何工作的。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/auto_drive.jpg"></p><p>这是一款配备了智能传感器的汽车，它能够感知从行人，到汽车、物体、路标等的道路环境。而且，一旦它能感知环境，就会将信息输入到后台，你就能利用机器学习算法做出决定和预测，辅助汽车驾驶。所以，我们受到这种思维以及“Ambient Intelligence”概念的启发，想要将 AI 注入到医疗服务的物理空间中，以便我们能够协助执行预期的步骤。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/healthcare_delivery.jpg"></p><p>这是一个医院单元的示意图：由许多传感器覆盖，可以观察不同的医疗服务情况。首先，我们需要通过传感器的性能来改造物理空间，如果是一家（设备）传统的医院，它可能就没有现代化的传感器以帮助收集并将这些潜在的信息传递给算法。接下来，一旦我们收集了数据，我们需要辨认出在这个环境里的活动，无论是手术室、病人康复室，还是在养老院里。</p><p>而辨认出该医疗活动的关键因素在于对人类活动的理解进行可视化。现在，如果你来自计算机视觉领域，那么你可以将医疗应用与计算机视觉的基础科学联系起来。事实上，多年以来，理解人类活动一直是计算机视觉的核心问题。所以，我将展示一些可以帮医疗服务环境提升的基础科学研究。最终我们希望整个医疗数据可以整合到整个医疗生态体系中。</p><h2 id="让AI注入医疗服务的每一环节"><a href="#让AI注入医疗服务的每一环节" class="headerlink" title="让AI注入医疗服务的每一环节"></a>让AI注入医疗服务的每一环节</h2><p>接下来的演讲中，通过展示我们最近的一些工作，我将分享到以下三个研究方向：感知、人类活动识别，以及医疗生态体系。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/feifeili-1.jpg"></p><h3 id="感知"><a href="#感知" class="headerlink" title="感知"></a>感知</h3><p>首先是感知，即将传感器集成到物理空间，并构建一个数据基础架构的过程。我们最近发表了一篇论文，讨论了我们在试点中所做的工作。我想问在座的各位：在医疗服务环境中，基于“Ambient Intelligence”的感应系统最重要的部分是什么？</p><p><strong>一是隐私，这是非常重要的</strong>。患者需要隐私，临床医生也需要隐私。</p><p><strong>二是通过空间进行感知</strong>。刚才提到的本地化解决方案，其部分问题是因为太过于本地化了，很难在空间上扩展。</p><p><strong>三是根据时间进行感知</strong>。如果人类来观测活动，他们往往会感到厌倦。</p><p>所以，我们想利用机器并且将其变得可扩展。在过去的数十年里，现代传感器已经有了很大的发展。那些曾经玩过 Xbox 视频游戏的玩家，应该都知道深度传感器，它可以用来保护隐私。</p><p><strong>如何通过深度传感器收集人类活动的数据？</strong></p><p>在我们的两所试点医院（犹他州儿童医院和承认重症监护病房）中，我们进行了深度传感器的试用。这些深度传感器被安装在医院的病房中。例如，在儿童医院，我们安装了将近30个不同的传感器，目的就是为了通过传感器获取更多的数据以理解人类的行为。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/sensors.jpg"></p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/sensors2.jpg"></p><p>还有一种传感器，它与前者相互补，主要作用于生理信息，即热传感器。通过深度传感器可以看到病人轮廓；而通过热传感器收集信息，你不仅可以看到病人的轮廓，你实际上还能看到其他关键的物体，如氧气管。这对病人而言是非常重要的。所以，在我们的试点研究中，我们同样也会用到热传感器。实际上，我们正在与旧金山的一所养老院合作，在养老院里安装了热传感器和深度传感器，以帮助医生监测老人的行为，帮助他们独立生活。</p><p>其实，将传感器投放在医疗环境中，数据基础架构的建设就已经面临着巨大挑战。例如，持续的数据源就意味着大量数据的涌入。如果我们使用传感器的原始分辨率，就会出现需要处理海量数据的问题。因此，我们进行了一些自适应抽样以减少要处理的数据。这些都是我们必须面临着的技术挑战。但我们依然保持着：对人类行为识别的计算机视觉研究的专注，也希望应对医疗环境下的种种挑战，为计算机视觉的基础科学研究做出贡献。</p><h3 id="人类活动识别"><a href="#人类活动识别" class="headerlink" title="人类活动识别"></a>人类活动识别</h3><p>视觉智能，指的是在动态物理世界中发生的过程。谈到动态这个概念，有很多的信息、事物转瞬即逝。这意味着：我们有时会处理之前从未见过的情况。例如，在医疗环境这种复杂的情况下，这名患者在地板上睡了会儿，在床上又睡了会儿。这并非是我们通常利用数据进行训练的场景。所以，这种问题有待解决。</p><p>在医疗场景中，我们还要处理物理空间的限制问题。比如一般计算机视觉处理的都是类似 YouTube 用户上传的视频，但是医院的空间有限，因此传感器的装设位置也受到限制，拍摄到的都是各种角度的画面，非常具有挑战性。同样重要的是，我们还会面临计算效率的问题，因为我们希望为临床医师提供实时反馈，因此计算效率极为重要。</p><p>人类活动识别是目前计算机视觉领域最受关注的方向之一，目前也已经一些公开的数据集，而且也有很多非常不错的工作。我想介绍的是，我们是如何把我们的工作和医疗健康应用相结合的。第一个是发表在 ECCV 16 上的一篇论文，论述了如何处理不同视野角度的问题，这只是最基础的。我们用到了很多深度学习结构，比如这个用来做图像分类的 Vanilla CNN 网络。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/Vanilla_CNN.jpg"></p><p>比如，我们希望检测临床医师在进出病人的房间前后是否都有洗手，就需要面临很多的挑战。首先，由于我们的传感器大都安在天花板上，因此画面的视角和正常的 YouTube 视频画面的视角非常不一样。此外，人是运动的，因此我们安装了很多传感器，来对人进行追踪。</p><p>我这里简单介绍下视角问题。我们使用了 Vanilla CNN 网络来做分类，唯一的变化就是我们增加了一个转换网络（transformer network），来解决训练数据的视角问题。然后，为了解决多个传感器的追踪问题，我们将不同的个体进行 ground projection，然后将整个 3D 空间的投影结合起来，进行联合优化，以此来追踪不同的个体。</p><p>为什么我们会选择手部卫生作为第一个应用案例呢？因为不注意手部卫生是病人死亡的重要元凶之一。实际上，因为每年死于医院获得性感染的病人是交通事故致死人数的三倍。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/death.jpg"></p><p>而大多数的医院获得性感染都是没有注意手部卫生导致的。这是医疗系统里的一个顽疾，解决这个问题的唯一办法是派人到医院里监督医生和护士，督促他们洗手。但是这种方法非常低效，不仅不能做到实施监督，也非常耗费时间，而且人也可能会犯错误。因此，通过使用深度学习和智能传感器系统来对医务人员进行追踪，我们取得了非常好的结果。和人类检查员相比，我们的方法观察到与事实更接近。</p><p>我们的系统可以追踪医务人员的行动轨迹，而这些数据对医疗系统来说非常宝贵，不仅仅可以用来追踪医务人员洗手了没，还可以用来优化工作流程。结合智能传感器和计算机视觉识别系统，我们在手部卫生检测领域取得了鼓舞人心的结果。下一步，我们将会把反馈信号实时传递到周遭环境中，以此来督促大家洗手。不过，这还远远不够，我们还需要理解各种不同的行为，观察它们，最终帮助医生和护士优化治疗和护理流程，这一点很重要。这就引出了我们的下一项工作：密集多标签活动识别。</p><p>比如，在 ICU 里面经常会涉及到测血压、绑止血带、用医用究竟喷雾消毒等等一系列的动作，我们希望最终能够映射所有的医疗活动，帮助医生和护士更好地照顾病人。为了做到这一点，仅仅用 CNN 是不够的，特别是对于静态帧分类。我们真正想做的，是将其扩展到时域，利用视频数据来识别人类活动。很多人都对此领域做出过相关贡献，这里我就不展开了。</p><p>但是总的来说，利用视频来进行活动识别的工作仍然很少，大部分工作都是活动分类，比如为潜水视频打一个单一的标签，或者是活动检测，为视频中不同活动分配相应的标签。不过，时域理解仍然处在比较初始的水平，比如很多帧都没有标签，而且大多数的测试视频都只有一种或少数几种活动类型。在医疗健康领域，活动要密集的多，因此我们需要识别更多不同种类的活动。</p><p>为了解决这个问题，我们开发了基于 RNN 序列模型的网络，使用 multilevel loss 来预测同时发生的活动。我们在 MultiHUMOS 数据集上进行了计算机视觉基准测试，与Vanilla LSTM 或者Stream CNN 相比，我们的算法在多活动标记领域取得了领先的成果。我们正试着在 InterMountain 医院的 ICU 里部署我们的模型，来观察病人的活动。我们选择从病人的四种活动开始，比如上床、下床等动作。知晓病人的活动水平，对医务人员提供更好的医疗服务至关重要。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/senior_pop_growth.jpg"></p><p>最后，我还想介绍下我们在减少训练数据量方面的工作，这在养老院的应用中非常有用。人口老龄化是世界性的问题，我们真的需要做一些事情来帮助老人们。</p><p>我们和很多老年病患交谈过，并确定了十几种和老年人健康息息相关的行为。我们希望最终使用计算机视觉系统来识别他们的行为，帮助患者和医生。举例来说，跌倒对老年人说是一个大问题，它甚至有可能夺走老人的生命。我们正在尝试解决跌倒检测的问题，但是在这个领域，我们不可能收集到大量的训练数据，因此我们的一个想法是使用自监督学习系统，比如我们去年发布在 CVPR 上的一项工作。另外，我们还尝试了迁移学习的想法。</p><h3 id="医疗生态系统"><a href="#医疗生态系统" class="headerlink" title="医疗生态系统"></a>医疗生态系统</h3><p>最后，融入整个医疗生态系统也非常重要。为此，我们与斯坦福大学的其他小组展开合作，在为医院赋能时，我们不仅考虑智能传感器本身，还与病理学、放射学、医疗文献、图片等相结合。比如，我们和皮肤科医生一起研究烧伤患者的图像分割，又比如，我们也一直在寻找手术视频来识别手术中的活动。</p><p>总的来说，这是一个非常新兴的研究领域，它使用计算机视觉和机器学习算法来改善医疗保健服务，并帮助医生和护士观察病人活动，提高护理质量，从挽救更多的生命。从感知到人类活动识别到生态系统，以及建立大型合作关系，未来还有许多工作要做。</p><h1 id="Bill-Freeman：视觉信息促进语音辨识"><a href="#Bill-Freeman：视觉信息促进语音辨识" class="headerlink" title="Bill Freeman：视觉信息促进语音辨识"></a>Bill Freeman：视觉信息促进语音辨识</h1><p>第二天的主题演讲人是谷歌研究科学家、MIT教授Bill Freeman，题目是“Look to Listen: Using Vision to Improve Speech Understanding”。</p><blockquote><p>人类拥有卓越的人声辨识能力，甚至在嘈杂的多人环境中识别出特定的一个人。但，计算机很难做到这点。我们最近的一项研究工作就是让计算机具备这种能力，我们借鉴了人类在这个过程中利用到的线索：“看”着说话人讲话。我们算法的输入时两个或者多于两个人的说话视频，输出就是被挑选出的某个说话人的声音。这个技术我们把它命名为“Looking to Listen”，可以应用于多个领域例如：语音识别、翻译以及辅助听力等。</p></blockquote><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/freeman-3.jpg"></p><p>主题报告＂Look to Listen: Using Vision to Improve Speech Understanding＂</p><p>人类在识别和理解人类语音方面有着极强的能力，哪怕是好几个人同时间在嘈杂的环境中说话，也能分清楚谁在说什么。对于计算机而言，这个任务还很艰巨。</p><p>最近，Freeman教授的团队通过让计算机“看”，也即观察说话者来辅助语音识别，大幅提升了计算机语言识别的性能。实际上，这也是人类在语音识别时常常采用的方法。他们的研究论文<a href="https://arxiv.org/pdf/1804.03619.pdf" target="_blank" rel="noopener">“Looking to Listen at the Cocktail Party”</a>，已经被SIGGRAPH 2018接收。这项研究的起点，是MIT的研究人员发现，视频信息实际上可以充当一种“视觉麦克风”。一袋放在桌上的薯片，在旁边播放音乐，观察高速摄像头拍摄下的薯片包装袋，能发现包装袋在颤动，从而推理出音频信息。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/freeman-1.jpg"></p><p>在此基础上，Freeman教授带领的Google Research团队，通过计算生成视频，使用视觉信息，加强其中特定人物的语音，同时抑制其他的所有声音。这个方法适用于带有单个音频轨道的普通视频，用户需要的只是选择他们想要听的视频中人物的脸部，或者根据上下文在算法上选择这样的人物就行了。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/Looking2Listen-1.png"></p><p>他们设计了一种算法，输入有两个及更多人同时说话的视频，算法能够输出其中被选定的那个人的音频，非常清晰</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/Looking2Listen-2.png"></p><p>“鸡尾酒效应”论文提出的基于神经网络的多数据流架构</p><p>他们把这种技术成为Looking to Listen，在语音识别、会议转录和视频会议等场景中，有着巨大的应用潜力。<br>(墙Video: <a href="https://www.youtube.com/watch?v=rVQVAPiJWKU" target="_blank" rel="noopener">https://www.youtube.com/watch?v=rVQVAPiJWKU</a>)</p><p>除了“从看到听”，在更早一些的时候，Freeman教授的团队还做了“从听到看”的研究，也即从声音中学习画面（Learning Sight from Sound）。在一项工作中，他们表明环境声音可以用作学习视觉模型的监督信号。他们训练了一个卷积神经网络来预测与视频帧相关的声音的统计汇总，网络学会了关于某个物体（对象）和场景有关声音信息的表示。结果发现，具有类似声音特征的视频，比如海边和河边，虽然视觉信息非常不同，但在网络学会的声音信号空间中，却是十分类似的。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/audio-video.png"></p><p>通过这个过程，网络学会了关于某个物体（对象）和场景有关声音信息的表示。实验结果显示，这种方法的性能与其他最先进的无监督学习方法相当。图像是声音的补充，从一种模态（比如图像）中能够得到一些很难或者无法从另一种模态（比如语音）分析中得到的信息。反之也一样。通过这样将视觉和语音信号相结合，能够彼此促进。此外，如果能够确定哪些视觉信号能在训练过程中帮助检测特定的声音信号，将进一步提升语音识别的效果。</p><h1 id="大合影"><a href="#大合影" class="headerlink" title="大合影"></a>大合影</h1><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/tsinghua-google-ai/group2.jpg"></p><p>距离较远，只能拍成这样了</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li>Serena Yeung. <a href="http://ai.stanford.edu/~syyeung/" target="_blank" rel="noopener">http://ai.stanford.edu/~syyeung/</a></li><li>Yeung S, Downing N L, Fei-Fei L, et al. Bedside Computer Vision-Moving Artificial Intelligence from Driver Assistance to Patient Safety[J]. The New England journal of medicine, 2018, 378(14): 1271.</li><li>William T. Freeman. <a href="https://billf.mit.edu/publications/all" target="_blank" rel="noopener">https://billf.mit.edu/publications/all</a></li><li>Ambient Sound Provides Supervision for Visual Learning. <a href="http://andrewowens.com/ambient/index.html" target="_blank" rel="noopener">http://andrewowens.com/ambient/index.html</a></li><li>Jeff Dean主页：<a href="https://ai.google/research/people/jeff" target="_blank" rel="noopener">https://ai.google/research/people/jeff</a></li><li>Feifei Li主页：<a href="http://vision.stanford.edu/feifeili/" target="_blank" rel="noopener">http://vision.stanford.edu/feifeili/</a></li><li>电子书： <a href="http://g.cheerue.com/#/index" target="_blank" rel="noopener">http://g.cheerue.com/#/index</a></li></ul><p>特别感谢：新智元、人工智能学家、机器之心等公众号的支持！</p>]]></content>
      
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Think Different</title>
      <link href="/posts/news/"/>
      <url>/posts/news/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><!--![](https://vincentqin.gitee.io/blogresource-1/gifs/5.gif)--><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/news/greatpeople.jpg"></p><div class="note primary">            <p>“Here’s to the crazy ones. The misfits. The rebels. The troublemakers. The round pegs in the square holes. The ones who see things differently. They’re not fond of rules. And they have no respect for the status quo. You can quote them, disagree with them, glorify or vilify them. About the only thing you can’t do is ignore them. Because they change things. They push the human race forward. And while some may see them as the crazy ones, we see genius. Because the people who are crazy enough to think they can change the world are the ones who do.”</p><center>— Apple’s “Think Different” commercial, 1997</center>          </div><a id="more"></a><!--<blockquote class="blockquote-center"> 都点进来了，说点啥呗 </blockquote>-->]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Light Field Depth Estimation</title>
      <link href="/posts/light-field-depth-estimation/"/>
      <url>/posts/light-field-depth-estimation/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/street.jpg"></p><div class="note success">            <p>本文将介绍光场领域进行深度估计的相关研究。<br>In this post, I’ll introduce some depth estimation algorithms using Light field information. Here is some of the <a href="https://github.com/Vincentqyw/Depth-Estimation-Light-Field" target="_blank" rel="noopener">code</a>.<br>研究生阶段的研究方向是光场深度信息的恢复。再此做一些总结，以便于让大家了解光场数据处理的一般步骤以及深度估计的相关的知识，光场可视化部分代码见<a href="https://github.com/Vincentqyw/light-field-Processing" target="_blank" rel="noopener">light-field-Processing</a>，收集的部分光场深度估计代码见<a href="https://github.com/Vincentqyw/Depth-Estimation-Light-Field" target="_blank" rel="noopener">Depth-Estimation-Light-Field</a>。如有任何疑问或者建议，请大家在评论区提出。</p>          </div><a id="more"></a><h1 id="什么是光场？"><a href="#什么是光场？" class="headerlink" title="什么是光场？"></a>什么是光场？</h1><p>提到光场，很多人对它的解释模糊不清，在此我对它的概念进行统一表述。它的故事可以追溯到1936年，那是一个春天，Gershun写了一本名为<strong>The Light Field</strong><sup><a href="#fn_1" id="reffn_1">1</a></sup>的鸿篇巨著（感兴趣的同学可以看看那个年代的论文），于是光场的概念就此诞生，但它并没有因此被世人熟知。经过了近六十年的沉寂，1991年Adelson<sup><a href="#fn_2" id="reffn_2">2</a></sup>等一帮帅小伙将光场表示成了如下的7维函数：</p><script type="math/tex; mode=display">P(\theta,\phi,\lambda,t,V_x,V_y,V_z). \tag{1}</script><p>其中$(\theta,\phi)$表示球面坐标，$\lambda$表示光线的波长，$t$表示时间，$(V_x,V_y,V_z)$表示观察者的位置。<br>可以想象假如有这样一张由针孔相机拍摄的黑白照片，它表示：我们从<strong>某个时刻</strong>、<strong>单一视角</strong>观察到的<strong>可见光谱</strong>中某个<strong>波长</strong>的光线的平均。也就是说，它记录了通过$P$点的光强分布，光线方向可以由球面坐标$P(\theta,\phi)$或者笛卡尔坐标$P(x,y)$来表示。对于彩色图片而言，我们要添加光线的波长$\lambda$信息即变为$P(\theta,\phi,\lambda)$。按照同样的思路，彩色电影也就是增加了时间维度$t$，因此$P(\theta,\phi,\lambda,t)$。对于彩色全息电影而言，我们可以从任意空间位置$(V_x,V_y,V_z)$进行观看，于是其可以表达为最终的形式$P(\theta,\phi,\lambda,t,V_x,V_y,V_z)$。这个函数又被成为全光函数（Plenoptic Function）。<br>但是以上的七维的全光函数过于复杂，难以记录以及编程实现。所以在实际应用中我们对其进行简化处理。第一个简化是单色光以及时不变。可分别记录3原色以简化掉波长$\lambda$，可以通过记录不同帧以简化$t$，这样全光函数就变成了5D。第二个简化是Levoy<sup><a href="#fn_3" id="reffn_3">3</a></sup>等人（1996年）认为5D光场中还有一定的冗余，可以在自由空间（光线在传播过程中能量保持不变）中简化成4D。</p><h2 id="光场参数化表示"><a href="#光场参数化表示" class="headerlink" title="光场参数化表示"></a>光场参数化表示</h2><p>参数化表示要解决的问题包括：1. 计算高效；2. 光线可控；3. 光线均匀采样。目前比较常用的表示方式是双平面法（$2PP$）<sup><a href="#fn_3" id="reffn_3">3</a></sup>，利用双平面法可以将光场表示为：$L(u,v,s,t)$。其中$(u,v)$为第一个平面，$(s,t)$是第二个平面。那么一条有方向的直线可以表示为连接$uv$以及$st$平面上任意两点确定的线，如下图所示：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/2pp-v1.png"></p><p>【注】：Levoy<sup><a href="#fn_3" id="reffn_3">3</a></sup>首先利用双平面法对光场进行表示，光线首先通过$uv$平面，然后再通过$st$平面。但是后来（同年）Gortler<sup><a href="#fn_4" id="reffn_4">4</a></sup>等人将其传播方向反了过来，导致后续研究者对此表述并不一致。与此同时，也有不少文献中也引入了$xy$坐标，例如著名的光场相机的缔造者N.G.博士的毕业论文。通常情况下，这指的是像平面坐标，即指的是由传感器得到的图像中像素的位置坐标。由于后续处理中都是针对图像而言，而对于光学结构以及光线的传播过程并不感兴趣。所以为了方便起见，我们在本文中统一采用Levoy<sup><a href="#fn_3" id="reffn_3">3</a></sup>的方式对<strong>光场图像</strong>进行表示，即$uv$表示角度分辨率，$xy$表示空间分辨率，即$L(u,v,x,y)$。同时在表示<strong>光场</strong>时用$L(u,v,s,t)$。有时候二者不做区分，注意即可。</p><h2 id="光场的可视化"><a href="#光场的可视化" class="headerlink" title="光场的可视化"></a>光场的可视化</h2><p>虽然光场由$7D$全光函数降维到$4D$，但是其结构还是很难直观想象。通过固定4D光场参数化表示$L(u,v,s,t)$中的某些变量，我们可以很容易地对光场进行可视化。我们通常认为$(u,v)$控制着某个视角的位置，即相机平面；而$(s,t)$控制着从某个视角观察到的图像。说简单点：$uv$控制角度分辨率，$st$控制空间分辨率（视野）。注意上式描述的是光线的表示方法，并没有涉及图像处理，所以没有$xy$。</p><p><img width="600" alt="uvst2images" data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/uvst2images.png"></p><p>接下来讲解，几种常见的可视化方式（图片来源<sup><a href="#fn_5" id="reffn_5">5</a></sup>）。首先是<strong>多视图法</strong>。很容易理解，对于最简单的情况，首先固定$u=u^*,v=v^*$，我们可以得到多视角的某个视图$L(u^*,v^*,s,t)$，如下图所示：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/allviews.png"></p><p>第二种表示方法是<strong>角度域法</strong>，通过固定$s=s^*,t=t^*$可以得到某个宏像素$L(u,v,s^*,t^*)$，如下图所示：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/angular-patch.png"></p><p>第三种表示方法是<strong>极线图法</strong>，通过固定$v=v^*,t=t^*$可以得到极线图：$L(u,v^*,s,t^*)$，如下图中水平方向的图所示；同理固定$u=u^*,s=s^*$可以得到极线图：$L(u^*,v,s^*,t)$，如下图中竖直方向的图所示：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/epi.png"></p><p>最后，给出这几种方式的对应关系图（注意图中，$xy$对应于以上$st$，$st$对应于$uv$）。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/lf-all-view.png"></p><h1 id="光场的获取"><a href="#光场的获取" class="headerlink" title="光场的获取"></a>光场的获取</h1><p>我们知道传统的相机只能采集来自场景某个方向的$2D$信息，那怎么才能够采集到光场信息呢？试想一下，当多个相机在多个不同视角同时拍摄时，这样我们就可以得到一个光场的采样（多视角图像）了。当然，这是容易想到的方法，目前已有多种获得光场的方式，如下表格中列举了其中具有代表性的方式<sup><a href="#fn_5" id="reffn_5">5</a></sup>。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/lf-acquisition.png"></p><h1 id="光场深度估计算法分类"><a href="#光场深度估计算法分类" class="headerlink" title="光场深度估计算法分类"></a>光场深度估计算法分类</h1><p>由上可知，光场图像中包含来自场景的多视角信息，这使得深度估计成为可能。相较于传统的多视角深度估计算法而言，基于光场的深度估计算法无需进行相机标定，这大大简化的深度估计的流程。但是由于光场图像巨大导致了深度估计过程占用大量的计算资源。同时这些所谓的多个视角之间虚拟相机的基线过短，从而可能导致误匹配的问题。以下将对多种深度估计算法进行分类并挑选具有代表性的算法进行介绍。</p><h2 id="多视角立体匹配"><a href="#多视角立体匹配" class="headerlink" title="多视角立体匹配"></a>多视角立体匹配</h2><p>根据光场相机的成像原理，我们可以将光场图像想像成为多个虚拟相机在多个不同视角拍摄同一场景得到图像的集合，那么此时的深度估计问题就转换成为多视角立体匹配问题。以下列举几种基于多视角立体匹配算法的深度估计算法<sup><a href="#fn_8" id="reffn_8">8</a></sup> <sup><a href="#fn_9" id="reffn_9">9</a></sup> <sup><a href="#fn_10" id="reffn_10">10</a></sup> <sup><a href="#fn_20" id="reffn_20">20</a></sup> <sup><a href="#fn_21" id="reffn_21">21</a></sup>。</p><table>    <tr>        <td rowspan="6"> MVS-based<br>        </td><td><b>Approach</b></td>        <td><b>Main Feature</b></td>    </tr>    <tr>        <td>Jeon et al. <sup><a href="#fn_8" id="reffn_8">8</a></sup></td>        <td>Phase shift Sub-pixel</td>    </tr>    <tr>        <td>Yu et al. <sup><a href="#fn_9" id="reffn_9">9</a></sup></td>        <td>Line-assisted graph cut</td>    </tr>    <tr>        <td>Heber et al. <sup><a href="#fn_10" id="reffn_10">10</a></sup> <sup><a href="#fn_2" id="reffn_20">20</a></sup></td>        <td>PCA matching term</td>    </tr>    <tr>        <td>Chen et al. <sup><a href="#fn_21" id="reffn_21">21</a></sup></td>        <td>Scam: Bilateral Consistency Metric</td>    </tr></table><p>在这里介绍Jeon等人<sup><a href="#fn_8" id="reffn_8">8</a></sup>提出的基于相移的亚像素多视角立体匹配算法。</p><h3 id="相移理论"><a href="#相移理论" class="headerlink" title="相移理论"></a>相移理论</h3><p>该算法的核心就是用到了相移理论，即空域的一个小的位移在频域为原始信号的频域表达与位移的指数的幂乘积，即如下公式：</p><script type="math/tex; mode=display">\mathcal{F}\left\{I(x+\Delta x)\right\} = \mathcal{F}\left\{I(x)\right\}\exp^{2\pi j\Delta x}. \tag{2}</script><p>所以，经过位移后图像可以表示为：</p><script type="math/tex; mode=display">I'(x)=I(x+\Delta x)={\mathcal{F}^{-1}\left\{\mathcal{F}\left\{I(x)\right\}\exp^{2 \pi j \Delta x}\right\}},\tag{3}</script><p>面对Lytro相机窄基线的难点，通过相移的思想能够实现亚像素精度的匹配，在一定程度上解决了基线短的问题。那么大家可能好奇的是，如何将这个理论用在多视角立体匹配中呢？带着这样的疑问，继续介绍该算法。</p><h3 id="匹配代价构建"><a href="#匹配代价构建" class="headerlink" title="匹配代价构建"></a>匹配代价构建</h3><p>为了能够使子视角图像之间进行匹配，作者设计了2中不同的代价量：Sum of Absolute Differences (SAD)以及Sum of Gradient Differences (GRAD)，最终通过加权的方式获得最终的匹配量$C$，它是位点$x$以及损失编号（可以理解成深度/视差层）$l$的函数，具体形式如下公式所示：</p><script type="math/tex; mode=display">C(x,l) = \alpha C_A(x,l)+(1-\alpha)C_G(x,l),\tag{4}</script><p>其中$\alpha \in [0,1]$表示SAD损失量$C_A$以及SGD损失量$C_G$之间的权重。同时其中的$C_A$被定义为如下形式：</p><script type="math/tex; mode=display">C_A(x,l) = \sum_{u \in V}\sum_{x \in R_x}{\min\left( | I(u_c,x)-I(u,x+\Delta x(u,l))|,\tau _1\right)},\tag{5}</script><p>其中的$R_x$表示在$x$点邻域的矩形区域；$\tau _1$是代价的截断值（为了增加算法鲁棒性）；$V$表示除了中心视角$u_c$之外的其余视角。上述公式通过比较中心视角图像$I(u_c,x)$与其余视角$I(u,x)$的差异来构建损失量，具体而言就是通过不断地在某个视角$I(u_i,x)$上$x$点的周围移动一个<strong>小的距离</strong>并于中心视角做差；重复这个过程直到比较完所有的视角(i=1…视角数目N)为止。此时会用到上节提及的相移理论以得到移动后的像素强度，注意上面提到的<strong>小的距离</strong>实际上就是公式中的$\Delta x$，它被定义为如下形式：</p><script type="math/tex; mode=display">\Delta x(u,l) = lk(u-u_c),\tag{6}</script><p>其中k表示深度/视差层的单位（像素），$\Delta x$会随着任意视角与中心视角之间距离的增大而线性增加。同理，可以构造出第二个匹配代价量SGD，其基本形式如下所示：</p><script type="math/tex; mode=display">C_G(x,l) = \sum_{u \in V}\sum_{x \in R_x}\beta (u){\min\left( Diff_x(u_c,u,x,l),\tau _2\right)}+ \\ \ \ \ (1-\beta (u)){\min\left( Diff_y(u_c,u,x,l),\tau _2\right)},\tag{7}</script><p>其中的$Diff_x(u_c,u,x,l)=|I_x(u_c,x)-I_x(u,x+\Delta x(u,l))|$表示子视角图像在x方向的上的梯度，同理$Diff_y$表示子孔径图像在y方向上的梯度；$\beta (u)$控制着这两个方向代价量的权重，它由任意视角与中心视角之间的相对距离表示：</p><script type="math/tex; mode=display">\beta (u) = \frac{|u-u_c|}{|u-u_c|+|v-v_c|}.\tag{8}</script><p>至此，代价函数构建完毕。随后对于该代价函数利用边缘保持滤波器进行损失聚合，得到优化后的代价量。紧接着作者建立了一个多标签优化模型（GC求解）以及迭代优化模型对深度图进行优化，再此不做详细介绍。下面是其算法的分部结果：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/jeon-depth-step.png"></p><h2 id="基于EPI的方法"><a href="#基于EPI的方法" class="headerlink" title="基于EPI的方法"></a>基于EPI的方法</h2><p><img width="100%" data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/2pp-epi-depth.png"></p><p>不同于多视角立体匹配的方式，EPI的方式是通过分析光场数据结构的从而进行深度估计的方式。EPI图像中斜线的斜率就能够反映出场景的深度。上图中点P为空间点，平面$\Pi$为相机平面，平面$\Omega$为像平面。图中$\Delta u$与$\Delta x$的关系可以表示为如下公式<sup><a href="#fn_6" id="reffn_6">6</a></sup>：</p><script type="math/tex; mode=display">\Delta x=- \frac{f}{Z}\Delta u,\tag{9}</script><p>假如固定相同的$\Delta u$，水平方向位移较大的EPI图中斜线所对应的视差就越大，即深度就越小。如下图所示，$\Delta x_2$&gt;$\Delta x_1$，那么绿色线所对应的空间点要比红色线所对应的空间点深度小。</p><p><img width="100%" data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/epi-depth.png"></p><p>以下列举几种基于EPI的深度估计算法<sup><a href="#fn_11" id="reffn_11">11</a></sup> <sup><a href="#fn_12" id="reffn_12">12</a></sup> <sup><a href="#fn_13" id="reffn_13">13</a></sup> <sup><a href="#fn_14" id="reffn_14">14</a></sup> <sup><a href="#fn_15" id="reffn_15">15</a></sup> <sup><a href="#fn_24" id="reffn_24">24</a></sup>。</p><table>    <tr>        <td rowspan="9"> EPI-based<br>        </td><td><b>Approach</b></td>        <td><b>Main Feature</b></td>    </tr>    <tr>        <td>Kim et al. <sup><a href="#fn_11" id="reffn_11">11</a></sup></td>        <td>Large scene reconstruction</td>    </tr>    <tr>        <td>Li et al. <sup><a href="#fn_12" id="reffn_12">12</a></sup></td>        <td>Sparse linear optimization</td>    </tr>    <tr>        <td>Krolla et al. <sup><a href="#fn_13" id="reffn_13">13</a></sup></td>        <td>Spherical light field</td>    </tr>    <tr>        <td>Wanner et al. <sup><a href="#fn_14" id="reffn_14">14</a></sup> <sup><a href="#fn_26" id="reffn_26">26</a></sup></td>        <td>Total variation(TV)</td>    </tr>    <tr>        <td>Diebode et al. <sup><a href="#fn_15" id="reffn_15">15</a></sup></td>        <td>Modified structure tensor</td>    </tr>    <tr>        <td>Zhang et al. <sup><a href="#fn_24" id="reffn_24">24</a></sup></td>        <td>Spinning Parallelogram Operator(SPO)</td>    </tr></table><p>在以上表格中最具代表性的算法是由wanner<sup><a href="#fn_14" id="reffn_14">14</a></sup>提出的结构张量法得到EPI图中线的斜率，如下公式所示：</p><script type="math/tex; mode=display"> J= \left[ \begin{matrix}   G_{\sigma}*(S_xS_x) & G_{\sigma}*(S_xS_y)  \\   G_{\sigma}*(S_xS_y) & G_{\sigma}*(S_yS_y)  \end{matrix}  \right]=  \left[ \begin{matrix}   J_{xx} & J_{xy}\\   J_{xy} & J_{yy}  \end{matrix}  \right],  \tag{10}</script><p>其中<script type="math/tex">S=S_{y^*,v^*}</script>为极线图。<script type="math/tex">S_x</script>以及<script type="math/tex">S_y</script>表示极线图在x以及y方向上的梯度，<script type="math/tex">G_{\sigma}</script>表示高斯平滑算子。最终极线图中局部斜线的斜率可以表示成如下形式：</p><script type="math/tex; mode=display"> J=\left[ \begin{matrix}   \Delta x  \\    \Delta v  \end{matrix}  \right]=  \left[ \begin{matrix}  \sin \varphi\\   \cos \varphi  \end{matrix}  \right],  \tag{11}</script><p>其中<script type="math/tex">\varphi = \frac{1}{2}\arctan\left(\frac{J_{yy}-J_{xx}}{2J_{xy}}\right)</script>。因此深度可以由公式（9）推出：</p><script type="math/tex; mode=display">Z=-f\frac{\Delta v}{\Delta x},  \tag{12}</script><p>通常情况下，可以用一种更加简单的形式，如视差对其进行表示：</p><script type="math/tex; mode=display">d_{y^*,v^*}=-f/Z=\frac{\Delta x}{\Delta v}=\tan \phi .  \tag{13}</script><p>至此，利用上述公式可以从EPI中估计出视差。</p><h2 id="散焦及融合的方法"><a href="#散焦及融合的方法" class="headerlink" title="散焦及融合的方法"></a>散焦及融合的方法</h2><p>光场相机一个很重要的卖点是先拍照后对焦，这其实是根据光场剪切原理<sup><a href="#fn_31" id="reffn_31">31</a></sup>得到的。通过衡量像素在不同焦栈处的“模糊度”可以得到其对应的深度。以下列举几种基于散焦的深度估计算法<sup><a href="#fn_7" id="reffn_7">7</a></sup> <sup><a href="#fn_16" id="reffn_16">16</a></sup> <sup><a href="#fn_17" id="reffn_17">17</a></sup> <sup><a href="#fn_22" id="reffn_22">22</a></sup> <sup><a href="#fn_23" id="reffn_23">23</a></sup>。</p><table>    <tr>        <td rowspan="5"> Defocus-based<br>        </td><td><b>Approach</b></td>        <td><b>Main Feature</b></td>    </tr>    <tr>        <td>Wang et al. <sup><a href="#fn_7" id="reffn_7">7</a></sup></td>        <td>Occlusion-aware</td>    </tr>    <tr>        <td>Tao et al. <sup><a href="#fn_16" id="reffn_16">16</a></sup></td>        <td>Defocus cues & Correspondence cues</td>    </tr>    <tr>        <td>Tao et al. <sup><a href="#fn_17" id="reffn_17">17</a></sup></td>        <td>Angular Coherence</td>    </tr>    <tr>        <td>Williem et al. <sup><a href="#fn_22" id="reffn_22">22</a></sup> <sup><a href="#fn_23" id="reffn_23">23</a></sup></td>        <td>Angular Entropy(AD, AE, CAD, CAE)</td>    </tr></table><p>这里介绍一个最具代表性的工作，由Tao等人<sup><a href="#fn_16" id="reffn_16">16</a></sup>在2013年提出，下图为其算法框架以及分部结果。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/tao2013.png"><br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/tao2013-results-step-by-step.png"></p><p>该工作其实就做了2件事情：1. 设计两种深度线索并估计原始深度；2. 置信度分析及MRF融合。以下对其进行具体介绍。</p><h3 id="双线索提取"><a href="#双线索提取" class="headerlink" title="双线索提取"></a>双线索提取</h3><p>首先对光场图像进行重聚焦，然后得到一系列具有不同深度的焦栈。然后对该焦栈分别提取2个线索：散焦量以及匹配量。其中散焦量被定义为：</p><script type="math/tex; mode=display">D_{\alpha}(x)=\frac{1}{|W_{D}|}{\sum _{x' \in W_D} {|\Delta _x{L}_{\alpha}(x')|}},\tag{14}</script><p>其中，$W_D$表示为当前像素领域窗口大小，$\Delta _x$表示水平方向拉式算子，$\overline{L}_{\alpha}(x)$为每个经过平均化后的重聚焦后光场图像，其表达式如下：</p><script type="math/tex; mode=display">\overline{L}_{\alpha}(x)=\frac{1}{N_{u}}\sum _{u'} {L}_{\alpha}(x,u'),\tag{15}</script><p>其中$N_{u}$表示每一个角度域内像素的数目。然后匹配量被定义成如下形式：</p><script type="math/tex; mode=display">{C}_{\alpha}(x)=\frac{1}{|W_{C}|}\sum _{x' \in W_C} {\sigma}_{\alpha}(x'),\tag{16}</script><p>其中，$W_C$表示为当前像素领域窗口大小，${\sigma}_{\alpha}(x)$表示每个宏像素强度的标准差，其表达式为：</p><script type="math/tex; mode=display">{\sigma}_{\alpha}(x)^2=\frac{1}{N_{u}}\sum _{u'} \left({L}_{\alpha}(x,u')-\overline{L}_{\alpha}(x)\right)^2.\tag{17}</script><p>经过以上两个线索可以通过赢者通吃（Winner Takes All，WTA）得到两张原始深度图。注意：对这两个线索使用WTA时略有不同，通过最大化空间对比度可以得到散焦线索对应的深度，最小化角度域方差能够获得匹配量对应的深度。因此二者深度可以分别表示为如下公式：</p><script type="math/tex; mode=display">\alpha ^{*}_D(x)=\mathop{\arg\max}_{\alpha} \ \ {D}_{\alpha}(x).\tag{18}</script><script type="math/tex; mode=display">\alpha ^{*}_C(x)=\mathop{\arg\min}_{\alpha} \ \ {C}_{\alpha}(x).\tag{19}</script><h3 id="置信度分析及深度融合"><a href="#置信度分析及深度融合" class="headerlink" title="置信度分析及深度融合"></a>置信度分析及深度融合</h3><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/tao2013-confidence-analysis.png"></p><p>上图中显示了两个线索随着深度层次而变化的曲线。接下来的置信度分析用<strong>主次峰值比例</strong>（Peak Ratio）来定义每种线索的置信度，可表示为如下公式，其中的$\alpha ^{*2}_D(x)$以及$\alpha ^{*2}_C(x)$分别表示曲线的次峰值对应的深度层次。</p><script type="math/tex; mode=display">D_{conf}(x)=\frac{D_{\alpha ^{*}_D}(x)}{D_{\alpha ^{*2}_D}(x)}.\tag{20}</script><script type="math/tex; mode=display">C_{conf}(x)=\frac{C_{\alpha ^{*}_C}(x)}{C_{\alpha ^{*2}_C}(x)}.\tag{21}</script><p>接下来对原始深度进行MRF置信度融合：</p><script type="math/tex; mode=display">\mathop{minimize}_{Z} \ \ \sum_{source}\lambda _{source} \sum _i W_i|Z_i-Z_i^{source}|</script><script type="math/tex; mode=display">+\lambda _{flat} \sum _{(x,y)}\left( \left |\frac{\partial Z_i}{\partial x}\right|_{(x,y)}+\left|\frac{\partial Z_i}{\partial y}\right|_{(x,y)}\right)</script><script type="math/tex; mode=display"> + \lambda _{smooth} \sum _{(x,y)}|\Delta Z_i|_{(x,y)}.\tag{22}</script><p>其中，$source$控制着数据项，即优化后的深度要与原始深度尽量保持一致。第二项与第三项分别控制着平坦性（flatness）与平滑性（smoothness）。注意：<strong>平坦</strong>的意思是物体表面没有凹凸变化的沟壑，例如魔方任一侧面，无论是否拼好（忽略中间黑线）。而<strong>平滑</strong>则表示在平坦的基础上物体表面没有花纹，如拼好的魔方的一个侧面。另外的$W$是权重量，此处选用的是每个线索的置信度。</p><script type="math/tex; mode=display"> \{Z_1^{source},Z_2^{source}\}=\{\alpha_C^{*},\alpha_D^{*}\}.\tag{23}</script><script type="math/tex; mode=display"> \{W_1^{source},W_2^{source}\}=\{C_{conf},D_{conf}\}.\tag{24}</script><p>至此，该算法介绍完毕，其代码已经放在我的<a href="https://github.com/Vincentqyw/Depth-Estimation-Light-Field/tree/master/LF_DC" target="_blank" rel="noopener">Github</a>。</p><h2 id="学习的方法"><a href="#学习的方法" class="headerlink" title="学习的方法"></a>学习的方法</h2><p>目前而言，将深度学习应用于从双目或者单目中恢复深度已经不再新鲜，我在之前的<a href="https://www.vincentqin.tech/2017/12/06/depth-estimation-using-deeplearning-1/">博文1</a>&amp;<a href="https://www.vincentqin.tech/2017/12/10/depth-estimation-using-deeplearning-2/">博文2</a>中有过对这类算法的介绍。但是将其应用于光场领域进行深度估计的算法还真是寥寥无几。不过总有一些勇敢的践行者去探索如何将二者结合，以下列举几种基于学习的深度估计算法<sup><a href="#fn_18" id="reffn_18">18</a></sup> <sup><a href="#fn_19" id="reffn_19">19</a></sup> <sup><a href="#fn_25" id="reffn_25">25</a></sup> <sup><a href="#fn_27" id="reffn_27">27</a></sup> <sup><a href="#fn_28" id="reffn_28">28</a></sup> <sup><a href="#fn_29" id="reffn_29">29</a></sup> <sup><a href="#fn_30" id="reffn_30">30</a></sup>。</p><table>    <tr>        <td rowspan="5"> Learning-based<br>           </td><td><b>Approach</b></td>        <td><b>Main Feature</b></td>    </tr>    <tr>        <td>Johannsen et al. <sup><a href="#fn_18" id="reffn_18">18</a></sup> <sup><a href="#fn_25" id="reffn_25">25</a></sup></td>        <td>Sparse coding</td>    </tr>    <tr>        <td>Heber et al. <sup><a href="#fn_19" id="reffn_19">19</a></sup> <sup><a href="#fn_27" id="reffn_27">27</a></sup> <sup><a href="#fn_28" id="reffn_28">28</a></sup></td>        <td>CNN-based</td>    </tr>    <tr>        <td>Jeon et al. <sup><a href="#fn_29" id="reffn_29">29</a></sup></td>        <td>SAD, SGD, ZNCC, CT, Random Forests</td>    </tr>    <tr>        <td>Shin et al. <sup><a href="#fn_30" id="reffn_30">30</a></sup></td>        <td>4-Directions EPIs & CNN-based</td>    </tr></table><p>在此，我将对截止目前（2018年5月29日）而言，在HCI新数据集上表现最好的<a href="https://arxiv.org/abs/1804.02379" target="_blank" rel="noopener">EPINET: A Fully-Convolutional Neural Network Using Epipolar Geometry for Depth from Light Field Images</a><sup><a href="#fn_30" id="reffn_30">30</a></sup>算法进行介绍，下图为该算法在各个指标上的表现情况。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/epinet-architecture.png"></p><p>算法摘要：光场相机能够同时采集空间光线的空域以及角度域信息，因此可以根据这种特性恢复出空间场景的涉深度。在本文中，作者提出了一种基于CNN的快速准确的光场深度估计算法。作者在设计网络时将光场的几何结构加入考虑，同时提出了一种新的数据增强算法以克服训练数据不足的缺陷。作者提出的算法能够在HCI 4D-LFB上在多个指标上取得Top1的成绩。作者指出，光场相机存在优势的同时也有诸多缺点，例如：基线超级短且空间&amp;角度分辨率有一定的权衡关系。目前已有很多工作去克服这些问题，这样一来，深度图像的精度提升了，但是带来的后果就是计算量超级大，无法快速地估计出深度。因此作者为了解决精度以及速度之间权衡关系设计了该算法（感觉很有意义吧）。</p><p>上面表格中提到的诸如Johannsen<sup><a href="#fn_18" id="reffn_18">18</a></sup> <sup><a href="#fn_25" id="reffn_25">25</a></sup>以及Heber<sup><a href="#fn_19" id="reffn_19">19</a></sup> <sup><a href="#fn_27" id="reffn_27">27</a></sup> <sup><a href="#fn_28" id="reffn_28">28</a></sup>等人设计的算法仅仅考虑到了一个极线方向，从而容易导致低置信度的深度估计。为了解决他们算法中存在的问题，作者通过一种多流网络将不同的极线图像分别进行编码去预测深度。因为，每个极线图都有属于自己的集合特征，将这些极线图放入网络训练能够充分地利用其提供的信息。</p><h3 id="光场图像几何特征"><a href="#光场图像几何特征" class="headerlink" title="光场图像几何特征"></a>光场图像几何特征</h3><p>由于光场图像可以等效成多个视角图像的集合，这里的视角数目通常要比传统的立体匹配算法需要的视角数目多得多。所以，如果利用全部的视角做深度估计将会相当耗时，所以在实际情况下并不需要用到全部的视角。作者的思路就是想办法尽量减少实际要使用的视角数目，所以作者探究了不同角度域方向光场图像的特征。中心视角图像与其余视角的关系可以表示成如下公式：</p><script type="math/tex; mode=display">L(x,y,0,0)=L(x+d(x,y)*u,y+d(x,y)*v,u,v),\tag{25}</script><p>其中$d(x,y)$表示中心视角到其相应相邻视角之间的视差（disparity）。令角度方向为$\theta$（$\tan \theta=v/u$），我们可以将上式改写成如下公式：</p><script type="math/tex; mode=display">L(x,y,0,0)=L(x+d(x,y)*u,y+d(x,y)*u \tan \theta,u,u \tan \theta).\tag{26}</script><p>作者选择了四个方向$\theta$: 0<sup>o</sup>，45<sup>o</sup>，90<sup>o</sup>，135<sup>o</sup>，同时假设光场图像总视角数为$(2N+1)\times(2N+1)$。</p><h3 id="网络设计"><a href="#网络设计" class="headerlink" title="网络设计"></a>网络设计</h3><p>如本节开始的图所示的网络结构，该网络的开始为多路编码网络（类似于Flownet以及<a href="https://www.cs.toronto.edu/~urtasun/publications/luo_etal_cvpr16.pdf" target="_blank" rel="noopener">Efficient Deep Learning for Stereo Matching</a><sup><a href="#fn_32" id="reffn_32">32</a></sup>），其输入为4个不同方向视角图像集合，每个方向对应于一路网络，每一路都可以对其对应方向上图像进行编码提取特征。每一路网络都由3个全卷积模块组成，因为全卷积层对逐点稠密预测问题卓有成效，所以作者将每一个全卷积模块定义为这样的卷积层的集合：<strong>Conv-ReLU-Conv-BN-ReLU</strong>，这样的话就可以在局部块中预逐点预测视差。为了解决基线短的问题，作者设计了非常小的卷积核：$2\times 2$，同时stride = 1，这样的话就可以测量$\pm 4$的视差。为了验证这种多路网络的有效性，作者同单路的网络做了对比试验，其结果如下表所示，可见多路网络相对于单路网络有10%的误差降低。</p><p><img width="60%" data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/viewpoints-effect.png"></p><p>在完成多路编码之后，网络将这些特征串联起来组成更维度更高的特征。后面的融合网络包含8个卷积块，其目的是寻找经多路编码之后特征之间的相关性。注意除了最后一个卷积块之外，其余的卷积块全部相同。为了推断得到亚像素精度的视差图，作者将最后一个卷积块设计为<strong>Conv-ReLU-Conv</strong>结构。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/data-augmentation.png"></p><p>最后，图像增强方式包括视角偏移（从9*9视角中选7*7，可扩展3*3倍数据），图像旋转（90<sup>o</sup>，180<sup>o</sup>，270<sup>o</sup>），图像缩放（[0.25,1]），色彩值域变化（[0.5,2]），随机灰度变化，gamma变换（[0.8,1.2]）以及翻转，最终扩充了288倍。</p><p>以下为其各个指标上的性能表现：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/benchmark-ranking.png"></p><p>以上介绍了目前已有的深度估计算法不同类别中具有代表性的算法，它们不一定是最优的，但绝对是最容易理解其精髓的。到目前为止，光场领域已经有一大波人做深度估计的工作，利用传统的方式其精度很难再往上提高。随着深度学习的大热，已经有一批先驱开始用深度学习做深度估计，虽然在仿真数据上可以表现得很好，但实际场景千变万化，即使是深度学习的策略也不敢保证对所有的场景都有效。路漫漫其修远兮，深度估计道路阻且长。我认为以后的趋势应该是从EPI图像下手，然后利用CNN提feature（或者响应）；此时可供选择的工具有<a href="http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo" target="_blank" rel="noopener">KITTI Stereo</a>/<a href="http://hci-lightfield.iwr.uni-heidelberg.de/" target="_blank" rel="noopener">HCI新数据集算法比较</a>/<a href="http://vision.middlebury.edu/stereo/" target="_blank" rel="noopener">Middlebury Stereo</a>中较好的几种算法，我们需要总结其算法优势并迁移到光场领域中来。GPU这个Powerful的计算工具一定要用到光场领域中来，发挥出多线程的优势。否则传统的CPU对于动辄上百兆的数据有心无力。这样一来，深度图像不仅仅可以从精度上得以提高，而且深度估计的速度也会更快。至此，本文介绍到此结束。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><blockquote id="fn_1"><sup>1</sup>. Gershun, A. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/1.Gershun-1939-Journal_of_Mathematics_and_Physics.pdf" target="_blank" rel="noopener">The Light Field</a>.” Studies in Applied Mathematics 18.1-4(1939):51–151.<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. Adelson, Edward H, and J. R. Bergen. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/2.The%20plenoptic%20function%20and%20the%20elements%20of%20early%20vision.pdf" target="_blank" rel="noopener">The plenoptic function and the elements of early vision</a>. “ Computational Models of Visual Processing (1991):3-20.<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. Levoy, Marc. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/3.Light_Field_Rendering.pdf" target="_blank" rel="noopener">Light field rendering</a>.” Conference on Computer Graphics and Interactive Techniques ACM, 1996:31-42.<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. Gortler, Steven J., et al. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/4.The%20lumigraph.pdf" target="_blank" rel="noopener">The Lumigraph</a>.” Proc Siggraph 96(1996):43-54.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. Wu, Gaochang, et al. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/5.Light-Field-Image-Processing-An%20Overview.pdf" target="_blank" rel="noopener">Light Field Image Processing: An Overview</a>.” IEEE Journal of Selected Topics in Signal Processing PP.99(2017):1-1.<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. Wanner, Sven, and B. Goldluecke. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/6.Variational%20Light%20Field%20Analysis%20for%20Disparity%20Estimation%20and%20Super-Resolution.pdf" target="_blank" rel="noopener">Variational Light Field Analysis for Disparity Estimation and Super-Resolution</a>.” IEEE Transactions on Pattern Analysis &amp; Machine Intelligence 36.3(2013):1.<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote><blockquote id="fn_7"><sup>7</sup>. Wang, Ting Chun, A. A. Efros, and R. Ramamoorthi. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/7.Occlusion-aware%20Depth%20Estimation%20Using%20Light-field%20Cameras.pdf" target="_blank" rel="noopener">Occlusion-Aware Depth Estimation Using Light-Field Cameras</a>.” IEEE International Conference on Computer Vision IEEE, 2016:3487-3495.<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a></blockquote><blockquote id="fn_8"><sup>8</sup>. Jeon, Hae Gon, et al. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/8.Accurate%20Depth%20Map%20Estimation%20from%20a%20Lenslet%20Light%20Field%20Camera.pdf" target="_blank" rel="noopener">Accurate depth map estimation from a lenslet light field camera</a>.” Computer Vision and Pattern Recognition IEEE, 2015:1547-1555.<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a></blockquote><blockquote id="fn_9"><sup>9</sup>. Yu, Zhan, et al. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/9.Line%20Assisted%20Light%20Field%20Triangulation%20and%20Stereo%20Matching.pdf" target="_blank" rel="noopener">Line Assisted Light Field Triangulation and Stereo Matching</a>.” IEEE International Conference on Computer Vision IEEE, 2014:2792-2799.<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a></blockquote><blockquote id="fn_10"><sup>10</sup>. Heber, Stefan, and T. Pock. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/10.Shape%20from%20Light%20Field%20meets%20Robust%20PCA.pdf" target="_blank" rel="noopener">Shape from Light Field Meets Robust PCA</a>.” Computer Vision – ECCV 2014. 2014:751-767.<a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a></blockquote><blockquote id="fn_11"><sup>11</sup>. Kim, Changil, et al. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/11.scene-reconstruction-from-high-spatio-angular-resolution-light-fields-siggraph-2013-compressed-kim-et-al.pdf" target="_blank" rel="noopener">Scene reconstruction from high spatio-angular resolution light fields</a>.” Acm Transactions on Graphics 32.4(2017):1-12.<a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a></blockquote><blockquote id="fn_12"><sup>12</sup>. Li, J., M. Lu, and Z. N. Li. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/12.Continuous%20Depth%20Map%20Reconstruction%20From%20Light%20Fields.pdf" target="_blank" rel="noopener">Continuous Depth Map Reconstruction From Light Fields</a>.” IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society 24.11(2015):3257.<a href="#reffn_12" title="Jump back to footnote [12] in the text."> &#8617;</a></blockquote><blockquote id="fn_13"><sup>13</sup>. Krolla, Bernd, et al. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/13.Spherical%20light%20field.pdf" target="_blank" rel="noopener">Spherical Light Fields</a>.” British Machine Vision Conference 2014.<a href="#reffn_13" title="Jump back to footnote [13] in the text."> &#8617;</a></blockquote><blockquote id="fn_14"><sup>14</sup>. Wanner, Sven, C. Straehle, and B. Goldluecke. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/14.Globally%20consistent%20multi-label%20assignment%20on%20the%20ray%20space%20of%204D%20light%20field.pdf" target="_blank" rel="noopener">Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a>.” IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society, 2013:1011-1018.<a href="#reffn_14" title="Jump back to footnote [14] in the text."> &#8617;</a></blockquote><blockquote id="fn_15"><sup>15</sup>. Diebold, Maximilian, B. Jahne, and A. Gatto. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/15.Heterogeneous%20Light%20Fields.pdf" target="_blank" rel="noopener">Heterogeneous Light Fields</a>.” Computer Vision and Pattern Recognition IEEE, 2016:1745-1753.<a href="#reffn_15" title="Jump back to footnote [15] in the text."> &#8617;</a></blockquote><blockquote id="fn_16"><sup>16</sup>. Tao, M. W, et al. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/16.Depth%20from%20Combining%20Defocus%20and%20Correspondence%20Using%20Light-Field%20Cameras.pdf" target="_blank" rel="noopener">Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a>.” IEEE International Conference on Computer Vision IEEE Computer Society, 2013:673-680.<a href="#reffn_16" title="Jump back to footnote [16] in the text."> &#8617;</a></blockquote><blockquote id="fn_17"><sup>17</sup>. Tao, Michael W., et al. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/17.Depth%20from%20Shading,%20Defocus,%20and%20Correspondence%20Using%20Light-Field%20Angular%20Coherence.pdf" target="_blank" rel="noopener">Depth from shading, defocus, and correspondence using light-field angular coherence</a>.” Computer Vision and Pattern Recognition IEEE, 2015:1940-1948.<a href="#reffn_17" title="Jump back to footnote [17] in the text."> &#8617;</a></blockquote><blockquote id="fn_18"><sup>18</sup>. Johannsen, Ole, A. Sulc, and B. Goldluecke. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/18.Variational%20separation%20of%20light%20field%20layers.pdf" target="_blank" rel="noopener">Variational Separation of Light Field Layers</a>.” (2015).<a href="#reffn_18" title="Jump back to footnote [18] in the text."> &#8617;</a></blockquote><blockquote id="fn_19"><sup>19</sup>. Heber, Stefan, and T. Pock. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/19.Heber_Convolutional_Networks_for_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Convolutional Networks for Shape from Light Field</a>.” Computer Vision and Pattern Recognition IEEE, 2016:3746-3754.<a href="#reffn_19" title="Jump back to footnote [19] in the text."> &#8617;</a></blockquote><blockquote id="fn_20"><sup>20</sup>. Heber, Stefan, R. Ranftl, and T. Pock. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/20.Variational%20Shape%20from%20Light%20Field.pdf" target="_blank" rel="noopener">Variational Shape from Light Field</a>.” Energy Minimization Methods in Computer Vision and Pattern Recognition. Springer Berlin Heidelberg, 2013:66-79.<a href="#reffn_20" title="Jump back to footnote [20] in the text."> &#8617;</a></blockquote><blockquote id="fn_21"><sup>21</sup>. Chen, Can, et al. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/21.Light%20Field%20Stereo%20Matching%20Using%20Bilateral%20Statistics%20of%20Surface%20Cameras-Can_CVPR14_stereo.pdf" target="_blank" rel="noopener">Light Field Stereo Matching Using Bilateral Statistics of Surface Cameras</a>.” IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society, 2014:1518-1525.<a href="#reffn_21" title="Jump back to footnote [21] in the text."> &#8617;</a></blockquote><blockquote id="fn_22"><sup>22</sup>. Williem W, Kyu P I. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/22.Williem_Robust_Light_Field_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Robust light field depth estimation for noisy scene with occlusion</a>.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016:4396-4404.<a href="#reffn_22" title="Jump back to footnote [22] in the text."> &#8617;</a></blockquote><blockquote id="fn_23"><sup>23</sup>. Williem W, Park I K, Lee K M. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/23.TPAMI2017_Williem.pdf" target="_blank" rel="noopener">Robust light field depth estimation using occlusion-noise aware data costs</a>.” IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017(99):1-1.<a href="#reffn_23" title="Jump back to footnote [23] in the text."> &#8617;</a></blockquote><blockquote id="fn_24"><sup>24</sup>. Zhang S, Sheng H, Li C, et al. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/24.Robust%20Depth%20Estimation%20for%20Light%20Field%20via%20Spinning%20Parallelogram%20Operator.pdf" target="_blank" rel="noopener">Robust depth estimation for light field via spinning parallelogram operator</a>.” Computer Vision and Image Understanding, 2016, 145:148-159.<a href="#reffn_24" title="Jump back to footnote [24] in the text."> &#8617;</a></blockquote><blockquote id="fn_25"><sup>25</sup>. Johannsen O, Sulc A, Goldluecke B. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/25.What%20Sparse%20Light%20Field%20Coding%20Reveals%20about%20Scene%20Structure.pdf" target="_blank" rel="noopener">What sparse light field coding reveals about scene structure</a>.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016(1/3/4):3262-3270.<a href="#reffn_25" title="Jump back to footnote [25] in the text."> &#8617;</a></blockquote><blockquote id="fn_26"><sup>26</sup>. Wanner S, Goldluecke B. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/26.Reconstructing%20reflective%20and%20transparent%20surfaces%20from%20epipolar%20plane%20images.pdf" target="_blank" rel="noopener">Reconstructing reflective and transparent surfaces from epipolar plane images</a>.” In German Conference on Pattern Recognition (Proc. GCPR), 2013:1-10.<a href="#reffn_26" title="Jump back to footnote [26] in the text."> &#8617;</a></blockquote><blockquote id="fn_27"><sup>27</sup>. Heber S, Yu W, Pock T. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/27.U-shaped%20Networks%20for%20Shape%20from%20Light%20Field.pdf" target="_blank" rel="noopener">U-shaped networks for shape from light field</a>.” British Machine Vision Conference, 2016, 37:1-12.<a href="#reffn_27" title="Jump back to footnote [27] in the text."> &#8617;</a></blockquote><blockquote id="fn_28"><sup>28</sup>. Heber S, Yu W, Pock T. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/28.Neural%20EPI-volume%20Networks%20for%20Shape%20from%20Light%20Field.pdf" target="_blank" rel="noopener">Neural EPI-Volume networks for shape from light field</a>.” IEEE International Conference on Computer Vision (ICCV), IEEE Computer Society, 2017:2271-2279.<a href="#reffn_28" title="Jump back to footnote [28] in the text."> &#8617;</a></blockquote><blockquote id="fn_29"><sup>29</sup>. Jeon H G, Park J, Choe G, et.al. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/29.Depth%20from%20a%20Light%20Field%20Image%20with%20Learning-based%20Matching%20Costs.pdf" target="_blank" rel="noopener">Depth from a Light Field Image with Learning-based Matching Costs</a>.” IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018.<a href="#reffn_29" title="Jump back to footnote [29] in the text."> &#8617;</a></blockquote><blockquote id="fn_30"><sup>30</sup>. Shin C, Jeon H G, Yoon Y. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/30.EPINET%20A%20fully-Convolutional%20Neural%20Network%20Using%20Epipolar%20Geometry%20for%20Depth%20from%20Light%20Field%20Images.pdf" target="_blank" rel="noopener">EPINET: A Fully-Convolutional Neural Network for Light Field Depth Estimation Using Epipolar Geometry</a>.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.<a href="#reffn_30" title="Jump back to footnote [30] in the text."> &#8617;</a></blockquote><blockquote id="fn_31"><sup>31</sup>. Ng, Ren. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/31.Digital%20light%20field%20photography.pdf" target="_blank" rel="noopener">Digital light field photography</a>.” 2006, 115(3):38-39.<a href="#reffn_31" title="Jump back to footnote [31] in the text."> &#8617;</a></blockquote><blockquote id="fn_32"><sup>32</sup>. Luo, Wenjie, A. G. Schwing, and R. Urtasun. “<a href="https://vincentqin.gitee.io/blogresource-1/light-field-depth-estimation/32.Efficient%20deep%20learning%20for%20stereo%20matching.pdf" target="_blank" rel="noopener">Efficient Deep Learning for Stereo Matching</a>.” IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society, 2016:5695-5703.<a href="#reffn_32" title="Jump back to footnote [32] in the text."> &#8617;</a></blockquote>]]></content>
      
      
      <categories>
          
          <category> 光场 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> depth estimation </tag>
            
            <tag> light field </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CV Related References</title>
      <link href="/posts/cv-books/"/>
      <url>/posts/cv-books/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/cv-books/moono.jpg"></p><a id="more"></a><h2 id="视觉"><a href="#视觉" class="headerlink" title="视觉"></a>视觉</h2><ul><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/Multiple_View_Geometry_in_Computer_Vision-cn.pdf" target="_blank" rel="noopener">Multiple View Geometry in Computer Vision：计算机视觉中的多视图几何</a>【69.52 MB】</li></ul><h2 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h2><ul><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/1.%20Selected%20Applications%20of%20Convex%20Optimization-Springer-Verlag%20Berlin%20Heidelberg%20%282015%29.pdf" target="_blank" rel="noopener">Selected Applications of Convex Optimization：凸优化应用讲义</a></li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/CVX.pdf" target="_blank" rel="noopener">CVX:凸优化问题求解</a></li></ul><h2 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h2><ul><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/2.%20Iterative%20Methods%20for%20Sparse%20Linear%20Systems.pdf" target="_blank" rel="noopener">Iterative Methods for Sparse Linear Systems：稀疏线性系统求解</a></li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/3.%20The%20Matrix%20Cookbook.pdf" target="_blank" rel="noopener">The Matrix Cookbook：矩阵分析</a></li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/MRF.pdf" target="_blank" rel="noopener">Markov Random Field Image Modelling：马尔科夫随机场模型</a></li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/numerical_recipes-3E.pdf" target="_blank" rel="noopener">Numerical Recipes：数值运算</a>【20.41 MB】</li></ul><h2 id="机器学习算法"><a href="#机器学习算法" class="headerlink" title="机器学习算法"></a>机器学习算法</h2><ul><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/machine-learning.pdf" target="_blank" rel="noopener">Machine Learning：机器学习中文版-周志华</a>【85.68 MB】</li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/1-C4.5.pdf" target="_blank" rel="noopener">C4.5</a></li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/1-C4.5.pdf" target="_blank" rel="noopener">K-means</a></li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/3-SVM.pdf" target="_blank" rel="noopener">SVM</a></li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/4-Apriori.pdf" target="_blank" rel="noopener">Apriori</a></li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/5-EM.pdf" target="_blank" rel="noopener">EM</a></li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/6-PageRank.pdf" target="_blank" rel="noopener">PageRank</a></li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/8-kNN.pdf" target="_blank" rel="noopener">kNN</a></li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/9-Naive-Bayes.pdf" target="_blank" rel="noopener">Naive-Bayes</a></li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/10-CART.pdf" target="_blank" rel="noopener">CART</a></li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/Deep%20Learning-Yoshua%20Bengio.pdf" target="_blank" rel="noopener">Deep Learning：深度学习中文版</a>【30.90 MB】</li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/Building%20Machine%20Learning%20Projects%20with%20TensorFlow.pdf" target="_blank" rel="noopener">Building Machine Learning Projects with TensorFlow</a>【13.42 MB】</li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/statistical-learning-method.pdf" target="_blank" rel="noopener">统计学习方法-李航</a>【17.56 MB】</li></ul><h2 id="OpenCV"><a href="#OpenCV" class="headerlink" title="OpenCV"></a>OpenCV</h2><ul><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/OpenCV-Guide-Primer.pdf" target="_blank" rel="noopener">OpenCV Guide：Opencv简明教程</a></li></ul><h2 id="会议"><a href="#会议" class="headerlink" title="会议"></a>会议</h2><ul><li><a href="http://www.cvpapers.com/" target="_blank" rel="noopener">CVPapers - Computer Vision Resource</a></li><li><a href="https://vincentqin.gitee.io/blogresource-2/cv-books/ccf_conferences.pdf" target="_blank" rel="noopener">中国计算机学会推荐国际会议及期刊目录</a></li></ul><p>To be continued, welcome to commit.</p>]]></content>
      
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>立体视觉综述：Stereo Vision Overview</title>
      <link href="/posts/stereo-vision-overview/"/>
      <url>/posts/stereo-vision-overview/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img width="1600px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/depth-overview-cover.jpg"></p><p>本文主要翻译自<a href="www.vision.deis.unibo.it/smatt">Mattoccia</a>的双目视差估计综述，对于刚刚接触立体深度估计方向的小伙伴会有帮助；如果你是专家也可以做一下复习，如有错误请在评论中指出。文中主要介绍以下几个方面的内容：</p><ul><li>Introduction to stereo vision</li><li>Overview of a stereo vision system</li><li>Algorithms for visual correspondence</li><li>Computational optimizations</li><li>Hardware implementation</li><li>Applications</li></ul><a id="more"></a><h2 id="什么是立体视觉"><a href="#什么是立体视觉" class="headerlink" title="什么是立体视觉"></a>什么是立体视觉</h2><ul><li>是一个能够从双目或者多目相机中提取深度图像的技术</li><li>在计算机视觉领域很火爆的研究话题</li><li>这与以下几个方面的相关：双目立体视觉系统、稠密立体算法、立体视觉应用</li><li>偏好能够实时或者硬件实现</li></ul><h2 id="单目相机"><a href="#单目相机" class="headerlink" title="单目相机"></a>单目相机</h2><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p6.png"></p><p>如图所示的是单目摄像机的拍摄原理，右侧实际场景可以抽象成左侧的模型。可以发现场景中的P点与Q点会同时汇聚在成像平面中的一点，同样遮挡问题出现在PQ连线的所有点。</p><h2 id="双目相机"><a href="#双目相机" class="headerlink" title="双目相机"></a>双目相机</h2><p>对于双目相机，$O_R$和$O_T$分别是左右相机的光学中心，对于在参考相机像平面上被汇聚的两点（p和q），在目标相机像平面上会被区分开来，那么我们可以找到双目或者多目相机中匹配的点利用三角相似原理来估计深度。那么我们怎么寻找相对应的点呢？一个直观的想法就是固定两幅图中的一幅，然后在另外一幅图中 进行2D范围的搜索匹配点。<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p8.png"></p><p>但实际情况这样做的代价非常大。不过多亏了有<strong>极线约束</strong>，我们可以在图像的<strong>1D</strong>范围上进行搜索。以下将要对极线约束进行解释。</p><h3 id="极线约束-对极几何"><a href="#极线约束-对极几何" class="headerlink" title="极线约束(对极几何)"></a>极线约束(对极几何)</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p9.png"></p><ul><li>对于参考图像R而言，现实场景中的P与Q点在其像平面${\pi}_R$上被投影成为一个点p=q。</li><li>极线约束规定，属于（红色）视线的点对应位于目标图像T的图像平面${\pi}_T$上的绿线上。</li></ul><p>我们可以在维基百科上找到更为详细的<a href="https://en.wikipedia.org/wiki/Epipolar_geometry" target="_blank" rel="noopener">介绍</a>，具体描述可见下图。特别感谢<a href="https://blog.csdn.net/lin453701006/article/details/55096777" target="_blank" rel="noopener">@岳麓吹雪同学</a>的帮忙，以下是他已经整理好的译文。下图是针孔相机模型图。两个针孔相机看向空间点，实际相机的像面位于焦点中心后面，生成了一幅关于透镜的焦点中心对称的图像。<strong>这个问题可以简化为在焦点中心前方放置一个虚拟像面来生成正立图像，而不需要对称变换得到</strong>。$O_L$和$O_R$表示两个相机透镜中心，$X$表示两个相机共同的目标点，$X_L$和$X_R$是点$X$在两像面上的投影。<br><img width="1600px" data-src="/stereo-vision-overview/Epipolar_geometry.svg"></p><ul><li><p><strong>epipolar points极点</strong><br>每一个相机的透镜中心是不同的，会投射到另一个相机像面的不同点上。这两个像点用$e_L$和$e_R$表示，被称为<strong>epipolar points极点</strong>。两个极点$e_L$、$e_R$分别与透镜中心$O_L$、$O_R$在空间中位于一条直线上。</p></li><li><p><strong>epipolar plane极面</strong><br>将$X$、$O_L$和$O_R$三点形成的面称为epipolar plane极面。</p></li><li><p><strong>epipolar line极线</strong><br>直线$O_LX$被左侧相机看做一个点，因为它和透镜中心位于一条线上。然而，从右相机看直线$O_LX$，则是像面上的一条线直线$e_RX_R$，被称为epipolar line极线。从另一个角度看，极面$XO_LO_R$与相机像面相交形成极线。极线是3D空间中点X的位置函数，随$X$变化，两幅图像会生成一组极线。直线$O_LX$通过透镜中心$O_L$，右像面中对应的极线必然通过极点$e_R$。一幅图像中的所有极线包含了该图像的所有极点。实际上，任意一条包含极点的线都是由空间中某一点$X$推导出的一条极线。</p></li></ul><p>如果两个相机位置已知，则：<br>1.如果投影点$X_L$已知，则极线$e_RX_R$已知，点X必定投影到右像面极线上的$X_R$处。这就意味着，在一个图像中观察到的每个点，在已知的极线上观察到该点的其他图像。这就是Epipolar constraint极线约束：<strong>$X$在右像面上的投影$X_R$必然被约束在$e_RX_R$极线上</strong>。对于$O_LX_L$上的$X$，$X_1$，$X_2$，$X_3$都受该约束。极线约束可以用于测试两点是否对应同一3D点。极线约束也可以用两相机间的基本矩阵来描述。<br>2.如果$X_L$和$X_R$已知，他们的投影线已知。如果两幅图像上的点对应同一点X，则投影线必然交于$X$。这就意味着$X$可以用两个像点的坐标计算得到。</p><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p10.png"><br>由极线约束可知，我们可以将原来的匹配点搜索范围由2D转换成1D，这样做可以很大程度上减少计算量。我们将左右视图摆放成更容易理解的形式，可以发现对应点的匹配问题转换成了在同一条扫描线上（极线）的匹配问题。</p><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p11.png"><br>可以发现相机的摆放姿势影响着扫描线的方向。在上图A中，相机与水平呈一定角度地摆放，其扫描线为右图所示，同样是与水平倾斜的扫描线。假如两个相机平行摆放的话，其拍出来匹配对是扫描线已经对齐了的。</p><h3 id="深度与视差"><a href="#深度与视差" class="headerlink" title="深度与视差"></a>深度与视差</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p12.png"><br>如上图所示为扫描线已经对齐了的匹配图像对（以下简称<strong>匹配对</strong>）。可以发现：$PO_RO_T$与$Ppp’$是相似三角形，由于相似三角形原理，我们可以很容易知道：</p><script type="math/tex; mode=display">\frac{b}{Z}=\frac{(b+x_T)-x_R}{Z-f}</script><p>其中，$x_R-x_T$就是视差，Z表示深度，B为基线，f是焦距。</p><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p13.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p14-1.png"></p><p>所谓<strong>视差就是匹配对中对应点之间x方向上的差异</strong>，我们可以将这种差异转换成为灰度图（越近越白），如上最后一个图所示。<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p14-2.png"></p><p>上图展示了物体距离相机越近的话，视差就越大。其实很容易理解，将人的双眼比作成双目相机，对比将手指放在双眼前方近处与远处晃动的区别，可以发现在近处的话人眼感知到手指的晃动是比远处晃动的“程度”明显的，那么这种程度就是视差在人脑中的反映。</p><h3 id="视界"><a href="#视界" class="headerlink" title="视界"></a>视界</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p15.png"></p><p>图为双摄装置，基线为b，焦距为f，那么双摄的视界被视差范围所限定{$d_{min},d_{max}$}，如图中绿色包裹的区域。</p><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p16.png"></p><ul><li>深度是通过利用立体匹配系统将视差离散成一系列平行的平面来测量的；每一层平面对应着一个视差。</li><li>可以通过超像素的方法得到效果更好的深度图。</li></ul><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p17.png"><br>图为5个视差{$d_{min},d_{min}+4$}组成的视场。</p><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p18.png"></p><ul><li>图为5个视差{$\Delta+d_\{min\},\Delta+d_\{min}+4$\}组成的视场</li><li>$\Delta&gt;0$时，视场收缩并向相机靠近</li></ul><h2 id="深度估计"><a href="#深度估计" class="headerlink" title="深度估计"></a>深度估计</h2><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p21.png"><br>图中为传统算法以及ICCV2011当时最好的结果。可以发现，能够达到较好的视差是具有挑战性的。下面将要展示视差估计的基本流程。</p><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p22.png"><br>通过双摄设备采集图像，此时图像是存在镜头畸变的，在进行扫描线对齐之前要进行离线标定以消除镜头畸变。扫描线对齐的过程叫做<strong>镜头矫正</strong>（rectificaition），经过这步之后就可以进行1D的匹配点搜索（stereo correspondence）了。随后通过三角形相似原理得到相应的深度/视差图。</p><h3 id="离线标定"><a href="#离线标定" class="headerlink" title="离线标定"></a>离线标定</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p23.png"><br>标定的目标是寻找：</p><ul><li>相机内参：焦距、图像中心、镜头畸变参数</li><li>相机外参：排列相机使其对齐的参数</li></ul><p>注意的是，相机标定的话一般需要10对以上的图像（通常拍摄棋盘格图像，利用张氏标定法进行标定）。</p><ul><li>标定程序可以见Opencv<sup><a href="#fn_39" id="reffn_39">39</a></sup>和Matlab<sup><a href="#fn_40" id="reffn_40">40</a></sup>。</li><li>更为详细的介绍参见<sup><a href="#fn_20" id="reffn_20">20</a></sup> <sup><a href="#fn_21" id="reffn_21">21</a></sup> <sup><a href="#fn_22" id="reffn_22">22</a></sup>。</li></ul><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p25.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p26.png"></p><h3 id="匹配矫正"><a href="#匹配矫正" class="headerlink" title="匹配矫正"></a>匹配矫正</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p27.png"><br>利用标定步骤得到的相机的内参对相机镜头畸变进行校正，同时对其扫描线。</p><h3 id="立体匹配"><a href="#立体匹配" class="headerlink" title="立体匹配"></a>立体匹配</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p28.png"><br>目标：从匹配对中寻找对应的点，反映在图像中就是视差图像。</p><h3 id="三角测量"><a href="#三角测量" class="headerlink" title="三角测量"></a>三角测量</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p29.png"><br>给定视差图像，基线长度以及焦距可以通过三角计算得到当前位置对应的3D位置。</p><h2 id="立体匹配的挑战性"><a href="#立体匹配的挑战性" class="headerlink" title="立体匹配的挑战性"></a>立体匹配的挑战性</h2><h3 id="光度失真以及噪声"><a href="#光度失真以及噪声" class="headerlink" title="光度失真以及噪声"></a>光度失真以及噪声</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p37-1.png"></p><h3 id="高光表面"><a href="#高光表面" class="headerlink" title="高光表面"></a>高光表面</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p37-2.png"></p><h3 id="透视收缩"><a href="#透视收缩" class="headerlink" title="透视收缩"></a>透视收缩</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p38.png"></p><h3 id="透视变形"><a href="#透视变形" class="headerlink" title="透视变形"></a>透视变形</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p39-1.png"></p><h3 id="无纹理区域"><a href="#无纹理区域" class="headerlink" title="无纹理区域"></a>无纹理区域</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p39-2.png"></p><h3 id="重复-混淆区域"><a href="#重复-混淆区域" class="headerlink" title="重复/混淆区域"></a>重复/混淆区域</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p40.png"></p><h3 id="透明物体"><a href="#透明物体" class="headerlink" title="透明物体"></a>透明物体</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p41-1.png"></p><h3 id="遮挡区以及不连续区域（1）"><a href="#遮挡区以及不连续区域（1）" class="headerlink" title="遮挡区以及不连续区域（1）"></a>遮挡区以及不连续区域（1）</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p41-2.png"></p><h3 id="遮挡区以及不连续区域（2）"><a href="#遮挡区以及不连续区域（2）" class="headerlink" title="遮挡区以及不连续区域（2）"></a>遮挡区以及不连续区域（2）</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p42.png"></p><h2 id="Middlebury数据集"><a href="#Middlebury数据集" class="headerlink" title="Middlebury数据集"></a>Middlebury数据集</h2><p><a href="http://vision.middlebury.edu/stereo/eval3/" target="_blank" rel="noopener">Middlebury数据集</a>提供了一套可供深度估计的数据集以及评价系统，深度估计算法可在该数据集上进行测试性能。2003年的数据集提供了Tsukuba, Venus, Teddy and Cones这几个场景的匹配对。</p><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p44.png"></p><h2 id="匹配问题"><a href="#匹配问题" class="headerlink" title="匹配问题"></a>匹配问题</h2><p>立体匹配的算法可以分成以下几个步骤：</p><ol><li>匹配量/损失计算</li><li>损失聚合</li><li>视差计算/优化</li><li>视差精化</li></ol><ul><li>局部算法包括：<br>1-&gt;2-&gt;3（简单的WTA算法）</li><li>全局算法包括：<br>1（-&gt;2）-&gt;3（全局或者半全局算法）</li></ul><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>数据预处理是为了消除图像的光度失真。常见的操作有：</p><ul><li>LoG滤波器<sup><a href="#fn_41" id="reffn_41">41</a></sup></li><li>消减附近像素中计算的平均值<sup><a href="#fn_42" id="reffn_42">42</a></sup></li><li>双边滤波</li><li>统计变换</li></ul><p>最简单的立体匹配算法如下图所示，逐像素地计算SAD匹配损失；然后通过WTA得到初始视差，但是此时得到的视差质量是很差的。那么如何提高深度图像的质量呢？通常来说有两种不同类别的策略。<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p47.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p48.png"></p><ul><li>局部算法。同样是利用到了简单的WTA提取到初始视差，但是通过计算窗口内的损失量提高了信噪比。有时需会加入平滑项。Steps 1+2 (+ WTA)<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p50.png"></li><li>全局/半全局算法。寻找能够使得能量函数的最小值的视差以得到逐点视差。Steps 1+ Step3。（有时，损失函数需要聚合）</li></ul><p>两种算法都假设了匹配对是平滑的，但有时，该假设并不成立。这个假设在局部算法中隐晦地提及，却在全局算法中明确地建模，如下形式。</p><script type="math/tex; mode=display">E(d)=E_{data}(d)+E_{smooth}(d)</script><h2 id="损失量的计算"><a href="#损失量的计算" class="headerlink" title="损失量的计算"></a>损失量的计算</h2><h3 id="逐像素的匹配误差"><a href="#逐像素的匹配误差" class="headerlink" title="逐像素的匹配误差"></a>逐像素的匹配误差</h3><ul><li><p>绝对值误差</p><script type="math/tex; mode=display">e(x,y,d)=|I_R(x,y)-I_T(x+d,y)|</script></li><li><p>平方误差</p><script type="math/tex; mode=display">e(x,y,d)=(I_R(x,y)-I_T(x+d,y))^2</script></li><li><p>鲁棒匹配子（M-estimators）<br>如截断绝对误差（truncated absolute differences (TAD)）可以减少离群点的干扰：</p><script type="math/tex; mode=display">e(x,y,d)=min\{|I_R(x,y)-I_T(x+d,y),T\}</script></li><li><p>相异性测量对于图像噪声不敏感（Birchfield and Tomasi<sup><a href="#fn_27" id="reffn_27">27</a></sup>）</p></li></ul><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p52.png"><br>视差空间图像（DSI）是一个如下图所示张量$W\times H\times(d_{max}-d_{min})$，其中的每一个元素$C(x,y,d)$表示$I_R(x_R,y)$与$I_T(x_R+d,y)$之间的匹配度。</p><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p53.png"></p><h3 id="区域匹配损失"><a href="#区域匹配损失" class="headerlink" title="区域匹配损失"></a>区域匹配损失</h3><ul><li>绝对误差和（Sum of Absolute differences (SAD)）<script type="math/tex; mode=display">C(x,y,d)=\sum_{x\in S}|I_R(x,y)-I_T(x+d,y)|</script></li><li>绝对平方和（Sum of Squared differences (SSD)）<script type="math/tex; mode=display">C(x,y,d)=\sum_{x\in S}\left(I_R(x,y)-I_T(x+d,y)\right)^2</script></li><li>截断绝对误差和（Sum of truncated absolute differences (STAD)）<script type="math/tex; mode=display">C(x,y,d)=\sum_{x\in S}\{|I_R(x,y)-I_T(x+d,y),T\}</script></li><li>Normalized Cross Correlation <sup><a href="#fn_57" id="reffn_57">57</a></sup></li><li>Zero mean Normalized Cross Correlation <sup><a href="#fn_58" id="reffn_58">58</a></sup></li><li>Gradient based MF <sup><a href="#fn_59" id="reffn_59">59</a></sup></li><li>Non parametric <sup><a href="#fn_60" id="reffn_60">60</a></sup> <sup><a href="#fn_61" id="reffn_61">61</a></sup></li><li>Mutual Information <sup><a href="#fn_30" id="reffn_30">30</a></sup></li><li>Combination of matching costs</li></ul><h2 id="损失聚合"><a href="#损失聚合" class="headerlink" title="损失聚合"></a>损失聚合</h2><p>那么从最简单的固定窗口（FW）损失聚合开始，以下为利用FW聚合的TAD损失然后利用WTA得到的深度图。理想是完美的，但现实是骨感的，可以看到下图给出的结果并不佳，这是什么原因呢？<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p57.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p58.png"><br>a. 隐含地假设前额表面处于同一视差<br>b. 忽略了深度的非连续性<br>c. 平坦区域的处理不佳<br>d. 重复的区域</p><p>对于a. 隐含地假设前额表面处于同一视差，很多即使是当前最好的损失聚合算法也会假设：在一个小的支持域里面的所有点所处的视差是相同的。但实际情况并非如此，可以观察以上两图，人体头像模型的面部是不规则的表面，展现出来的是视差的不断变化；下面的图是桌子平面，它表面是倾斜的，同样表现出来的是视差的变化。</p><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p59.png"></p><p>对于b. 忽略了深度的非连续性，原本假设真实场景中的正面平行表面在支持域内深度不会变化，但是这个假设在深度不连续处的附近被打破。可以看到下图中在台灯灯罩的边界处出现了深度的间断，这样经过损失聚合之后得到的深度就会出现边界误匹配的现象，表现在图中为边界没有很好的对齐。不过利用TAD可以在一定程度上减少这种现象。</p><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p60-1.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p60-2.png"></p><p>目前最好的损失聚合算法都在想方设法地改变支持域的形状以适应在仅在相同的已知视差上做匹配。对于FW而言，就是减小其窗口大小，来减少边界定位问题。但是与此同时，这个改变使得匹配问题变得含糊不清，特别是对于有重复区域以及平滑区域的情形。<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p61.png"></p><p>对于c与d，FW并不能很好地处理。在这两种情况下，损失聚合算法应该不断地加大支持域的尺寸以获得更多的相同深度上的点。<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p62.png"></p><p>以上为FW所面对的诸多问题。令人吃惊的是，虽然FW看起来如此不堪一击，但是其应用却是如此广泛。原因可能有以下几点：</p><ol><li>容易实现；</li><li>快！(特别感谢增量计算框架)；</li><li>可以在传统的处理器上实时完成计算；</li><li>仅需要很小的内存；</li><li>可硬件（FPGA）实时实现，且功率小（&lt;1W）</li></ol><p>在介绍更加复杂的算法之前，我们首先介绍积分图像（Integral Images (II)）以及箱滤波（Box-Filtering (BF)）。</p><h3 id="积分图像"><a href="#积分图像" class="headerlink" title="积分图像"></a>积分图像</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p64.png"></p><h3 id="箱滤波器"><a href="#箱滤波器" class="headerlink" title="箱滤波器"></a>箱滤波器</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p65.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p66.png"></p><p>可以总结出积分图与箱滤波器的关系：</p><ol><li>每个点需要4个运算</li><li>积分图可以支持不同的支持域尺寸</li><li>积分图有溢出风险</li><li>积分图对内存消耗较大</li></ol><p>在实际应用当中，积分图对于可变支持域的情况会有帮助。</p><h2 id="立体匹配中损失聚合策略的分类及评估"><a href="#立体匹配中损失聚合策略的分类及评估" class="headerlink" title="立体匹配中损失聚合策略的分类及评估"></a>立体匹配中损失聚合策略的分类及评估</h2><p>在文献<sup><a href="#fn_1" id="reffn_1">1</a></sup>中，作者实现、分类以及评估了超过10种损失聚合算法。这些损失聚合的策略包含几种方式：</p><ul><li>位置</li><li>方向</li><li>位置与方向</li><li>权重</li></ul><p>接下来就对文中但不限于文中提到的诸多算法进行介绍 (i.e. Fast Aggregation <sup><a href="#fn_64" id="reffn_64">64</a></sup>, Fast Bilateral Stereo (FBS) <sup><a href="#fn_65" id="reffn_65">65</a></sup> and the Locally Consistent (LC) methodology <sup><a href="#fn_66" id="reffn_66">66</a></sup>)。</p><h3 id="固定窗口"><a href="#固定窗口" class="headerlink" title="固定窗口"></a>固定窗口</h3><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p87.png"></p><h3 id="可移动窗口11"><a href="#可移动窗口11" class="headerlink" title="可移动窗口11"></a>可移动窗口<sup><a href="#fn_11" id="reffn_11">11</a></sup></h3><p>这种方法是为了应对场景边界定位问题，这种算法不限制当前位置位于支持域中心。<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p88.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p89.png"></p><h3 id="多窗口7"><a href="#多窗口7" class="headerlink" title="多窗口7"></a>多窗口<sup><a href="#fn_7" id="reffn_7">7</a></sup></h3><p>支持域内元素个数为常数；支持域的形状不限于为矩形；支持域大小可为5、9、25（5W,9W,25W）。下图所示的为9W：<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p90.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p91.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p92.png"></p><h3 id="可变窗口12"><a href="#可变窗口12" class="headerlink" title="可变窗口12"></a>可变窗口<sup><a href="#fn_12" id="reffn_12">12</a></sup></h3><p>这种方式，支持域的形状是固定的但是尺寸是变化的。支持域的位置是可变的。<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p94.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p95.png"></p><h3 id="基于分割的窗口5"><a href="#基于分割的窗口5" class="headerlink" title="基于分割的窗口5"></a>基于分割的窗口<sup><a href="#fn_5" id="reffn_5">5</a></sup></h3><p>这种方式根据图像的颜色相似性将其分割成一系列图像块，这对于损失聚合、深度图像优化以及离群点检测都有帮助。这种算法假设：每个分割块内深度平滑变化。由于涉及到图像分割此时要求分割的精度很高，并且分割后的每个支持域的形状也是不规则的。如下图所示，对于一个可允许的最大支持域范围内，包含支持域中心点所在的分割所覆盖支持域权重赋值为1，支持域的其余部分赋值为$\lambda$，其中$\lambda&lt;<1$。 <img src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p98-1.png" width="1200px"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p98-2.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p99.png"></1$。></p><h3 id="自适应加权14-51"><a href="#自适应加权14-51" class="headerlink" title="自适应加权14 51"></a>自适应加权<sup><a href="#fn_14" id="reffn_14">14</a></sup> <sup><a href="#fn_51" id="reffn_51">51</a></sup></h3><p>首先介绍双边滤波，双边滤波是一种边缘保持滤波器，它是根据图像的邻域的颜色以及空间相关性对每个中心点进行加权。类似于双边滤波，自适应加权对其进行了简化，只考虑颜色的相关性。每一个损失都被乘以了一个这样的权重可以得到$C(p_c,q_c)$。<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p101.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p102.png"><br>以下是自适应加权的结果：<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p103.png"></p><h3 id="可分支持域10"><a href="#可分支持域10" class="headerlink" title="可分支持域10"></a>可分支持域<sup><a href="#fn_10" id="reffn_10">10</a></sup></h3><h3 id="快速聚合64"><a href="#快速聚合64" class="headerlink" title="快速聚合64"></a>快速聚合<sup><a href="#fn_64" id="reffn_64">64</a></sup></h3><ul><li>假设：在每个分割块内的深度变化平缓；</li><li>损失量：TAD；</li><li>只对参考图像进行聚合；</li><li>对称的支持域</li><li>支持域覆盖整个分割块<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p109.png"><script type="math/tex; mode=display">C_{agg}(p,q,d)=\frac{C_S(p,q,d)}{|S_p|}+\frac{C_W(p,q,d)}{|r^2|}</script><script type="math/tex; mode=display">C_S(p,q,d)=\sum_{p_i \in S_p}{TAD(p_i,q_{i+d})}</script><script type="math/tex; mode=display">C_W(p,q,d)=\sum_{p_i \in W_p}{TAD(p_i,q_{i+d})}</script>其中$C_W$为了消除“分割锁定”，这一项可能在纹理稠密的区域很有帮助，但是这一项有可能带来深度的不连续性。<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p111.png"></li></ul><h3 id="快速双边滤波65"><a href="#快速双边滤波65" class="headerlink" title="快速双边滤波65"></a><a href="http://vision.deis.unibo.it/~smatt/fast_bilateral_stereo.htm" target="_blank" rel="noopener">快速双边滤波</a><sup><a href="#fn_65" id="reffn_65">65</a></sup></h3><p>该方法兼顾了自适应加权的精度以及传统方法的效率。通过逐块的利用双边滤波对损失进行规范化，通过这种方式可以增加对噪声的鲁棒性。可以利用前面提及的积分图或者箱滤波的方式快速计算。由于双边滤波的局部计算特性，可以利用GPU进行加速，<a href="http://vision.deis.unibo.it/~smatt/FBS_GPU.html" target="_blank" rel="noopener">GPU加速版本</a>。结果如下所示：<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p115.png"></p><h3 id="局部一致性"><a href="#局部一致性" class="headerlink" title="局部一致性"></a><a href="http://vision.deis.unibo.it/~smatt/lc_stereo.htm" target="_blank" rel="noopener">局部一致性</a></h3><p>通过对像素之间的一致性约束进行建模寻找像素之间的相关关系，这种方法对于目前最好的方法具有显著的效果提升。<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p122.png"></p><h3 id="O-1-adaptive-cost-aggregation75"><a href="#O-1-adaptive-cost-aggregation75" class="headerlink" title="O(1) adaptive cost aggregation75"></a>O(1) adaptive cost aggregation<sup><a href="#fn_75" id="reffn_75">75</a></sup></h3><p>该方法受引导滤波的启发，效果还不错，可与最佳效果相媲美。<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p127.png"></p><h2 id="视差-深度计算以及优化"><a href="#视差-深度计算以及优化" class="headerlink" title="视差/深度计算以及优化"></a>视差/深度计算以及优化</h2><p>该步骤是为了寻找可最小化损失函数的视差（或者得到DSI图像的最佳路径以最小化能量函数）。通常情况下，能量函数可以表示为以下形式</p><script type="math/tex; mode=display">E(d)=E_{data}(d)+E_{smooth}(d)</script><p>其中的数据项$E_{data}(d)$为了衡量目前假定的视差能够以何等程度接近真实视差。目前已经有不少逐像素的损失构造策略，但是目前也涌现了许多基于支持域的数据项。<br>另外一项是平滑项或者叫做正则项$E_{smooth}(d)$，它可以对像素之间的连续性或者相似性进行约束：这一项对大的视差给予大的惩罚，同时对于边界处的大视差变化以及小的惩罚。也就是说，视差的变化在边界处是被允许的。</p><p>以上模型的求解是个NP-hard问题，在这里可以借助几种常用的策略对其进行求解。</p><ul><li>Graph Cuts <sup><a href="#fn_52" id="reffn_52">52</a></sup></li><li>Belief Propagation <sup><a href="#fn_53" id="reffn_53">53</a></sup></li><li>Cooperative optimization <sup><a href="#fn_54" id="reffn_54">54</a></sup></li></ul><p>这些方法的比较在[63]中进行了详述。有意思的是，上述问题的解决可以由动态规划以及扫描线优化来解决。</p><h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><ul><li>高效 (polynomial time) ≈ 1 sec</li><li>边界以及纹理稀疏区域有所帮助</li><li>条带现象<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p135.png"></li></ul><h3 id="扫描线优化（Scanline-Optimization，SO30）"><a href="#扫描线优化（Scanline-Optimization，SO30）" class="headerlink" title="扫描线优化（Scanline Optimization，SO30）"></a>扫描线优化（Scanline Optimization，SO<sup><a href="#fn_30" id="reffn_30">30</a></sup>）</h3><ul><li>高效 (polynomial time) ≈ few seconds</li><li>边界以及纹理稀疏区域有所帮助</li><li>条带现象</li><li>高内存消耗<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p139.png"></li></ul><h2 id="视差精化"><a href="#视差精化" class="headerlink" title="视差精化"></a>视差精化</h2><p>原始视差中包含的离群点需要被检测以及被移除；同时，视差是离散的数据点，有时需要更高的精度；以下将会介绍亚像素插值、图像滤波、双向验证。</p><p><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p166.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p167.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p168.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p169.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p170.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p181.png"><br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/p183.png"></p><font color="F08080" size="5">未完待续...</font><p><strong>注：</strong>特此感谢Stefano Mattoccia给出如此良心的立体视觉综述，<a href="http://www.vision.deis.unibo.it/smatt/Seminars/StereoVision.pdf" target="_blank" rel="noopener">本文最新版本在此</a>(速度较慢)或者<a href="https://vincentqin.gitee.io/blogresource-1/stereo-vision-overview/StereoVision.pdf" target="_blank" rel="noopener">这里</a>(较快，大小51.61M)。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><blockquote id="fn_1"><sup>1</sup>. F. Tombari, S. Mattoccia, L. Di Stefano, E. Addimanda, Classification and evaluation of cost aggregation methods for stereo correspondence, IEEE International Conference on Computer Vision and Pattern Recognition (CVPR 2008)<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. Y. Boykov, O. Veksler, and R. Zabih, A variable window approach to early vision IEEE Trans. PAMI, 20(12):1283–1294, 1998<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. S. Chan, Y. Wong, and J. Daniel, Dense stereo correspondence based on recursive adaptive size multi-windowing In Proc. Image and Vision Computing New Zealand (IVCNZ’03), volume 1, pages 256–260, 2003<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. C. Demoulin and M. Van Droogenbroeck, A method based on multiple adaptive windows to improve the determination of disparity maps. In Proc. IEEE Workshop on Circuit, Systems and Signal Processing, pages 615–618, 2005<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. M. Gerrits and P. Bekaert. Local Stereo Matching with Segmentation-based Outlier Rejection In Proc. Canadian Conf. on Computer and Robot Vision (CRV 2006), pages 66-66, 2006.<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. M. Gong and R. Yang. Image-gradient-guided real-time stereo on graphics hardware In Proc. Int. Conf. 3D Digital Imaging and Modeling (3DIM), pages 548–555, 2005<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote><blockquote id="fn_7"><sup>7</sup>. H. Hirschmuller, P. Innocent, and J. Garibaldi, Real-time correlation-based stereo vision with reduced border errors Int. Journ. of Computer Vision, 47:1–3, 2002<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a></blockquote><blockquote id="fn_8"><sup>8</sup>. S. Kang, R. Szeliski, and J. Chai, Handling occlusions in dense multi-view stereo In Proc. Conf. on Computer Vision and Pattern Recognition (CVPR 2001), pages 103–110, 2001<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a></blockquote><blockquote id="fn_9"><sup>9</sup>. J. Kim, K. Lee, B. Choi, and S. Lee. A dense stereo matching using two-pass dynamic programming with generalized ground control points, In Proc. Conf. on Computer Vision and Pattern Recognition (CVPR 2005), pages 1075–1082, 2005<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a></blockquote><blockquote id="fn_10"><sup>10</sup>. F. Tombari, S. Mattoccia, and L. Di Stefano, Segmentation-based adaptive support for accurate stereo correspondence PSIVT 2007<a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a></blockquote><blockquote id="fn_11"><sup>11</sup>. D. Scharstein and R. Szeliski, A taxonomy and evaluation of dense two-frame stereo correspondence algorithms Int. Jour. Computer Vision, 47(1/2/3):7–42, 2002.<a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a></blockquote><blockquote id="fn_12"><sup>12</sup>. O. Veksler. Fast variable window for stereo correspondence using integral images, In Proc. Conf. on Computer Vision and Pattern Recognition (CVPR 2003), pages 556–561, 2003<a href="#reffn_12" title="Jump back to footnote [12] in the text."> &#8617;</a></blockquote><blockquote id="fn_13"><sup>13</sup>. Y. Xu, D. Wang, T. Feng, and H. Shum, Stereo computation using radial adaptive windows, In Proc. Int. Conf. on Pattern Recognition (ICPR 2002), volume 3, pages 595–598, 2002<a href="#reffn_13" title="Jump back to footnote [13] in the text."> &#8617;</a></blockquote><blockquote id="fn_14"><sup>14</sup>. K. Yoon and I. Kweon, Adaptive support-weight approach for correspondence search, IEEE Trans. PAMI, 28(4):650–656,2006<a href="#reffn_14" title="Jump back to footnote [14] in the text."> &#8617;</a></blockquote><blockquote id="fn_15"><sup>15</sup>. D. Scharstein and R. Szeliski, <a href="http://vision.middlebury.edu/stereo/eval/" target="_blank" rel="noopener">http://vision.middlebury.edu/stereo/eval/</a><a href="#reffn_15" title="Jump back to footnote [15] in the text."> &#8617;</a></blockquote><blockquote id="fn_16"><sup>16</sup>. A. Ansar, A. Castano, L. Matthies, Enhanced real-time stereo using bilateral filtering IEEE Conference on Computer Vision and Pattern Recognition 2004<a href="#reffn_16" title="Jump back to footnote [16] in the text."> &#8617;</a></blockquote><blockquote id="fn_17"><sup>17</sup>. D. Scharstein and R. Szeliski, 􀀃High-accuracy stereo depth maps using structured light. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2003), volume 1, pages 195-202<a href="#reffn_17" title="Jump back to footnote [17] in the text."> &#8617;</a></blockquote><blockquote id="fn_18"><sup>18</sup>. D. Scharstein and C. Pal. Learning conditional random fields for stereo.In IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2007)<a href="#reffn_18" title="Jump back to footnote [18] in the text."> &#8617;</a></blockquote><blockquote id="fn_19"><sup>19</sup>. H. Hirschmüller and D. Scharstein. Evaluation of cost functions for stereo matching.In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2007)<a href="#reffn_19" title="Jump back to footnote [19] in the text."> &#8617;</a></blockquote><blockquote id="fn_20"><sup>20</sup>. E. Trucco, A. Verri, Introductory Techniques for 3-D Computer Vision, Prentice Hall, 1998<a href="#reffn_20" title="Jump back to footnote [20] in the text."> &#8617;</a></blockquote><blockquote id="fn_21"><sup>21</sup>. R.I.Hartley, A. Zisserman, Multiple View Geometry in Computer Vision, Cambridge University Press, 2000<a href="#reffn_21" title="Jump back to footnote [21] in the text."> &#8617;</a></blockquote><blockquote id="fn_22"><sup>22</sup>. G. Bradsky, A. Kaehler, Learning Opencv, O’Reilly, 2008<a href="#reffn_22" title="Jump back to footnote [22] in the text."> &#8617;</a></blockquote><blockquote id="fn_23"><sup>23</sup>. OpenCV Computer Vision Library, <a href="http://sourceforge.net/projects/opencvlibrary/" target="_blank" rel="noopener">http://sourceforge.net/projects/opencvlibrary/</a><a href="#reffn_23" title="Jump back to footnote [23] in the text."> &#8617;</a></blockquote><blockquote id="fn_24"><sup>24</sup>. Jean-Yves Bouguet , Camera Calibration Toolbox for Matlab, <a href="http://www.vision.caltech.edu/bouguetj/calib_doc/" target="_blank" rel="noopener">http://www.vision.caltech.edu/bouguetj/calib_doc/</a><a href="#reffn_24" title="Jump back to footnote [24] in the text."> &#8617;</a></blockquote><blockquote id="fn_25"><sup>25</sup>. M. A. Fischler and R. C. Bolles, Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography, Comm. of the ACM 24: 381–395, June 1981<a href="#reffn_25" title="Jump back to footnote [25] in the text."> &#8617;</a></blockquote><blockquote id="fn_26"><sup>26</sup>. Z. Wang and Z. Zheng, A region based stereo matching algorithm using cooperative optimization IEEE CVPR 2008<a href="#reffn_26" title="Jump back to footnote [26] in the text."> &#8617;</a></blockquote><blockquote id="fn_27"><sup>27</sup>. S. Birchfield and C. Tomasi. A pixel dissimilarity measure that is insensitive to image sampling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(4):401-406, April 1998<a href="#reffn_27" title="Jump back to footnote [27] in the text."> &#8617;</a></blockquote><blockquote id="fn_28"><sup>28</sup>. J. Zabih, J. Woodfill, Non-parametric local transforms for computing visual correspondence. European Conf. on Computer Vision, Stockholm, Sweden, 151–158<a href="#reffn_28" title="Jump back to footnote [28] in the text."> &#8617;</a></blockquote><blockquote id="fn_29"><sup>29</sup>. S. Mattoccia, F. Tombari, and L. Di Stefano, Stereo vision enabling precise border localization within a scanline optimization framework, ACCV 2007<a href="#reffn_29" title="Jump back to footnote [29] in the text."> &#8617;</a></blockquote><blockquote id="fn_30"><sup>30</sup>. H. Hirschmüller. Stereo vision in structured environments by consistent semi-global matching. CVPR 2006, PAMI 30(2):328-341, 2008<a href="#reffn_30" title="Jump back to footnote [30] in the text."> &#8617;</a></blockquote><blockquote id="fn_31"><sup>31</sup>. F. Tombari, S. Mattoccia, L. Di Stefano, F. Tonelli, Detecting motion by means of 2D and 3D information ACCV’07 Workshop on Multi-dimensional and Multi-view Image Processing (ACCV 2007 WS)<a href="#reffn_31" title="Jump back to footnote [31] in the text."> &#8617;</a></blockquote><blockquote id="fn_32"><sup>32</sup>. P. Azzari, L. Di Stefano, F. Tombari, S. Mattoccia, Markerless augmented reality using image mosaics International Conference on Image and Signal Processing (ICISP 2008)<a href="#reffn_32" title="Jump back to footnote [32] in the text."> &#8617;</a></blockquote><blockquote id="fn_33"><sup>33</sup>. Li Zhang, Brian Curless, and Steven M. Seitz Spacetime Stereo: Shape Recovery for Dynamic Scenes IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2003), pp. 367-374<a href="#reffn_33" title="Jump back to footnote [33] in the text."> &#8617;</a></blockquote><blockquote id="fn_34"><sup>34</sup>. J. Davis, D. Nehab, R. Ramamoothi, S. Rusinkiewicz. Spacetime Stereo : A Unifying Framework for Depth from Triangulation, IEEE Trans. On Pattern Analysis and Machine Intelligence (PAMI), vol. 27, no. 2, Feb 2005<a href="#reffn_34" title="Jump back to footnote [34] in the text."> &#8617;</a></blockquote><blockquote id="fn_35"><sup>35</sup>. F. Tombari, L. Di Stefano, S. Mattoccia, A. Zanetti, Graffiti detection using a Time-Of-Flight camera Advanced Concepts for Intelligent Vision Systems (ACIVS 2008)<a href="#reffn_35" title="Jump back to footnote [35] in the text."> &#8617;</a></blockquote><blockquote id="fn_36"><sup>36</sup>. L. Di Stefano, F. Tombari, A. Lanza, S. Mattoccia, S. Monti, Graffiti detection using two views ECCV 2008 - 8th International Workshop on Visual Surveillance (VS 2008)<a href="#reffn_36" title="Jump back to footnote [36] in the text."> &#8617;</a></blockquote><blockquote id="fn_37"><sup>37</sup>. T. Darrell, D. Demirdijan, N. Checka, P. Felzenszwalb, Plan-view trajectory estimation with dense stereo background models, International Conference on Computer Vision (ICCV 2001), 2001<a href="#reffn_37" title="Jump back to footnote [37] in the text."> &#8617;</a></blockquote><blockquote id="fn_38"><sup>38</sup>. M. Harville, Stereo person tracking with adaptive plan-view templates of height and occupancy statistics Image and Vision Computing 22(2) pp 127-142, February 2004<a href="#reffn_38" title="Jump back to footnote [38] in the text."> &#8617;</a></blockquote><blockquote id="fn_39"><sup>39</sup>. OpenCV Computer Vision Library, <a href="http://sourceforge.net/projects/opencvlibrary/" target="_blank" rel="noopener">http://sourceforge.net/projects/opencvlibrary/</a><a href="#reffn_39" title="Jump back to footnote [39] in the text."> &#8617;</a></blockquote><blockquote id="fn_40"><sup>40</sup>. Jean-Yves Bouguet , Camera Calibration Toolbox for Matlab, <a href="http://www.vision.caltech.edu/bouguetj/calib_doc/" target="_blank" rel="noopener">http://www.vision.caltech.edu/bouguetj/calib_doc/</a><a href="#reffn_40" title="Jump back to footnote [40] in the text."> &#8617;</a></blockquote><blockquote id="fn_41"><sup>41</sup>. T. Kanade, H. Kato, S. Kimura, A. Yoshida, and K. Oda, Development of a Video-Rate Stereo Machine International Robotics and Systems Conference (IROS ‘95), Human Robot Interaction and Cooperative Robots, 1995<a href="#reffn_41" title="Jump back to footnote [41] in the text."> &#8617;</a></blockquote><blockquote id="fn_42"><sup>42</sup>. O. Faugeras, B. Hotz, H. Mathieu, T. Viville, Z. Zhang, P. Fua, E. Thron, L. Moll, G. Berry, Real-time correlation-based stereo: Algorithm. Implementation and Applications, INRIA TR n. 2013, 1993<a href="#reffn_42" title="Jump back to footnote [42] in the text."> &#8617;</a></blockquote><blockquote id="fn_43"><sup>43</sup>. F. Crow, Summed-area tables for texture mapping, Computer Graphics, 18(3):207–212, 1984<a href="#reffn_43" title="Jump back to footnote [43] in the text."> &#8617;</a></blockquote><blockquote id="fn_44"><sup>44</sup>. M. Mc Donnel. Box-filtering techniques, Computer Graphics and Image Processing, 17:65–70, 1981<a href="#reffn_44" title="Jump back to footnote [44] in the text."> &#8617;</a></blockquote><blockquote id="fn_45"><sup>45</sup>. A. Goshtasby, 2-D and 3-D Image Registration for Medical, Remote Sensing and Industrial Applications New York: Wiley, 2005<a href="#reffn_45" title="Jump back to footnote [45] in the text."> &#8617;</a></blockquote><blockquote id="fn_46"><sup>46</sup>. B. Zitova and J. Flusser, Image registration methods:A survey, Image Vision Computing, vol. 21, no. 11, pp. 977–1000, 2003<a href="#reffn_46" title="Jump back to footnote [46] in the text."> &#8617;</a></blockquote><blockquote id="fn_47"><sup>47</sup>. Changming Sun, Recursive Algorithms for Diamond, Hexagon and General Polygonal Shaped Window Operations Pattern Recognition Letters, 27(6):556-566, April 2006<a href="#reffn_47" title="Jump back to footnote [47] in the text."> &#8617;</a></blockquote><blockquote id="fn_48"><sup>48</sup>. L. Di Stefano, M. Marchionni, S. Mattoccia, A fast area-based stereo matching algorithm, Image and Vision Computing, 22(12), pp 983-1005, October 2004<a href="#reffn_48" title="Jump back to footnote [48] in the text."> &#8617;</a></blockquote><blockquote id="fn_49"><sup>49</sup>. L. Di Stefano, M. Marchionni, S. Mattoccia, A PC-based real-time stereo vision system, Machine Graphics &amp; Vision, 13(3), pp. 197-220, January 2004<a href="#reffn_49" title="Jump back to footnote [49] in the text."> &#8617;</a></blockquote><blockquote id="fn_50"><sup>50</sup>. D. Comaniciu and P. Meer, Mean shift: A robust approach toward feature space analysis, IEEE Transactions on Pattern Analysis and Machine Intelligence, 24:603–619, 2002<a href="#reffn_50" title="Jump back to footnote [50] in the text."> &#8617;</a></blockquote><blockquote id="fn_51"><sup>51</sup>. C. Tomasi and R. Manduchi. Bilateral filtering for gray and color images. In ICCV98, pages 839–846, 1998<a href="#reffn_51" title="Jump back to footnote [51] in the text."> &#8617;</a></blockquote><blockquote id="fn_52"><sup>52</sup>. V. Kolmogorov and R. Zabih, Computing visual correspondence with occlusions using graph cuts, ICCV 2001<a href="#reffn_52" title="Jump back to footnote [52] in the text."> &#8617;</a></blockquote><blockquote id="fn_53"><sup>53</sup>. A. Klaus, M. Sormann and K. Karner, Segment-based stereo matching using belief propagation and a self-adapting dissimilarity measure, ICPR 2006<a href="#reffn_53" title="Jump back to footnote [53] in the text."> &#8617;</a></blockquote><blockquote id="fn_54"><sup>54</sup>. Z. Wang and Z. Zheng, A region based stereo matching algorithm using cooperative optimization, CVPR 2008<a href="#reffn_54" title="Jump back to footnote [54] in the text."> &#8617;</a></blockquote><blockquote id="fn_55"><sup>55</sup>. L. Di Stefano, S. Mattoccia, Real-time stereo within the VIDET project Real-Time Imaging, 8(5), pp. 439-453, Oct. 2002<a href="#reffn_55" title="Jump back to footnote [55] in the text."> &#8617;</a></blockquote><blockquote id="fn_56"><sup>56</sup>. F. Tombari, S. Mattoccia, L. Di Stefano, Full search-equivalent pattern matching with Incremental Dissimilarity Approximations, IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(1), pp 129-141, January 2009<a href="#reffn_56" title="Jump back to footnote [56] in the text."> &#8617;</a></blockquote><blockquote id="fn_57"><sup>57</sup>. S. Mattoccia, F. Tombari, L. Di Stefano, Fast full-search equivalent template matching by Enhanced Bounded Correlation, IEEE Transactions on Image Processing, 17(4), pp 528-538, April 2008<a href="#reffn_57" title="Jump back to footnote [57] in the text."> &#8617;</a></blockquote><blockquote id="fn_58"><sup>58</sup>. L. Di Stefano, S. Mattoccia, F. Tombari, ZNCC-based template matching using Bounded Partial Correlation Pattern Recognition Letters, 16(14), pp 2129-2134, October 2005<a href="#reffn_58" title="Jump back to footnote [58] in the text."> &#8617;</a></blockquote><blockquote id="fn_59"><sup>59</sup>. F. Tombari, L. Di Stefano, S. Mattoccia, A. Galanti, Performance evaluation of robust matching measures 3rd International Conference on Computer Vision Theory and Applications (VISAPP 2008)<a href="#reffn_59" title="Jump back to footnote [59] in the text."> &#8617;</a></blockquote><blockquote id="fn_60"><sup>60</sup>. R. Zabih, J John Woodll Non-parametric Local Transforms for Computing Visual Correspondence, ECCV 1994<a href="#reffn_60" title="Jump back to footnote [60] in the text."> &#8617;</a></blockquote><blockquote id="fn_61"><sup>61</sup>. D. N. Bhat, S. K. Nayar, Ordinal measures for visual correspondence, CVPR 1996<a href="#reffn_61" title="Jump back to footnote [61] in the text."> &#8617;</a></blockquote><blockquote id="fn_62"><sup>62</sup>. D. G. Lowe, Distinctive image features from scale-invariant keypoints, International Journal of Computer Vision, 60, 2 (2004), pp. 91-110<a href="#reffn_62" title="Jump back to footnote [62] in the text."> &#8617;</a></blockquote><blockquote id="fn_63"><sup>63</sup>. R.Szeliski, R. Zabih, D. Scharstein, O. Veksler, V. Kolmogorov, A. Agarwala, M. Tappen, C. Rother, A Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors, IEEE Transactions on Pattern Analysis and Machine Intelligence, 30, 6, June 2008, pp 1068-1080<a href="#reffn_63" title="Jump back to footnote [63] in the text."> &#8617;</a></blockquote><blockquote id="fn_64"><sup>64</sup>. F. Tombari, S. Mattoccia, L. Di Stefano, E. Addimanda, Near real-time stereo based on effective cost aggregation International Conference on Pattern Recognition (ICPR 2008)<a href="#reffn_64" title="Jump back to footnote [64] in the text."> &#8617;</a></blockquote><blockquote id="fn_65"><sup>65</sup>. S. Mattoccia, S. Giardino,A. Gambini, Accurate and efficient cost aggregation strategy for stereo correspondence based on approximated joint bilateral filtering, Asian Conference on Computer Vision (ACCV 2009), September 23-27 2009, Xiang, China<a href="#reffn_65" title="Jump back to footnote [65] in the text."> &#8617;</a></blockquote><blockquote id="fn_66"><sup>66</sup>. S. Mattoccia, A locally global approach to stereo correspondence, 3D Digital Imaging and Modeling (3DIM 2009), pp 1763-1770, October 3-4, 2009, Kyoto, Japan<a href="#reffn_66" title="Jump back to footnote [66] in the text."> &#8617;</a></blockquote><blockquote id="fn_67"><sup>67</sup>. S. Mattoccia, Improving the accuracy of fast dense stereo correspondence algorithms by enforcing local consistency of disparity fields, 3D Data Processing, Visualization, and Transmission (3DPVT 2010), 17-20 May 2010, Paris, France<a href="#reffn_67" title="Jump back to footnote [67] in the text."> &#8617;</a></blockquote><blockquote id="fn_68"><sup>68</sup>. S. Mattoccia, Fast locally consistent dense stereo on multicore, Sixth IEEE Embedded Computer Vision Workshop (ECVW2010), CVPR workshop, June 13, 2010, San Francisco, USA<a href="#reffn_68" title="Jump back to footnote [68] in the text."> &#8617;</a></blockquote><blockquote id="fn_69"><sup>69</sup>. S. Mattoccia, Accurate dense stereo by constraining local consistency on superpixels, 20th International Conference on Pattern Recognition (ICPR2010), August 23-26, 2010, Istanbul, Turkey<a href="#reffn_69" title="Jump back to footnote [69] in the text."> &#8617;</a></blockquote><blockquote id="fn_70"><sup>70</sup>. L. Wang, M. Liao, M. Gong, R. Yang, and D. Nistér. High-quality real-time stereo using adaptive cost aggregation and dynamic programming. 3DPVT 2006<a href="#reffn_70" title="Jump back to footnote [70] in the text."> &#8617;</a></blockquote><blockquote id="fn_71"><sup>71</sup>. S. Mattoccia, M. Viti, F. Ries,. Near real-time Fast Bilateral Stereo on the GPU, 7th IEEE Workshop on Embedded Computer Vision (ECVW20011), CVPR Workshop, June 20, 2011, Colorado Springs (CO), USA<a href="#reffn_71" title="Jump back to footnote [71] in the text."> &#8617;</a></blockquote><blockquote id="fn_72"><sup>72</sup>. S. Mattoccia, L. De-Maeztu, “A fast segmentation-driven algorithm for stereo correspondence”, International Conference on 3D (IC3D 2011), December 7-8, 2011, Liege, Belgium<a href="#reffn_72" title="Jump back to footnote [72] in the text."> &#8617;</a></blockquote><blockquote id="fn_73"><sup>73</sup>. L. De-Maeztu, S. Mattoccia, A. Villanueva, R. Cabeza, “Efficient aggregation via iterative block-based adapting support weight”,International Conference on 3D (IC3D 2011), December 7-8, 2011, Liege, Belgium<a href="#reffn_73" title="Jump back to footnote [73] in the text."> &#8617;</a></blockquote><blockquote id="fn_74"><sup>74</sup>. D. Min, J. Lu, and M. Do, A revisit to cost aggregation in stereo matching: how far can we reduce its computational redundancy?, ICCV 2011<a href="#reffn_74" title="Jump back to footnote [74] in the text."> &#8617;</a></blockquote><blockquote id="fn_75"><sup>75</sup>. L. De-Maeztu, S. Mattoccia, A. Villanueva, R. Cabeza, “Linear stereo matching”, International Conference on Computer Vision (ICCV 2011), November 6-13, 2011, Barcelona, Spain<a href="#reffn_75" title="Jump back to footnote [75] in the text."> &#8617;</a></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> computer vision </tag>
            
            <tag> stereo matching </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lytro的光场AR之路：从巅峰到死亡</title>
      <link href="/posts/lytro-light-field/"/>
      <url>/posts/lytro-light-field/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/lytro_cover.jpg"></p><div class="note success">            <p>这篇文章将会介绍Lytro公司在光场以及VR领域的进展。我们知道NG博士在06年创办了这家公司，曾经被誉为硅谷最优潜力的公司之一。它曾经推出了世界上第一款商用的手持式光场相机，进而推出了第二代产品。但是低调的Lytro已经许久不出现大众的视野中，难道是盛名之下其实难副？<br><span id="inline-red">声明</span>：<u><strong>一切理解都是本人观点，如有疑问，还望在</strong>评论<strong>中留言。如需转载请与本人联系，谢谢合作</strong></u>! 邮箱：<a href="/about">点我</a></p>          </div><a id="more"></a><p>其实不然，原来人家在搞大事情，Lytro公司在用光场技术搞AR！</p><h2 id="6自由度"><a href="#6自由度" class="headerlink" title="6自由度"></a>6自由度</h2><p>言归正传，提到AR首先讲下自由度的概念。人类在地球上的运动可以由6自由度来描述。6自由度描述了一个人在空间中的自然运动，6自由度会比3自由度提供两倍的自由度感受空间！具体而言，前三个自由度是大多数VR都支持的围着轴的旋转运动，后三个是沿着轴向的平移运动，这需要配套的位置跟踪设备的支持，如Oculus Rift, HTC Vive 或者Playstation VR。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/6DOF_Lytro_Color_Version-930x1024.jpg"></p><p>接下来，详细介绍一下这六个神奇的自由度到底包括哪些。你想或者不想，我们每天都在6个自由度（6DoF）里运动着。</p><h3 id="Yaw（平摇）"><a href="#Yaw（平摇）" class="headerlink" title="Yaw（平摇）"></a>Yaw（平摇）</h3><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/Yaw.gif"></p><h3 id="Pitch（俯仰）"><a href="#Pitch（俯仰）" class="headerlink" title="Pitch（俯仰）"></a>Pitch（俯仰）</h3><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/Pitch.gif"></p><h3 id="Roll（翻滚）"><a href="#Roll（翻滚）" class="headerlink" title="Roll（翻滚）"></a>Roll（翻滚）</h3><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/roll.gif"></p><h3 id="Left-Right（左右）"><a href="#Left-Right（左右）" class="headerlink" title="Left/Right（左右）"></a>Left/Right（左右）</h3><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/left-right.gif"></p><h3 id="Up-Down（上下）"><a href="#Up-Down（上下）" class="headerlink" title="Up/Down（上下）"></a>Up/Down（上下）</h3><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/updown.gif"></p><h3 id="Forward-Backward（前后）"><a href="#Forward-Backward（前后）" class="headerlink" title="Forward/Backward（前后）"></a>Forward/Backward（前后）</h3><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/Forward-Backward.gif"></p><h2 id="光场与AR"><a href="#光场与AR" class="headerlink" title="光场与AR"></a>光场与AR</h2><p>我们生活在一个充满光明的世界，无论是自然光还是人造光，光无时无刻不照普照大地。</p><p><img width="1000" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/LF_Volume_Video_ScreenGrab_900W-1-678x381.png"></p><p>光的定向传播会照射在我们周围的物体表面，从而物体才能够显示出其纹理特征。即使是在虚拟的环境中，光场也可以通过计算机图形学(CG)的模拟光线通过在虚拟场景中反射3D对象来进行创建。在光场的所有无限光线中，我们只能看到那些照在我们眼睛上光线，这些光线穿过瞳孔打在视网膜上从而让我们感知光线的存在。对于这些光线，我们的眼睛会根据光的刺激产生某种神经信号，这些信号中包括光线的颜色，强度以及方向等信息。我们的大脑正是通过这些信号来感知大千世界。</p><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/Screen-Shot-2016-09-28-at-11.17.02-AM.png"></p><p>当我们在光场中移动时，不同的光线会通过我们的瞳孔，给我们的大脑提供额外的信息，使我们能够理解物体在空间中的位置，同时也会了解物体的其他信息例如：材质、反射、折射等等。</p><h3 id="记录光线角度-方向"><a href="#记录光线角度-方向" class="headerlink" title="记录光线角度/方向"></a>记录光线角度/方向</h3><p>为了捕获和再现光场，需要记录<strong>光线的颜色及其路径（方向/角度）</strong>。确定光线的颜色和亮度很简单，那么问题在于如何记录光线的方向/角度。在一个光场内捕获的2D图像中的任何像素（实际动作或被渲染），可以提供与该像素所有相交光线的颜色和亮度信息。这通常被称为“光束”（所有的光线被一个像素捕获），但为了简单起见，我们将使用术语“光线”来描述由光线捕获的光线单个像素。</p><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/Light_Field_two_planes_FINAL-1024x802.png"></p><p>那么如何记录光线的角度和方向呢？目前已经有很多种记录光线的角度和方向路径的技术，但这些技术都需要至少两个点来确定实际的方向/角度。一种常见的方法是使用<strong>双平面法</strong>（2PP）来计算光线路径。光线穿过两个平面并相交于两点，利用这两个交点，可以确定射线的角度和方向。</p><h3 id="视差"><a href="#视差" class="headerlink" title="视差"></a>视差</h3><p><strong>视差也是记录光场的比较常用的技术</strong>。通过计算两个或者多个相邻相机之间拍摄的2D图像的差异可以得到视差信息。这些2D图像是由场景中物体光的颜色亮度组成的彩色像素组成。通过对一系列图像的显著特征进行三角测量，同时比较图像之间像素间的视差，可以计算出各个物体在空间中的位置和距相机的距离（深度）。这种方式可以从2D数据恢复光场。对3D场景的计算机图形渲染，通常会提供对象与摄像机的距离（深度信息），并作为渲染过程的一部分。在以下场景中，我们使用三个相邻的相机对苹果和橘子进行记录。每个单独的相机从不同的位置记录该场景，这会产生图像之间的差异（视差）。</p><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/Disparity_Light_Field_A_FINAL-1024x896.png"></p><p>每个2D图像是一组彩色像素的集合，仅仅表示场景中苹果和橙色表面的颜色和亮度。视差必须通过分析这些2D图像之间的差异得到。<br><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/Disparity_Light_Field_B_FINAL-631x1024.png"></p><p>利用三个2D图像之间的视差，可以确定苹果和橘子在空间中的位置以及与三个视点之间的距离。随后经过处理的光线角度和颜色信息会记录在光场体（Light Field Volume）中。在VR中，光场体能够为我们提供沉浸式的高质量视觉体验。为了能够达到这个水平，光场体验需要囊括多种视觉效果。例如每个方向上的完美立体感，光场体的全视差和六个自由度，以及正确的场景流，以实现视觉相关效果（镜面反射和折射）。光场无论在实景动作还是计算机渲染，都可以产生最为优秀的VR电影体验。</p><h2 id="光场VR设备"><a href="#光场VR设备" class="headerlink" title="光场VR设备"></a>光场VR设备</h2><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/Tracing-Explanation-Hero-678x381.png"></p><h3 id="Lytro-VT"><a href="#Lytro-VT" class="headerlink" title="Lytro VT"></a>Lytro VT</h3><p>不安分的Lytro最近发布了名为“Lytro Volume Tracer”(Lytro VT)的产品，它作为一套强大的工具可以用于CG 3D场景的光场体的创建，同时能够为用户提供视觉高质量以及完全沉浸式的VR体验。</p><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/vr1.jpg"></p><p>Lytro VT可以使用任何DCC和渲染引擎（例如Maya和VRay）来生成一组3D场景的2D采样。首先，Lytro VT将虚拟相机放置于CG场景中，虚拟相机包含场景中任何可能的视角，需要注意的是这些场景已经包含在定义好的光场体中，并且虚拟相机可以根据需要调整以最大限度地提高显示质量和性能。渲染引擎用于追踪场景中的虚拟光线，并从设备中每个摄像头捕获一定数量的2D图像样本。Lytro VT通过追踪从每个被渲染的像素到其相机的原点的光线(光积跟踪)来创建<strong>视觉体</strong>，通过以上神操作就可以感受到沉浸式的光场VR体验。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/10x10x10_Sequence_700px_copyright_2018.gif"></p><p>以上是由<strong>1000个视点组成的视觉体</strong>（图片加载慢，24.74M）。在该视觉体中，VR HMD中的观看者可以体验具有最高级别的光线追踪光学效果，每个方向上完美的视差以及六个自由度（6DOF）的重建虚拟场景。</p><p>光线跟踪的样本包括对<strong>颜色和深度信息（RGBZ等数据）的跟踪</strong>。摄像机的数量及其配置取决于场景的视觉复杂程度以及播放过程中所需视图的预定大小。</p><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/devices.jpg"></p><p>Lytro VT处理来自于该2D样本的颜色以及深度信息，并通过Lytro Player创建用于在VR中展示的光场体。</p><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/Tracing-Explanation-1-copyright-2018-1024x697.png"></p><p>该3D场景中的视图体由白色立方体表示。单个相机由绿色球体表示，它具有自己单独的视点。虚拟的Lytro VT摄像机包含有成百上千个独立的摄像机。2D场景样本渲染使用虚拟装备中每个独立像机进行光线追踪。</p><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/Tracing-Explanation-Close-Up-2-copyright-2018-1024x697.png"></p><p>以上是一个相机跟踪的来自于场景中5个不同位置的光线的局部放大图，通过对每个独立相机进行光线跟踪就可以重建光场。</p><h3 id="光线追迹"><a href="#光线追迹" class="headerlink" title="光线追迹"></a>光线追迹</h3><p>在将来，Lytro VT与渲染可以和并为一个无缝过程，允许光场直接进行光线跟踪，而不需要2D图像样本的中间步骤。然而这是需要代价的，这一过程需要很强的渲染器集成，并且要放弃这个如今如此灵活的Lytro VT。</p><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/lytro_vt.jpg"></p><p>作为从虚拟3D场景创建真实2D图像的渲染技术，光线追踪能够产生极高质量的图像。用最简单的术语来说，基于模拟光线与3D场景中的物体表面的相互作用，反映在2D图像平面就是被渲染的彩色像素。</p><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/vr_experience.jpg"></p><p>光线追踪适用于精确渲染某些光学效果，例如如反射，折射和散射（光度），但这些需要大量的计算时间。具有全光学效果的光线追踪对于实时帧率而言简直太慢。但是不得不说，光线追踪非常适合需要最高级别图像质量并可以脱机的应用，如电影视觉效果。</p><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/Raytracing-copyright-2018-1024x729.png"></p><p>上图为光线跟踪的过程：<strong>通过虚拟相机的视角可以看到，虚拟相机跟踪到了物体与物体之间的光线反复反射，并最终到达光源的位置</strong>。如果有些物体遮挡了光线，那么就会产生被遮挡的光线。这种技术的计算效率很高，因为它只需追踪相机通过虚拟镜头看到的光线路径。Lytro VT和光线追踪是相辅相成的，然而在光线追踪的概念方向上形成对比。如上所示，光线跟踪通过跟踪从固定摄像机向外看光线的路径，从而呈现图像中的彩色像素。<strong>相反，Lytro VT通过从一个视觉体内的每个视点向内朝着观察者，去追踪来自每个渲染像素的光线来重建光场体</strong>（这句话翻译的不佳，原因是我没太理解VT与光线跟踪的区别…有大神能够理解的话，请在评论区给出）。于是在Lytro Player中，观众在这些密集的光线的移动，沉浸在具有最高级视觉质量的重建CG场景中，并且在每个方向都具有完美的视差和六个自由度。</p><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/lytro-light-field/cinema.jpg"></p><p>在这种体验中，光线不是实时呈现，而是从大量预先渲染的光线中实时获取，为视图体积内每个位置的每只眼睛组成一张图像。</p><p>PS: Lytro公司在2017年11月30号之后停止了对lytro live photo的线上支持，其相机业务至此告一段落。通过光场VR转型，不知Lytro能否再次创造辉煌？这里留下一个疑问，等待时间的检验吧！</p><h2 id="从巅峰到倒闭"><a href="#从巅峰到倒闭" class="headerlink" title="从巅峰到倒闭"></a>从巅峰到倒闭</h2><p>3月29日更新。</p><p>世界上第一个光场技术初创公司Lytro昨日发表声明，<strong>正式宣布倒闭</strong>！</p><p>看来时间并不允许Lytro继续存活，光场进阶之路就此截止了吗？早些时间就有传闻称Lytro即将倒闭，Google或将接盘。起因是Google早前公布了一款能够显示沉浸式VR场景的App，这种VR场景据说是由多摄像机采集得到，貌似用到了第三方公司的技术。有人猜测这个第三方公司就是Lytro，这是一家以光场技术著称的公司，它利用其光场采集设备获取场景深度，并将其利用到了VR技术之中。</p><p>但这只是猜测，并没有得到印证。有来源显示Lytro早前进行的属于“<strong>资产抛售</strong>”，抛售额度不超过<strong>4000万美元</strong>。也有人说，这个额度更低，不超过2500万美元。</p><p>有可能接盘的公司包括Google，Facebook，Apple等。有知情人透露，Lytro内部员工已经陆续离职，与此抛售的还有Lytro公司的59项光场专利。这个抛售金额对于Lytro而言简直是低价销售，因为在其成立之初融资金额已经达到了2亿美元，并且到其2017年最后一次融资时已经达到了3.6亿美元！</p><p>投资者多是科技投资巨头，例如Andreessen Horowitz，富士康，GSV，Greylock，NEA，Qualcomm Ventures等。创业艰辛，Lytro同样面对。从2006年成立之初，Lytro就面临着创业圈共同面对的问题。<strong>光场技术的硬件实现异常艰难，同时VR技术的发展并没有想像中那么快。同时，大型平台逐渐成为具有说服力的整合商，这是其发展的一大阻力</strong>。</p><p>与此同时，Lytro的推出的光场相机迷之昂贵，这是VR技术的重要技术支点，同时也成为了其发展中最大的短板。如今看来，Lytro完全有足够的时间和资金提供一个面向大众更具有说服力的报价，以等待在正确的市场条件下推出正确的产品。同时，Lytro公司应该考虑把光场应用到更加广阔的领域，例如无人汽车、智能导航、地图以及游戏等。</p><p>一年前Lytro CEO Jason Rosenthal 在其官方博客中写道：“我认为我们有能力重新定义下一代Lytro的产品线、技术以及品控”。仅仅时隔一年，此时或许该轮到Google来完成Lytro的雄心壮志了。虽然Google对Lytro具体有何企图不得而知，但是我们可以确定的是，借助Google这个世界上最大的移动手机系统提供商，如果光场技术能够成功整合，这将是科技界的一大奇迹！</p><p>以下是Lytro官方通告原文（大意是：我虽已死，光场犹存）：<br><div class="note ">            <p>At Lytro, we believe that Light Field will continue to shape the course of Virtual and Augmented Reality, and we’re incredibly proud of the role we’ve been able to play in pushing the boundaries of what’s possible. We’ve uncovered challenges we never dreamed of and made breakthroughs at a seemingly impossible pace. We’ve had some spectacular successes, and built entire systems that no one thought possible. More importantly, we built a team that was singularly unified in its focus and unrivaled in its dedication. It has been an honor and a pleasure to contribute to the cinema and Virtual Reality communities, but starting today we will not be taking on new productions or providing professional services as we prepare to wind down the company. We’re excited to see what new opportunities the future brings for the Lytro team as we go our separate ways. We would like to thank the various communities that have supported us and hope that our paths will cross in the future.</p><p>Lytro was founded in 2006 by Executive Chairman Ren Ng, whose Ph.D. research on Light Field imaging won Stanford University’s prize for best thesis in computer science. In late 2015, Lytro announced the world’s first Light Field solution for Virtual Reality (VR), Lytro Immerge, that was quickly followed by the 2016 launch of Lytro Cinema, the world’s first Light Field capture system for cinematic content. With these products, Lytro pioneered the generational shift from legacy 2D imaging to 3D volumetric video.</p>          </div></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>光场民用领域的践行者离我们而去，不知光场的未来将何去何从？敢问Raytrix和Magic Leap你们可好？</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="http://blog.lytro.com/glossary/6dof/" target="_blank" rel="noopener">6DoF</a></li><li><a href="http://blog.lytro.com/holiday-edition-what-are-the-six-degrees-of-freedom/" target="_blank" rel="noopener">Holiday Edition: What Are the Six Degrees of Freedom?</a></li><li><a href="http://blog.lytro.com/what-is-a-light-field/" target="_blank" rel="noopener">What is a Light Field?</a></li><li><a href="http://blog.lytro.com/ray-tracing-lytro-volume-tracing-and-cg-generated-light-fields-in-vr/" target="_blank" rel="noopener">Ray tracing, Lytro Volume Tracing and CG generated Light Fields in VR</a></li><li><a href="http://blog.lytro.com/primer-on-360-video-for-vr/" target="_blank" rel="noopener">Primer on Types of 360° Video for VR</a></li><li><a href="https://techcrunch.com/" target="_blank" rel="noopener">Homepage: Techcrunch</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 光场 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 光场 </tag>
            
            <tag> 计算成像 </tag>
            
            <tag> Light Field </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stephen Hawking</title>
      <link href="/posts/stephen-hawking/"/>
      <url>/posts/stephen-hawking/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img width="1600px" data-src="//www.vincentqin.tech/posts/stephen-hawking/Hawkingsig.svg"></p><!--![](Hawkingsig.svg)--><a id="more"></a><center><p>昨天晚上，拜访许久没去的羽毛球馆</p><p>在去之前有种不好的预感</p><p>结果真出事了</p><p>脚踝意外扭伤</p><p>今天上午去医院拍片</p><p>人很多，需要排队</p><p>谁也没料到</p><p>就在这等待的过程中</p><p>霍金去世了<img width="1600px" data-src="https://vincentqin.gitee.io/blogresource-1/stephen-hawking/stephen-hawking-child.jpg"></p><p>这条消息惊得我一身冷汗</p><p>整个人半天没有反应过来</p><p>我确认了几遍</p><p>无法挽回，这就是事实</p><p>维基百科霍金主页瞬间把</p><p>1942年1月8日－至今</p><p>改成了</p><p>2018年3月14日<img width="1600px" data-src="https://vincentqin.gitee.io/blogresource-1/stephen-hawking/stephen-hawking.jpg"></p><p>巧合的是</p><p>他的生日恰好是伽利略·伽利莱的忌日</p><p>而忌日亦恰好是阿尔伯特·爱因斯坦的生日</p><p>世界圆周率日</p><p>或许在世界的某个角落</p><p>一个婴儿降临在地球</p><p>若干年后</p><p>响彻寰宇</p><p>桌上还有一本他写的</p><p>《A Brief History of Time》</p><p>至今没有读完...<img width="1600px" data-src="https://vincentqin.gitee.io/blogresource-1/stephen-hawking/stephen-hawking-a-brief-history-of-time.jpg"></p><p>高中起迷恋你的宇宙大爆炸和虫洞理论</p><p>晚自习第三节课</p><p>经常会拿出日记本</p><p>书写着由你构建的虫洞世界</p><p>大学看了</p><p>《万物理论》</p><p>你作为一个立体的人</p><p>重新定义了我对你的认知<img width="1600px" data-src="https://vincentqin.gitee.io/blogresource-1/stephen-hawking/stephen-hawking-the-theory-of-everything.jpg"></p><p>考研时英语作文里写到了你</p><p>一个永垂不朽的凡人<img width="1600px" data-src="https://vincentqin.gitee.io/blogresource-1/stephen-hawking/stephen-hawking-wedding.jpeg"></p><p>代表了人类好奇心的极值</p><p>你是最接近于外星人的人类</p><p>我的偶像</p><p>霍金老爷子，一路走好</p><p>时间简史，霍耀苍穹<img width="1600px" data-src="https://vincentqin.gitee.io/blogresource-1/stephen-hawking/stephen-hawking-student.jpg"></p><p>冥想</p><p>也许在如此浩瀚的宇宙中</p><p>我们人类</p><p>仅仅作为宇宙大爆炸数百亿年后</p><p>出现的粒子随机组合<img width="1600px" data-src="https://vincentqin.gitee.io/blogresource-1/stephen-hawking/stephen-hawking-black-hole-2.jpg"></p><p>相比于宇宙的无穷</p><p>人类的寿命抑或人类的历史</p><p>只是这其中的</p><p>一朵不经意的涟漪<img width="1600px" data-src="https://vincentqin.gitee.io/blogresource-1/stephen-hawking/stephen-hawking-smile.jpg"></p><p>但</p><p>这何尝不是一种幸运呢</p><p>对吧？</p><p>霍老爷子<img width="1600px" data-src="https://vincentqin.gitee.io/blogresource-1/stephen-hawking/stephen-hawking-life.jpeg"></p></center>]]></content>
      
      
      
        <tags>
            
            <tag> Stephen Hawking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用的生产力工具</title>
      <link href="/posts/some-useful-tools/"/>
      <url>/posts/some-useful-tools/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/some-useful-tools/42665014.jpg"></p><div class="note success">            <p>强大的工具能够极大地提高生产力，而我们的要求似乎会更多一些：我们总是在寻找功能与美观兼得的工具。我是一个喜欢折腾的人，正好我有几个不错的工具推荐给大家。需要注意的是，这些软件是为提高生产力服务的，一味地追求炫酷而不求效率可谓顾此失彼，总之，不可亵玩。</p>          </div><a id="more"></a><h2 id="Writing"><a href="#Writing" class="headerlink" title="Writing"></a>Writing</h2><ul><li>目前写作使用的是<strong>Markdown</strong>文本标记语言，比较好的编辑软件有<strong><a href="https://www.zybuluo.com" target="_blank" rel="noopener">作业部落</a></strong>和<strong><a href="https://note.youdao.com" target="_blank" rel="noopener">有道云笔记</a></strong>。除此之外，印象笔记也不错。这些工具都能够实现不同客户端之间的同步，将使我们不会受制于空间与时间。</li><li>桌面端Markdown编辑器使用<a href="http://pad.haroopress.com/" target="_blank" rel="noopener">Haroopad</a>，但有个缺点就是速度比较慢。</li></ul><p><img width="800px" data-src="https://vincentqin.gitee.io/blogresource-1/some-useful-tools/haroopad.png"></p><ul><li><a href="http://p7itjlzj6.bkt.clouddn.com/MathType6.9.rar" target="_blank" rel="noopener">Mathtype破解版</a></li><li><a href="http://p7itjlzj6.bkt.clouddn.com/Microsoft-Toolkit-2.6-beta-5.exe" target="_blank" rel="noopener">Windows/Office大礼包破解工具</a></li><li><h2 id="Programing"><a href="#Programing" class="headerlink" title="Programing"></a>Programing</h2></li><li><p>推荐一款跨平台的终端<a href="http://www.termius.com/" target="_blank" rel="noopener">Termius</a>。它同时支持Andriod、IOS以及PC端（Ubuntu &amp; Windows），可以说很好用了。</p></li></ul><p><img width="800px" data-src="https://vincentqin.gitee.io/blogresource-1/some-useful-tools/Termius.png"></p><ul><li><a href="http://cmder.net/" target="_blank" rel="noopener">Cmder</a>完全可以取代Windows下的<code>cmd</code>，同时它还集成了<code>git shell</code>和<code>power shell</code>，可以说很好用很强大了。</li></ul><p><img width="800px" data-src="https://vincentqin.gitee.io/blogresource-1/some-useful-tools/69629280.jpg"></p><ul><li>另外推荐一个可以在windows系统操纵Linux的神器——<a href="https://mobaxterm.mobatek.net/#" target="_blank" rel="noopener">MobaXterm</a>，支持直接拖动粘贴复制。这里是<a href="http://p7itjlzj6.bkt.clouddn.com/MobaXterm_v8.2.rar" target="_blank" rel="noopener">v8.2版本</a>。</li></ul><p><img width="800px" data-src="https://vincentqin.gitee.io/blogresource-1/some-useful-tools/mobaxterm.png"></p><ul><li><a href="https://www.jetbrains.com/clion/" target="_blank" rel="noopener">Clion大礼包</a>，C/C++多平台支持集成开发环境，学生可以申请免费使用。</li></ul><p><img width="800px" data-src="https://vincentqin.gitee.io/blogresource-1/some-useful-tools/clion.png"></p><ul><li><a href="https://code.visualstudio.com/" target="_blank" rel="noopener">Visual Studio Code</a>，一个强大的轻量级文本编辑器，支持多插件下载。它将开发者的重心放在了coding上，并没有涉及编译。类似的还有<a href="https://www.sublimetext.com/" target="_blank" rel="noopener">Sublime</a>，<a href="https://notepad-plus-plus.org/" target="_blank" rel="noopener">Notepad++</a>和<a href="https://atom.io/" target="_blank" rel="noopener">Atom</a>。</li></ul><p><img width="800px" data-src="https://vincentqin.gitee.io/blogresource-1/some-useful-tools/vscode.png"></p><h2 id="Screenshots"><a href="#Screenshots" class="headerlink" title="Screenshots"></a>Screenshots</h2><ul><li><a href="https://www.snipaste.com/index.html" target="_blank" rel="noopener">Snipaste</a>，一款支持多种自定义的Windows截图工具，颜值与高效并存。</li><li>轻量级录屏软件<a href="http://p7itjlzj6.bkt.clouddn.com/GifCam.exe" target="_blank" rel="noopener">GifCam</a></li></ul><p><img width="800px" data-src="https://vincentqin.gitee.io/blogresource-1/some-useful-tools/79224982.png"></p><h2 id="Search"><a href="#Search" class="headerlink" title="Search"></a>Search</h2><ul><li><a href="https://www.voidtools.com/" target="_blank" rel="noopener">Everything</a>，在NTFS卷上快速地根据名称查找文件和目录，全盘秒搜！类似的还有<a href="http://www.listary.com/" target="_blank" rel="noopener">Listary</a>等。</li></ul><h2 id="APPs-on-Phone"><a href="#APPs-on-Phone" class="headerlink" title="APPs on Phone"></a>APPs on Phone</h2><ul><li>我觉得GitHub能设计个手机客户端就好了，目前App store里有几个非官方版本的GitHub客户端，但是用户参差不齐。我目前发现的比较好用的有<strong>GitBucket</strong> (free)，<strong>PPHub for Github</strong> (￥12)。<br><img width="50%" data-src="https://vincentqin.gitee.io/blogresource-1/some-useful-tools/96412734.png"></li><li><strong>Coding</strong>手机客户端，能够很方便地查看自己以及他人的coding仓库。界面小清新，还加入了社交功能，但是貌似不太活跃。<br><img width="50%" data-src="https://vincentqin.gitee.io/blogresource-1/some-useful-tools/74869287.png"></li><li><p><strong><a href="https://gitter.im/" target="_blank" rel="noopener">Gitter</a></strong>一款聊天社交平台，我的网站（右下角）就用了Gitter提供的API。<br><img width="50%" data-src="https://vincentqin.gitee.io/blogresource-1/some-useful-tools/12300517.png"></p></li><li><p><strong>MeassureKit</strong>，能够利用手机摄像头测距，绘制运动轨迹等非常炫酷的功能。这是一款在内侧阶段一直在玩的AR应用，应该是最早的一批使用Apple提供的ARKIT的APP了吧。<br><img width="50%" data-src="https://vincentqin.gitee.io/blogresource-1/some-useful-tools/58186493.jpg"><br>世上神器千万，如有推荐，欢迎在评论区中出现。</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Tools </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习在深度(视差)估计中的应用(2)</title>
      <link href="/posts/depth-estimation-using-deeplearning-2/"/>
      <url>/posts/depth-estimation-using-deeplearning-2/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/depth-estimation-using-deeplearning-2/wonderwomanv3.jpg"></p><div class="note success">            <p>最近一段时间一直痴迷于如何将深度学习用于深度估计，看了不少关于该方面的介绍，再次做一个简单的总结。虽说<code>深度学习</code>和<code>深度估计</code>都有<code>深度</code>二字，但是其意义确是完全不一样。一个是<code>deep</code>一个是<code>depth</code>，前者表示网络层纵向的延伸度，后者表示三维场景中物体距离摄像头的距离。这两个差异如此之大的名词是如何结合在一起的呢？且听我慢慢解释。</p>          </div><a id="more"></a><p>深度学习的历史在此不做介绍，我们只关心深度学习在深度估计的方面的成果。在开始之前要到一个著名的网络<strong><a href="https://lmb.informatik.uni-freiburg.de/Publications/2015/DFIB15/" target="_blank" rel="noopener">FlowNet</a></strong>，这是<em>Dosovitskiy</em>等人发表在<strong>ICCV2015</strong>上的作品。这篇文章其实就是做了两件事情：1. 建立了两种结构的FlowNet；2. 建立了一个虚拟场景训练集（Flying Chairs）。最后的测试效果还不错，虽说仅仅在这个数据集上进行了训练，但是泛化能力能够达到业界水平。</p><h2 id="FlowNet"><a href="#FlowNet" class="headerlink" title="FlowNet"></a>FlowNet</h2><p>首先大体看一下这个可以end-to-end训练的网络长得如何，如下图所示：对于输入的图像对，依次经过一个收缩（contractive）网络以及放大（expanding）网络，最后输出得到对应的光流。很难想象CNN可以用来做classification的同时，也可以做到寻找图像之间的相关信息。作者这么做的目的就是为了验证CNN这种强大的特性。原话如是说“<em>The idea is to exploit the ability of convolutional networks to learn strong features at multiple levels of scale and abstraction and to help it with finding the actual correspondences based on these features</em>”。</p><p><img width="60%" alt="A Encoder-Decoder Network for Disparity Estimation" data-src="https://vincentqin.gitee.io/blogresource-2/depth-estimation-using-deeplearning-2/flow-net-structure.png"></p><p>接下来就是网络的设计环节，首先作者回顾了之前的网络设计策略。一种是最简单的“sliding window”方式，但这种方式的缺点在于计算量很大，它使用了各种优化包括重用网络的临时输出；另外一种对各个层的临时输出做上采样到全分辨率，然后将这些图叠起来，这行对于每一点而言，都能够得到相应的多级特征向量，这个向量可用来学习想要的信息。</p><h3 id="Contrasting-Part"><a href="#Contrasting-Part" class="headerlink" title="Contrasting Part"></a>Contrasting Part</h3><p>作者受到“per-pixel prediction tasks”的相关工作的启发，设计了两种光流网络框架。一种是相对简单的实现：首先将输入的图像对叠加起来作为输入，然后输入一个网络，让网络自己学，最后提取运动信息。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-2/depth-estimation-using-deeplearning-2/flownet-simple.png"></p><p>另外一种方式就是将输入的图像pair（左图&amp;右图）分开训练，提取出高维丰富的信息之后再做相关性连接，即增加了<code>correlation layer</code>。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-2/depth-estimation-using-deeplearning-2/flownet-corr.png"><br>这个correlation layer是为了衡量左右图相应位置的相似度而设置的。一个的很直观地理解就是，在左图选取一个patch，同时在右图中的可能的位置选择同样大小的patch进行匹配运算(点积运算或者说是卷积运算)。<br>具体而言，分别在左右<code>feature map</code>（$f_1$和$f_2$）以$x_1$和$x_2$为中心的块之间进行卷积运算。<code>correlation</code>的定义如下：</p><script type="math/tex; mode=display">c(x_1,x_2)=\sum_{o \in [-k,k] \times [-k,k]}{<f_1(x_1+o),f_2(x_2+o)>}</script><p>其中<script type="math/tex">f_1</script>和<script type="math/tex">f_2</script>的维度为<script type="math/tex">w \times h \times c</script>。可以看到计算一次$c(x_1,x_2)$需要$c \times K^2$次乘法，$K:=2k+1$。而对于所有的位置则需要$w^2 \times h^2 \times c \times K^2 $次乘法，可想而知，这个计算量是巨大的。于是作者为了减少运算量，对搜素窗口进行了限制，设置了最大的搜索半径为$d$，则$x_2$就能在窗口大小是$D=2d+1$里计算<code>correlation</code>了。另外值得一提的是，我们以上的过程是在计算光流信息，所以应该在某个<strong>某个窗口</strong>内进行匹配，而不是在某个方向，而后续即将提到的<code>DispNet</code>的 <code>correlation</code> 是在某一个方向上进行搜索。那么最后得到的<code>correlation</code>的维度是$w \times h \times D^2$。</p><!--![](flow-net-simple-corr.png)--><h3 id="Expanding-Part"><a href="#Expanding-Part" class="headerlink" title="Expanding Part"></a>Expanding Part</h3><p>如下是优化网络的结构，大部分都是结合缩放阶段信息的<strong>反卷积</strong>操作。这里不再赘述，最后得到的结果图像大小是输入图像的$1/4$。</p><p><img alt="refinement" data-src="https://vincentqin.gitee.io/blogresource-2/depth-estimation-using-deeplearning-2/refinement.png"></p><p><del>最后作者利用<code>variational approach</code>对低分辨率的输出做了20次迭代以得到高分辨率的光流图，之后又对全分辨率的光流图做了进一步优化。</del></p><p>很多深度估计的工作受到这一篇论文启发，特别是<code>correlation layer</code>实现了寻找图像对之间的相关性这一点，对于后续<code>DispNet</code>的诞生起到重要作用。其实<code>DispNet</code>就是在<code>FlowNet</code>的基础上进行的改进，接下来就会详细的介绍<code>DispNet</code>。</p><h2 id="DispNet"><a href="#DispNet" class="headerlink" title="DispNet"></a>DispNet</h2><p>受到<strong>FlowNet</strong>的启发，另外一篇论文将光流估计拓广到了视差以及场景流的估计。2015年12月放在arxiv上的大作<em><a href="https://arxiv.org/abs/1512.02134" target="_blank" rel="noopener">A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation</a></em>便是具体代表性的作品。这篇文章有两个主要贡献：</p><ol><li>建立了三个仿真视差数据集（3 stereo video datasets），这是当时第一个超大规模的用于视差以及光流场景流训练以及评估的数据集；</li><li>设计了<strong>DispNet</strong>，<strong>SceneFlownet</strong>。</li></ol><p><img width="50%" data-src="https://vincentqin.gitee.io/blogresource-2/depth-estimation-using-deeplearning-2/disp-flow.png"></p><h3 id="开篇"><a href="#开篇" class="headerlink" title="开篇"></a>开篇</h3><p>文章一开始就说估计场景流这个问题，堪称“皇家赛事”级别的任务（场景流是估计空间三维物体的运动场，相比于光流多了一个深度信息）。然后又提到没有好的数据集无法做到完美的训练，那么这么好的数据集又不是天上掉下来的，那只能自食其力，自力更生，自己造吧，于是他们的超大规模（<strong>35000</strong> stereo frames）的数据集便成功诞生！</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>参考了前方开创性的<strong>FlowNet</strong>工作之后，他们就把<strong>FlowNet</strong>改造成<strong>DispNet</strong>，真是华丽丽的变身。然后对于场景流模型的构建，作者提到：虽然在以往的成千上万的论文中均有涉及到光流的估计，但是仅仅有极少数的工作敢去尝试估计场景流。然后作者就是厉害，他们足够相信CNN具有强大的学习与抽象能力，能够通过某种方式的组合使场景流的估计问题转化成为学习问题。文中提到“相机无关的场景流的学习可以转化成视差，光流以及光流变化学习问题”，于是在实际上文中提到的<strong>SceneFlownet</strong>就是<strong>FlowNet</strong>与<strong>DispNet</strong>的组合。以下重点介绍<strong>DispNet</strong>的实现。</p><h3 id="DispNet-amp-DispNetC"><a href="#DispNet-amp-DispNetC" class="headerlink" title="DispNet &amp; DispNetC"></a>DispNet &amp; DispNetC</h3><p><img alt="dispnet" data-src="https://vincentqin.gitee.io/blogresource-2/depth-estimation-using-deeplearning-2/dispnet.png"><br>收缩部分从conv1到conv6b；在放大部分，upconvolutions (upconvN), convolutions (iconvN, prN)和loss layers是交替出现的。从低层提取的特征与高层的特征进行串联，增加特征的多样性。最后的网络输出是pr1。</p><p>二者与<em>Dosovitskiy</em>提出的<strong>FlowNet</strong>两种结构差不多，总结起来共有3个变化：</p><ol><li>对原来的<strong>FLowNet</strong>进行了改造，在上卷积层之间增加了卷积层，这样可以使得最终的深度图像更加的平滑；</li><li>将原来的<code>2D correlation</code> 改造成了<code>1D correlation</code>；并且发现加入<code>correlation</code> 层之后会有普遍的效果提升；原因在于左右图均进行了<code>rectify</code>，基于极线约束，我们就可以在一个方向进行搜索。所以类似于<code>FlowNet</code>，我们得到的<code>correlation</code> map的大小是$D \times w  \times h$。</li><li>放大部分比Flownet多做了一次deconv，使输出为原来的$1/2$。<br>注意：DispNet对应于FlowNet的第一种实现，DispNetC对应于FlowNet的第二种实现；</li></ol><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>虽然文中提出了一个超大的数据集，但是仍然需要一定的数据增强以获得更加多样的训练数据。具体方法：空间变化（旋转，变形，裁剪，缩放），色度变换（颜色，对比度，明暗）。</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/depth-estimation-using-deeplearning-2/dispnet-res.png"><br>文中比较了Zbontar&amp;LeCun的<a href="https://github.com/jzbontar/mc-cnn" target="_blank" rel="noopener">MC-CNN</a>以及opencv中<strong>SGBM</strong>方法。<strong>DispNet</strong>是在<code>FlyingThings3D</code>数据集上做得训练，然后在<code>KITTI 2015</code>数据集上做了优化，注意“K”表示优化之后的网络。</p><h2 id="其他网络"><a href="#其他网络" class="headerlink" title="其他网络"></a>其他网络</h2><p>To be continued, more depth net pls refer to this <a href="https://sites.google.com/site/yorkyuhuang/home/research/machine-learning-information-retrieval/disparityestimationbydeeplearning" target="_blank" rel="noopener">link</a>.</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul><li>此处对<code>correlation</code>的理解还不透彻;</li><li>待补充其他类型深度/视差估计网络；</li></ul><h2 id="Change-Log"><a href="#Change-Log" class="headerlink" title="Change Log"></a>Change Log</h2><ul><li>添加了对<code>correlation</code>的解释。</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://lmb.informatik.uni-freiburg.de/resources/binaries/" target="_blank" rel="noopener">Homepage: Freiburg: Pattern Recognition and Image Processing</a></li><li><a href="https://lmb.informatik.uni-freiburg.de/Publications/2015/DFIB15/" target="_blank" rel="noopener">Homepage: FlowNet: Learning Optical Flow with Convolutional Networks</a></li><li><a href="FlowNet-Learning-Optical-Flow-with-Convolutional-Networks.pdf">Paper: FlowNet</a></li><li><a href="A%20Large%20Dataset%20to%20Train%20Convolutional%20Networks%20for%20Disparity,%20Optical%20Flow,%20and%20Scene%20Flow%20Estimation.pdf">Paper: A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation</a></li><li><a href="https://github.com/jzbontar/mc-cnn" target="_blank" rel="noopener">Github: Stereo matching by training a convolutional neural network to compare image patches</a></li><li><a href="https://sites.google.com/site/yorkyuhuang/home/research/machine-learning-information-retrieval/disparityestimationbydeeplearning" target="_blank" rel="noopener">Blog: Disparity Estimation by Deep Learning</a></li><li><a href="https://vincentqin.gitee.io/blogresource-2/depth-estimation-using-deeplearning-2/Unsupervised%20Adaptation%20for%20Deep%20Stereo.pdf" target="_blank" rel="noopener">Paper: Unsupervised Adaptation for Deep Stereo</a></li><li><a href="https://github.com/CVLAB-Unibo/Unsupervised-Adaptation-for-Deep-Stereo" target="_blank" rel="noopener">Code: Unsupervised Adaptation for Deep Stereo</a></li><li><a href="http://blog.csdn.net/hysteric314/article/details/50529804" target="_blank" rel="noopener">Blog: 【论文学习】神经光流网络——用卷积网络实现光流预测（FlowNet: Learning Optical Flow with Convolutional Networks）</a></li><li><a href="http://blog.csdn.net/kongfy4307/article/details/75212800" target="_blank" rel="noopener">Blog: 论文阅读笔记之Dispnet</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> depth estimation </tag>
            
            <tag> disparity </tag>
            
            <tag> DispNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习在深度(视差)估计中的应用(1)</title>
      <link href="/posts/depth-estimation-using-deeplearning-1/"/>
      <url>/posts/depth-estimation-using-deeplearning-1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><!--![Kitti](17-12-7/11107720.jpg)--><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/depth-estimation-using-deeplearning-1/game-jaime.jpg"></p><div class="note ">            <p>本文对KITTI stereo 2015 datasets 冠军之作<a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w17/Pang_Cascade_Residual_Learning_ICCV_2017_paper.pdf" target="_blank" rel="noopener">Cascade Residual Learning: A Two-stage Convolutional Neural Network for Stereo Matching</a>进行简要解读。</p>          </div><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>目前深度学习发展的如火如荼，利用CNN可以将图像对的<strong>匹配问题</strong>看成一个学习问题。但是如何能够得到高质量的深度图像仍然是一个普世问题。本文作者提出了一种新型的层叠式（cascade）CNN结构（CRL:Cascade Residual Learning）去估计深度信息。深度估计的过程大致可以分成两个步骤：</p><ol><li>在现有的DispNet的基础上添加几个反卷积模块，目的是为了得到full resolution的初始的深度信息，同时能够学习到更多的细节信息；</li><li>第二步是对第一步中学习到的深度信息进行校准（rectify）；这一步利用到了第一步得到的多尺度的深度信息，然后并非是直接学习到优化后的深度信息，而是学习了每个尺度下的深度残差，然后结合第一步中得到的多尺度深度信息合成最终的深度图（这里有点类似于何凯明的residual的思想： It is easier to learn the residual than to learn the disparity directly）。</li></ol><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>下面详细的介绍下这个网络的结构:<br><img ) width="120%/" data-src="https://vincentqin.gitee.io/blogresource-2/depth-estimation-using-deeplearning-1/38104630.png"></p><p>可以很清楚地在上图中看到这两个不同的阶段。对于第一个阶段，类似于文献[1]中提到的<strong>DispNetC</strong>结构（C是correlation层的意思），本文作者同样采取了沙漏形的网络结构。但是<strong>DispNetC</strong>网络的输出图像的分辨率只有原始尺寸的一半！CRL中的<strong>DispFulNet</strong>在<strong>DispNetC</strong>的基础上，在最后的两个卷积层增加了添加了反卷积模块，然后再串联左图；通过再次添加一个额外的卷积层，可以使得网络输出为全分辨率（和左右图大小一致）。注意：每个尺度（共6个尺度）上的临时输出与其对应的ground truth之间计算$l_1$损失。<br>总结一下就是，这个<strong>DispFulNet</strong>学习了这样一个网络：通过输入一对图片$I_L$和$I_R$，学习到了视差$d_1$，使得：</p><script type="math/tex; mode=display">\tilde{I}_L(x,y)=I_R(x+d_1(x,y),y)</script><p>上式中的$\tilde{I}_L$就是把右图根据视差移动后的结果，我们的目标就是$\tilde{I}_L$越来越接近$I_L$。</p><p>接下来就是第二阶段，将$I_L$,$I_R$,$\tilde{I}_L$,$d_1$以及$e_L=|I_L-\tilde{I}_L|$串联起来[2]作为<strong>dispResNet</strong>的输入。此优化网络最后学到的是多尺度的残差$ \{r_2^{(s)} \} _s^S$，其中s=0时表示全尺度残差。最后与<strong>DispFulNet</strong>输出的多尺度深度图$\{d_1^{(s)}\}_s^S$做和运算得到最后优化后的深度$\{d_2^{(s)}\}_s^S$：</p><script type="math/tex; mode=display">d_2^{(s)}=d_1^{(s)}+r_2^{(s)},0 \leq s \leq S</script><p>于是$d_2^{(0)}$就是最后的全尺度输出。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>以下是对其结果展示：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-2/depth-estimation-using-deeplearning-1/resluts.png"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]. N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. <a href="https://arxiv.org/abs/1512.02134" target="_blank" rel="noopener"><strong>A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</strong></a>. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4040–4048, 2016.<br>[2]. E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. <a href="https://arxiv.org/abs/1612.01925" target="_blank" rel="noopener"><strong>Flownet 2.0: Evolution of optical flow estimation with deep networks</strong></a>. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2462–2470, 2017.<br>[3]. <a href="http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo" target="_blank" rel="noopener">KITTI: Stereo Evaluation 2015</a><br>[4]. <a href="https://github.com/Artifineuro/crl" target="_blank" rel="noopener">code: Cascade Residual Learning (CRL)</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> depth estimation </tag>
            
            <tag> disparity </tag>
            
            <tag> DispNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Matlab Deep Learning学习笔记</title>
      <link href="/posts/Matlab-Deep-Learning/"/>
      <url>/posts/Matlab-Deep-Learning/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/Matlab-Deep-Learning/cifar10-fig.jpg"></p><div class="note ">            <p>最近对深度学习尤其着迷，是时候用万能的Matlab去践行我的DL学习之路了。之所以用Matlab，是因为Matlab真的太强大了！自从大学开始我就一直用这个神奇的软件，算是最熟悉的编程工具。加上最近mathworks公司一大波大佬的不懈努力，在今年下半年发行的R2017b版本中又加入了诸多新颖的<a href="https://cn.mathworks.com/products/new_products/latest_features.html?s_tid=hp_release_2017b&amp;from=timeline&amp;isappinstalled=0" target="_blank" rel="noopener">特性</a>，尤其在<a href="https://cn.mathworks.com/solutions/deep-learning.html" target="_blank" rel="noopener">DL</a>方面，可以发现：仅仅几条简单的代码，就能够实现复杂的功能。基于以上，我在本文列举了几个在Matlab上学习Deep Learning的例子：1. <a href="#example1">手写字符识别</a>；2. <a href="#example2">搭建网络对CIFAR10分类</a>；3.<a href="#example3">搭建一个Resnet</a>。务必保证主机已经安装Matlab 2017a及以上。</p>          </div><a id="more"></a><h2 id="手写字符识别"><a href="#手写字符识别" class="headerlink" title="手写字符识别"></a><span id="example1">手写字符识别</span></h2><p>利用CNN做数字分类实验。</p><p>接下来的实验会阐明如何进行：</p><ul><li>加载图像数据</li><li>设计网络结构</li><li>设置网络训练参数</li><li>训练网络</li><li>预测新数据的类别</li></ul><h3 id="加载图像数据"><a href="#加载图像数据" class="headerlink" title="加载图像数据"></a>加载图像数据</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">digitDatasetPath = fullfile(matlabroot,<span class="string">'toolbox'</span>,<span class="string">'nnet'</span>,<span class="string">'nndemos'</span>,...</span><br><span class="line">    <span class="string">'nndatasets'</span>,<span class="string">'DigitDataset'</span>);</span><br><span class="line"><span class="comment">% imageDatastore函数 能够通过文件夹名自动地把数据存储成ImageDatastore 对象</span></span><br><span class="line">digitData = imageDatastore(digitDatasetPath,...</span><br><span class="line">    <span class="string">'IncludeSubfolders'</span>,<span class="built_in">true</span>,<span class="string">'LabelSource'</span>,<span class="string">'foldernames'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% Display some of the images in the datastore.</span></span><br><span class="line"><span class="built_in">figure</span>;</span><br><span class="line">perm = randperm(<span class="number">10000</span>,<span class="number">25</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="number">25</span></span><br><span class="line">    subplot(<span class="number">5</span>,<span class="number">5</span>,<span class="built_in">i</span>);</span><br><span class="line">    imshow(digitData.Files&#123;perm(<span class="built_in">i</span>)&#125;);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>以下是手写字符的部分数据：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/Matlab-Deep-Learning/63361958.png"></p><h3 id="创建训练集与验证集"><a href="#创建训练集与验证集" class="headerlink" title="创建训练集与验证集"></a>创建训练集与验证集</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainNumFiles = <span class="number">750</span>;</span><br><span class="line">[trainDigitData,valDigitData] = splitEachLabel(digitData,<span class="number">750</span>,<span class="string">'randomize'</span>); </span><br><span class="line"><span class="comment">% 每类有1000个，选择其中的750类作为训练集，剩下的作为验证集；此处750可以换成一个比例：75%</span></span><br></pre></td></tr></table></figure><p>注意Matlab里面支持的<strong>层</strong>的类型，包括：<a href="https://cn.mathworks.com/help/nnet/ref/nnet.cnn.layer.layer.html?searchHighlight=softmaxlayer&amp;s_tid=doc_srchtitle" target="_blank" rel="noopener">CLICK THIS LINK</a>。如下所示：</p><div class="table-container"><table><thead><tr><th>Epoch</th><th>Iteration</th></tr></thead><tbody><tr><td>Layer Type</td><td>Function</td></tr><tr><td>Image input layer</td><td>imageInputLayer</td></tr><tr><td>Sequence input layer</td><td>sequenceInputLayer</td></tr><tr><td>2-D convolutional layer</td><td>convolution2dLayer</td></tr><tr><td>2-D transposed convolutional layer</td><td>transposedConv2dLayer</td></tr><tr><td>Fully connected layer</td><td>fullyConnectedLayer</td></tr><tr><td>Long short-term memory (LSTM) layer</td><td>LSTMLayer</td></tr><tr><td>Rectified linear unit (ReLU) layer</td><td>reluLayer</td></tr><tr><td>Leaky rectified linear unit (ReLU) layer</td><td>leakyReluLayer</td></tr><tr><td>Clipped rectified linear unit (ReLU) layer</td><td>clippedReluLayer</td></tr><tr><td>Batch normalization layer</td><td>batchNormalizationLayer</td></tr><tr><td>Channel-wise local response normalization (LRN) layer</td><td>crossChannelNormalizationLayer</td></tr><tr><td>Dropout layer</td><td>dropoutLayer</td></tr><tr><td>Addition layer</td><td>additionLayer</td></tr><tr><td>Depth concatenation layer</td><td>depthConcatenationLayer</td></tr><tr><td>Average pooling layer</td><td>averagePooling2dLayer</td></tr><tr><td>Max pooling layer</td><td>maxPooling2dLayer</td></tr><tr><td>Max unpooling layer</td><td>maxUnpooling2dLayer</td></tr><tr><td>Softmax layer</td><td>softmaxLayer</td></tr><tr><td>Classification layer</td><td>classificationLayer</td></tr><tr><td>Regression layer</td><td>regressionLayer</td></tr></tbody></table></div><h3 id="创建自己的网络结构"><a href="#创建自己的网络结构" class="headerlink" title="创建自己的网络结构"></a>创建自己的网络结构</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% Define Network Architecture</span></span><br><span class="line"><span class="comment">% Define the convolutional neural network architecture.</span></span><br><span class="line">layers = </span><br><span class="line">    [</span><br><span class="line">    imageInputLayer([<span class="number">28</span> <span class="number">28</span> <span class="number">1</span>])</span><br><span class="line">    convolution2dLayer(<span class="number">3</span>,<span class="number">16</span>,<span class="string">'Padding'</span>,<span class="number">1</span>)</span><br><span class="line">    batchNormalizationLayer()</span><br><span class="line">    reluLayer()</span><br><span class="line">    maxPooling2dLayer(<span class="number">2</span>,<span class="string">'Stride'</span>,<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    convolution2dLayer(<span class="number">3</span>,<span class="number">32</span>,<span class="string">'Padding'</span>,<span class="number">1</span>)</span><br><span class="line">    batchNormalizationLayer()</span><br><span class="line">    reluLayer()</span><br><span class="line">    </span><br><span class="line">    maxPooling2dLayer(<span class="number">2</span>,<span class="string">'Stride'</span>,<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    convolution2dLayer(<span class="number">3</span>,<span class="number">64</span>,<span class="string">'Padding'</span>,<span class="number">1</span>)</span><br><span class="line">    batchNormalizationLayer()</span><br><span class="line">    reluLayer()</span><br><span class="line">    </span><br><span class="line">    fullyConnectedLayer(<span class="number">10</span>)</span><br><span class="line">    softmaxLayer()</span><br><span class="line">    classificationLayer()</span><br><span class="line">    ];</span><br></pre></td></tr></table></figure><p>以下就是该网络结构及参数设置：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"> <span class="number">1</span>   <span class="string">''</span>   Image Input             <span class="number">28</span>x28x1 images <span class="keyword">with</span> <span class="string">'zerocenter'</span> normalization</span><br><span class="line"> <span class="number">2</span>   <span class="string">''</span>   Convolution             <span class="number">16</span> <span class="number">3</span>x3 convolutions <span class="keyword">with</span> stride [<span class="number">1</span>  <span class="number">1</span>] and padding [<span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>]</span><br><span class="line"> <span class="number">3</span>   <span class="string">''</span>   Batch Normalization     Batch normalization</span><br><span class="line"> <span class="number">4</span>   <span class="string">''</span>   ReLU                    ReLU</span><br><span class="line"> <span class="number">5</span>   <span class="string">''</span>   Max Pooling             <span class="number">2</span>x2 max pooling <span class="keyword">with</span> stride [<span class="number">2</span>  <span class="number">2</span>] and padding [<span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>]</span><br><span class="line"> <span class="number">6</span>   <span class="string">''</span>   Convolution             <span class="number">32</span> <span class="number">3</span>x3 convolutions <span class="keyword">with</span> stride [<span class="number">1</span>  <span class="number">1</span>] and padding [<span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>]</span><br><span class="line"> <span class="number">7</span>   <span class="string">''</span>   Batch Normalization     Batch normalization</span><br><span class="line"> <span class="number">8</span>   <span class="string">''</span>   ReLU                    ReLU</span><br><span class="line"> <span class="number">9</span>   <span class="string">''</span>   Max Pooling             <span class="number">2</span>x2 max pooling <span class="keyword">with</span> stride [<span class="number">2</span>  <span class="number">2</span>] and padding [<span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>]</span><br><span class="line"><span class="number">10</span>   <span class="string">''</span>   Convolution             <span class="number">64</span> <span class="number">3</span>x3 convolutions <span class="keyword">with</span> stride [<span class="number">1</span>  <span class="number">1</span>] and padding [<span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>]</span><br><span class="line"><span class="number">11</span>   <span class="string">''</span>   Batch Normalization     Batch normalization</span><br><span class="line"><span class="number">12</span>   <span class="string">''</span>   ReLU                    ReLU</span><br><span class="line"><span class="number">13</span>   <span class="string">''</span>   Fully Connected         <span class="number">10</span> fully connected layer</span><br><span class="line"><span class="number">14</span>   <span class="string">''</span>   Softmax                 softmax</span><br><span class="line"><span class="number">15</span>   <span class="string">''</span>   Classification Output   crossentropyex</span><br></pre></td></tr></table></figure><h3 id="网络训练参数设计"><a href="#网络训练参数设计" class="headerlink" title="网络训练参数设计"></a>网络训练参数设计</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> options = trainingOptions(<span class="string">'sgdm'</span>,...</span><br><span class="line"><span class="string">'MaxEpochs'</span>,<span class="number">3</span>, ...                 <span class="comment">% 训练最大轮回</span></span><br><span class="line"><span class="string">'ValidationData'</span>,valDigitData,...  <span class="comment">% 验证集</span></span><br><span class="line"><span class="string">'ValidationFrequency'</span>,<span class="number">30</span>,...</span><br><span class="line"><span class="string">'Verbose'</span>,<span class="built_in">false</span>,...</span><br><span class="line"><span class="string">'Plots'</span>,<span class="string">'training-progress'</span>);</span><br></pre></td></tr></table></figure><h3 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = trainNetwork(trainDigitData,layers,options);</span><br></pre></td></tr></table></figure><h3 id="测试新的数据"><a href="#测试新的数据" class="headerlink" title="测试新的数据"></a>测试新的数据</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">predictedLabels = classify(net,valDigitData);</span><br><span class="line">valLabels = valDigitData.Labels;</span><br><span class="line">accuracy = sum(predictedLabels == valLabels)/<span class="built_in">numel</span>(valLabels)</span><br></pre></td></tr></table></figure><h3 id="查看某层参数"><a href="#查看某层参数" class="headerlink" title="查看某层参数"></a>查看某层参数</h3><p>例如查看第2层的weight参数，输入以下命令：<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">montage(imresize(mat2gray(net.Layers(<span class="number">2</span>).Weights),[<span class="number">128</span> <span class="number">128</span>]));</span><br><span class="line">set(gcf,<span class="string">'color'</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]); </span><br><span class="line">frame=getframe(gcf); <span class="comment">% get the frame</span></span><br><span class="line">image=frame.cdata;</span><br><span class="line">[image,map]     =  rgb2ind(image,<span class="number">256</span>);  </span><br><span class="line">imwrite(image,map,<span class="string">'weight-layer2.png'</span>);</span><br></pre></td></tr></table></figure></p><p>图像如下所示：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/Matlab-Deep-Learning/weight-layer2.png"></p><p>再看一下第10层的参数：<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[~,~,iter,~]=<span class="built_in">size</span>(net.Layers(<span class="number">10</span>).Weights);</span><br><span class="line">name=<span class="string">'weight.gif'</span>;</span><br><span class="line">dt=<span class="number">0.4</span>;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:iter</span><br><span class="line">montage(imresize(mat2gray(net.Layers(<span class="number">10</span>).Weights(:,:,<span class="built_in">i</span>,:)),[<span class="number">128</span> <span class="number">128</span>]));</span><br><span class="line">    set(gcf,<span class="string">'color'</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]); <span class="comment">%变白</span></span><br><span class="line">title([<span class="string">'Layer(10), Channel: '</span>,num2str(<span class="built_in">i</span>)]);</span><br><span class="line">axis normal</span><br><span class="line"><span class="built_in">true</span><span class="built_in">size</span></span><br><span class="line"><span class="comment">%Creat GIF</span></span><br><span class="line">frame(<span class="built_in">i</span>)=getframe(gcf); <span class="comment">% get the frame</span></span><br><span class="line">image=frame(<span class="built_in">i</span>).cdata;</span><br><span class="line">[image,map]     =  rgb2ind(image,<span class="number">256</span>);  </span><br><span class="line"><span class="keyword">if</span> <span class="built_in">i</span>==<span class="number">1</span></span><br><span class="line"> imwrite(image,map,name,<span class="string">'gif'</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"> imwrite(image,map,name,<span class="string">'gif'</span>,<span class="string">'WriteMode'</span>,<span class="string">'append'</span>,<span class="string">'DelayTime'</span>,dt);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/Matlab-Deep-Learning/weight-inf.gif"></p><h2 id="搭建网络对CIFAR10分类"><a href="#搭建网络对CIFAR10分类" class="headerlink" title="搭建网络对CIFAR10分类"></a><span id="example2">搭建网络对CIFAR10分类</span></h2><p>CIFAR10和CIFAR100是<a href="http://groups.csail.mit.edu/vision/TinyImages/" target="_blank" rel="noopener">80 million tiny images</a>的子集，是由Geoffrey Hinton的弟子们Alex Krizhevsky和Vinod Nair共同采集。</p><h3 id="CIFAR10"><a href="#CIFAR10" class="headerlink" title="CIFAR10"></a><a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">CIFAR10</a></h3><p>CIFAR10由60000张32*32的彩色图像组成，一种分成10类，平均每类图像6000张。共有50000张训练图像，10000张测试图像。这个数据集被分成了5个分支，其中每个分支10000张。测试集包含每类中随机选择的1000张图像。训练集就是剩下的那些图像。<br>对于每个分支的数据的大小是：10000*3072；其中3072=32*32*3。数据以行优先的顺序存储，所以前1024个数据是r通道的数据，接下来的1024个数据是g通道的数据，最后1024个数据是b通道的。<br>假如原始的数据是data，我们想要将其重新排列成我们需要的数据。首先对其进行转置，然后再用reshape函数对图像重组（可选：最后将图像前两维互换（转置），之所以这么做，可以更好的可视化）。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">XBatch = data';</span><br><span class="line">XBatch = <span class="built_in">reshape</span>(XBatch, <span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>,[]);</span><br><span class="line">XBatch = <span class="built_in">permute</span>(XBatch, [<span class="number">2</span> <span class="number">1</span> <span class="number">3</span> <span class="number">4</span>]);</span><br></pre></td></tr></table></figure><p>以下是cifar10的部分数据。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/Matlab-Deep-Learning/cifar10-images.png"><br>共有10类，包括：airplane，automobile，bird，cat，deer，dog，frog，horse，ship，truck。</p><h3 id="Just-run-it"><a href="#Just-run-it" class="headerlink" title="Just run it"></a>Just run it</h3><p>接下来我们就开始运行以下代码，来训练我们的网络。闲话少说，我把代码放在了<a href="https://github.com/Vincentqyw/DeepLearning/blob/master/demo_cifar10.m" target="_blank" rel="noopener">Github</a>，欢迎$star$。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>   <span class="string">'imageinput'</span>    Image Input         <span class="number">28</span>x28x1 images <span class="keyword">with</span> <span class="string">'zerocenter'</span> normalization</span><br><span class="line"><span class="number">2</span>   <span class="string">'conv_1'</span>        Convolution         <span class="number">16</span> <span class="number">3</span>x3x1 convolutions <span class="keyword">with</span> stride [<span class="number">1</span>  <span class="number">1</span>] and padding [<span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>]</span><br><span class="line"><span class="number">3</span>   <span class="string">'batchnorm_1'</span>   Batch Normalization Batch normalization <span class="keyword">with</span> <span class="number">16</span> channels</span><br><span class="line"><span class="number">4</span>   <span class="string">'relu_1'</span>        ReLU               ReLU</span><br><span class="line"><span class="number">5</span>   <span class="string">'maxpool_1'</span>     Max Pooling         <span class="number">2</span>x2 max pooling <span class="keyword">with</span> stride [<span class="number">2</span>  <span class="number">2</span>] and padding [<span class="number">0</span> <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>]</span><br><span class="line"><span class="number">6</span>   <span class="string">'conv_2'</span>        Convolution         <span class="number">32</span> <span class="number">3</span>x3x16 convolutions <span class="keyword">with</span> stride [<span class="number">1</span>  <span class="number">1</span>] and padding [<span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>]</span><br><span class="line"><span class="number">7</span>   <span class="string">'batchnorm_2'</span>   Batch Normalization Batch normalization <span class="keyword">with</span> <span class="number">32</span> channels</span><br><span class="line"><span class="number">8</span>   <span class="string">'relu_2'</span>        ReLU                ReLU</span><br><span class="line"><span class="number">9</span>   <span class="string">'maxpool_2'</span>     Max Pooling         <span class="number">2</span>x2 max pooling <span class="keyword">with</span> stride [<span class="number">2</span>  <span class="number">2</span>] and padding [<span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"><span class="number">10</span>   <span class="string">'conv_3'</span>       Convolution         <span class="number">64</span> <span class="number">3</span>x3x32 convolutions <span class="keyword">with</span> stride [<span class="number">1</span>  <span class="number">1</span>] and padding [<span class="number">1</span> <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>]</span><br><span class="line"><span class="number">11</span>   <span class="string">'batchnorm_3'</span>  Batch Normalization Batch normalization <span class="keyword">with</span> <span class="number">64</span> channels</span><br><span class="line"><span class="number">12</span>   <span class="string">'relu_3'</span>       ReLUReLU</span><br><span class="line"><span class="number">13</span>   <span class="string">'fc'</span>           Fully Connected<span class="number">10</span> fully connected layer</span><br><span class="line"><span class="number">14</span>   <span class="string">'softmax'</span>      Softmaxsoftmax</span><br><span class="line"><span class="number">15</span>   <span class="string">'classoutput'</span>  Classification Outputcrossentropyex <span class="keyword">with</span> <span class="string">'0'</span>, <span class="string">'1'</span>, and <span class="number">8</span> other classes</span><br></pre></td></tr></table></figure><p>以下是训练过程输出：</p><div class="table-container"><table><thead><tr><th>Epoch</th><th>Iteration</th><th style="text-align:center">Time Elapsed (seconds)</th><th style="text-align:center">Mini-batch Loss</th><th style="text-align:center">Mini-batch Accuracy</th><th style="text-align:center">Base Learning Rate</th></tr></thead><tbody><tr><td>1</td><td>1</td><td style="text-align:center">0.06</td><td style="text-align:center">2.3026</td><td style="text-align:center">8.59%</td><td style="text-align:center">0.0020</td></tr><tr><td>1</td><td>50</td><td style="text-align:center">1.27</td><td style="text-align:center">2.3026</td><td style="text-align:center">14.06%</td><td style="text-align:center">0.0020</td></tr><tr><td>1</td><td>100</td><td style="text-align:center">2.52</td><td style="text-align:center">2.3024</td><td style="text-align:center">7.81%</td><td style="text-align:center">0.0020</td></tr><tr><td>1</td><td>150</td><td style="text-align:center">3.73</td><td style="text-align:center">2.2999</td><td style="text-align:center">20.31%</td><td style="text-align:center">0.0020</td></tr><tr><td>1</td><td>200</td><td style="text-align:center">5.01</td><td style="text-align:center">2.2740</td><td style="text-align:center">15.63%</td><td style="text-align:center">0.0020</td></tr><tr><td>1</td><td>250</td><td style="text-align:center">6.28</td><td style="text-align:center">2.1194</td><td style="text-align:center">21.09%</td><td style="text-align:center">0.0020</td></tr><tr><td>1</td><td>300</td><td style="text-align:center">7.58</td><td style="text-align:center">1.9100</td><td style="text-align:center">23.44%</td><td style="text-align:center">0.0020</td></tr><tr><td>1</td><td>350</td><td style="text-align:center">8.86</td><td style="text-align:center">1.8892</td><td style="text-align:center">28.13%</td><td style="text-align:center">0.0020</td></tr><tr><td>2</td><td>400</td><td style="text-align:center">10.08</td><td style="text-align:center">1.7490</td><td style="text-align:center">29.69%</td><td style="text-align:center">0.0020</td></tr><tr><td>2</td><td>450</td><td style="text-align:center">11.32</td><td style="text-align:center">1.8377</td><td style="text-align:center">31.25%</td><td style="text-align:center">0.0020</td></tr><tr><td>2</td><td>500</td><td style="text-align:center">12.57</td><td style="text-align:center">1.6073</td><td style="text-align:center">39.84%</td><td style="text-align:center">0.0020</td></tr><tr><td>…</td><td>…</td><td style="text-align:center">…</td><td style="text-align:center">…</td><td style="text-align:center">…</td><td style="text-align:center">…</td></tr><tr><td>20</td><td>7650</td><td style="text-align:center">407.74</td><td style="text-align:center">0.2858</td><td style="text-align:center">93.75%</td><td style="text-align:center">2.00e-05</td></tr><tr><td>20</td><td>7700</td><td style="text-align:center">409.06</td><td style="text-align:center">0.3127</td><td style="text-align:center">89.84%</td><td style="text-align:center">2.00e-05</td></tr><tr><td>20</td><td>7750</td><td style="text-align:center">410.38</td><td style="text-align:center">0.3254</td><td style="text-align:center">87.50%</td><td style="text-align:center">2.00e-05</td></tr><tr><td>20</td><td>7800</td><td style="text-align:center">411.64</td><td style="text-align:center">0.2456</td><td style="text-align:center">92.19%</td><td style="text-align:center">2.00e-05</td></tr></tbody></table></div><p>最后测试我们的模型的性能，accuracy=76%左右。但是训练时，我们的batch-accuracy已经达到了90%以上，说明我们的<strong>模型过拟合</strong>了。显然这不是我们想要的结果，进一步的调参将会在此补充。</p><h3 id="可视化某层的参数"><a href="#可视化某层的参数" class="headerlink" title="可视化某层的参数"></a>可视化某层的参数</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% Extract the first convolutional layer weights</span></span><br><span class="line">w = cifar10Net.Layers(<span class="number">2</span>).Weights;</span><br><span class="line"></span><br><span class="line"><span class="comment">% rescale and resize the weights for better visualization</span></span><br><span class="line">w = mat2gray(w);</span><br><span class="line">w = imresize(w, [<span class="number">100</span> <span class="number">100</span>]);</span><br><span class="line"></span><br><span class="line"><span class="built_in">figure</span></span><br><span class="line">montage(w)</span><br><span class="line">name=<span class="string">'cifar10-weight-layer2'</span>;</span><br><span class="line">set(gcf,<span class="string">'color'</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]);</span><br><span class="line">frame=getframe(gcf); <span class="comment">% get the frame</span></span><br><span class="line">image=frame.cdata;</span><br><span class="line">[image,map]     =  rgb2ind(image,<span class="number">256</span>);  </span><br><span class="line">imwrite(image,map,[name,<span class="string">'.png'</span>]);</span><br></pre></td></tr></table></figure><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/Matlab-Deep-Learning/cifar10-weight-layer2.png"></p><h2 id="搭建一个Resnet"><a href="#搭建一个Resnet" class="headerlink" title="搭建一个Resnet"></a><span id="example3">搭建一个Resnet</span></h2><p>接下来，为了验证下这个DL工具包的强大之处，我打算纯手工建一个Resnet。为方便起见，我搭了一个Resnet34（更深的网络敬请期待吧）。这里是它的<a href="./resnet34.prototxt">prototxt</a>，我们可以用<a href="http://ethereon.github.io/netscope/#/editor" target="_blank" rel="noopener">网络可视化工具</a>进行查看resnet34的结构。以下是Resnet34的一部分（太长了没有截下全部视图）。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/Matlab-Deep-Learning/resnet34.png"></p><h3 id="定义每一层与连接层"><a href="#定义每一层与连接层" class="headerlink" title="定义每一层与连接层"></a>定义每一层与连接层</h3><p>以从pool1到res2a为例子建立网络。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">layers_example=[</span><br><span class="line">    <span class="comment">% pool1 - res2a</span></span><br><span class="line">    maxPooling2dLayer(<span class="number">3</span>, <span class="string">'Stride'</span>, <span class="number">2</span>,<span class="string">'Name'</span>,<span class="string">'pool1'</span>);</span><br><span class="line">    <span class="comment">%  branch2a </span></span><br><span class="line">    convolution2dLayer(<span class="number">3</span>,<span class="number">64</span>,<span class="string">'Stride'</span>, <span class="number">1</span>,<span class="string">'Padding'</span>, <span class="number">1</span>,<span class="string">'Name'</span>,<span class="string">'res2a_branch2a'</span>)</span><br><span class="line">    batchNormalizationLayer(<span class="string">'Name'</span>,<span class="string">'bn2a_branch2a'</span>)</span><br><span class="line">    reluLayer(<span class="string">'Name'</span>,<span class="string">'res2a_branch2a_relu'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">%  branch2b</span></span><br><span class="line">    convolution2dLayer(<span class="number">3</span>,<span class="number">64</span>,<span class="string">'Stride'</span>, <span class="number">1</span>,<span class="string">'Padding'</span>, <span class="number">1</span>,<span class="string">'Name'</span>,<span class="string">'res2a_branch2b'</span>)</span><br><span class="line">    batchNormalizationLayer(<span class="string">'Name'</span>,<span class="string">'bn2a_branch2b'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">% add together</span></span><br><span class="line">    additionLayer(<span class="number">2</span>,<span class="string">'Name'</span>,<span class="string">'res2a'</span>)</span><br><span class="line">    reluLayer(<span class="string">'Name'</span>,<span class="string">'res2a_relu'</span>)</span><br><span class="line">];</span><br></pre></td></tr></table></figure><p>上述过程仅仅完成了网络的一个小分支，记下来要完成<code>res2a_branch1</code>这部分的连接。这时候要用到<strong>DAG</strong>的<a href="https://cn.mathworks.com/help/nnet/ref/dagnetwork.html" target="_blank" rel="noopener">一些方法</a>。通过添加新层同时建立新的连接即可，方式如下。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">lgraph = layerGraph(layers_example);</span><br><span class="line"><span class="built_in">figure</span></span><br><span class="line"><span class="built_in">plot</span>(lgraph)</span><br><span class="line"></span><br><span class="line"><span class="comment">%% add some connections (shortcut)</span></span><br><span class="line">layers_2a=[</span><br><span class="line">    convolution2dLayer(<span class="number">1</span>,<span class="number">64</span>,<span class="string">'Stride'</span>, <span class="number">1</span>,<span class="string">'Padding'</span>, <span class="number">0</span>,<span class="string">'Name'</span>,<span class="string">'res2a_branch1'</span>)</span><br><span class="line">    batchNormalizationLayer(<span class="string">'Name'</span>,<span class="string">'bn2a_branch1'</span>)</span><br><span class="line">];</span><br><span class="line">lgraph = addLayers(lgraph,layers_2a);</span><br><span class="line">lgraph = connectLayers(lgraph,<span class="string">'pool1'</span>,<span class="string">'res2a_branch1'</span>);</span><br><span class="line">lgraph = connectLayers(lgraph,<span class="string">'bn2a_branch1'</span>,<span class="string">'res2a/in2'</span>);</span><br><span class="line"><span class="comment">% show net</span></span><br><span class="line"><span class="built_in">plot</span>(lgraph)</span><br></pre></td></tr></table></figure><p>其他部分的构建同上，经过一系列重复的工作，我们可以构建出这个不太深的Resnet34，全部代码见我的<a href="https://github.com/Vincentqyw/DeepLearning" target="_blank" rel="noopener">Github</a>。</p><h2 id="一些基本问题"><a href="#一些基本问题" class="headerlink" title="一些基本问题"></a>一些基本问题</h2><ul><li>参数的基本格式</li></ul><script type="math/tex; mode=display">Height \times Width \times (\#Channels) \times (\#Filters)</script><ul><li>SGD是什么？<br>可以参见好友写的一篇<a href="https://sttomato.github.io/2017/08/13/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/#随机梯度下降" target="_blank" rel="noopener">博文</a>。</li><li>什么是epoch？<br>模型训练的时候一般采用stochastic gradient descent（SGD），一次迭代选取一个batch进行update。一个epoch的意思就是迭代次数*batch的数目 和训练数据的个数一样，就是一个epoch。</li><li>为什么要是用BN？<br>Batch normalization layers normalize the activations and gradients propagating through a network, making network training an easier optimization problem. Use batch normalization layers between convolutional layers and nonlinearities, such as ReLU layers, to speed up network training and reduce the sensitivity to network initialization.</li><li>RELU的作用？<br><em>Max-Pooling Layer</em> Convolutional layers (with activation functions) are sometimes followed by a down-sampling operation that reduces the spatial size of the feature map and removes redundant spatial information. Down-sampling makes it possible to increase the number of filters in deeper convolutional layers without increasing the required amount of computation per layer. One way of down-sampling is using a max pooling. The max pooling layer returns the maximum values of rectangular regions of inputs.</li><li><p><em>add more</em></p></li><li><p>Resnet中scale层是如何定义的？有什么用途？ </p></li><li><p>Resnet中为何残差$F(x)$比$H(x)$好学？ </p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Matlab </tag>
            
            <tag> CNN </tag>
            
            <tag> CIFAR10 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN框架(CNN Architectures)</title>
      <link href="/posts/CNN-Architectures/"/>
      <url>/posts/CNN-Architectures/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><center><img width="75%" dense-net data-src="https://vincentqin.gitee.io/blogresource-2/CNN-Architectures/9721733.png"></center><div class="note ">            <p>本文来自于CS231N（2017 Spring），将介绍几种较为常见的CNN结构。以下网络均是ImageNet比赛的冠军之作，我们将从网络结构，参数规模，运算量等来描述各个网络的特点。</p>          </div><a id="more"></a><ul><li>AlexNet</li><li>VGG</li><li>GoogLeNet</li><li>ResNet</li></ul><p>后续将补充以下几种网络：</p><ul><li>NiN(Network in Network)</li><li>wide ResNet</li><li>ResNeXT</li><li>stochastic Depth</li><li><a href="https://github.com/liuzhuang13/DenseNet" target="_blank" rel="noopener">DenseNet</a></li><li>FractalNet</li><li>SqueezeNet</li></ul><p>以下是正文。</p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img alt="AlexNet" data-src="https://vincentqin.gitee.io/blogresource-2/CNN-Architectures/99013631.png"><br>网络的输入大小为：227*227*3，每一层的结构以及参数设置如下：</p><div class="table-container"><table><thead><tr><th style="text-align:left">Layer Type</th><th style="text-align:left">#Filters</th><th style="text-align:left">Stride</th><th style="text-align:left">Pading</th><th style="text-align:left">OUTPUT SIZE</th><th style="text-align:left">Parameters</th></tr></thead><tbody><tr><td style="text-align:left">CONV1</td><td style="text-align:left">#96 @11*11</td><td style="text-align:left">4</td><td style="text-align:left">0</td><td style="text-align:left">55*55*96</td><td style="text-align:left">11*11*3*96</td></tr><tr><td style="text-align:left">MAXPOOL1</td><td style="text-align:left">3*3</td><td style="text-align:left">2</td><td style="text-align:left">0</td><td style="text-align:left">27*27*96</td><td style="text-align:left">0</td></tr><tr><td style="text-align:left">NORM1</td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left">27*27*96</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">CONV1</td><td style="text-align:left">#256 @5*5</td><td style="text-align:left">1</td><td style="text-align:left">2</td><td style="text-align:left">27*27*256</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">MAXPOOL2</td><td style="text-align:left">3*3</td><td style="text-align:left">2</td><td style="text-align:left">0</td><td style="text-align:left">13*13*256</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">NORM2</td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left">13*13*256</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">CONV3</td><td style="text-align:left">#384 @3*3</td><td style="text-align:left">1</td><td style="text-align:left">1</td><td style="text-align:left">13*13*384</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">CONV4</td><td style="text-align:left">#384 @3*3</td><td style="text-align:left">1</td><td style="text-align:left">1</td><td style="text-align:left">13*13*384</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">CONV5</td><td style="text-align:left">#256 @3*3</td><td style="text-align:left">1</td><td style="text-align:left">1</td><td style="text-align:left">13*13*256</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">MAXPOOL3</td><td style="text-align:left">3*3</td><td style="text-align:left">2</td><td style="text-align:left">0</td><td style="text-align:left">6*6*256</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">FC6</td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left">4096</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">FC7</td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left">4096</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">FC8</td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left">1000</td><td style="text-align:left">55*55*96</td></tr></tbody></table></div><p>The size of output image is $\frac{N-Conv+2\times Pading}{stride}+1$</p><p><img alt="AlexNet-details" data-src="https://vincentqin.gitee.io/blogresource-2/CNN-Architectures/33818113.png"></p><p><em>后续将使用<a href="https://cn.mathworks.com/help/nnet/examples.html?s_cid=doc_flyout#bvljehw" target="_blank" rel="noopener">Matlab DL 工具包</a>补充Alexnet实验…</em></p><h2 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h2><p>The winner of ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014.</p><h3 id="网络结构-1"><a href="#网络结构-1" class="headerlink" title="网络结构"></a>网络结构</h3><p><strong>small filters, deeper networks</strong>。<br>将原来8层的AlexNet扩展到了16&amp;19层。卷积层的大小仅仅有3*3，stride=1，pad=1；池化层仅仅有stride=2的2*2的MAXPOOL。以下是其与AlexNet的结构对比图。<br><img alt="VGG" data-src="https://vincentqin.gitee.io/blogresource-2/CNN-Architectures/89475867.png"></p><p>更加具体的，VGG16的网络的参数个数以及内存消耗如下：<br><img alt="VGG-details" data-src="https://vincentqin.gitee.io/blogresource-2/CNN-Architectures/31257061.png"></p><p>Q：为何采用更小的CONV？<br>A：几个3*3的CONV叠加后的接受域和一个7*7大小的CONV的接受域一致，但是与此同时，<strong>网络层数变深，引入了更多的非线性，参数数量更少</strong>。（Stack of three 3x3 conv (stride 1) layers has same effective receptive field as one 7x7 conv layer，But deeper, more non-linearities. And fewer parameters: $3\times3^2C^2$ vs. $7^2C^2$ for C channels per layer）</p><h3 id="更多细节"><a href="#更多细节" class="headerlink" title="更多细节"></a>更多细节</h3><ul><li>ILSVRC’14 2nd in classification, 1st in localization Similar training</li><li>procedure as Krizhevsky 2012 No Local Response Normalisation (LRN)</li><li>Use VGG16 or VGG19 (VGG19 only slightly better, more memory) </li><li>Use ensembles for best results </li><li>FC7 features generalize well to other tasks</li></ul><h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p>论文地址：<a href="https://arxiv.org/pdf/1409.4842.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1409.4842.pdf</a><br>代码地址：NULL<br><strong>Deeper networks, with computational efficiency</strong>。GoogLeNet是ILSVRC’14的图像分类冠军网络，它加入了<strong>Inception</strong>模块，并且去除了全连接层，大大减少了参数的个数。</p><ul><li>22 layers (with weights)</li><li>Efficient “Inception” module</li><li>No FC layers</li><li>Only 5 million parameters! 12x less than AlexNet</li><li>ILSVRC’14 classification winner (6.7% top 5 error)</li></ul><h3 id="“Inception-module”"><a href="#“Inception-module”" class="headerlink" title="“Inception module”"></a>“Inception module”</h3><p>精心设计了一个局部网络模块，并且将这些模块叠加构成GoolgeNet。这种经过精心设计的模块就是Inception。（design a good local network topology (network within a network) and then stack these modules on top of each other）。<br>Inception包含几个接受域不同的CONV核（1*1，3*3，5*5）以及池化操作（3*3）；最终将这些操作后的输出在depth方向串联。以下是两种两种不同的实现方式，左图时原始的inception模块，右图是改进版的inception模块。<br><img alt="inception" data-src="https://vincentqin.gitee.io/blogresource-2/CNN-Architectures/33298717.png"><br>对于naive inception而言，它面临这运算量巨大的问题。由于池化层的输出会保留原始输入的depth，所以经过CONV&amp;MAXPOOL过后的输出的feature map势必比原始输入的depth更深。<br><img alt="inception-naive" data-src="https://vincentqin.gitee.io/blogresource-2/CNN-Architectures/53626485.png"><br>那么如何去解决以上问题呢，一个通常的方式就是降维。我们在每个CONV前加上1*1的CONV（“bottleneck” layers）来减少feature map的维度。所谓的1*1CONV就是在保持输入的空间分辨率不变的情况下来减小depth维度，即通过将不同depth上的feature map进行组合，从而将输入的feature map映射到更低的depth维度上。经过以上操作就可以将运算的操作次数大大降低。<br><img alt="inception-improve" data-src="https://vincentqin.gitee.io/blogresource-2/CNN-Architectures/95592762.png"></p><p>于是GoogLeNet的全貌如下：<br><img alt="googlenet" data-src="https://vincentqin.gitee.io/blogresource-2/CNN-Architectures/75689322.jpg"></p><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>利用<strong>残差</strong>连接成的超级深网络。<br>这里有一个何凯明在ICML2016的Tutorial，内容比较详细。<a href="http://kaiminghe.com/icml16tutorial/index.html" target="_blank" rel="noopener">ICML 2016 Tutorial on Deep Residual Networks</a><br>代码在这里<a href="https://github.com/KaimingHe/deep-residual-networks" target="_blank" rel="noopener">Code: deep-residual-networks</a></p><h3 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h3><ul><li>152-layer model for ImageNet</li><li>ILSVRC’ 15 classification winner (3.57% top 5 error)</li><li>Swept all classification and detection competitions in ILSVRC’ 15 and COCO’ 15!</li></ul><h3 id="深度增加带来的问题"><a href="#深度增加带来的问题" class="headerlink" title="深度增加带来的问题"></a>深度增加带来的问题</h3><p><img alt="deeper-nets-problems" data-src="https://vincentqin.gitee.io/blogresource-2/CNN-Architectures/22279699.png"><br>从上图可以发现，当网络层数增加时，训练误差和测试误差都有所下降。这并不符合以往的经验，我们会想，既然网络层数增加了，那么模型参数势必增多，此时会造成过拟合。然而过拟合的表现是：训练误差减小，测试误差增大。但是事实和分析并不吻合。<br>何凯明认为：<strong>The problem is an optimization problem, deeper models are harder to optimize</strong>。这是一个优化问题，更深的网络更难优化。并且，更深的网络应该至少比浅层网络不差，这是因为我们可以通过拷贝浅层网络+identity mapping（恒等映射）来构造一个更深的网络，这个结构化的方案表明深层网络可以达到和浅层网络一致的性能。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p><img alt="resnet-layer" data-src="https://vincentqin.gitee.io/blogresource-2/CNN-Architectures/resnet.png"><br>Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping.<br>作者假设：<strong>相较于最优化最初的无参照映射（残差函数以输入x作为参照），最优化残差映射是更容易的</strong>。利用网络去拟合残差$F(x)$，并非直接拟合$H(x)$。</p><h3 id="整个ResNet框架"><a href="#整个ResNet框架" class="headerlink" title="整个ResNet框架"></a>整个ResNet框架</h3><p><img alt="resnet-structure" data-src="https://vincentqin.gitee.io/blogresource-2/CNN-Architectures/82101608.png"></p><ul><li>Stack residual blocks</li><li>Every residual block has two 3x3 conv layers</li><li>Periodically, double # of filters and downsample spatially using stride 2 (/2 in each dimension)</li><li>Additional conv layer at the beginning</li><li>No FC layers at the end (only FC 1000 to output classes)</li></ul><p>对于ImageNet比赛而言，ResNet设置的网络深度有34、50、101以及152层。对于层数较多的网络，利用“bottleneck”（类似于GoogLeNet的1*1卷积操作）来提高效率。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>论文<a href="https://arxiv.org/pdf/1605.07678.pdf" target="_blank" rel="noopener">An Analysis of Deep Neural Network Models for Practical Applications</a> 比较了2016年以来的一些神经网络的规模、运算量、能耗以及精度等项目。<br><img alt="complexity-compare" data-src="https://vincentqin.gitee.io/blogresource-2/CNN-Architectures/40479799.png"><br>可以从上图总结出以下几点：</p><ul><li>GoogLeNet: most efficient</li><li>VGG: Highest memory, most operations</li><li>AlexNet: Smaller compute, still memory heavy, lower accuracy</li><li>ResNet: Moderate efficiency depending on model, highest accuracy</li></ul><h2 id="其他网络变体"><a href="#其他网络变体" class="headerlink" title="其他网络变体"></a>其他网络变体</h2><p>后续补充。</p><h2 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h2><ul><li><strong><font color="red">ResNet为何能够使网络层数更深，应如何正确理解残差网络？He是受何启发从而发明了这种结构？</font></strong></li><li>more questions will be added…</li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="http://deeplearning.net/" target="_blank" rel="noopener">DeepLearning.net</a></li><li><a href="http://deeplearning.net/reading-list/" target="_blank" rel="noopener">Reading List</a></li><li><a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a></li><li><a href="https://zhuanlan.zhihu.com/p/28124810" target="_blank" rel="noopener">为什么ResNet和DenseNet可以这么深？一文详解残差块为何有助于解决梯度弥散问题</a></li><li><a href="https://arxiv.org/pdf/1605.07678.pdf" target="_blank" rel="noopener">An Analysis of Deep Neural Network Models for Practical Applications</a></li><li><a href="http://cs231n.stanford.edu/" target="_blank" rel="noopener">CS231n: Convolutional Neural Networks for Visual Recognition</a></li><li><a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank" rel="noopener">Densely Connected Convolutional Networks</a></li><li><a href="http://kaiminghe.com/icml16tutorial/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf" target="_blank" rel="noopener">Deep Residual Networks (Deep Learning Gets Way Deeper)</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AlexNet </tag>
            
            <tag> VGG </tag>
            
            <tag> GoogLeNet </tag>
            
            <tag> ResNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>理解LSTM网络【译】</title>
      <link href="/posts/lstm/"/>
      <url>/posts/lstm/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/lstm/honghong.jpg"></p><p>本文是我对大神<a href="http://colah.github.io/about.html" target="_blank" rel="noopener">Christopher Olah</a>的<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a>的译文。<br><a id="more"></a></p><h2 id="循环神经网络（Recurrent-Neural-Networks）"><a href="#循环神经网络（Recurrent-Neural-Networks）" class="headerlink" title="循环神经网络（Recurrent Neural Networks）"></a>循环神经网络（Recurrent Neural Networks）</h2><p>人们并非每时每刻都在大脑一片空白时开始思考。当我们读这篇文章的时候，我们会根据之前学习的知识来理解当前你在阅读的内容。我们不是把原来的知识丢的一干二净来重新学习，我们的记忆有一定的<strong>持久性</strong>。<br>传统的神经网络做不到这些，这是它的一大缺陷。比如说，可以想象有这样一种情况，我们想知道一部电影的每一帧画面正在发生什么。使用传统的神经网络很难通过理解电影之前的画面来推断以后将要发生的事件。（传统的神经网络不能处理带有时序的样本）<br>循环神经网络（Recurrent Neural Networks）尝试解决了以上问题。这种网络是一种带有循环结构的网络，可以使得信息得以持久保持。</p><center><img width="15%" rnn有循环结构 data-src="https://vincentqin.gitee.io/blogresource-1/lstm/RNN-rolled.png"></center><p>上图是一个RNN模块，$A$读取输入$x_i$，同时输出$h_t$，$A$这个循环允许信息从网络的当前步骤传递到下一步骤。<br>上述过程把RNN的过程讲的有些神秘感。但是，如果我们仔细想想，这也不比一个正常的神经网络难以理解。一个RNN可以看成是多个相同网络的拷贝，每一个拷贝都会向后续网络传递信息，下图我们把RNN展开。<br><img alt="展开RNN" data-src="https://vincentqin.gitee.io/blogresource-1/lstm/RNN-unrolled.png"><br>这种链式的特性揭示了RNN与序列和列表有关，RNN是对这些数据最自然的表达。RNN目前已经被广泛使用！在过去的几年间，RNN在很多领域都有着出色的表现：语音识别，语言建模，翻译，图像描述…推荐大家阅读大神Andrej Karpathy的博文<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener"> The Unreasonable Effectiveness of Recurrent Neural Networks</a>，来领略下RNN的诸多应用，这简直不能太棒！<br>以上的成功案例离不开使用<strong>长短期记忆</strong>（LSTM: Long Short Term Memory）网络，这是一种特殊的RNN网络并且能够应对更多任务，相比于标准的RNN网络，它具有更为出色的表现。几乎所有的RNN都是基于LSTM来实现的，接下来我们讨论下LSTM的奥义。</p><h2 id="长期依赖（Long-Term-Dependencies）问题"><a href="#长期依赖（Long-Term-Dependencies）问题" class="headerlink" title="长期依赖（Long-Term Dependencies）问题"></a>长期依赖（Long-Term Dependencies）问题</h2><p>RNNs的要求之一就是它能够连接之前的信息到当前的任务之上，例如利用之前的视频帧来理解当前帧的内容。如果RNNs能够做到这一点的话，它将会超级有用。但是它真的可以吗？看情况而定。<br>有时，我们仅仅需要离当前任务最近的几个任务信息。例如，我们有一个<strong>语言模型</strong>，它的目标是根据当前已有的词语去预测接下来将要出现的词语。如果我们尝试去预测“the clouds are in the sky”中的最后一个单词，我们不需要任何更多的语料，很明显最后一个单词将会是“sky”。在这种情况下，相关信息和当前需要预测词的位置的间隔是非常小的，这时RNNs可以去利用过去的信息。<br><img alt="短期记忆" data-src="https://vincentqin.gitee.io/blogresource-1/lstm/RNN-shorttermdepdencies.png"><br>但是也有一种情况是，我们需要更多的信息才能够做预测。例如我们的<strong>语言模型</strong>需要预测下面句子的最后一个单词“I grew up in France… I speak fluent French.”。从相邻近的几个单词可以推断最后一个单词可能是一种语言，但如果我们想要知道到底是哪种语言的话，我们需要句子最前面的一个单词“France”。这会使得相关信息以及需要预测词的位置的间隔变得很大。<br>遗憾的是，随着间隔的逐渐增大，RNNs不能够去关联有用的信息。<br><img alt="长期记忆" data-src="https://vincentqin.gitee.io/blogresource-1/lstm/RNN-longtermdependencies.png"><br>在理论上，RNNs能够解决长期依赖的问题。人们可以通过仔细选取参数来解决这类问题。但是实际上RNNs并不能去解决这个问题。<a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="noopener"> Hochreiter (1991) [German] and Bengio(1994)</a>等人深入研究了该问题为何如此艰难。<br>阿弥陀佛，LSTMs并没有上述问题。</p><h2 id="LSTM网络"><a href="#LSTM网络" class="headerlink" title="LSTM网络"></a>LSTM网络</h2><p>长短期记忆网络——经常被叫做“LSTM”——是RNN的这一种特殊的形式，它能够解决长期依赖的问题。LSTM是由<a href="https://www.researchgate.net/publication/13853244_Long_Short-term_Memory" target="_blank" rel="noopener">Hochreiter &amp; Schmidhuber (1997)</a>提出，并由很多后来者完善以及推广。LSTM能够在很多问题上取得优秀的结果，现如今被广泛引用。<br>LSTM被设计成防止长期依赖问题的发生。在实践中，LSTM的长期记忆是默认行为，而并非艰苦习得！所有的RNN都有链式重复的神经网络模块。在标准的RNN中，这些重复的模块仅仅有简单的结构，例如$tanh$层。<br><img alt="标准RNN中重复的简单模块" data-src="https://vincentqin.gitee.io/blogresource-1/lstm/LSTM3-SimpleRNN.png"><br>当然，LSTM中也存在这样的链式结构，但是其中的重复模块就大为不同了。LSTM的重复的模块中包含4种不同的层，它们以一种特殊的结构交错着。<br><img alt="LSTM中重复的模块中包含4种不同层" data-src="https://vincentqin.gitee.io/blogresource-1/lstm/LSTM3-chain.png"><br>看不懂，不用担心，细节即将展开。接下来，我们会一步步来讲解LSTM的网络结构。首先我们先明确几个会经常用到的表示方法。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/lstm/LSTM2-notation.png"><br>在以上的图示中，每条实线传输着整个向量，从一个节点的输出到其他节点的输入。粉红色的圆圈表示的是<strong>逐点操作</strong>，例如向量的加法，黄色的方形表示已经学习了的网络层。汇集的线表示<strong>串联</strong>，分叉的线表示<strong>复制</strong>操作，这些复制的内容流向不同的位置。</p><h2 id="LSTM背后的核心技术"><a href="#LSTM背后的核心技术" class="headerlink" title="LSTM背后的核心技术"></a>LSTM背后的核心技术</h2><p>LSTM的关键技术在于细胞（cell）状态，也就是图表中最上方水平穿行的直线。细胞状态可以理解成是一种传送带。它仅仅以少量的线性相交，贯穿整个链式结构上方。信息沿着这条传动带很容易保持不变。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/lstm/LSTM3-C-line.png"><br>LSTM有一种能向细胞增加或者移除信息的能力，这种经过精心设计的结构称作<strong>门</strong>（gates）。所谓的门就是一种让信息选择性通过的方法。它是由一个$sigmoid$层和一个逐点乘法单元构成。如下图：</p><center><img width="15%" rnn有循环结构 data-src="https://vincentqin.gitee.io/blogresource-1/lstm/LSTM3-gate.png"></center><p>$sigmoid$层的输出是$0$-$1$之间的数字，它描述了每个部分可以有多少量能够通过。$0$代表“啥都不能通过”，$1$代表“啥都能通过”！一个LSTM有三种这样的门结构，用来保证以及控制细胞状态。</p><h2 id="逐步理解LSTM"><a href="#逐步理解LSTM" class="headerlink" title="逐步理解LSTM"></a>逐步理解LSTM</h2><p>LSTM的第一步是来决定<strong>啥信息将要从细胞状态中丢弃</strong>。这个决定是由一个叫做“遗忘门”（“forget gate layer”）的$sigmoid$层来决定。它的输入是$h_{t-1}$和$x_t$，输出是一个介于$0$到$1$之间数值，给每个在状态$C_{t-1}$的数值。$1$表示“完全保留”，$0$表示“完全丢弃”。<br>让我们回到之前的的语言模型的例子中，我们还是基于以前的词语来预测后续的单词。在这样一个问题中，细胞状态可能会包含当前主语的性别信息，所以正确的<strong>介词</strong>将会被使用。当我们看到一个新的主语时，我们要遗忘掉旧的的主语。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/lstm/LSTM3-focus-f.png"></p><p>下一步就是决定<strong>啥新信息将要存储在细胞状态中</strong>。这包括两个方面。第一，一个叫做“输入门层”的$sigmoid$层来决定哪些值我们要更新；第二，一个$tanh$层创造了新的候选值$\tilde{C}_t$，这个值将会加入到新的状态中去。进一步，我们要把上述两个方面结合起来来更新细胞状态。<br>在我们的语言模型中，我们想要在新的主语对应的细胞状态中加入性别信息，去代替我们遗忘掉的那个旧主语状态。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/lstm/LSTM3-focus-i.png"></p><p>我们现在更新旧的细胞状态，从状态$C_{t-1}$到状态$C_{t}$。上述步骤已经详述了具体如何操作，我们现在就开始更新！<br>我们将旧的细胞状态乘以$f_t$，目的是忘记我们要忘记的旧状态。然后我们加上$i_t*\tilde{C}_t$，这就是我们创造新的候选值，这个值根据我们想要更新每个状态值的程度进行伸缩变化（这就是$i_t$的意义）。<br>在我们的语言模型中，这就是我们根据最开始确定的目标，丢弃旧主语性别以及增加新主语信息的地方。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/lstm/LSTM3-focus-C.png"></p><p>最后一步我们要决定到底输出什么信息。这个输出信息会基于细胞状态，但将会是一个经过过滤后的结果。首先，我们用一个$sigmoid$层去决定细胞状态的哪一部分将会被输出。然后，我们将细胞状态通过$tanh$（将其值规范到$-1$到$1$之间）。最后我们将这个值与$sigmoid$输出相乘，这样我们仅仅输出我们想要输出的部分。<br>还是对于之前提到的语言模型，因为它只看到了一个主语，它可能会输出一个与动词相关的信息。例如，可能输出这个主语是单数还是复数，所以我们会知道紧跟的动词是何种形式。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/lstm/LSTM3-focus-o.png"><br>至此，基本的LSTM介绍完毕。</p><h2 id="LSTM的变体"><a href="#LSTM的变体" class="headerlink" title="LSTM的变体"></a>LSTM的变体</h2><p>我们以上描述的均是最为普通的LSTM。但是并不是所有的LSTM都是以上那个样子。事实上，似乎每一篇涉及LSTMs的论文均对其做了细微的修改。其中的差别不大，以下列举几种LSTM的变体。</p><p>其中之一就是一种特别流行的LSTM变体，它由<a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="noopener"> Gers &amp; Schmidhuber (2000)</a>提出，加入一种<strong>窥视孔连接</strong>（peephole connections）的结构。这使得细胞状态可以作为门层（译者：gete layers:$sigmoid$ layers &amp; $tanh$ layer）输入。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/lstm/LSTM3-var-peepholes.png"><br>以上的图示为每个门层加入了窥视孔连接，但是也有一些论文并不是所有的门层都加。</p><p>另外一种变体是加入了双遗忘门（coupled forget）以及输入门。我们同时考虑了何时遗忘以及应该加入何种新信息，而并非分别考虑。我们仅仅在我们需要就地输入信息的时候才会遗忘，同时我们仅仅在遗忘掉旧的信息的时候才会加入新的信息（译者：此时$f_t=0$，表示遗忘旧的细胞状态，同时加入新的输入$\tilde{C}_t$）。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/lstm/LSTM3-var-tied.png"></p><p>另外一种改动较大的变体是门控循环单元（Gated Recurrent Unit）即GRU。这个算法由<a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener">Cho,et al.(2014)</a>提出，它把遗忘门和输入门结合起来构成一个“更新门”（update gate）。与此同时，它还将细胞状态和隐含状态合并起来，当然还有一些其他变化在此不一一赘述。最终的变体比标准的LSTM简单，这使得它很受欢迎。<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/lstm/LSTM3-var-GRU.png"></p><p>以上均是最近比较劲爆的LSTM变体。当然也有很多其他形式的变体，如<a href="http://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="noopener">Yao,et al. (2015)</a>提出的<strong>深度门RNN</strong>（Depth Gated RNNs）。还有一些变体用完全不同的方式来解决长期依赖问题，例如<a href="http://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="noopener">Koutnik,et al.(2014)</a>提出的<strong>时钟频率驱动RNN</strong>（Clockwork RNNs ）。</p><p>列举了诸多LSTM变体，那么到底哪一种变体是最好的呢？其中的差异真的很重要吗？<a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="noopener">Greff, et al.(2015)</a>做了一个非常棒了比较，发现这些变体几乎都是一样一样的。<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="noopener">Jozefowicz,et al.(2015)</a>测试了上万种RNN框架，发现了一些框架在特定任务上会比LSTM表现出色。（译者：没有一种算法一统江湖）</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>以上，我提到了人们利用RNNs得到了很多优秀的结果。在本质上说，几乎所有的RNNs都使用了LSTMs。LSTMs在诸多任务上表现优异。<br>在介绍LSTMs的过程中写了很多公式，这让它看起来很吓人。幸运的是，我们在文中通过一步步地探索，让LSTM看起来平易近人。<br>LSTMs是我们完成RNNs的重大成果。我们很自然地想：还有没有其他的重大成果？研究员们的共识是：当然有！下一个重大成果就是——<strong>注意力</strong>。这个观点是让RNNs的每一步都能够从更大的数据集中挑选信息。例如，如果你想利用RNNs去给一幅图像创造标题来描述它，这就可能会选择图像的一部分作为输入，然后根据这些输入来得到每个单词。事实上，<a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="noopener">Xu,et al.(2015)</a>就是这么做的——这可能你探究<strong>注意力</strong>这个领域的起点。还有诸多使用<strong>注意力</strong>取得的令人激动的研究结果，看起来还有更多需要探索。<br><strong>注意力</strong>并非RNN唯一令人激动的研究方向。例如，<a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="noopener">Kalchbrenner, et al. (2015)</a>提出的网格LSTM（Grid LSTMs）看似非常有前景。<a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="noopener">Gregor, et al.(2015)</a>，<a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="noopener">Chung, et al.(2015)</a>和<a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="noopener"> Bayer &amp; Osendorfer (2015)</a>等人的研究工作是在生成模型中使用RNNs，这些工作都看起来非常有趣。过去几年是RNNs异常火爆的时期，未来也会有更多更加有意义的成果出现。</p><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>我非常感谢那些帮助我去理解LSTMs的大佬们，同时感谢对可视化进行评论并在这篇博文提供反馈的网友们。非常感激谷歌同事们的反馈，尤其感谢<a href="http://research.google.com/pubs/OriolVinyals.html" target="_blank" rel="noopener">Oriol Vinyals</a>，<a href="http://research.google.com/pubs/GregCorrado.html" target="_blank" rel="noopener">Greg Corrado</a>，<a href="http://research.google.com/pubs/JonathonShlens.html" target="_blank" rel="noopener">Jon Shlens</a>，<a href="http://people.cs.umass.edu/~luke/" target="_blank" rel="noopener">Luke Vilnis</a>以及<a href="http://www.cs.toronto.edu/~ilya/" target="_blank" rel="noopener">Ilya Sutskever</a>。我也由衷感谢很多同事朋友的帮助，包括<a href="https://www.linkedin.com/pub/dario-amodei/4/493/393" target="_blank" rel="noopener">Dario Amodei</a>和<a href="http://cs.stanford.edu/~jsteinhardt/" target="_blank" rel="noopener">Jacob Steinhardt</a>。值得特别感谢还有<a href="http://www.kyunghyuncho.me/" target="_blank" rel="noopener">Kyunghyun Cho</a>，这哥们对图表的绘制给了我极大的帮助。<br>在写这篇博文之前，我尝试在我讲授的神经网络课程中利用两系列研讨会的时间来解释LSTMs。感谢每一位参与者，感觉大家的反馈。</p>]]></content>
      
      
      <categories>
          
          <category> LSTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> LSTM </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>降维之PCA主成分分析原理</title>
      <link href="/posts/pca/"/>
      <url>/posts/pca/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><!-- ![](https://vincentqin.gitee.io/blogresource-1/pca/pca-cover.jpg)[//]:![](https://vincentqin.gitee.io/blogresource-1/pca/smile.jpg) --><p><img alt data-src="https://gitee.com//vincentqin/BlogResource-5/raw/master/images/pca-cover.jpg"></p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在许多领域的研究与应用中，往往需要对反映事物的多个变量进行大量的观测，收集大量数据以便进行分析寻找规律。多变量大样本无疑会为研究和应用提供了丰富的信息，但也在一定程度上增加了数据采集的工作量，更重要的是在多数情况下，许多变量之间可能存在相关性，从而增加了问题分析的复杂性，同时对分析带来不便。如果分别对每个指标进行分析，分析往往是孤立的，而不是综合的。盲目减少指标会损失很多信息，容易产生错误的结论。<br>因此需要找到一个合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量间存在一定的相关关系，因此有可能用较少的综合指标分别综合存在于各变量中的各类信息。主成分分析与因子分析就属于这类降维的方法。</p><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。能够对高维数据降维的算法包括：</p><ul><li>LASSO</li><li>主成分分析法</li><li>聚类分析</li><li>小波分析法</li><li>线性判别法</li><li>拉普拉斯特征映射</li></ul><h2 id="降维有什么作用"><a href="#降维有什么作用" class="headerlink" title="降维有什么作用"></a>降维有什么作用</h2><p>降维有什么作用呢？</p><ul><li>数据在低维下更容易处理、更容易使用</li><li>相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示</li><li>去除数据噪声</li><li>降低算法开销</li></ul><p>常见的降维算法有主成分分析（principal component analysis,PCA）、因子分析（Factor Analysis）和独立成分分析（Independent Component Analysis，ICA）。</p><center></center><h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。<br>注意：PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。</p><h2 id="PCA原理"><a href="#PCA原理" class="headerlink" title="PCA原理"></a>PCA原理</h2><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/pca/projection.png"></p><p>最大化样本点在基上的投影，使得数据点尽量的分离。令第一主成分的方向是<script type="math/tex">u_1</script>，我们的目标就是将样本点在该方向上的投影最大化，即：</p><script type="math/tex; mode=display">\max \frac{1}{n}\sum_{i=1}^n<u_1,x_i>^2</script><script type="math/tex; mode=display"> \frac{1}{n}\sum_{i=1}^n<u_1,x_i> \rightarrow \frac{1}{n}\sum_{i=1}^n(x_1^Tu_1)^2=\frac{1}{n}\sum_{i=1}^n(x_1^Tu_1)^T(x_1^Tu_1)</script><script type="math/tex; mode=display">=\frac{1}{n}\sum_{i=1}^n(u_1^Tx_1x_1^Tu_1)=\frac{1}{n}u_1^T\left(\sum_{i=1}^nx_1x_1^T\right)u_1=\frac{1}{n}u_1^T\left(XX^T\right)u_1</script><p>其中的<script type="math/tex">X=[x_1,x_2,...,x_n]^T,x_i\in R^{m}</script>。那么优化函数就变成了：</p><script type="math/tex; mode=display">\max u_1^T\left(XX^T\right)u_1</script><p>以上式子是个二次型，可以证明<script type="math/tex">XX^T</script>是半正定矩阵，所以上式必然有最大值。</p><script type="math/tex; mode=display">\max u_1^T\left(XX^T\right)u_1=\max ||X^Tu_1||_2^2</script><h2 id="优化函数"><a href="#优化函数" class="headerlink" title="优化函数"></a>优化函数</h2><script type="math/tex; mode=display">max||Wx||_2</script><script type="math/tex; mode=display">s.t.  W^TW=I</script><p>解释：<strong>最大化方差同时最小化协方差</strong>（PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”）。最大化方差意味着，使得每个样本点在每个维度上与均值有很大差异，就是说非常有个性，有个性才能分辨出来；同时协方差越小的话表明样本之间的互相影响就非常小，如果协方差是0的话，表示两个字段完全独立。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/pca/explain.gif"></p><p>寻找协方差矩阵的特征向量和特征值就等价于拟合一条能保留最大方差的直线或主成分。因为特征向量追踪到了主成分的方向，而最大方差和协方差的轴线表明了数据最容易改变的方向。根据上述推导，我们发现达到优化目标就等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将特征值按大小从上到下排列。协方差矩阵作为实对称矩阵，其主要性质之一就是可以正交对角化，因此就一定可以分解为特征向量和特征值。</p><h2 id="具体实施步骤"><a href="#具体实施步骤" class="headerlink" title="具体实施步骤"></a>具体实施步骤</h2><p>总结一下PCA的算法步骤，设有<script type="math/tex">m</script>条<script type="math/tex">n</script>维(字段数)数据，我们采用以下步骤对数据降维。</p><ol><li>将原始数据按列组成<script type="math/tex">n</script>行<script type="math/tex">m</script>列矩阵X. (行数代表字段数目，一个字段就是取每个样本的该维度的数值；列数代表样本数目)</li><li>将<script type="math/tex">X</script>的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</li><li>求出协方差矩阵<script type="math/tex">C=\frac{1}{m}XX^T</script></li><li>求出协方差矩阵的特征值及对应的特征向量</li><li>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P</li><li><script type="math/tex">Y=PX</script>即为降维到k维后的数据</li></ol><h2 id="去均值化的目的"><a href="#去均值化的目的" class="headerlink" title="去均值化的目的"></a>去均值化的目的</h2><p>下面两幅图是数据做中心化（centering）前后的对比，可以看到其实就是一个平移的过程，平移后所有数据的中心是<script type="math/tex">(0,0)</script>.</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/pca/centering_1.jpg"></p><p>在做PCA的时候，我们需要找出矩阵的特征向量，也就是主成分（PC）。比如说找到的第一个特征向量是a = [1, 2]，a在坐标平面上就是从原点出发到点（1，2）的一个向量。如果没有对数据做中心化，那算出来的第一主成分的方向可能就不是一个可以“描述”（或者说“概括”）数据的方向了。还是看图比较清楚。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/pca/centering_2.jpg"></p><p>黑色线就是第一主成分的方向。只有中心化数据之后，计算得到的方向才能比较好的“概括”原来的数据。</p><h2 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h2><p>PCA虽可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="http://blog.codinglabs.org/articles/pca-tutorial.html" target="_blank" rel="noopener">PCA的数学原理 </a></li><li><a href="http://blog.csdn.net/oldmonkeyyu_s/article/details/45766543" target="_blank" rel="noopener"> K-L变换和PCA的区别</a></li><li><a href="http://blog.csdn.net/Dark_Scope/article/details/53150883" target="_blank" rel="noopener">从PCA和SVD的关系拾遗</a></li><li><a href="https://www.zhihu.com/question/37069477/answer/132387124" target="_blank" rel="noopener">数据什么时候需要做中心化和标准化处理</a></li><li><a href="http://blog.jobbole.com/109015/" target="_blank" rel="noopener">主成分分析（PCA）原理详解</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> PCA </tag>
            
            <tag> LDA </tag>
            
            <tag> 降维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SIFT和SURF特性提取总结</title>
      <link href="/posts/SIFT-and-SURF/"/>
      <url>/posts/SIFT-and-SURF/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/SIFT-and-SURF/match_before_v2.png"></p><blockquote><p>SIFT（Scale-invariant feature transform）是一种检测局部特征的算法，该算法通过求一幅图中的特征点（interest points,or corner points）及其有关<strong>scale</strong> 和 <strong>orientation</strong> 的描述子得到特征并进行图像特征点匹配</p></blockquote><a id="more"></a><h2 id="什么是SIFT"><a href="#什么是SIFT" class="headerlink" title="什么是SIFT"></a>什么是SIFT</h2><p>先看看上图利用sift进行匹配的结果：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/SIFT-and-SURF/matches_adjust_contrast.png"></p><p>这个结果应该可以很好的解释sift的尺度、旋转以及光照不变性。接下来就介绍一下这个神奇的算法的奥义。我把代码放在了<a href="https://github.com/Vincentqyw/siftDemo" target="_blank" rel="noopener">Github</a>，感兴趣的同学自己下载下来试试。</p><h3 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h3><p>SIFT特征具有尺度不变性，旋转不变性，光照不变性。</p><h3 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h3><h4 id="构建尺度空间"><a href="#构建尺度空间" class="headerlink" title="构建尺度空间"></a>构建尺度空间</h4><p>尺度空间的目的是模拟图像的多尺度特性。<br><strong>高斯卷积核是实现尺度变换的唯一线性核</strong>，于是 一副二维图像的尺度空间定义为：</p><script type="math/tex; mode=display">L(x,y,\sigma)=G(x,y,\sigma)*I(x,y)</script><p>其中的<script type="math/tex">G(x,y,\sigma)</script>是尺度可以发生变化的高斯函数<script type="math/tex">G(x,y,\sigma)=\frac{1}{2\pi{\sigma}^2}e^{-\frac{x^2+y^2}{2{\sigma}^2}}</script>。<script type="math/tex">(x,y)</script>表示空间坐标，<script type="math/tex">\sigma</script>是尺度系数，描述了图像的模糊程度。<br>为了能够更为有效的提取出特征点，提出了DOG（高斯差分尺度空间）的概念。通过不同尺度下的高斯差分核与图像卷积形成：</p><script type="math/tex; mode=display">D(x,y,\sigma)=(G(x,y,k\sigma)-G(x,y,\sigma))*I(x,y) =L(x,y,k\sigma)-L(x,y,\sigma)</script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/SIFT-and-SURF/scale_space.png"></p><p>图像金字塔的建立：为了实现尺度不变特性，对于每一幅图像<script type="math/tex">I(x,y)</script>，分成<strong>子八度（octave）</strong>，第一个子八度的scale为原图大小，后面每个octave为上一个octave降采样的结果，即原图size的1/4（长宽分别减半），构成下一个子八度（高一层金字塔）。此时要强烈注意size和尺度空间的概念。size是图像大小，而尺度空间表示不同<script type="math/tex">\sigma</script>的图像的集合。那么尺度空间的集合是：</p><script type="math/tex; mode=display">2^{i-1}(\sigma, k*\sigma,k^2*\sigma,k^3*\sigma,...,k^{n-1}*\sigma)</script><p>其中的 <script type="math/tex">k=2^{1/S}</script>，<script type="math/tex">S</script>表示尺度金字塔每个octave的层数，<script type="math/tex">n</script>表示尺度金字塔的总层数，<script type="math/tex">i</script>表示的是在某个octave的第<script type="math/tex">i</script>层，<script type="math/tex">i\in[1,2,3,...n]</script>。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/SIFT-and-SURF/DoG.jpg"></p><p>由图片size决定建几个塔，每塔几层图像(S一般为3-5层)。0塔的第0层是原始图像(或你double后的图像)，往上每一层是对其下一层进行Laplacian变换（高斯卷积，其中σ值渐大，例如可以是σ, k*σ, k*k*σ…），直观上看来越往上图片越模糊。塔间的图片是降采样关系，例如1塔的第0层可以由0塔的第3层down sample得到，然后进行与0塔类似的高斯卷积操作。</p><h4 id="在DoG空间找到关键点"><a href="#在DoG空间找到关键点" class="headerlink" title="在DoG空间找到关键点"></a>在DoG空间找到关键点</h4><p>为了寻找尺度空间的极值点，每一个采样点要和它所有的相邻点比较，看其是否比它的图像域和尺度域的相邻点大或者小。如图所示，中间的检测点和它同尺度的8个相邻点和上下相邻尺度对应的9×2个点共26个点比较，以确保在尺度空间和二维图像空间都检测到极值点。 一个点如果在DOG尺度空间本层以及上下两层的26个领域中是最大或最小值时，就认为该点是图像在该尺度下的一个特征点,如图所示。使用Laplacian of Gaussian能够很好地找到找到图像中的兴趣点，但是需要大量的计算量，所以使用Difference of Gaussian图像的极大极小值近似寻找特征点.DOG算子计算简单，是尺度归一化的LoG算子的近似。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/SIFT-and-SURF/DoG_Space.jpg"></p><h4 id="去除不好的点"><a href="#去除不好的点" class="headerlink" title="去除不好的点"></a>去除不好的点</h4><blockquote><p>这一步本质上要去掉DoG局部曲率非常不对称的像素。通过拟和三维二次函数以精确确定关键点的位置和尺度（达到亚像素精度），同时去除低对比度的关键点和不稳定的边缘响应点(因为DoG算子会产生较强的边缘响应)，以增强匹配稳定性、提高抗噪声能力，在这里使用近似Harris Corner检测器。</p></blockquote><h4 id="给特征点赋值一个128维方向参数并描述"><a href="#给特征点赋值一个128维方向参数并描述" class="headerlink" title="给特征点赋值一个128维方向参数并描述"></a>给特征点赋值一个128维方向参数并描述</h4><p>前面的几个步骤确定了特征点到底在哪里，此步骤是为了<strong>描述特征点</strong>。<br>(x,y)处梯度的模值和方向公式为：</p><script type="math/tex; mode=display">m(x,y)=\sqrt{(L(x+1,y)-L(x-1,y))^2+(L(x,y+1)-L(x,y-1))^2}</script><script type="math/tex; mode=display">\theta(x,y)=tan^{-1}\left(\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}\right)</script><blockquote><p>利用关键点邻域像素的梯度方向分布特性为每个关键点指定方向参数，使算子具备旋转不变性。</p></blockquote><p>其中L所用的尺度为每个关键点各自所在的尺度。至此，图像的关键点已经检测完毕，每个关键点有三个信息：<strong>位置，所处尺度、方向</strong>，由此可以确定一个SIFT特征区域。</p><p>在实际计算时，我们在以关键点为中心的邻域窗口内采样，并用直方图统计邻域像素的梯度方向。梯度直方图的范围是0～360度，其中每45度一个柱，总共8个柱, 或者每10度一个柱，总共36个柱。Lowe论文中还提到要使用高斯函数对直方图进行平滑，减少突变的影响。直方图的峰值则代表了该关键点处邻域梯度的主方向，即作为该关键点的方向。直方图中的峰值就是主方向，其他的达到最大值80%的方向可作为辅助方向。</p><p>计算keypoint周围的16*16的window中每一个像素的梯度，而且使用高斯下降函数降低远离中心的权重。图左部分的中央为当前关键点的位置，每个小格代表关键点邻域所在尺度空间的一个像素，利用公式求得每个像素的梯度幅值与梯度方向，箭头方向代表该像素的梯度方向，箭头长度代表梯度模值，然后用高斯窗口对其进行加权运算。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/SIFT-and-SURF/keypoints.jpg"></p><p>该图是8*8的区域计算得到2*2描述子向量的过程。但是在实际中使用的是在16*16的区域计算得到4*4的特征描述子，如下图：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/SIFT-and-SURF/descriptor.jpg"></p><p>这样就可以对每个feature形成一个4*4*8=128维的描述子，每一维都可以表示4*4个格子中一个的scale/orientation。将这个<strong>向量归一化之后，就进一步去除了光照的影响</strong>。</p><h3 id="sift的缺点"><a href="#sift的缺点" class="headerlink" title="sift的缺点"></a>sift的缺点</h3><p>SIFT在图像的不变特征提取方面拥有无与伦比的优势，但并不完美，仍然存在：</p><ol><li>实时性不高。</li><li>有时特征点较少。</li><li>对边缘光滑的目标无法准确提取特征点。</li></ol><p>PS: 论文见这里：<a href="http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf" target="_blank" rel="noopener">Distinctive Image Features from Scale-Invariant Keypoints</a>，这里是<a href="http://www.cs.ubc.ca/~lowe/home.html" target="_blank" rel="noopener">David Lowe大神</a>做的一个<a href="http://www.cs.ubc.ca/~lowe/keypoints/" target="_blank" rel="noopener">Demo Software: SIFT Keypoint Detector</a>.</p><h2 id="SURF-简介"><a href="#SURF-简介" class="headerlink" title="SURF 简介"></a>SURF 简介</h2><p>参考了好友整理的一篇文章<a href="http://simtalk.cn/2017/08/18/%E7%89%B9%E5%BE%81%E4%B8%8E%E5%8C%B9%E9%85%8D/#ORB" target="_blank" rel="noopener">特征与匹配</a></p><ol><li>整体的思路就是将计算DOG的一整套东西来检测关键点的理论替换成了利用hessian矩阵来检测关键点，因为当Hessian矩阵的判别式取得局部极大值时，判定当前点是比周围邻域内其他点更亮或更暗的点，由此来定位关键点的位置。<br>上述过程要进行Hessian判别式的计算，可以采用box filter的方式进行加速。</li><li>构建尺度金字塔的方式不同，具体见下图：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/SIFT-and-SURF/diff.png"></li><li>Sift特征点方向分配是采用在特征点邻域内统计其梯度直方图，取直方图bin值最大的以及超过最大bin值80%的那些方向作为特征点的主方向。而在Surf中，采用的是统计特征点圆形邻域内的harr小波特征。即在特征点的圆形邻域内，统计60度扇形内所有点的水平、垂直harr小波特征总和，然后扇形以0.2弧度大小的间隔进行旋转并再次统计该区域内harr小波特征值之后，最后将值最大的那个扇形的方向作为该特征点的主方向。该过程示意图如下：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/SIFT-and-SURF/direction.jpg"></li><li>生成特征点描述子: 在Sift中，是取特征点周围4*4个区域块，统计每小块内8个梯度方向，用着4*4*8=128维向量作为Sift特征的描述子。surf算法中，也是在特征点周围取一个4*4的矩形区域块，但是所取得矩形区域方向是沿着特征点的主方向。每个子区域统计25个像素的水平方向和垂直方向的haar小波特征，这里的水平和垂直方向都是相对主方向而言的。该haar小波特征为水平方向值之后、垂直方向值之后、水平方向绝对值之后以及垂直方向绝对值之和4个方向。<br>把这4个值作为每个子块区域的特征向量，所以一共有4*4*4=64维向量作为Surf特征的描述子，比Sift特征的描述子减少了2倍。</li></ol><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/SIFT-and-SURF/diff_more.jpg"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="http://blog.csdn.net/abcjennifer/article/details/7639681/" target="_blank" rel="noopener">SIFT特征提取分析</a></li><li><a href="http://blog.csdn.net/luoshixian099/article/details/47807103" target="_blank" rel="noopener">特征匹配-SURF原理与源码解析（一）</a></li><li><a href="http://simtalk.cn/2017/08/18/%E7%89%B9%E5%BE%81%E4%B8%8E%E5%8C%B9%E9%85%8D/#ORB" target="_blank" rel="noopener">特征与匹配</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sift </tag>
            
            <tag> surf </tag>
            
            <tag> 特征提取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计学习方法总结</title>
      <link href="/posts/summary-statistical-learning/"/>
      <url>/posts/summary-statistical-learning/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/summary-statistical-learning/Captain_America.jpg"></p><p>本文主要研究监督学习，所谓的监督学习就是在给定的，有限的，用于学习的训练数据集合（training data）出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个集合，即<strong>假设空间</strong>；我们根据一定的评价准则，从假设空间中选取一个最优的模型，使它对已知的训练数据以及未知的测试数据在给定评价准则下有最优的预测，最优模型的选取由<strong>算法</strong>实现。<br>所以统计学习方法有三个要素：<strong>模型</strong>、<strong>策略</strong>、<strong>算法</strong>。</p><a id="more"></a><h2 id="统计学习"><a href="#统计学习" class="headerlink" title="统计学习"></a>统计学习</h2><ul><li>监督学习</li><li>半监督学习</li><li>无监督学习</li><li>强化学习</li></ul><h2 id="输入空间、特征空间与输出空间"><a href="#输入空间、特征空间与输出空间" class="headerlink" title="输入空间、特征空间与输出空间"></a>输入空间、特征空间与输出空间</h2><ol><li>输入变量&amp;输出变量均连续-&gt; 回归问题</li><li>输出空间为有限个离散变量的预测问题-&gt; 分类问题</li><li>输入与输出均为变量序列的预测问题-&gt; 标注问题</li></ol><h2 id="风险函数"><a href="#风险函数" class="headerlink" title="风险函数"></a>风险函数</h2><ul><li>期望风险：模型关于联合分布的期望损失</li><li>经验风险：模型关于训练样本的平均损失<br>按照大数定律，当样本数据量区域无穷时，经验风险趋近于期望风险；<br>但是当样本容量很小时，经验风险的效果就不会太好，此时容易出现过拟合现象。<br>此时，结构风险就被提出。结构风险是在经验风险的基础上添加上表示模型复杂度的正则化项/罚项。<br>极大似然估计是经验风险最小化的一个特例。<br>最大后验概率估计是结构风险最小化的一个特例；</li></ul><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>监督学习里要学习的模型就是<strong>决策函数或者条件概率分布</strong>。</p><p>此时不得不提到生成方法以及判别方法。</p><ul><li>生成方法，由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布模型，即生成模型：<script type="math/tex; mode=display">P(Y|X)=\frac{P(X,Y)}{P(X)}</script></li><li>判别方法是由数据直接学习决策函数或者条件概率分布作为预测模型，即判别模型。</li></ul><h3 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h3><ul><li>生成模型常见的主要有：</li></ul><ol><li>高斯混合模型</li><li>朴素贝叶斯</li><li>混合高斯模型GMM</li><li>隐马尔可夫模型HMM</li><li>马尔可夫的随机场</li><li>KNN</li></ol><ul><li>常见的判别模型有：</li></ul><ol><li>支持向量机</li><li>传统的神经网络</li><li>线性判别分析</li><li>线性回归</li><li>条件随机场</li><li>最大熵模型</li><li>逻辑斯特回归</li></ol><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/summary-statistical-learning/norm12.jpg"></p><h2 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h2><p>指定策略的目的就是为了挑选出假设空间中到底哪个模型才是我们真正需要的。此时会用到损失函数以及风险函数的概念。</p><ul><li><p>0-1 损失函数</p><script type="math/tex; mode=display">L(Y,f(X))=\left\{\begin{aligned}1,    && Y \neq f(X)\\0,    &&Y = f(X) \end{aligned}\right.</script></li><li><p>平法损失函数</p><script type="math/tex; mode=display">L(Y,f(X))=(Y-f(X)^2</script></li><li><p>绝对值损失函数</p></li></ul><script type="math/tex; mode=display">L(Y,f(X))=|Y-f(X)|</script><ul><li>对数损失函数<script type="math/tex; mode=display">L(Y,P(Y|X))=-logP(Y|X)</script></li></ul><p>损失函数越小的话代表模型越好。$(X,Y)$是随机变量符合联合分布概率$P(X,Y)$，所以损失函数的期望被定义为：</p><script type="math/tex; mode=display">R_{ref}(f)=E_p[L(Y,f(X))]=\int_{\mathcal{X} \times \mathcal{Y}}L(y,f(x))P(x,y)dxdy</script><p>以上是模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失，称为风险函数或者<strong>期望损失</strong>（expected loss）。<br>但是呢，期望损失不易求解，我们一般用<strong>模型关于训练数据集的平均损失</strong>来逼近期望损失，即：</p><script type="math/tex; mode=display">R_{emp}(f)=E_p[L(Y,f(X))]=\frac{1}{N} \sum_{i=1}^NL(y_i,f(x_i))</script><p><strong>经验风险最小化</strong>的策略认为，经验风险最小的模型就是最优的模型，于是按照这种定义，我们有：</p><script type="math/tex; mode=display">f^*={argmin}_{f \in \mathcal{F} } R_{emp}</script><p>其中的$\mathcal{F}$是假设空间。<br>最大似然估计就是经验风险最小化的一个例子：当模型为条件概率，损失函数是对数损失时，经验风险最小化就等价于极大似然估计。<br>根据大数定理可知，当样本容量N趋近于无限时，经验风险趋近于期望风险。但是如果样本数量是有限时，此时会出现过拟合现象，那么这时候需要结构风险的帮助。<br>结构风险是为了防止过拟合而提出的策略，结构风险最小化等价于正则化（regularization）。其定义是</p><script type="math/tex; mode=display">R_{srm}(f)=\frac{1}{N} \sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)</script><p>其中的$J(f)$是衡量模型复杂度的项，也叫罚项。当模型越复杂时，$J(f)$越大；模型越简单时，$J(f)$越小。<br>最大后验概率估计（MAP）就是结构风险最小化的一个例子：当模型时条件概率，损失函数是对数损失函数，模型复杂度由先验概率表示时，结构风险最小化就等价于MAP。</p><p>监督学习的模型可以分为概率模型与非概率模型，由条件概率分布$P(Y|X)$或者决策函数$Y=f(X)$表示。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>算法是指学习模型的具体计算方法： <strong>find global optimization solution efficiently</strong>.</p><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><p><strong>模型选择的主要方式有：正则化与交叉验证</strong>。</p><p>在经验风险最小化时已经了解到正则化的由来，正则化就是针对过拟合现象提出的解决策略。</p><p><strong>过拟合</strong>是指学习模型时选择的模型包含的参数过多，以致于这一模型对已知数据的预测很好，而对未知数据的预测能力变得很差。 以下介绍两种正则化的范数：$L_1$ and $L_2$</p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><h4 id="L2范数正则"><a href="#L2范数正则" class="headerlink" title="L2范数正则"></a>L2范数正则</h4><script type="math/tex; mode=display">C=C_0+ \frac{\alpha}{2n}\sum_ww^2</script><p>对$w$以及$b$求导如下：</p><script type="math/tex; mode=display"> \frac{\partial C}{\partial w}= \frac{\partial C_0}{\partial w}+\frac{\lambda}{n}w \frac{\partial C}{\partial b}= \frac{\partial C_0}{\partial b}</script><p>由梯度下降法可知：</p><script type="math/tex; mode=display">w \rightarrow w-\eta\frac{\partial C}{\partial w}=\left(1-\frac{\eta\lambda}{n} \right)w-\eta\frac{\partial C_0}{\partial w}</script><p>系数$\left(1-\frac{\eta\lambda}{n} \right)$是小于1的，相比于原来的系数等于一，此时的效果相当于就是权值衰减（weight decay）。</p><p>注意到<strong>过拟合</strong>的时候，我们的假设函数要顾及到每一个数据点，模型就会尝试对所有数据点进行拟合，包括一些异常点；此时形成的拟合函数的波动性会非常大，可以看到此时的拟合参数会异常大。通过L2正则化处理可以使这些参数变小，解释如下：</p><blockquote><p>更小的权重，表示网络的复杂组更低，对数据拟合的刚刚好（奥卡姆剃须刀原理）</p></blockquote><h4 id="L1范数正则"><a href="#L1范数正则" class="headerlink" title="L1范数正则"></a>L1范数正则</h4><p>L1正则假设参数的先验分布是Laplace分布，可以保证模型的稀疏性，也就是某些参数等于0；<br>L2正则假设参数的先验分布是Gaussian分布，可以保证模型的稳定性，也就是参数的值不会太大或太小</p><p><img alt="两种正则化的对比" data-src="https://gss0.baidu.com/-fo3dSag_xI4khGko9WTAnF6hhy/zhidao/pic/item/359b033b5bb5c9ea238f2f6cdd39b6003bf3b3c4.jpg"></p><h4 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h4><p><a href="http://freemind.pluskid.org/machine-learning/sparsity-and-some-basics-of-l1-regularization/#ed61992b37932e208ae114be75e42a3e6dc34cb3" target="_blank" rel="noopener">lasso and ridge regularization</a></p><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>就是将整个数据集切分成三个部分，分别是训练集、验证集和测试集。训练集用来训练模型，验证集用于模型 的选择，而测试集用于对学习方法的评估。</p><p>但是一般情况下，训练数据是不足的，那么此时可以重复的利用数据，进行反复训练以得到最优模型。</p><p>常见的方法有：</p><ul><li>S折交叉验证（S-fold cross validation）</li><li>留一交叉验证（S=N，当S为数据集的容量时，S折交叉验证就变成了留一交叉验证）</li></ul><h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>最大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”</p><blockquote><p>举个别人博客中的例子，假如有一个罐子，里面有黑白两种颜色的球，数目多少不知，两种颜色的比例也不知。我 们想知道罐中白球和黑球的比例，但我们不能把罐中的球全部拿出来数。现在我们可以每次任意从已经摇匀的罐中拿一个球出来，记录球的颜色，然后把拿出来的球 再放回罐中。这个过程可以重复，我们可以用记录的球的颜色来估计罐中黑白球的比例。假如在前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例最有可能是多少？很多人马上就有答案了：70%。而其后的理论支撑是什么呢？</p></blockquote><p>我们假设罐中白球的比例是$p$，那么黑球的比例就是$1-p$。因为每抽一个球出来，在记录颜色之后，我们把抽出的球放回了罐中并摇匀，所以每次抽出来的球的颜 色服从同一独立分布。这里我们把一次抽出来球的颜色称为一次抽样。题目中在一百次抽样中，七十次是白球的概率$P(Data|M)$，这里Data是所有的数据，M是所给出的模型，表示每次抽出来的球是白色的概率为$p$。如果第一抽样的结果记为<script type="math/tex">x_1</script>，第二抽样的结果记为<script type="math/tex">x_2</script>… 那么<script type="math/tex">Data = (x_1,x_2,...,x_{100})</script>。这样，</p><script type="math/tex; mode=display">P(Data|M)= P(x_1,x_2,...,x_{100}|M)= P(x_1|M)P(x_2|M)...P(x_{100}|M)= p^{70}(1-p)^{30}</script><p>那么p在取什么值的时候，$P(Data|M)$的值最大呢？将$p^{70}(1-p)^{30}$对$p$求导，并其等于零。</p><script type="math/tex; mode=display">70p^{69}(1-p)^{30}-p^{70}*30(1-p)^{29}=0</script><p>解方程可以得到<script type="math/tex">p=0.7</script>。　　　　</p><h3 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h3><p>最大熵原理使概率学习中的一个准则。学习概率模型时，在所有的可能概率模型（分布）中，熵最大的模型是最好的模型。最大熵原理也可以理解成在满足约束条件的模型中选择熵最大的模型！</p><script type="math/tex; mode=display">H(p)=-\sum_x{log(P(x))P(x)}</script><p>其中$X$服从的概率分布为$P(X)$，$X$服从均匀分布时，熵最大。当没有其他已知的约束时，$\Sigma{P(x)}=1$，此时按照最大熵原理，当$P(x_1)=P(x_2)=…=P(x_n)$时，熵最大；此时等概论，等概论意味着对事实的无知，因为没有更多可能的信息，所以此时的判断也是合情合理的。</p><h3 id="线性分类器"><a href="#线性分类器" class="headerlink" title="线性分类器"></a>线性分类器</h3><p>线性分类器有三大类：感知器准则函数、$SVM$、$Fisher$准则，而贝叶斯分类器不是线性分类器。</p><ul><li>感知器准则函数：代价函数$J=-(W*X+w_0)$，分类的准则是最小化代价函数。感知器是神经网络（$NN$）的基础，网上有很多介绍。</li><li>$SVM$：支持向量机也是很经典的算法，优化目标是最大化间隔（margin），又称最大间隔分类器，是一种典型的线性分类器。（使用核函数可解决非线性问题）</li><li>$Fisher$准则：更广泛的称呼是线性判别分析（$LDA$），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。</li></ul><h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><ul><li>召回率就是有多少正确的被你找回来了；准确率就是你找到的有多少是正确的；(一般情况下召回率和准确率呈负相关，所以可以用F值衡量整体效果)</li><li>TP(True Positive)是你判断为正确的，实际就是正确的；</li><li>FP(False Positive)是你判断是错误的，实际也是错误的；</li><li>TN(True Negative)是你判断为正确的，但实际是错误的；</li><li>FN(False Negative)是你判断是错误的，但实际是正确的；</li></ul><h2 id="朴素贝叶斯-Naive-Bayes"><a href="#朴素贝叶斯-Naive-Bayes" class="headerlink" title="朴素贝叶斯 Naive Bayes"></a>朴素贝叶斯 <script type="math/tex">Naive Bayes</script></h2><h3 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h3><p>朴素贝叶斯是基于贝叶斯定理与特征条件独立假设的分类方法。NB的核心在于它假设向量的所有分量之间是独立的。在贝叶斯理论系统中，都有一个重要的条件独立性假设：假设所有特征之间相互独立，这样才能将联合概率拆分。<br>对于由$P(X,Y)$独立产生的训练集$T=\{(x_1,y_1),(x_2,y_2),…, (x_N,y_N),\}$而言通过朴素贝叶斯的方法学习这个联合概率分布。大致分两步：</p><ol><li>计算先验分布：<script type="math/tex; mode=display">P(Y=c_k),k=1,2...,K</script></li><li>条件概率分布：<script type="math/tex; mode=display">P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)}|Y=c_k),k=1,2...,K</script>其中的$x \in  R^{n}$，$n$表示这个样本的维度。</li></ol><p>但是在计算条件概率时因为朴素贝叶斯做了条件独立性假设，那么该条件概率分布可以写成：</p><script type="math/tex; mode=display">\begin{aligned}P(X=x|Y=c_k)&=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)}|Y=c_k)\\&=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)\end{aligned}</script><p>在实际分类时，对于给定的输入x，通过学习到的模型估计后验概率$P(Y=c_k|X=x)$将后验概率最大的类作为x的类的输出。</p><script type="math/tex; mode=display">\begin{aligned}P(Y=c_k|X=x) &=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_{k}P(X=x|Y=c_k)P(Y=c_k)}\\&=\frac{P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_{k}P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}\end{aligned}</script><p>因为分母就是$P(X=x)$的概率，这是不变的。因此我们仅需要知道分子哪个大就可以，也就是：</p><script type="math/tex; mode=display">y={argmax}_{c_k} P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)</script><p>上式的意思是求解到底是哪些类别<script type="math/tex">c_k</script>能够让最大后验概率最大化。</p><h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><h4 id="极大似然估计-1"><a href="#极大似然估计-1" class="headerlink" title="极大似然估计"></a>极大似然估计</h4><ol><li><p>计算先验概率</p><script type="math/tex; mode=display">P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N}</script></li><li><p>计算条件概率</p><script type="math/tex; mode=display">P(X=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}</script><p>其中</p><script type="math/tex; mode=display">j=1,2...n;l=1,2...S_j;k=1,2...K</script></li><li><p>对于给定的实例x，计算</p><script type="math/tex; mode=display">P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)</script></li><li><p>确定实例的类别</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习方法 </tag>
            
            <tag> 概率论 </tag>
            
            <tag> SVM </tag>
            
            <tag> 过拟合 </tag>
            
            <tag> 朴素贝叶斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu上使用Git以及GitHub</title>
      <link href="/posts/git-notebook/"/>
      <url>/posts/git-notebook/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/git-notebook/github-octocat.png"></p><a id="more"></a><h2 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h2><p>在Ubuntu上安装Git的命令为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install git</span><br></pre></td></tr></table></figure><h2 id="配置GitHub"><a href="#配置GitHub" class="headerlink" title="配置GitHub"></a>配置GitHub</h2><p>安装git结束之后就是配置github用户资料，如下：<br>将其中的 “user_name”替换成自己 GitHub的用户名并且将”email_id” 换成你创建GitHub账号所用的邮箱.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;user_name&quot;</span><br><span class="line"></span><br><span class="line">git config --global user.email &quot;email_id&quot;</span><br></pre></td></tr></table></figure></p><h2 id="建立本地仓库（repository）"><a href="#建立本地仓库（repository）" class="headerlink" title="建立本地仓库（repository）"></a>建立本地仓库（repository）</h2><p>在自己的电脑上建立本地仓库，这个仓库将会在后续推送到GitHub上，语句如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init Mytest</span><br></pre></td></tr></table></figure></p><p>如果初始化成功，你将会得到以下提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Initialized empty Git repository in /home/user_name/Mytest/.git/</span><br></pre></td></tr></table></figure><p>其中的user_name为本地计算机用户名，因人而异。<br>Mytest是”init”创建的文件夹，然后进入该文件所在目录：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd Mytest</span><br></pre></td></tr></table></figure></p><h2 id="创建README来描述这个仓库"><a href="#创建README来描述这个仓库" class="headerlink" title="创建README来描述这个仓库"></a>创建README来描述这个仓库</h2><p>该步骤可有可无，但是作为一个优秀的工程师还是写点东西比较好。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit README</span><br></pre></td></tr></table></figure><p>然后输入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">This is a git repo</span><br></pre></td></tr></table></figure></p><h2 id="将仓库文件加入index（缓存）"><a href="#将仓库文件加入index（缓存）" class="headerlink" title="将仓库文件加入index（缓存）"></a>将仓库文件加入index（缓存）</h2><p>这一步尤其重要，我们将需要上载到GitHub的文件们添加到index。这些文件可以是文本文档，m/c/c++/PDF/jpg…几乎任何类型文件，一般而言我们可以把需要上载的文件拷贝到这个文件夹内，然后再用一个语句来把需要上传到文件添加到index，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git add file1.txt</span><br><span class="line">git add file2.c</span><br><span class="line">git and file3.m</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><h2 id="提交到本地仓库"><a href="#提交到本地仓库" class="headerlink" title="提交到本地仓库"></a>提交到本地仓库</h2><p>当我们已经把文件添加／修改到index后，就可以进行提交了；利用如下语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &quot;some_message&quot;</span><br></pre></td></tr></table></figure><p>其中some_message可以是任何字符，例如：”my first commit” 或者”edit in readme”等。上面代码的-m参数，就是用来指定这个mesage 的。</p><p>注意：git是分为三部分，一部分是文件，另外一个是缓存区，最后一个是本地库。当你修改了自己的文件后，你会git add xx将修改保存到缓存区，然后再用commit推送修改到本地库中。git push 将本地仓库修改推送到服务器上的仓库中commit是将本地修改保存到本地仓库中。</p><h2 id="在GitHub创建仓库"><a href="#在GitHub创建仓库" class="headerlink" title="在GitHub创建仓库"></a>在GitHub创建仓库</h2><p>这个仓库的名字要和本地的一致，该部分不做展开。然后连接到远程仓库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin https://github.com/user_name/Mytest.git</span><br></pre></td></tr></table></figure><p>其中user_name就是自己的GitHub的用户名。</p><h2 id="推送到远程仓库"><a href="#推送到远程仓库" class="headerlink" title="推送到远程仓库"></a>推送到远程仓库</h2><p>最后的一步就是提交到远程仓库，用以下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin master</span><br></pre></td></tr></table></figure></p><p><a href="https://www.howtoforge.com/tutorial/install-git-and-github-on-ubuntu-14.04/" target="_blank" rel="noopener">原文地址</a></p>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ubuntu </tag>
            
            <tag> Linux </tag>
            
            <tag> GitHub </tag>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习修炼手册</title>
      <link href="/posts/machine-learning/"/>
      <url>/posts/machine-learning/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/machine-learning/ML.jpg"></p><p id="div-border-left-green">对机器学习的学习我开始于二年级的《数据挖掘》课，当时袁老师对数据挖掘中的常用的算法做了一些介绍，但是这仅仅是个入门教学，我并没有深入了解的其中的原理。到现在我才深刻的意识到ML的重要性，我就抽空看了一些这方面的资料，整理了这一份文档。</p><a id="more"></a><p>机器学习算法包括，<span id="inline-red">监督学习</span>(回归、分类)以及<span id="inline-red">非监督学习</span>(聚类)。</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><script type="math/tex; mode=display">\bbox[5px,border:2px solid red]{    \theta_j:=\theta_j-\alpha\frac{\partial}{\partial \theta}J(\theta)}</script><p>其中$\alpha$为学习率一般为很小的数字(0.001-0.1)，$\theta$为我们需要求解的参数，$J(\theta)$为能量函数或者为损失函数，通过上述公式可知，梯度下降是沿着损失函数梯度的反方向寻找迭代寻找最优值的过程。梯度下降容易陷入局部最极小点，所以我们要采取一定的措施来阻止这种现象的发生。</p><h2 id="过拟合（Overfitting）"><a href="#过拟合（Overfitting）" class="headerlink" title="过拟合（Overfitting）"></a>过拟合（Overfitting）</h2><p></p><p id="div-border-left-red">如果训练样本的特征过多，我们学习的假设可能在训练集上表现地很好，但是在验证集上表现地就不尽人意</p><p></p><h3 id="避免过拟合"><a href="#避免过拟合" class="headerlink" title="避免过拟合"></a>避免过拟合</h3><ul><li>减少特征的大小</li><li>正则化<ul><li>在保证所有特征都保留的情况下，限制$\theta$的大小，即Small values for parameters $ \theta_0,\theta_1,\theta_2…\theta_n$</li><li>当特征量很多时，该方式仍然表现的很好</li></ul></li><li>交叉验证(Cross Validation)</li></ul><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><h4 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h4><p>对于线性回归而言，其损失函数形式如下：</p><script type="math/tex; mode=display">    J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2</script><p>引入正则化之后的损失函数的形式为：</p><script type="math/tex; mode=display">    J(\theta)=\frac{1}{2m}\left(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_{j}^2\right)</script><h5 id="GD迭代求解参数"><a href="#GD迭代求解参数" class="headerlink" title="GD迭代求解参数"></a>GD迭代求解参数</h5><p><strong>Repeat</strong>{</p><script type="math/tex; mode=display">    \theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)x_0^{(i)}</script><script type="math/tex; mode=display">    \theta_j:=\theta_j-\alpha\frac{1}{m}\left(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\lambda\theta_j\right)</script><p>}<br>梯度下降法的学习率$\alpha$需要提前指定，并且还要制定收敛标准。</p><h5 id="Normal-Equation"><a href="#Normal-Equation" class="headerlink" title="Normal Equation"></a>Normal Equation</h5><script type="math/tex; mode=display">\theta=\left(x^Tx+\lambda\begin{bmatrix}{0}&{0}&{\cdots}&{0}\\{0}&{1}&{\cdots}&{0}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{0}&{0}&{\cdots}&{1}\\\end{bmatrix}_{(n+1)(n+1)}\right)^{-1}x^Ty</script><p>上式是对线性回归正则化后的矩阵解。可以证明的是当$\lambda&gt;0$时，求逆符号内部的式子总是可逆的。</p><h4 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h4><p>在没有加入正则化之前，逻辑回归的损失函数的形式是这样的：</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log\left(h_{\theta}(x^{(i)})\right)+(1-y^{(i)})\log\left(1-h_{\theta}(x^{(i)})\right)\right)</script><p>加入正则项之后的形式为：</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log\left(h_{\theta}(x^{(i)})\right)+(1-y^{(i)})\log\left(1-h_{\theta}(x^{(i)})\right)\right)+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2</script><h5 id="GD迭代求解参数-1"><a href="#GD迭代求解参数-1" class="headerlink" title="GD迭代求解参数"></a>GD迭代求解参数</h5><p><strong>Repeat</strong>{</p><script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)x_0^{(i)}</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{1}{m}\left(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\lambda\theta_j\right)</script><p>}</p><h2 id="SVM支持向量机"><a href="#SVM支持向量机" class="headerlink" title="SVM支持向量机"></a>SVM支持向量机</h2><p>支持向量机又被称作最大间距（Large Margin）分类器，损失函数的形式是：</p><script type="math/tex; mode=display">J(\theta)=C\sum_{i=1}^{m}\left(y^{(i)}cost_1\left(h_{\theta}(x^{(i)})\right)+(1-y^{(i)})cost_0\left(h_{\theta}(x^{(i)})\right)\right)+\frac{1}{2}\sum_{j=1}^{n}\theta_j^2</script><p>其中：$h_{\theta}(x^{(i)})=\theta^Tx^{i}$，$cost_1$以及$cost_0$的形式如下图所示：</p><script type="math/tex; mode=display">\begin{cases}\text{we want } \theta^Tx\ge1,  & \text{if $y$ =1} \\[2ex]\text{we want } \theta^Tx\le-1,  & \text{if $y$ =0}\end{cases}</script><p>在考虑到soft margin时的损失函数是hinge损失，<a href="http://breezedeus.github.io/2015/07/12/breezedeus-svm-is-hingeloss-with-l2regularization.html" target="_blank" rel="noopener">SVM就等价于Hinge损失函数+L2正则</a>。此时损失函数为0时候就对应着非支持向量样本的作用，“从而所有的普通样本都不参与最终超平面的决定，这才是支持向量机最大的优势所在，对训练样本数目的依赖大大减少，而且提高了训练效率”。<br>以下是七月在线大神July写的一篇关于SVM的介绍，个人觉得不错。分享下：<a href="https://coding.net/s/f03e79be-8200-46bb-b714-7bb4ef70c391" target="_blank" rel="noopener">支持向量机通俗导论（理解SVM的三层境界）</a>。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初试HCI光场数据集</title>
      <link href="/posts/new-hci-lightfield-datasets/"/>
      <url>/posts/new-hci-lightfield-datasets/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img width="80%" data-src="https://vincentqin.gitee.io/blogresource-1/new-hci-lightfield-datasets/buddha2.gif"></p><div class="note success">            <p>好的数据集是做出漂亮实验的必要条件.<br><span id="inline-red">声明</span>：<u>一切理解都是本人观点，如有疑问，还望在<strong>评论</strong>中留言。如需转载请与本人联系，谢谢合作</u>! 邮箱：<a href="/about#CONTACT INFORMATION">点我</a></p>          </div><a id="more"></a><h2 id="Wanner光场数据集"><a href="#Wanner光场数据集" class="headerlink" title="Wanner光场数据集"></a>Wanner光场数据集</h2><p>目前光场数据集有如下几种主流的数据集，</p><ol><li><a href="http://lightfield.stanford.edu/lfs.html" target="_blank" rel="noopener">斯坦福大学光场数据集</a>；</li><li><a href="http://lightfieldgroup.iwr.uni-heidelberg.de/?page_id=713" target="_blank" rel="noopener">Wanner(HCI)数据集</a>(Old 4D Light Field Benchmark)；</li><li><a href="http://hci-lightfield.iwr.uni-heidelberg.de/" target="_blank" rel="noopener">4D Light Field Dataset</a>(Konstanz大学与Heidelberg大学的HCI合作,New 4D Light Field Benchmark)。</li></ol><p>下面对Wanner数据集进行讨论。学习光场的同学应该很熟悉Wanner提供的数据集共有<strong>10</strong>个场景，分别是：</p><ol><li>Buddha</li><li>Buddha2</li><li>Couple</li><li>Cube</li><li>Mona</li><li>Medieval</li><li>Papillon</li><li>StillLife</li><li>Horses</li><li>rx_watch</li></ol><p>其中，1-8为仿真场景，9-10是由Raytrix拍摄的场景。他们的文件后缀为 .h5, 格式是HDF5，这是一种文件组织格式，可以很好的将数据组织在一起，具体不做展开。MATLAB 提供了一系列相应的读取该文件的函数，如：h5disp，hdf5info(新版本用h5info)，hdf5read等函数，如利用h5disp就可以得到HDF5文件的内容信息，如下图：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/new-hci-lightfield-datasets/Buddha2Hd5.gif"></p><p>以下给出解码HDF5文件得到子孔径图像以及重排图像的代码：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">input_file       = <span class="string">'Buddha2.h5'</span>; <span class="comment">% file name</span></span><br><span class="line">input_folder     = <span class="string">''</span>;           <span class="comment">% your datasets folder</span></span><br><span class="line"></span><br><span class="line">[pathstr,name,ext] = fileparts([input_folder <span class="string">'/'</span> input_file]);</span><br><span class="line">file_path=[pathstr,name,ext];</span><br><span class="line"></span><br><span class="line">hinfo_data = hdf5info(file_path);</span><br><span class="line"><span class="keyword">if</span> strcmp(file_path,<span class="string">'Cube'</span>) || strcmp(file_path,<span class="string">'Couple'</span>)</span><br><span class="line">data = hdf5read(hinfo_data.GroupHierarchy.Datasets(<span class="number">3</span>));</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">data = hdf5read(hinfo_data.GroupHierarchy.Datasets(<span class="number">2</span>));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">data = <span class="built_in">permute</span>(data, [<span class="number">3</span> <span class="number">2</span> <span class="number">1</span> <span class="number">5</span> <span class="number">4</span>]);</span><br><span class="line">data = im2double(data(:, :, :, :, <span class="keyword">end</span>:<span class="number">-1</span>:<span class="number">1</span>));</span><br><span class="line"><span class="comment">% parameters from input</span></span><br><span class="line">UV_diameter = <span class="built_in">size</span>(data, <span class="number">4</span>);                   <span class="comment">% angular resolution</span></span><br><span class="line">UV_radius    = <span class="built_in">floor</span>(UV_diameter/<span class="number">2</span>);           <span class="comment">% half angular resolution</span></span><br><span class="line">h                  = <span class="built_in">size</span>(data, <span class="number">1</span>);            <span class="comment">% spatial image height</span></span><br><span class="line">w                  = <span class="built_in">size</span>(data, <span class="number">2</span>);            <span class="comment">% spatial image width</span></span><br><span class="line">y_size=h;</span><br><span class="line">x_size=w;</span><br><span class="line">UV_size=UV_diameter^<span class="number">2</span>;</span><br><span class="line">LF_y_size   = h * UV_diameter;                 <span class="comment">% total image height</span></span><br><span class="line">LF_x_size   = w * UV_diameter;                 <span class="comment">% total image width</span></span><br><span class="line">LF_Remap    = <span class="built_in">reshape</span>(<span class="built_in">permute</span>(data, ...</span><br><span class="line">   [<span class="number">4</span> <span class="number">1</span> <span class="number">5</span> <span class="number">2</span> <span class="number">3</span>]), [LF_y_size LF_x_size <span class="number">3</span>]); <span class="comment">% the remap image</span></span><br><span class="line">IM_Pinhole = data(:,:,:,UV_radius+<span class="number">1</span>,UV_radius+<span class="number">1</span>); <span class="comment">% the pinhole image</span></span><br></pre></td></tr></table></figure><p>经过以上步骤可以得到相应的中心视角图像以及Remap（重排）之后的图像，从而进一步方便接下来的工作，例如基于该数据集的深度图像估计算法估计。</p><h2 id="HCI-4D光场数据集-4D-Light-Field-Benchmark"><a href="#HCI-4D光场数据集-4D-Light-Field-Benchmark" class="headerlink" title="HCI 4D光场数据集(4D Light Field Benchmark)"></a>HCI 4D光场数据集(4D Light Field Benchmark)</h2><p id="div-border-left-red">The 4D Light Field Benchmark was jointly created by the University of Konstanz and the HCI at Heidelberg University.</p><p>上周整理上一篇博客的时候，想再次查看HCI数据集是否更新，结果惊喜地看到它竟然更新了！激动之余，就连夜把数据以及代码下载了下来，看看这个数据集的庐山真面目。</p><h3 id="数据集概况"><a href="#数据集概况" class="headerlink" title="数据集概况"></a>数据集概况</h3><p>这个数据集共有4大类：</p><ul><li><p>Stratified（4）</p></li><li><p>training（4）</p></li><li><p>test（4）</p></li><li><p>additional（16）</p><p>​</p></li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/new-hci-lightfield-datasets/scenes.png"></p><p>总结而言这个4D光场数据集提供了如下信息：</p><ul><li>9x9x512x512x3 light fields as individual PNGs（角度分辨率：9×9，空间分辨率：512×512）</li><li>Config files with camera settings and disparity ranges（相机配置文件以及视差范围）</li><li>Per center view (except for the 4 test scenes):（除了测试类外每类的中心视角图像）<ul><li>512×512 and 5120×5120 depth and disparity maps as PFMs（深度图像以及视差图：512×512低分辨率，以及5120×5120高分辨率）</li><li>512×512 and 5120×5120 evaluation masks as PNGs（png格式的评价掩膜：512×512低分辨率，以及5120×5120高分辨率）</li></ul></li><li>16组additional的每个视角的Ground Truth Depth图像（pfm格式）</li></ul><h3 id="数据集下载"><a href="#数据集下载" class="headerlink" title="数据集下载"></a>数据集下载</h3><p>开始下载吧！在<a href="http://hci-lightfield.iwr.uni-heidelberg.de/" target="_blank" rel="noopener">该页面</a>的<code>get the data</code>处填写自己的邮箱，然后点击<code>request download links</code>。接下来你的邮箱里就会出现这个数据集的下载链接，链接有点多，你可以选择性的下载或者全部下载。方便起见，我把邮件中提供的链接贴在了下面：</p><ul><li><p><a href="http://lightfield-analysis.net/benchmark/downloads/benchmark.zip" target="_blank" rel="noopener">Benchmark package with the 12 benchmark scenes</a></p></li><li><p><a href="http://lightfield-analysis.net/benchmark/downloads/full_data.zip" target="_blank" rel="noopener"><strong>Full package with all 28 scenes</strong></a>(这是全部的场景，共28类；注意不包含深度图像)</p></li></ul><ul><li>Packages per category:<ul><li><a href="http://lightfield-analysis.net/benchmark/downloads/stratified.zip" target="_blank" rel="noopener">stratified</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/test.zip" target="_blank" rel="noopener">test</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/training.zip" target="_blank" rel="noopener">training</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/additional.zip" target="_blank" rel="noopener">additional</a></li></ul></li></ul><ul><li>Stratified scenes:<ul><li><a href="http://lightfield-analysis.net/benchmark/downloads/backgammon.zip" target="_blank" rel="noopener">backgammon</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/dots.zip" target="_blank" rel="noopener">dots</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/pyramids.zip" target="_blank" rel="noopener">pyramids</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/stripes.zip" target="_blank" rel="noopener">stripes</a></li></ul></li></ul><ul><li>Test scenes:<ul><li><a href="http://lightfield-analysis.net/benchmark/downloads/bedroom.zip" target="_blank" rel="noopener">bedroom</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/bicycle.zip" target="_blank" rel="noopener">bicycle</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/herbs.zip" target="_blank" rel="noopener">herbs</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/origami.zip" target="_blank" rel="noopener">origami</a></li></ul></li></ul><ul><li>Training scenes:<ul><li><a href="http://lightfield-analysis.net/benchmark/downloads/boxes.zip" target="_blank" rel="noopener">boxes</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/cotton.zip" target="_blank" rel="noopener">cotton</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/dino.zip" target="_blank" rel="noopener">dino</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/sideboard.zip" target="_blank" rel="noopener">sideboard</a></li></ul></li></ul><ul><li><p>Additional scenes:</p><ul><li><a href="http://lightfield-analysis.net/benchmark/downloads/antinous.zip" target="_blank" rel="noopener">antinous</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/boardgames.zip" target="_blank" rel="noopener">boardgames</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/dishes.zip" target="_blank" rel="noopener">dishes</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/greek.zip" target="_blank" rel="noopener">greek</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/kitchen.zip" target="_blank" rel="noopener">kitchen</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/medieval2.zip" target="_blank" rel="noopener">medieval2</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/museum.zip" target="_blank" rel="noopener">museum</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/pens.zip" target="_blank" rel="noopener">pens</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/pillows.zip" target="_blank" rel="noopener">pillows</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/platonic.zip" target="_blank" rel="noopener">platonic</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/rosemary.zip" target="_blank" rel="noopener">rosemary</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/table.zip" target="_blank" rel="noopener">table</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/tomb.zip" target="_blank" rel="noopener">tomb</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/tower.zip" target="_blank" rel="noopener">tower</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/town.zip" target="_blank" rel="noopener">town</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/vinyl.zip" target="_blank" rel="noopener">vinyl</a></li></ul></li></ul><ul><li><a href="http://lightfield-analysis.net/benchmark/downloads/additional_depth_disp_all_views.zip" target="_blank" rel="noopener">Depth and disparity maps for all views of the additional scenes</a></li></ul><h3 id="数据集初体验"><a href="#数据集初体验" class="headerlink" title="数据集初体验"></a>数据集初体验</h3><h4 id="测试代码下载"><a href="#测试代码下载" class="headerlink" title="测试代码下载"></a>测试代码下载</h4><p>在其官方给出的<a href="https://github.com/lightfield-analysis/matlab-tools" target="_blank" rel="noopener">代码页面</a>下载测试程序，下载完毕后将convert*.m以及lib文件夹其放置在与上述数据集同级目录。例如：TEST目录下同时包括：convert.m 以及 lib/， 同样也包含 additional/, stratified/, test/, 以及 training/。</p><h4 id="生成LF-mat"><a href="#生成LF-mat" class="headerlink" title="生成LF.mat"></a>生成LF.mat</h4><ul><li>convertAll. 对于每一个场景都声称一个<code>LF.mat</code>文件</li></ul><p>如果我们仅仅下载了几个场景我们可以利用如下函数得到相应的<code>LF.mat</code></p><ul><li>convertBlenderTo5D(‘FOLDER’)</li></ul><p>这个LF.mat中包含该场景的光场信息诸如：</p><ul><li>光场数据 (LF.LF)</li><li>真实值 (LF.depth/disp_high/lowres)</li><li>评价掩膜（mask）</li><li>中心视角图像</li></ul><p><span id="inline-red">注意</span>：生成LF.mat的过程用到的参数通过加载相应文件夹下parameters.cfg得到，并将其存储在了LF.parameters中；H变换矩阵存储在了LF.H中（可以参考论文<a href="http://www-personal.acfr.usyd.edu.au/ddan1654/PlenCal.pdf" target="_blank" rel="noopener">“Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras” </a>）；两个平面的距离存储在LF.f, 单位 [mm]； 相机焦距：LF.parameters.intrinsics.focal_length_mm.</p><h4 id="生成点云（Point-Cloud）"><a href="#生成点云（Point-Cloud）" class="headerlink" title="生成点云（Point Cloud）"></a>生成点云（Point Cloud）</h4><p>接下来我以<strong>additional</strong>文件下的<strong><code>antinous</code></strong>为例子展示如何利用深度图像（官方利用视差）与纹理图像生成点云。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">filename=<span class="string">'antinous'</span>;</span><br><span class="line">addpath(<span class="string">'lib'</span>);</span><br><span class="line"><span class="comment">% 得到antinous的LF.mat</span></span><br><span class="line">convertBlenderTo5D([<span class="string">'additional/'</span>,filename])</span><br><span class="line">load([<span class="string">'additional/'</span>,filename,<span class="string">'/LF.mat'</span>]);</span><br><span class="line"></span><br><span class="line">img=LF.LF(<span class="number">5</span>,<span class="number">5</span>,:,:,:); <span class="comment">%中心视角，用于着色</span></span><br><span class="line">r=img(:,:,<span class="number">1</span>);</span><br><span class="line">g=img(:,:,<span class="number">2</span>);</span><br><span class="line">b=img(:,:,<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 深度图读取</span></span><br><span class="line">d=pfmread([<span class="string">'additional_depth_disp_all_views\',filename,'</span>\gt_disp_lowres_Cam025.pfm']);</span><br><span class="line">d=mat2gray(d);</span><br><span class="line"></span><br><span class="line">mkdir([<span class="string">'PointClouds-color/'</span>,filename]);建立一个文件夹存储图片</span><br><span class="line"></span><br><span class="line">[ X,Y,Z ] = getPointcloud(LF,<span class="string">'disp'</span>,d);</span><br><span class="line">ptCloud1 = pointCloud([X(:),Y(:),Z(:)],<span class="string">'color'</span>,[r(:) g(:) b(:)]);</span><br><span class="line"></span><br><span class="line">h1=<span class="built_in">figure</span>(<span class="number">1</span>);</span><br><span class="line">pcshow(ptCloud1);</span><br><span class="line"></span><br><span class="line">axis off</span><br><span class="line">set(gcf,<span class="string">'color'</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>])</span><br><span class="line">set(gcf,<span class="string">'Position'</span>,[<span class="number">800</span>,<span class="number">300</span>,<span class="number">600</span>,<span class="number">600</span>], <span class="string">'color'</span>,<span class="string">'w'</span>)</span><br><span class="line">view(<span class="number">90.6338</span>,  <span class="number">88.5605</span>);</span><br><span class="line">zoom(<span class="number">1.2</span>)</span><br></pre></td></tr></table></figure><p>结果如下所示：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/new-hci-lightfield-datasets/PCs.gif"></p><p><span id="inline-red">注意</span>：生成点云这一步，低版本的MATLAB（如R2014a）由于没有加入相应的函数所以不能够生成点云，高版本（R2016b）可以正常生成。另外，在此提供另外一个函数<code>visualizeZ_3D</code>，该函数将depth map当做彩色图像的z向延伸，然后构图。<br><img width="1200px" data-src="https://vincentqin.gitee.io/blogresource-1/new-hci-lightfield-datasets/3d-demo.jpg"></p><figure class="highlight matlab"><figcaption><span>文件名: visualizeZ_3D.m</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--The <span class="function"><span class="keyword">function</span> <span class="title">visualizeZ_3D</span>--&gt;</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">visualizeZ_3D</span><span class="params">(Z,im)</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (im == <span class="number">0</span>)</span><br><span class="line">    surf(Z, visualizeZ(Z), <span class="string">'EdgeColor'</span>, <span class="string">'none'</span>); imtight; axis image ij; <span class="comment">%view(-180, 91);</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    surf(Z, im, <span class="string">'EdgeColor'</span>, <span class="string">'none'</span>); imtight; axis image ij; <span class="comment">%view(-180, 91);</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">imtight</span></span></span><br><span class="line">axis image off;</span><br><span class="line">xMult = <span class="number">1</span>;</span><br><span class="line">yMult = <span class="number">1</span>;</span><br><span class="line">borderSize = <span class="number">0</span>;</span><br><span class="line">PLOTBASESIZE = <span class="number">500</span>;</span><br><span class="line">set(gca, <span class="string">'PlotBoxAspectRatio'</span>, [xMult yMult <span class="number">1</span>])</span><br><span class="line">set(gcf, <span class="string">'Position'</span>, get(gcf, <span class="string">'Position'</span>) .* [<span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>] + [<span class="number">0</span> <span class="number">0</span> PLOTBASESIZE*xMult PLOTBASESIZE*yMult]);</span><br><span class="line">set(gca, <span class="string">'Position'</span>, [borderSize borderSize <span class="number">1</span><span class="number">-2</span>*borderSize <span class="number">1</span><span class="number">-2</span>*borderSize]);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="旧数据集下载"><a href="#旧数据集下载" class="headerlink" title="旧数据集下载"></a>旧数据集下载</h3><ul><li>旧HCI数据集 : <a href="https://drive.google.com/open?id=1epj7GTDlCYTnlnG-TCvPmISBbBcWEhB4" target="_blank" rel="noopener">GoogleDrive</a>, <a href="https://pan.baidu.com/s/1EJ8lkAkR-R76jCaeJ_4Vlg" target="_blank" rel="noopener">BaiduWangPan</a>, 提取码: r4yb</li></ul>]]></content>
      
      
      <categories>
          
          <category> 光场 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 光场 </tag>
            
            <tag> 计算成像 </tag>
            
            <tag> 点云Point Cloud </tag>
            
            <tag> Light Field </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实习季到了，大家又浮躁了起来</title>
      <link href="/posts/internship/"/>
      <url>/posts/internship/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/internship/internship.png"></p><p id="div-border-left-red">实习季堪比就业季，今年的形势尤其严峻。伴随着忐忑的心情，我迎来了这个不得不面对的时期。</p><a id="more"></a><h1 id="阿里巴巴视觉算法工程师"><a href="#阿里巴巴视觉算法工程师" class="headerlink" title="阿里巴巴视觉算法工程师"></a>阿里巴巴视觉算法工程师</h1><h2 id="算法与视觉部分"><a href="#算法与视觉部分" class="headerlink" title="算法与视觉部分"></a>算法与视觉部分</h2><ul><li>BF，NN的区别</li><li>激活函数的种类</li><li>怎么防止过拟合</li><li>CUDA的内存模型</li><li>HMM是什么</li><li>SVM的优缺点</li><li>SVD分解的过程</li><li>PCA过程</li><li>光流法</li><li>模版匹配SSD与NCC的优缺点</li><li>有哪些形态学操作</li><li>相机畸变的参数到底有哪些</li><li>交叉熵的概念</li><li>Sift与Surf的区别</li><li>由前序遍历与中序遍历求后序遍历</li><li>深度优先遍历可能的顺序</li></ul><p><br></p><h1 id="腾讯基础研究实习生"><a href="#腾讯基础研究实习生" class="headerlink" title="腾讯基础研究实习生"></a>腾讯基础研究实习生</h1><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/internship/Tencent.png"></p><p id="div-border-left-green"> 上机考试包括很多数学方面的知识，比考研数学简单多了，但是范围很广，我想过了这么久大家都忘记了吧。概率论部分占了不少题目，尤其要注意后验概率以及假设检验的题目。基础研究没有编程题目！</p><h2 id="数学部分"><a href="#数学部分" class="headerlink" title="数学部分"></a>数学部分</h2><ul><li>特征值与特征向量：Ax=λx</li><li>假设检验，第一类错误与第二类错误</li><li>求解偏导数</li><li>切比雪夫不等式</li><li>F分布的性质</li></ul><h2 id="简答部分"><a href="#简答部分" class="headerlink" title="简答部分"></a>简答部分</h2><ul><li>假设检验来确定零件是否符合标准（可以查看概率论的部分例题）</li><li>神经网络以及SVM的对比，优缺点介绍</li><li>根据某项调查研究，来确定某结论的正确性；</li></ul><h2 id="现场面试部分"><a href="#现场面试部分" class="headerlink" title="现场面试部分"></a>现场面试部分</h2><h3 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h3><p>主要包括以下部分：</p><ul><li>自我介绍（1分钟内）</li><li>项目经历（占了60%时间）</li><li>编程题目（反转链表，可参考<strong>《剑指offer》</strong>）</li><li>意向，做工程还是做算法<br>(ps: 被腾讯挂掉了，惨啊)</li></ul><h3 id="一面（数据挖掘）"><a href="#一面（数据挖掘）" class="headerlink" title="一面（数据挖掘）"></a>一面（数据挖掘）</h3><p>猝不及防地又来了一波电话面试，我一脸懵逼的节奏，完全没有准备。我是小白有没有，面试官主要问了以下几个问题：</p><ul><li>解释方差，协方差以及样本方差的概念</li><li>解释过拟合以及过拟合的概念以及预防措施</li><li>解释TCP滑动窗口的概念（这是啥？）</li><li>求超级长数组的中位数</li><li>析构函数是否可以为虚函数（我是C++小白）</li><li>项目介绍</li></ul><p><br></p><h1 id="商汤算法实习生"><a href="#商汤算法实习生" class="headerlink" title="商汤算法实习生"></a>商汤算法实习生</h1><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/internship/sensetime.png"></p><h2 id="在线笔试"><a href="#在线笔试" class="headerlink" title="在线笔试"></a>在线笔试</h2><p id="div-border-left-green"> 本人申请的岗位是见习算法研究员，笔试1个小时，20道选择填空题，3道编程题。时间略紧。涉及面也非常广，数学，智力，概率统计，线代矩阵，图形学，机器学习，神经网络，C++，均有涉及。</p><h3 id="一、选择填空题-部分-："><a href="#一、选择填空题-部分-：" class="headerlink" title="一、选择填空题(部分)："></a>一、选择填空题(部分)：</h3><ol><li><p>S市A，B共有两个区，人口比例为3：5，据历史统计A的犯罪率为0.01%，B区为0.015%，现有一起新案件发生在S市，那么案件发生在A区的可能性有多大?  (概率题，考查贝叶斯公式，牛客网有)</p></li><li><p>A = [1, 2 ; 2, 1]，求A的k次方。(线代，对A进行对角化，求特征值以及特征方程)</p></li><li><p><strong>git</strong>常用命令，克隆到本地是（），提交到仓库区（），取回远程仓库的变化，并与本地分支合并（）,  推送所有分支到远程仓库（），显示有变更的文件（）(答：clone, commit, pull, push, status)</p></li><li><p>表示矩阵需要多少个数字，表示矩阵的投影需要多少个数字?</p></li><li><p>给出先序序列，中序序列，求后序序列。</p></li><li><p>一个关于继承和虚函数问题。</p></li><li><p>掷两个骰子，得到两个数字A,B，设 C = A+B，那么设 C 除以4 的余数为0，1，2，3的概率分别为p0, p1, p2, p3，求它们的大小关系。</p></li><li><p>图片分辨率为512x512，pad = 2, stride = 3, kernel_size = 9, group = 4, 求卷积后输出分辨率大小。</p></li><li><p>一个关于图形自由度的问题。（本人完全没概念，所以题目具体记不清了）</p></li><li><p>以下哪个不能使用迭代器？a) map, b) set, c) queue, d) vector.  (c)</p></li><li><p>有两个样本点，第一个点为正样本,它的特征向量是(0,-1);第二个点为负样本,它的特征向量是(2,3),从这两个样本点组成的训练集构建一个线性SVM分类器的分类面方程是() （<a href="https://www.nowcoder.com/questionTerminal/104e95c6a13d464a86eb6b657cc545c0" target="_blank" rel="noopener">答案猛击这里</a>）</p></li><li><p>在一个无序数组中，求前k个最小数字，复杂度最小为？</p></li><li><p>根据以下程序：求func(500)的值。(经典问题，相当于求500的二进制中1的个数，《剑指offer》)</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">func</span><span class="params">(x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> countx = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(x)</span><br><span class="line">    &#123;</span><br><span class="line">          countx ++;</span><br><span class="line">          x = x&amp;(x<span class="number">-1</span>);</span><br><span class="line">     &#125;</span><br><span class="line">    <span class="keyword">return</span> countx;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题（）<br>a) 增加训练集量<br>b) 减少神经网络隐藏层节点数<br>c) 删除稀疏的特征<br>d) SVM算法中使用高斯核/RBF核代替线性核</p></li><li><p>关于vector初始化的一个问题。</p></li><li><p>有4个车站连通情况如下，每辆车每天都会等概率随机从一个车站出发，然后在某个车站呆一夜，第二天再出发。求稳定之后，每个车站的车辆比例。</p></li></ol><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/internship/question.png"></p><p>( 根据马尔科夫链 平稳分布，π=πP( P为转移概率矩阵)，和π1+π2+π3+π4=1，同时π1=π4, π2 = π3。可以求得2:3:3:2 ）</p><h3 id="二、编程题"><a href="#二、编程题" class="headerlink" title="二、编程题:"></a>二、编程题:</h3><ol><li><p>连续子数组的最大和。（leetcode或剑指offer原题）</p></li><li><p>Minimum Window Substring .(leetcode 原题)</p></li></ol><p>特别鸣谢<a href="http://blog.csdn.net/smallplum123/article/details/69938232" target="_blank" rel="noopener">smallpum123</a>的商汤回忆版！</p><h2 id="现场面试"><a href="#现场面试" class="headerlink" title="现场面试"></a>现场面试</h2><blockquote><p>4月25日，从学校匆匆到了商汤科技进行面试，幸亏提前到了1个小时，要不然就被淋成落汤鸡了。</p></blockquote><p>一共有两个面试官依次面了我，这两名面试官的侧重点不同，第一位是大体了解面试对象，第二位面试官更加具体深入了解面试者。</p><ol><li>第一个面试官简单地聊了一段时间，首先是自我介绍，然后是项目经历，最常用的编程语言（我说的是Matlab），然后又问了我有没有用过Matlab的高级函数（bsxfun、ismember等），其他的没有很深入地讨论项目细节。（20 min）</li><li>接下来就是第二个面试官，还是重复前面的问题，自我介绍，项目经历，不过这次更加具体了。因为我的方向是做一些基于光场相机的深度图像估计研究的，面试官就问了我关于光场相机原理以及深度估计算法细节方面的东西；然后又问了我第二个基于GPU加速的项目，具体是如何加速代码的（该项目偏工程，没有具体展开）。项目的最后又问了我这些工程的代码量有多大，多少行的样子（我说最少得两、三千行吧）。</li><li>最后就是编程题目，面试官问我关于商汤在线评测代码书写的问题，我的回答是：对于<strong>连续子数组的最大和问题</strong>仅仅写了思路，没有写全代码。然后就是让我现场手写代码了，大概修改了4遍的样子，终于“调试”（所谓调试就是，现场测试代码一步一步写出结果）成功。（60 min）</li><li>当然，面试的最后通常面试官都会问面试者想要了解公司的事情，我就如实将我想要知道的事情想他请假了一下下，然后就没有然后了…</li></ol><p>经过大概一个半小时，面试结束。无论结果如何，我的心情瞬间轻松许多。还是静候佳音吧~</p><h2 id="电话面试"><a href="#电话面试" class="headerlink" title="电话面试"></a>电话面试</h2><p>由香港那边的负责人对我进行了远程电话面试，主要包括自我介绍以及项目介绍，重点在项目介绍上面。Ps：当时电话那头是两位面试官听着我的陈述，我竟然浑然不知。就这样过了40分钟，结束。等待的时间最为忐忑，我觉得自己表现平平，不知道给面试官留下了什么印象。</p><h2 id="顺利通过"><a href="#顺利通过" class="headerlink" title="顺利通过"></a>顺利通过</h2><p>经过一个漫长的劳动节并时逢校庆的假期，5月6号的下午收到了HR打来的电话，成功通过面试，现在心情还是特别激动。</p><p><br></p><h1 id="搜狐图像处理实习生"><a href="#搜狐图像处理实习生" class="headerlink" title="搜狐图像处理实习生"></a>搜狐图像处理实习生</h1><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/internship/qianfan.png"></p><p>初选简历过关之后进行面试。</p><h2 id="笔试（60min）"><a href="#笔试（60min）" class="headerlink" title="笔试（60min）"></a>笔试（60min）</h2><p>根据应聘的实习岗做不同的题目，因为我面的是千帆直播下的图像处理岗位，所以我的题目中有很多关于这方面的相关知识。以下是我的会议版本：</p><ol><li>do while 和while do 的区别</li><li>char const *p 与char * const p 的区别(答：p都是指向const char类型的指针, 不可以赋值给*p, 就是不可通过这个指针改变它指向的值; 第二个: char * const p是指向char的常指针, 指针需在声明时就初始化, 之后不可以改变它的指向)</li><li>创建并且初始化一个双向链表</li><li>代码实现二分查找</li><li>对一个WAV格式的文件头用适当的数据结构进行表示</li><li>队列与栈的区别，分别以什么数据结构表示</li><li>常见的视频压缩方法，视频格式，音频格式</li><li>汇编语言和C/C++混合编程有哪些方法</li><li>如何引用一个已经定义好的全局变量，并比较异同</li><li>gdb如何调试线程，多线程呢（ps:我根本不会这个题目）</li><li>解释“熵”的概念（答：熵，热力学中表征物质状态的参量之一，用符号S表示，其物理意义是体系混乱程度的度量。信息熵表示信息的丰富程度，定义为E=-plog(p)）</li><li>请解释1080p的含义（答：1080指的是分辨率1920*1080，p为扫描方式：逐行扫描）</li><li>请解释FPS的全称以及含义（答：Frames Per Second，帧率的意思）</li><li>解释“码率”的概念（答：即比特率，一秒钟处理的数据量大小，影响到视频的质量以及帧率）</li><li>MPEG标准中有哪些帧类型</li><li>有以下数据结构，请问最后输出结果的是？（注意共用体的大小）<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">union</span>&#123;</span><br><span class="line">    <span class="keyword">long</span> i;</span><br><span class="line">    <span class="keyword">int</span> j[<span class="number">3</span>];</span><br><span class="line">    <span class="keyword">char</span> k;</span><br><span class="line">&#125;DATA;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">data</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> m;</span><br><span class="line">    DATA n;</span><br><span class="line">    <span class="keyword">double</span> q;</span><br><span class="line">&#125;;</span><br><span class="line">data max;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%d/n"</span>,&amp;(<span class="keyword">sizeof</span>(DATA)+<span class="keyword">sizeof</span>(max));</span><br></pre></td></tr></table></figure></li></ol><p>还有其他的题目，记不太清了，但是主要就是以上的。难度适中，即有涉及到程序也有图像以及视频处理的相关知识。因为当时没有好好准备，猝不及防的给我了这些题目，感觉一脸懵逼。</p><h2 id="现场面试-1"><a href="#现场面试-1" class="headerlink" title="现场面试"></a>现场面试</h2><p>面试小哥很nice，人很好。首先是自我介绍，然后就是项目经历。基本上简历上的内容问了一遍，感觉还不错。问了我如果调试正在进行中的程序，如何用markdown语言引用代码，对Latex的了解等等。小哥面试结束后，以为女面试官姐姐再次对我的一些基本情况进行了询问，最后还送了一个可爱的小狐狸。<br>无论结果如何等结果吧，祝我好运！<br>ps：很幸运地被录为实习生，但是还是选择了商汤。</p><center><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="600" height="86" src="http://music.163.com/outchain/player?type=2&id=579954&auto=0&height=66"></iframe></center>]]></content>
      
      
      <categories>
          
          <category> 实习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实习 </tag>
            
            <tag> 腾讯基础研究 </tag>
            
            <tag> 阿里巴巴视觉算法 </tag>
            
            <tag> 商汤科技 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日本与美国之行</title>
      <link href="/posts/journal2JP-USA/"/>
      <url>/posts/journal2JP-USA/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Deer_in_Nara.jpg"></p><a id="more"></a><p>早上起得很早，坐车从深圳到香港乘坐飞机。一切就绪，出发！</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Hongkong_Airport.JPG"></p><blockquote class="blockquote-center"> 日本国之行 </blockquote><p>小时候因为极其喜欢动漫的缘故，我喜欢上了日本，从那时起已经向往能够进行一次日本之行。2017年1月8日早上的闹钟比以往想得更加早些。迎着黎明前第一缕阳光的到来，我们来到了香港机场。即将开始人生第一次出国高校访问之旅，我心情相当激动。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Narita_Japan_Airport.JPG"></p><p>飞机经历两个小时到达日本，偌大的东京比深圳多了一份凉意。拖着行李坐着日本地铁匆匆来到了池袋的旅馆，安顿下来，第一天就这样结束。接下来的几天我们团队依次到了东京、京都、奈良等地，参观日本几所具有代表性的大学，体验日本的文化以及感受日本的风土人情。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Meiji_Jingu_Shrine.jpg"></p><p>初来乍到，凛冬已至，虽寒意袭来，但心中激动之情让我变得火热。日本之行我们团队参观了5所大学（早稻田大学，东京大学，法政大学，京都大学，秋田大学）以及1个科研院所（NHK科技研究实验室）。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Waseda_University.jpg"></p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/The_University_of_Tokyo.jpg"></p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Kyoto_University_After_Snow.jpg"></p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Kyoto_University_Advancing_Titans.jpg"></p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/University_of_Political_Science_and_Law_on_the_way.jpg"></p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/University_of_Political_Science_and_Law.jpg"></p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/NHK.jpg"></p><p>日本同学介绍的科研内容并非高大上的东西，有些只是利用现有的算法来解决现实问题；但是他们做出的东西都是系统化的，最终的结果是以实物或者可视化的形式作为展示。具体而言，对于早稻田大学3个机器人实验室，他们的研究方向更加贴近于实际生活，而并非空谈理论，关注于“人”本身，而并非突发奇想。一位早稻田大学的师兄提到，他们的研究成果必须最终以Demo的形式展示，相比之下，国内科研更加注重仿真，如若计算机仿真能够成功的实现预期目标算是大功告成，省去了硬件实现或者可视化展示这一步。这对于该领域内的科研人员而言是及其容易理解的，但是对于大众而言，大都不能够理解算法的原理以及实现细节，他们关注的是研究的目的以及能够解决的问题。因此如何以一种通俗易懂的方式向大众介绍自己的研究内容成为了一个问题，日本高校相比国内高校在科研成果展示方面具有一定的领先。</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Akita_University.jpg"></p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Akita_University_Snow.jpg"></p><p>值得注意的是，最后一站的秋田大学与以前的几个大学有着明显差异，它位于日本的北端，抵达时正值秋田几十年来最大的一场雪，这场大雪增加了参观秋田大学的乐趣。起初，我们对于这个农业科学见长的高校知之甚少。通过对其参观，我了解到了这个新兴高校的发展历史以及现有科研水平，秋田大学如何在这十几年时间内迅速成长必然值得我国高校学习。</p><p>总结而言，东京大学与京都大学历史悠久，科研气氛浓厚且资源丰厚；早稻田大学注重实践，校企合作成为常态；政法大学兼容并包，面向世界；秋田大学后起之秀，努力吸收优秀资源。尽管参观时间有限，但是我们能够明显的感觉到日方高校对于我们来访的热情，同时也可以了解到日本最有代表性大学的科研水平。最大的收获在于，亲身感受到了跨文化交流。我们用英文与日本同学交流，语言不再是最大的障碍。保持好奇心一直是我的座右铭，尽管研究领域不尽一致，但我会努力的提出自己的疑问并聆听日本同学的讲解。起初时候会担心自己不能够很好的提出疑问或者不能够听懂他们的回答，但是主动尝试之后会发现，自己所担心的问题并不存在，同时我也逐渐喜欢上了与他们之间的交流的过程。</p><blockquote><p>京都游玩的地方很多，交流之余，游遍几个好玩的地方</p></blockquote><p>京都街头随拍：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Kyoto_Street.jpg"></p><p>京都伏见稻荷大社：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Rice_lotus.jpg"></p><p><img width="100%" data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Rice_lotus_1.jpg"></p><p>京都清水寺求姻缘之地：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Kiyomizu_Temple.jpg"></p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Kiyomizu_Temple_Far.jpg"></p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Kiyomizu_Temple_1.jpg"></p><p>此外，很早就听说日本民众素质很高，这次真切的感受到了。值得注意的是，他们在日常生活中表现出的礼貌也值得人敬佩，这很符合日本人不愿给别人添麻烦的特点。他们时常把“すみませ”挂在嘴边，比如进电梯时，进入餐厅时或者超市付款时；无论多赶时间也会排队，另外在公共场合他们不会大声喧哗，具体而言在地铁上无论人再多再挤，车厢内也是安静的让人感觉不可思议；他们的房屋建设的很低，密密麻麻的挤在一起，电线在房屋之间穿梭，但是让人感觉并不杂乱，“规矩”一词用在他们身上并不为过。同时日本对于古老传统的保留非常重视，以前就听说日本是保留中国古文化最好的非华人国家。果然，“相扑，茶道，和服，空手道”等诸多优良文化在日本保留的相当好，在日剧以及日本民族产业动画中都有很好体现。改革开放以来，我国的经济实力突飞猛进，如今已经跃居世界第二大经济体，但是国际上的文化软实力远远不如日本。很多欧美人士所了解的很多“日本传统”实际是源于中国，日本在国际上的形象是优于中国的。“择其善者而从之”，我认为国内的道德建设以及文化建设必须同经济发展相协调，提高全民素质，打造真正属于中国的国际形象。</p><p>大阪上空：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Osaka_Sky_Snow.jpg"></p><p>东京远望富士山：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Fuji.jpg"></p><blockquote class="blockquote-center"> 美利坚合众国之行 </blockquote><p>出国开会，顺便拍了几张图~</p><p>新奥尔良法国区一隅：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/New_Orleans_French_district.jpg"></p><p>教堂一隅：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/church.jpg"></p><p>杰克逊广场一隅:<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Jackson_square.jpg"></p><p>杜兰大学军事学院：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Military_College_of_Tulane_University.jpg"></p><p>杜兰大学一隅：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/Tulane_University.jpg"></p><p>二战博物馆：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/journal2JP-USA/World_War2Museum.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> 游行 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 美国 </tag>
            
            <tag> 日本 </tag>
            
            <tag> 东京大学 </tag>
            
            <tag> 早稻田大学 </tag>
            
            <tag> 法政大学 </tag>
            
            <tag> 京都大学 </tag>
            
            <tag> 秋田大学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Light Field 光场以及MATLAB光场工具包(LightField ToolBox)的使用说明</title>
      <link href="/posts/LightField-Toolbox/"/>
      <url>/posts/LightField-Toolbox/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt="Magic Leap" data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/91252468.png"></p><div class="note success">            <p><a href="https://www.vincentqin.tech/collections/">这里</a>鄙人汇总了有关光场（Light Field）一些有用的链接以及光场数据的处理过程。<br><span id="inline-red">声明</span>：<u><strong>一切理解都是本人观点，如有疑问，还望在</strong>评论<strong>中留言。如需转载请与本人联系，谢谢合作</strong></u>! )</p>          </div><a id="more"></a><h2 id="光场相机"><a href="#光场相机" class="headerlink" title="光场相机"></a>光场相机</h2><p>大家在刚刚入手光场领域的时候可能会用到目前消费级的手持光场相机，如Lytro或者ILLUM，如图（实验室的设备，鄙人可买不起ILLUM）：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/LF-cameras.jpg"></p><h2 id="获取-LFP-or-LFR-原文件"><a href="#获取-LFP-or-LFR-原文件" class="headerlink" title="获取.LFP(or .LFR)原文件"></a>获取.LFP(or .LFR)原文件</h2><p>由Lytro拍摄的图像的原格式是.lfp格式，我们要将其解码成我们需要的格式。<br>工具：<strong>Lytro Desktop</strong>，<strong>MATLAB光场工具包</strong>（很强大，推荐，本文介绍该方法）。</p><p>首先用数据线把设备连接到电脑，打开Lytro Desktop，点击想要导入的图片，选中点击右上角立即处理，然后打开我的电脑图片-&gt;Lytro Desktop\Libraries\Lytro 图库.lytrolibrary*，就可以发现有很多文件夹名字是一串很长数字字母云云。点击进去可以发现里面有几个文件，以lytro为例，这几个文件如下形式：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/lfp_list.png"></p><p><strong><font color="red">raw.lfp</font></strong>就是我们需要的原文件，之后我们就要利用Matlab光场工具包对其进行解码操作。</p><h2 id="Matlab-Light-Field-ToolBox-光场工具箱-的使用"><a href="#Matlab-Light-Field-ToolBox-光场工具箱-的使用" class="headerlink" title="Matlab Light Field ToolBox(光场工具箱)的使用"></a>Matlab Light Field ToolBox(光场工具箱)的使用</h2><h3 id="下载光场工具包（LFToolBox）"><a href="#下载光场工具包（LFToolBox）" class="headerlink" title="下载光场工具包（LFToolBox）"></a>下载光场工具包（LFToolBox）</h3><p>首先下载<a href="http://cn.mathworks.com/matlabcentral/fileexchange/49683-light-field-toolbox-v0-4" target="_blank" rel="noopener">光场工具箱</a>并仔细阅读说明文档，根据文档把相应的数据拷贝到工具箱的文件夹下(这一步很关键，要仔细配置)。<del>如果不想在官网下载的话我上传到了度娘的云盘链接：<a href="http://pan.baidu.com/s/1hsDo0ks" target="_blank" rel="noopener">链接</a> 密码：yykc。这是鄙人修改后的一个版本，可以直接运行。</del><strong><font color="red">另外鄙人在Github上传了一个版本，大家可以git clone<a href="https://github.com/Vincentqyw/Light_Field_TB" target="_blank" rel="noopener">链接</a></font></strong>。下载下来的工具包是这样的：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/LF-TB-main.png"></p><p>LFToolbox0.4就是我们需要的工具包，该工具包里包含很多函数，如下图：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/LF-TB-files.png"></p><p>在此，鄙人把一些比较常用的函数及文档用红色的框标注出来，其中PDF文档是该工具包的说明书。这个说明书中详细地介绍了该工具包的使用方法，我们完全可以根据该文档的介绍来实现自己想要的功能。如下是该说明书的截图：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/LF-TB.png"></p><p>该说明文档提供了各种函数用于从LFP文件中提取出自己需要的各种信息：白图像(white image)，Raw Image，对齐后的图像，以及颜色校正，频域滤波后的图像等。<br><del>因为时间不足没有整理的，感觉大家都对这个过程比较感兴趣，鄙人觉得有必要写一下到底如何读取lfp、lfr、raw文件了。好了言归正传，开写。</del></p><h3 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h3><h4 id="step-1-创建自己的工作目录"><a href="#step-1-创建自己的工作目录" class="headerlink" title="step 1: 创建自己的工作目录"></a>step 1: 创建自己的工作目录</h4><p><u>如果是直接clone鄙人在github上的<a href="https://github.com/Vincentqyw/Light_Field_TB" target="_blank" rel="noopener">工程</a>的话直接跳转<strong>step 2</strong></u>。如果没有，那就要建立自己的工作目录，便于文件的管理。这一步是必要的，如果建立的目录不一致，可能导致程序无法运行，这也是鄙人当时初次用这个工具包时常常出错的地方。好了，建立这样的目录结构：<br><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/folder-structure-raw.png"></p><h4 id="step-2-根据相机序列号修改文件名"><a href="#step-2-根据相机序列号修改文件名" class="headerlink" title="step 2: 根据相机序列号修改文件名"></a>step 2: 根据相机序列号修改文件名</h4><p><strong><font color="red">Sample_test</font></strong>表示我们的测试目录，里面包含了相机信息以及自己拍摄照片的图像（lfp/lfr）。<br><strong><font color="red">Cameras</font></strong> 这个目录中又包含了几个文件夹，它们分别是以“A”或者“B”开头，在其后面有一长串数字。这其实就是光场相机的serial number，我们可以从默认目录<u> c:\Users\<username>\AppData\Local\Lytro\cameras\sn- serial_number</username></u>中找到，这个数字每个相机不一样，大家根据自己的相机序列号修改这个目录哈。”A”表示的是LYTRO系列相机，“B”表示ILLUM系列相机；以上图为例，”A303134427”就是我相机的序列号。</p><h4 id="step-3-把白图像文件拷贝到相应的文件夹下"><a href="#step-3-把白图像文件拷贝到相应的文件夹下" class="headerlink" title="step 3: 把白图像文件拷贝到相应的文件夹下"></a>step 3: 把白图像文件拷贝到相应的文件夹下</h4><p>在每个序列号文件夹下又有一个文件夹<strong><font color="red">WhiteImages</font></strong>，这里面放着由该相机拍摄的白图像。所谓白图像就是一张由光场相机拍摄的白色的图像，当然自己也可以拿着光场相机对着白色的墙面拍几张，但是效果并不一定很好。庆幸的是LYTRO官方提供了白图像，以Lytro为例，我们可以从以下目录找到:<u> c:\Users\ <strong>username</strong>\AppData\Local\Lytro\cameras\sn- serial_number</u>。如下图所示：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/white-image-files-folders.png"></p><p>我们发现这里有以下4个文件：<strong>data.C.0/1/2/3</strong>，这是官方把白图像压缩成了这种格式，此时需要用工具箱进行解码。我们需要的正是这四个文件，拷贝出这4个文件，放在<strong><font color="red">WhiteImages</font></strong>文件夹里。这一步相当关键，一定要确保拷贝对了目录。<strong><font color="red">注意</font></strong>，Illum相机的白图像与Lytro的白图像的存放位置不一样，在<a href="#Whiteimage-Illum">相机的SD</a>卡里。</p><h4 id="step-4-将测试文件放到Images目录"><a href="#step-4-将测试文件放到Images目录" class="headerlink" title="step 4: 将测试文件放到Images目录"></a>step 4: 将测试文件放到Images目录</h4><p><strong><font color="red">Images</font></strong>文件夹下包含我们需要处理的文件们，<strong><font color="red">F01</font></strong>下存放LYTRO系列拍摄的文件，<strong><font color="red">B01</font></strong>下存放ILLUM系列拍摄的文件。以Lytro为例，由于前面已经有了测试文件<strong><font color="red">raw.lfp</font></strong>，我们就把这个文件放在<strong><font color="red">F01</font></strong>下。经过我们上述的过程之后，最后我们的目录会变成这样（注意：<u>Sample_test与LFToolBox0.4为同级目录，各个文件夹的名字务必正确</u>）：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/folder-structure.png"></p><h3 id="测试开始"><a href="#测试开始" class="headerlink" title="测试开始"></a>测试开始</h3><p>鄙人用的是实验室的电脑，配置是：<strong>Intel(R) Core(TM) i7-4790 CPU  @3.60GHz  3.60GHz RAM 16GB</strong>。其中的Demo文件是本人编写的一小段测试代码，已经贴在了文末。接下来的过程就是RUN CODE了。程序大致可以分为以下几个测试：</p><ol><li>处理白图像</li><li>解码LFP文件</li><li>频域滤波</li><li>颜色校正</li></ol><h4 id="处理白图像"><a href="#处理白图像" class="headerlink" title="处理白图像"></a>处理白图像</h4><p>处理白图像的目的是得到相机的某些参数，鄙人当时是为了获得每幅光场的中心点坐标才进行的这一步。白图像拍摄的场景没有纹理，此时可以清楚的观察到微透镜成像的边界信息。如下图所示：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/white-image-macro-pixels.png"></p><p>可以看到的是，微透镜下成像是这种正六边形的网格，类似于蜂窝的结构，感觉很酷有木有。需要注意的是，该过程不是简单地提取出一张白图像来，而是提取几十张白图像对（image pairs），这个过程运行起来有点久，以下是运行的截图：</p><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/running-process.png"></p><h4 id="解码LFP文件"><a href="#解码LFP文件" class="headerlink" title="解码LFP文件"></a>解码LFP文件</h4><p>如果只是单纯地读出LFP/LFR、RAW文件的数据的话可以分别用工具包提供的如下函数：LFReadLFP、LFReadRaw。注意两个函数的返回值不一样。LFReadLFP返回一个结构体类型的变量，它包含相机的各个信息，我们可以根据自己的需要保留数据。LFReadRaw返回的是一张uint16的灰度图，还没有经过demosaicing操作。去马赛克操作在malatb中有相应的函数，这点不用担心。我们在这里不是直接调用的LFReadLFP而是调用了工具箱提供的LFLytroDecodeImage函数。如果运行有问题（<u>若是直接clone鄙人github上项目的话，不需要修改</u>），将LFLytroDecodeImage中的WhiteImageDatabase路径由以下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DecodeOptions = LFDefaultField( <span class="string">'DecodeOptions'</span>, <span class="string">'WhiteImageDatabasePath'</span>...</span><br><span class="line">,fullfile(<span class="string">'Cameras'</span>,<span class="string">'WhiteImageDatabase.mat'</span>));<span class="comment">% line 71</span></span><br></pre></td></tr></table></figure><p>改为：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DecodeOptions = LFDefaultField( <span class="string">'DecodeOptions'</span>, <span class="string">'WhiteImageDatabasePath'</span>...</span><br><span class="line"> ,fullfile(<span class="string">'Cameras'</span>,LFMetadata.SerialData.camera.serialNumber,<span class="string">'WhiteImageDatabase.mat'</span>));</span><br><span class="line"><span class="comment">%== 注意，这条插在  ---Select appropriate white image---上行，而不是在原来的71行修改==。</span></span><br></pre></td></tr></table></figure><p>经过这样的修改之后，这下应该可以跑了。我们可以得到以下图像：</p><center><img width="80%" data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/deer.jpg"></center><p>局部放大效果图：</p><center><img width="80%" data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/deer-zoom-in.png"></center><p>所有视角的图像：</p><center><img width="80%" data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/all-views-raw.png"></center><p>这时候可以看到在边界视角上的图像比较黑，所以我们接下来要进行频域滤波，以及颜色校正。</p><h4 id="频域滤波以及颜色校正"><a href="#频域滤波以及颜色校正" class="headerlink" title="频域滤波以及颜色校正"></a>频域滤波以及颜色校正</h4><p>这部分分别用到了LFFilt4DFFT以及LFColourCorrect函数。以LYTRO 1.0 为例子，我们得到的光场图像一种有11*11个视角，但是这个121个视角子孔径图像的质量真的不敢恭维，尤其是边角处的视角(u=1,v=1)时，这个图像时完全变成黑色的。所以嘛，LFFilt4DFFT这个函数是将这些变成黑色的图像或者质量不好的图像进行校正的，具体原理不作展开。LFColourCorrect这个函数是利用gamma变化对原始图像进行颜色校正的，这一点比较简单。总之利用这两个函数能够让我们得到的光场图像的质量更好，当然你也可以选择不用。</p><p>以下是经过滤波之后的所有子孔径图像，可以发现边界的视角相比于频域滤波之前有了很好的可视性。</p><center><img width="80%" data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/all-views-freq-correction.png"></center><p>以下是经过颜色校正之后的所有所有子孔径图像。</p><center><img width="80%" data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/all-views-freq-color-correction.png"></center><p>经过以上的步骤我们可以学习到白图像的处理，以及光场图像的处理等操作。当然鄙人没有列出这个工具包所有的功能介绍，大家可以根据需要建立自己工程，对自己的数据进行测试，以上！</p><h2 id="注意事项及测试代码"><a href="#注意事项及测试代码" class="headerlink" title="注意事项及测试代码"></a><strong><font color="red">注意事项及测试代码</font></strong></h2><h3 id="参数设置好了再Run"><a href="#参数设置好了再Run" class="headerlink" title="参数设置好了再Run"></a><span id="Whiteimage-Illum">参数设置好了再Run</span></h3><p>不少同学是因为设置不当，导致运行错误，以下鄙人列举了可能出现错误的地方。</p><ul><li>务必在WhiteImagesPath处写明相机型号，确定好到底是Lytro还是Illum</li><li>注意Illum相机的配对数据在相机的SD卡中，解压<code>caldata-Bxxxxxxxxx.tar</code>，将里面的文件拷贝出来放在路径<strong>Sample_test\Cameras\Bxxxxxxxxx\</strong>下即可</li><li>白图像的处理过程比较久，耐心等待就行即可</li><li>Lytro与Illum的频域滤波调用函数是不同的，鄙人已经把代码添加在了相应位置；这个函数用时较久，耐心等待</li><li>结果存放在<strong>Results_saving</strong>文件夹下</li><li>再次提醒，由于Illum图像的分辨率比较大，所以当程序运行到LFLytroDecodeImage以及频域滤波时会造成内存以及磁盘的大量使用，慎重考虑。</li><li>如有Bug请及时联系鄙人，请在评论区留言。</li></ul><h3 id="没有图像文件怎么搞"><a href="#没有图像文件怎么搞" class="headerlink" title="没有图像文件怎么搞"></a>没有图像文件怎么搞</h3><ol><li>下载整个<a href="https://github.com/Vincentqyw/light-field-TB" target="_blank" rel="noopener">工程</a>；</li><li>下载数据集：可以在<a href="https://www.irisa.fr/temics/demos/lightField/index.html" target="_blank" rel="noopener">这里</a>下载Lytro数据集，该数据集包括白图像以及图像原文件；</li><li>然后将工程<code>Sample_test/Cameras/</code>下的文件<code>Axxxxxxxxxxx</code>修改为<code>A303134643</code>，然后将数据集<code>LytroDataset\sn-A303134643</code>文件夹下的<code>data.C.0.1/2/3</code>放在<code>Sample_test/Cameras/A303134643</code>文件夹下；</li><li>将<code>LytroDataset\raws</code>文件夹下的图像原文件放在工程<code>Sample_test/Images/A01/</code>文件夹下；</li><li>修改<code>Demo.m</code>中<code>WhiteImagesPath=&#39;Sample_test\Cameras\Axxxxxxxxxx&#39;</code>，以及<code>lfpname=&#39;test&#39;</code>改成步骤4中的任何一个图像原文件即可。</li><li>run起来吧~</li></ol><h3 id="解码效果为何不佳"><a href="#解码效果为何不佳" class="headerlink" title="解码效果为何不佳"></a>解码效果为何不佳</h3><p>另外，很多童鞋问过鄙人一些问题，例如<strong>为何光场工具包解码出来的图像质量如此之差，始终达不到Lytro Desktop导出图像的质量</strong>。其实该问题是个普遍现象，目前没有一个好的解决方法。光场工具包的发明者<strong>Donald Dansereau</strong>在<a href="https://plus.google.com/communities/114934462920613225440" target="_blank" rel="noopener">Google Plus</a>也是这么认为的，鄙人把原话附在下面：</p><div class="note ">            <p>Q from email: there are differences between the toolbox decoded output and the Lytro Desktop’s image. The differences involve color, intensity and noise. How can I fix this?</p><p>A: Thanks for the email. There will always be important differences between the Lytro software output and the toolbox output. The toolbox tries to generate a 4D light field that is as close as possible to the raw image measured by the camera, while still being a standard two-plane parameterized 4D light field. The Lytro software has a very different goal. They do not produce a 4D light field, they produce 2D renders. These are optimized to look nice, and evidently look much nicer than any 2D slice taken from the toolbox output. They use sophisticated decoding and denoising techniques to do this.<br>The philosophy of the toolbox is to provide a 4D light field close to the raw image captured by the camera, to allow researchers to explore the characteristics of this kind of signal. It should, in theory, be possible to go from the 4D light field output by the toolbox to nice 2D renderings like those produced by the Lytro software. Making nice 2D renderings can also be accomplished by working directly from the lenslet image, as has been demonstrated in a few papers by researchers at Lytro and elsewhere.</p>          </div><p>从上面的回复可以看到，他提供的<strong>光场工具包的目的是尽量提供光场相机采集的原汁原味的数据（raw data）用来逼近4D光场信息</strong>；然后让研究者根据数据恢复出自己想要的信息。但是Lytro公司提供<strong>Lytro desktop的目的是渲染出漂亮的2D图像，以供用户使用</strong>。这也反映了研究和商业区别，Lytro并未提供他们渲染的方式，属内部机密。</p><p>此外鄙人也单独问了Donald Dansereau同样的问题，作者的建议原话祭在下面：<br><div class="note success">            <p>If you want to make nice 2D images, I suggest</p><p><strong>Filter before colour correction</strong>.  Try a simple planar focus filter (LFFiltShiftSum, or one of the linear filters in the LFDemoBasicFilt* examples).</p><p><strong>Don’t use the toolbox colour correction</strong>, the code is extremely simplistic and is mostly meant to show you where the metadata is.<br>Look into some <strong>4D - to - 2D rendering techniques</strong> for light fields.</p><p>Look into some <strong>2D denoising techniques</strong> and apply them to your 2D render, or to the 4D light field slices.</p><p>If you do find something that works well for you please share, as this is a common question.</p>          </div></p><p>鄙人习惯的参数设置如下：颜色校正的参数设为<strong>Gamma=0.8或者小于0.8</strong>（这个参数你可以不停地试，满意即可）。另外，鄙人一般不用上述代码里的频域滤波，因为鄙人认为这步会很大程度地破坏原始数据。鄙人会选用中心视角邻域的几个视角，例如原来ILLUM提供的是15x15个视角，但是我们可以选用其中的11x11或者更少的视角，这样就可以不用考虑边界视角黑暗的问题了。当然大家可以尝试按照Donald Dansereau的说法进行尝试，如果各位有好的方法，也可以告诉鄙人。（PS：以上为回复@lixiaohao同学邮件部分内容）</p><h3 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h3><p>以下是Demo文件的代码，仅供学习使用。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line">clc;</span><br><span class="line">clear all;</span><br><span class="line">clc;</span><br><span class="line"></span><br><span class="line">addpath(genpath(<span class="string">'Sample_test'</span>));</span><br><span class="line">addpath(genpath(<span class="string">'LFToolbox0.4'</span>));</span><br><span class="line"></span><br><span class="line">LFMatlabPathSetup;</span><br><span class="line"></span><br><span class="line"><span class="comment">%% Step1: 解压data.C.0/1/2/3---&gt;white,结果存储在Cameras中</span></span><br><span class="line">fprintf(<span class="string">'===============Step1: Unpack Lytro Files===============\n\n '</span>);</span><br><span class="line">LFUtilUnpackLytroArchive(<span class="string">'Sample_test'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">%% Step2: 包含刚刚解压出来的文件的目录</span></span><br><span class="line"></span><br><span class="line">fprintf(<span class="string">'===============Step2: Process WhiteImages===============\n\n'</span>);</span><br><span class="line"></span><br><span class="line">WhiteImagesPath=<span class="string">'Sample_test\Cameras\B5151500510'</span>; <span class="comment">% 务必要设置这个值 B5151500510   A303134427</span></span><br><span class="line">LFUtilProcessWhiteImages( WhiteImagesPath);</span><br><span class="line"></span><br><span class="line"><span class="comment">%% Step3: 解码光场图像.lfp</span></span><br><span class="line">fprintf(<span class="string">'=====================Step3: Decode LFP===================\n\n'</span>);</span><br><span class="line">cd(<span class="string">'Sample_test'</span>);  <span class="comment">% 进入Sample_test目录</span></span><br><span class="line"></span><br><span class="line">lfpname=<span class="string">'baby'</span>; <span class="comment">%测试图像名称，改成你自己的</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> WhiteImagesPath(<span class="number">21</span>)==<span class="string">'A'</span>    <span class="comment">%找到型号  exist('LYTRO','var')</span></span><br><span class="line">    version=<span class="string">'F01'</span>;</span><br><span class="line"><span class="keyword">elseif</span> WhiteImagesPath(<span class="number">21</span>)==<span class="string">'B'</span><span class="comment">%找到型号  exist('ILLUM','var')</span></span><br><span class="line">    version=<span class="string">'B01'</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">InputFname=[<span class="string">'Images\',version,'</span>\<span class="string">',lfpname,'</span>.lfp'];</span><br><span class="line"></span><br><span class="line">[LF, LFMetadata,WhiteImage,CorrectedLensletImage, ...</span><br><span class="line">WhiteImageMetadata, LensletGridModel, DecodeOptions]...</span><br><span class="line">                              =  LFLytroDecodeImage(InputFname);</span><br><span class="line">cd(<span class="string">'..'</span>);</span><br><span class="line"></span><br><span class="line">imshow(CorrectedLensletImage) <span class="comment">%Raw Image</span></span><br><span class="line">mkdir([<span class="string">'Results_saving\',lfpname]);</span></span><br><span class="line"><span class="string">imwrite(CorrectedLensletImage,['</span>Results_saving\<span class="string">',lfpname,'</span>\<span class="string">',lfpname,'</span>.bmp']);</span><br><span class="line">save([<span class="string">'Results_saving\',lfpname,'</span>\<span class="string">',lfpname,'</span>.mat'],<span class="string">'CorrectedLensletImage'</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%% =======================频域滤波================================</span></span><br><span class="line"><span class="comment">%---Setup for linear filters---</span></span><br><span class="line">tic</span><br><span class="line"><span class="comment">% lytro</span></span><br><span class="line"><span class="keyword">if</span> strcmp(version,<span class="string">'F01'</span>)==<span class="number">1</span></span><br><span class="line">    LFPaddedSize = [<span class="number">16</span>, <span class="number">16</span>, <span class="number">400</span>, <span class="number">400</span>];</span><br><span class="line">    BW = <span class="number">0.03</span>;</span><br><span class="line">    FiltOptions = [];</span><br><span class="line">    FiltOptions.Rolloff = <span class="string">'Butter'</span>;</span><br><span class="line">    Slope1 = <span class="number">-3</span>/<span class="number">9</span>; <span class="comment">% Lorikeet</span></span><br><span class="line">    Slope2 = <span class="number">4</span>/<span class="number">9</span>;  <span class="comment">% Building</span></span><br><span class="line">    fprintf(<span class="string">'Building 4D frequency hyperfan... '</span>);</span><br><span class="line">    [H, FiltOptionsOut] = LFBuild4DFreqHyperfan( LFPaddedSize, Slope1, Slope2, BW, FiltOptions );</span><br><span class="line">    fprintf(<span class="string">'Applying filter'</span>);</span><br><span class="line">    [LFFilt, FiltOptionsOut] = LFFilt4DFFT( LF, H, FiltOptionsOut );</span><br><span class="line"></span><br><span class="line"><span class="comment">% illum</span></span><br><span class="line"><span class="keyword">elseif</span> strcmp(version,<span class="string">'B01'</span>)==<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    LFSize = <span class="built_in">size</span>(LF);</span><br><span class="line">    LFPaddedSize = LFSize;</span><br><span class="line">    BW = <span class="number">0.04</span>;</span><br><span class="line">    FiltOptions = [];</span><br><span class="line">    <span class="comment">%---Demonstrate 4D Hyperfan filter---</span></span><br><span class="line">    Slope1 = <span class="number">-4</span>/<span class="number">15</span>; <span class="comment">% Lorikeet</span></span><br><span class="line">    Slope2 = <span class="number">15</span>/<span class="number">15</span>; <span class="comment">% Far background</span></span><br><span class="line">    fprintf(<span class="string">'Building 4D frequency hyperfan... '</span>);</span><br><span class="line">    [H, FiltOptionsOut] = LFBuild4DFreqHyperfan( LFPaddedSize, Slope1, Slope2, BW, FiltOptions );</span><br><span class="line">    fprintf(<span class="string">'Applying filter'</span>);</span><br><span class="line">    [LFFilt, FiltOptionsOut] = LFFilt4DFFT( LF, H, FiltOptionsOut );</span><br><span class="line">    title(sprintf(<span class="string">'Frequency hyperfan filter, slopes %.3g, %.3g, BW %.3g'</span>, Slope1, Slope2, BW));</span><br><span class="line">    drawnow</span><br><span class="line">    save([<span class="string">'Results_saving\',lfpname,'</span>\<span class="string">',lfpname,'</span><span class="number">5</span>D.mat'],<span class="string">'LFFilt'</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% =======================颜色校正参数设置==========================                               </span></span><br><span class="line"></span><br><span class="line">ColMatrix = DecodeOptions.ColourMatrix;</span><br><span class="line">Gamma=DecodeOptions.Gamma;</span><br><span class="line">ColBalance=DecodeOptions.ColourBalance;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 对3280*3280的原始彩色图像进行颜色校正</span></span><br><span class="line">ColorCorrectedImage=LFColourCorrect(CorrectedLensletImage, ColMatrix, ColBalance, Gamma);</span><br><span class="line">imwrite(ColorCorrectedImage,[<span class="string">'Results_saving\',lfpname,'</span>\<span class="string">',lfpname,'</span>ColorCorrectedImage.bmp']);</span><br><span class="line">save([<span class="string">'Results_saving\',lfpname,'</span>\<span class="string">',lfpname,'</span>ColorCorrectedImage.mat'],<span class="string">'ColorCorrectedImage'</span>)</span><br><span class="line">imshow(ColorCorrectedImage);title(<span class="string">'Corrected Lenslet Image'</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 同样是颜色矫正， 为了得到光场数据。得到5-D LFColorCorrectedImage数据</span></span><br><span class="line">LFColorCorrectedImage=<span class="built_in">zeros</span>(<span class="built_in">size</span>(LF,<span class="number">1</span>),<span class="built_in">size</span>(LF,<span class="number">2</span>),<span class="built_in">size</span>(LF,<span class="number">3</span>),<span class="built_in">size</span>(LF,<span class="number">4</span>),<span class="built_in">size</span>(LF,<span class="number">5</span>));</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="built_in">size</span>(LF,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span>=<span class="number">1</span>:<span class="built_in">size</span>(LF,<span class="number">2</span>)</span><br><span class="line">        temp =<span class="built_in">squeeze</span>(LFFilt(<span class="built_in">i</span>,<span class="built_in">j</span>,:,:,<span class="number">1</span>:<span class="number">3</span>));</span><br><span class="line">        temp = LFColourCorrect(temp, ColMatrix, ColBalance, Gamma);</span><br><span class="line">        LFColorCorrectedImage(<span class="built_in">i</span>,<span class="built_in">j</span>,:,:,<span class="number">1</span>:<span class="number">3</span>)=temp;</span><br><span class="line">        imshow(temp);</span><br><span class="line">        pause(<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">LFColorCorrectedImage(:,:,:,:,<span class="number">4</span>)=LF(:,:,:,:,<span class="number">4</span>);</span><br><span class="line">save([<span class="string">'Results_saving\',lfpname,'</span>\<span class="string">',lfpname,'</span>RawLFColorCorrectedImage.mat'],<span class="string">'LFColorCorrectedImage'</span>);<span class="comment">% very important</span></span><br><span class="line"></span><br><span class="line">toc</span><br><span class="line"><span class="comment">%--------------------------------------------------------------------------------</span></span><br></pre></td></tr></table></figure><hr><p><del>## Lytro官网可视化工具</del></p><p><del>谁让人家Lytro不开源呢，人家自己做的Demo还不错。通过鼠标就可以对以下图像进行<strong>重聚焦，变化视角，以及缩放</strong>等操作。话不多说，上图！</del> 呵，人家公司在2017年11月30号之后停止了对lytro live photo的线上支持，所以以下啥都没有了。<br><!--<center><iframe width='600' height='434' src='https://pictures.lytro.com/89268543555/pictures/1083179/embed' frameborder='0' allowfullscreen scrolling='no'></iframe></center> <center><iframe width='600' height='434' src='https://pictures.lytro.com/michaelsternoffphoto/pictures/1030057/embed' frameborder='0' allowfullscreen scrolling='no'></iframe></center> <center><iframe width='600' height='434' src='https://pictures.lytro.com/karinaguillenphoto/pictures/926574/embed' frameborder='0' allowfullscreen scrolling='no'></iframe></center> <center><iframe width='600' height='434' src='https://pictures.lytro.com/karinaguillenphoto/pictures/1004450/embed' frameborder='0' allowfullscreen scrolling='no'></iframe></center>更多图像在[这里](https://pictures.lytro.com/)。--></p><h3 id="数字重聚焦"><a href="#数字重聚焦" class="headerlink" title="数字重聚焦"></a>数字重聚焦</h3><p>利用如下的重聚焦公式可以实现光场图像的重聚焦。</p><script type="math/tex; mode=display">L_{\alpha}(x,y,u,v)=L_0(x+(1-\frac{1}{\alpha})u,y+(1-\frac{1}{\alpha})v,u,v)</script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/concatImg.png"><br>左图为其中心视角图像，右图为重聚焦（参数 $\alpha$ =0.5）之后的图像。</p><p>给出部分测试代码如下，全部代码见<strong><a href="https://github.com/Vincentqyw/Light-Field-Refocusing" target="_blank" rel="noopener">Github</a></strong>;</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% This is a demo of light field refocusing</span></span><br><span class="line"><span class="comment">% input: LF </span></span><br><span class="line"><span class="comment">% ouput: refocused pinhole image</span></span><br><span class="line"><span class="comment">% Writen by: Vincent Qin</span></span><br><span class="line"><span class="comment">% Data: 2018 May 17th 21:35:14</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% Note: the input is 5D LF data  decoded from Matlab Light field Toolbox</span></span><br><span class="line"></span><br><span class="line">clc;</span><br><span class="line"></span><br><span class="line">addpath(genpath(pwd));</span><br><span class="line"><span class="comment">%% mex function</span></span><br><span class="line">cd(<span class="string">'src'</span>); </span><br><span class="line">mex REMAP2REFOCUS_mex.c </span><br><span class="line">mex BLOCKMEAN_mex.c </span><br><span class="line">cd ..</span><br><span class="line"></span><br><span class="line">use_vincent_data=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> use_vincent_data</span><br><span class="line">    <span class="built_in">disp</span>(<span class="string">'Downloading LF data, please wait...'</span>);</span><br><span class="line"></span><br><span class="line">    URL=<span class="string">'http://p8vl2tjcq.bkt.clouddn.com/LF.mat'</span>;</span><br><span class="line">    [f, status] = urlwrite(URL,<span class="string">'input/LF_web.mat'</span>);</span><br><span class="line">    <span class="keyword">if</span> status == <span class="number">1</span>;</span><br><span class="line">        fprintf(<span class="number">1</span>,<span class="string">'Success！\n'</span>);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        fprintf(<span class="number">1</span>,<span class="string">'Failed！\n'</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    load(<span class="string">'input/LF_web.mat'</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    load(<span class="string">'input/your_LF_data.mat'</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">disp</span>(<span class="string">'Processing LF to Remap image...'</span>);</span><br><span class="line"></span><br><span class="line">LF=LF(:,:,:,:,<span class="number">1</span>:<span class="number">3</span>);</span><br><span class="line">[UV_diameter,~,y_size,x_size,c]=<span class="built_in">size</span>(LF);</span><br><span class="line"></span><br><span class="line"><span class="comment">%  get LF remap and pinhole image before refocusing</span></span><br><span class="line">LF_Remap = LF2Remap(LF);</span><br><span class="line"><span class="comment">% IM_Refoc_1 = zeros(y_size, x_size,3);temp = zeros(y_size, x_size);</span></span><br><span class="line"><span class="comment">% BLOCKMEAN_mex(x_size, y_size, UV_diameter, LF_Remap(:,:,1), temp);IM_Refoc_1(:,:,1)=temp;</span></span><br><span class="line"><span class="comment">% BLOCKMEAN_mex(x_size, y_size, UV_diameter, LF_Remap(:,:,2), temp);IM_Refoc_1(:,:,2)=temp;</span></span><br><span class="line"><span class="comment">% BLOCKMEAN_mex(x_size, y_size, UV_diameter, LF_Remap(:,:,3), temp);IM_Refoc_1(:,:,3)=temp;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% get params</span></span><br><span class="line">LF_x_size = x_size * UV_diameter;</span><br><span class="line">LF_y_size = y_size * UV_diameter;</span><br><span class="line">UV_radius = (UV_diameter<span class="number">-1</span>)/<span class="number">2</span>;</span><br><span class="line">UV_size   = UV_diameter*UV_diameter;</span><br><span class="line"></span><br><span class="line"><span class="comment">% collect data</span></span><br><span class="line">LF_parameters       = struct(...</span><br><span class="line">                             <span class="string">'LF_x_size'</span>,LF_x_size,...</span><br><span class="line">                             <span class="string">'LF_y_size'</span>,LF_y_size,...</span><br><span class="line">                             <span class="string">'x_size'</span>,x_size,...</span><br><span class="line">                             <span class="string">'y_size'</span>,y_size,...</span><br><span class="line">                             <span class="string">'UV_radius'</span>,(UV_diameter<span class="number">-1</span>)/<span class="number">2</span>,...</span><br><span class="line">                             <span class="string">'UV_diameter'</span>,UV_diameter,...</span><br><span class="line">                             <span class="string">'UV_size'</span>,UV_diameter*UV_diameter) ;</span><br><span class="line"></span><br><span class="line"><span class="comment">% predefine output</span></span><br><span class="line">LF_Remap_alpha   = <span class="built_in">zeros</span>(LF_y_size,LF_x_size,<span class="number">3</span>) ;</span><br><span class="line">IM_Refoc_alpha   = <span class="built_in">zeros</span>(y_size,x_size,<span class="number">3</span>)       ;</span><br><span class="line"></span><br><span class="line"><span class="comment">% here begins refocusing</span></span><br><span class="line"><span class="built_in">disp</span>(<span class="string">'Processing refocusing...'</span>);</span><br><span class="line">alpha=<span class="number">0.5</span>;    <span class="comment">%　shearing number</span></span><br><span class="line">REMAP2REFOCUS_mex(x_size,y_size,UV_diameter,UV_radius,LF_Remap,LF_Remap_alpha,IM_Refoc_alpha,alpha);  </span><br><span class="line"></span><br><span class="line"><span class="comment">% show figure</span></span><br><span class="line">central_view=<span class="built_in">squeeze</span>(LF(UV_radius+<span class="number">1</span>,UV_radius+<span class="number">1</span>,:,:,:));</span><br><span class="line"><span class="built_in">figure</span>; imshow(central_view);</span><br><span class="line">title(<span class="string">'central view'</span>);set(gcf,<span class="string">'color'</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]);  </span><br><span class="line"><span class="built_in">figure</span>; imshow(IM_Refoc_alpha);</span><br><span class="line">title([<span class="string">'refocused image pinhole at alpha = '</span> num2str(alpha)]);</span><br><span class="line">set(gcf,<span class="string">'color'</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]);  </span><br><span class="line"></span><br><span class="line"><span class="comment">% concat them</span></span><br><span class="line">concatImg=[central_view,IM_Refoc_alpha];</span><br><span class="line"><span class="built_in">figure</span>;imshow(concatImg);set(gcf,<span class="string">'color'</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]); </span><br><span class="line">imwrite(concatImg,<span class="string">'concatImg.png'</span>);</span><br></pre></td></tr></table></figure><h3 id="Lytro-Desktop"><a href="#Lytro-Desktop" class="headerlink" title="Lytro Desktop"></a>Lytro Desktop</h3><p>Lytro Desktop是曾经的Lytro官方提供的软件，可以处理由Lytro系列相机拍摄到的图像。利用该软件能够导出相机配对数据、重聚焦图像、全聚焦图、深度图、原始文件等。下面给出一些常用的导出文件。</p><ul><li>导入需要处理的图像</li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/lytro-desktop-1.jpg"></p><ul><li>导出配对数据</li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/lytro-desktop-3.jpg"></p><ul><li>导出待处理图像的各种格式</li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/lytro-desktop-2.jpg"></p><ul><li>全聚焦彩色图与内置深度图</li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/color-depth.jpg"></p><ul><li>重聚焦</li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/LightField-Toolbox/refocusing.jpg"></p><p><strong><center>以上，如有问题欢迎评论</center></strong></p>]]></content>
      
      
      <categories>
          
          <category> 光场 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 光场 </tag>
            
            <tag> 计算成像 </tag>
            
            <tag> Light Field </tag>
            
            <tag> Matlab光场工具包 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MATLAB:多个不同维度的箱线图画在一起</title>
      <link href="/posts/issues-Matlab/"/>
      <url>/posts/issues-Matlab/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-1/issues-Matlab/matlab-logo.jpg"></p><p>以下是我在使用Matlab编程时遇到的问题以及解决方法，最后彩蛋随时补充。<br><a id="more"></a></p><h2 id="多个不同维度的箱线图画在一起"><a href="#多个不同维度的箱线图画在一起" class="headerlink" title="多个不同维度的箱线图画在一起"></a>多个不同维度的箱线图画在一起</h2><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">%%  GENERATE RANDOM DATA</span></span><br><span class="line"></span><br><span class="line">dataA <span class="built_in">=</span> <span class="number">10</span> *rand(<span class="number">100</span>,<span class="number">1</span>);</span><br><span class="line"><span class="comment">% subplot(121);boxplot(dataA)</span></span><br><span class="line"></span><br><span class="line">dataB <span class="built_in">=</span> <span class="number">10</span> *rand(<span class="number">200</span>,<span class="number">1</span>) - <span class="number">5</span>*rand(<span class="number">200</span>,<span class="number">1</span>);</span><br><span class="line"><span class="comment">% subplot(122);boxplot(dataB)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% CONSTRUCT MPGs</span></span><br><span class="line"></span><br><span class="line">MPGs&#123;<span class="number">1</span>&#125; <span class="built_in">=</span> dataA;</span><br><span class="line">MPGs&#123;<span class="number">2</span>&#125; <span class="built_in">=</span> dataB;</span><br><span class="line"></span><br><span class="line">figure;</span><br><span class="line">data <span class="built_in">=</span> [MPGs&#123;<span class="number">1</span>&#125; ;MPGs&#123;<span class="number">2</span>&#125; ];</span><br><span class="line"></span><br><span class="line">s_groupA <span class="built_in">=</span> repmat(&#123;<span class="string">'item 1'</span>&#125;,numel(MPGs&#123;<span class="number">1</span>&#125;),<span class="number">1</span>);</span><br><span class="line">s_groupB <span class="built_in">=</span> repmat(&#123;<span class="string">'item 2'</span>&#125;,numel(MPGs&#123;<span class="number">2</span>&#125;),<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">group <span class="built_in">=</span> [s_groupA;s_groupB];</span><br><span class="line"></span><br><span class="line"><span class="comment">%% BEGIN DRAW</span></span><br><span class="line">boxplot(data,group);</span><br><span class="line"></span><br><span class="line">FONTSIZE <span class="built_in">=</span> <span class="number">30</span>;</span><br><span class="line">set(gcf,<span class="string">'color'</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>])</span><br><span class="line">ylabel(<span class="string">'items value'</span>);</span><br><span class="line">set(gca,<span class="string">'fontsize'</span>,FONTSIZE)</span><br><span class="line">set(gcf,<span class="string">'pos'</span>,[ <span class="number">986</span>,  <span class="number">414</span>, <span class="number">1274</span>, <span class="number">826</span>])</span><br></pre></td></tr></table></figure><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/issues-Matlab/multi-boxplot.png"></p><h2 id="Matlab绘制GIF动图"><a href="#Matlab绘制GIF动图" class="headerlink" title="Matlab绘制GIF动图"></a>Matlab绘制GIF动图</h2><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">input_file_path=<span class="string">'your image folder'</span>;</span><br><span class="line">out_name = <span class="string">'out'</span>; <span class="comment">%output name</span></span><br><span class="line"></span><br><span class="line">img_path_list=dir(strcat(input_file_path,<span class="string">'*.jpg'</span>)); <span class="comment">%image format</span></span><br><span class="line">[~, ind] = <span class="built_in">sort</span>([img_path_list(:).datenum], <span class="string">'ascend'</span>);</span><br><span class="line">img_path_list = img_path_list(ind);</span><br><span class="line">img_num=<span class="built_in">length</span>(img_path_list);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> img_num == <span class="number">0</span></span><br><span class="line">    <span class="built_in">disp</span>(<span class="string">'No images in the folder!'</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span>=<span class="number">1</span>:img_num</span><br><span class="line">        image_name = img_path_list(<span class="built_in">j</span>).name;</span><br><span class="line">        read_image = imread(strcat(input_file_path,image_name));</span><br><span class="line">        imshow(read_image,<span class="string">'border'</span>,<span class="string">'tight'</span>,<span class="string">'initialmagnification'</span>,<span class="string">'fit'</span>);</span><br><span class="line">        axis normal;</span><br><span class="line">        <span class="built_in">true</span><span class="built_in">size</span>;</span><br><span class="line"></span><br><span class="line">        n=<span class="built_in">j</span>;</span><br><span class="line">        frame(n)=getframe(gcf); <span class="comment">% get the frame</span></span><br><span class="line">        image=frame(n).cdata;</span><br><span class="line">        [image,map]     =  rgb2ind(image,<span class="number">256</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> n==<span class="number">1</span></span><br><span class="line"><span class="comment">%              imwrite(image,map,outname,'gif');</span></span><br><span class="line">             imwrite(image,map,[out_name <span class="string">'.gif'</span>],<span class="string">'gif'</span>,<span class="string">'Loopcount'</span>,<span class="built_in">inf</span>);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">             imwrite(image,map,[out_name <span class="string">'.gif'</span>],<span class="string">'WriteMode'</span>,<span class="string">'append'</span>,<span class="string">'DelayTime'</span>,<span class="number">0.2</span>);</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="Matlab-局部放大图像"><a href="#Matlab-局部放大图像" class="headerlink" title="Matlab 局部放大图像"></a>Matlab 局部放大图像</h2><p>论文里的局部放大图，再也不用每次手动截了。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">small_im</span>=<span class="title">ZoomIm_</span><span class="params">(im,pos)</span></span></span><br><span class="line"><span class="comment">% im= imread('lena.jpg');</span></span><br><span class="line"><span class="comment">% pos=[226,221,340,384];</span></span><br><span class="line"><span class="comment">% [左上X，左上Y，右下X，右下Y]</span></span><br><span class="line"></span><br><span class="line">clc</span><br><span class="line">close all;</span><br><span class="line"><span class="built_in">figure</span>;</span><br><span class="line"></span><br><span class="line">up_leftX=pos(<span class="number">1</span>);</span><br><span class="line">up_leftY=pos(<span class="number">2</span>);</span><br><span class="line">down_rightX=pos(<span class="number">3</span>);</span><br><span class="line">down_rightY=pos(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">%% mark rectangle in source image</span></span><br><span class="line">imshow(im);<span class="built_in">hold</span> on;</span><br><span class="line">set(gcf,<span class="string">'color'</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]);</span><br><span class="line">line([up_leftX, down_rightX]   ,[up_leftY   ,up_leftY],<span class="string">'linestyle'</span>,<span class="string">'-'</span>,<span class="string">'linewidth'</span>,<span class="number">3</span>,<span class="string">'color'</span>,<span class="string">'r'</span>);</span><br><span class="line">line([up_leftX, up_leftX]      ,[up_leftY   ,down_rightY],<span class="string">'linestyle'</span>,<span class="string">'-'</span>,<span class="string">'linewidth'</span>,<span class="number">3</span>,<span class="string">'color'</span>,<span class="string">'r'</span>);</span><br><span class="line">line([down_rightX, down_rightX],[up_leftY   ,down_rightY],<span class="string">'linestyle'</span>,<span class="string">'-'</span>,<span class="string">'linewidth'</span>,<span class="number">3</span>,<span class="string">'color'</span>,<span class="string">'r'</span>);</span><br><span class="line">line([up_leftX, down_rightX]   ,[down_rightY,down_rightY],<span class="string">'linestyle'</span>,<span class="string">'-'</span>,<span class="string">'linewidth'</span>,<span class="number">3</span>,<span class="string">'color'</span>,<span class="string">'r'</span>);</span><br><span class="line"><span class="built_in">hold</span> off;</span><br><span class="line"></span><br><span class="line"><span class="comment">%% get rect image</span></span><br><span class="line"><span class="built_in">figure</span>;</span><br><span class="line">small_im=im(up_leftY:down_rightY,up_leftX:down_rightX,:);</span><br><span class="line">imagesc(small_im);</span><br><span class="line">axis equal</span><br><span class="line">axis off</span><br><span class="line">set(gcf,<span class="string">'color'</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]);</span><br><span class="line">set(gca,<span class="string">'xtick'</span>,[],<span class="string">'ytick'</span>,[]);</span><br><span class="line"><span class="comment">% set(gca,'position',[0.1 0.1,0.8 0.8])</span></span><br><span class="line">set(<span class="number">0</span>,<span class="string">'DefaultFigureMenu'</span>,<span class="string">'figure'</span>);</span><br><span class="line"><span class="comment">% figure('menubar','on');</span></span><br><span class="line"><span class="comment">% set(0,'Default');</span></span><br><span class="line">set(gcf,<span class="string">'Position'</span>,[<span class="number">500</span>,<span class="number">200</span>,<span class="number">800</span>,<span class="number">500</span>])</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="Matlab-保存超高质量图像"><a href="#Matlab-保存超高质量图像" class="headerlink" title="Matlab 保存超高质量图像"></a>Matlab 保存超高质量图像</h2><p>除了直接保存成<code>eps</code>或者<code>emf</code>格式之外，也可直接在绘制完图像之后，保持当前绘图窗口不要关闭，在<strong>命令行窗口</strong>键入如下命令，然后在word文档/PPT里<code>ctrl+v</code>粘贴即可。<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uimenufcn(gcf,<span class="string">'EditCopyFigure'</span>)</span><br></pre></td></tr></table></figure></p><h2 id="Matlab-写入Excel错误"><a href="#Matlab-写入Excel错误" class="headerlink" title="Matlab 写入Excel错误"></a>Matlab 写入Excel错误</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><blockquote><p>Matlab 在创建EXCEL文件的时候总是出错，即使使用MATLAB自带的程序。 问题描述：在Matlab中使用xlswrite函数时，如果excel文件存在时，则程序能够正常运行；当excel文件不存在时，则会出现错误：</p></blockquote><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error using xlswrite (line <span class="number">220</span>) Error: 服务器出现意外情况。</span><br></pre></td></tr></table></figure><h3 id="解决之道"><a href="#解决之道" class="headerlink" title="解决之道"></a>解决之道</h3><p>xlswrite函数在调用时会占用Excel的com端口，所以要保证在调用时这个端口是开放的，也就是没有被其他程序占用。打开任意一个Excel（我的是16版）文档，点击<strong>文件</strong>—<strong>选项</strong>，弹出Excel选项卡，在<strong>加载项</strong>中可以看到，活动应用程序加载项，以及非活动应用程序加载项；<br>由于我的系统中装了一个福昕阅读器，该程序占用了Excel的com端口，所以当Matlab再去调用这个端口时就会出现异常。具体解决方法：点击管理旁边的下拉菜单，选择COM加载项，点击转到，把福昕阅读器的前面的勾去掉，然后确定。</p><p><img width="100%" data-src="https://vincentqin.gitee.io/blogresource-1/issues-Matlab/issue-matlab-1.png"><br><img width="100%" data-src="https://vincentqin.gitee.io/blogresource-1/issues-Matlab/issue-matlab-2.png"></p><h2 id="Matlab设置绘图坐标轴信息"><a href="#Matlab设置绘图坐标轴信息" class="headerlink" title="Matlab设置绘图坐标轴信息"></a>Matlab设置绘图坐标轴信息</h2><h3 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h3><blockquote><p>Matlab 作图时更改纵轴刻度为科学计数法，指数放在框左上方</p></blockquote><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">plot</span>([<span class="number">0</span> <span class="number">1</span>],[<span class="number">0</span> <span class="number">.02</span>]) <span class="comment">% 作图，换成自己的图像就可以~</span></span><br><span class="line">oldLabels = str2num(get(gca,<span class="string">'YTickLabel'</span>));</span><br><span class="line">scale = <span class="number">10</span>^<span class="number">2</span>;newLabels = num2str(oldLabels*scale);</span><br><span class="line">set(gca,<span class="string">'YTickLabel'</span>,newLabels,<span class="string">'units'</span>,<span class="string">'normalized'</span>);</span><br><span class="line">posAxes = get(gca,<span class="string">'position'</span>);</span><br><span class="line">textBox = annotation(<span class="string">'textbox'</span>,<span class="string">'linestyle'</span>,<span class="string">'none'</span>,<span class="string">'string'</span>,[<span class="string">'x 10\it^&#123;'</span> sprintf(<span class="string">'%d'</span>,<span class="built_in">log10</span>(<span class="number">1.</span>/scale)) <span class="string">'&#125;'</span>]);</span><br><span class="line">posAn = get(textBox,<span class="string">'position'</span>);</span><br><span class="line">set(textBox,<span class="string">'position'</span>,[posAxes(<span class="number">1</span>) posAxes(<span class="number">2</span>)+posAxes(<span class="number">4</span>) posAn(<span class="number">3</span>) posAn(<span class="number">4</span>)],<span class="string">'VerticalAlignment'</span>,<span class="string">'cap'</span>);</span><br></pre></td></tr></table></figure><h2 id="Matlab显示图片错误"><a href="#Matlab显示图片错误" class="headerlink" title="Matlab显示图片错误"></a>Matlab显示图片错误</h2><h3 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h3><blockquote><p>MATLAB图像显示总是白色</p></blockquote><p><code>imshow</code>是一个很强大的”武器”，显示图像简直无所不能，但这其中往往会出现问题。在处理图像时，我们的图像经常是经过了某种运算，为了保证其精度，系统会自动的将<code>uint8</code>型数据类型转化成<code>double</code>型。</p><p>如果直接运行<code>imshow(I)</code>，我们会发现显示的是一个白色的图像。这是因为imshow()显示图像时对<code>double</code>型是认为在0~1范围内，即大于1时都是显示为白色，而<code>imshow</code>显示<code>uint8</code>型时是0~255范围。而经过运算的范围在0-255之间的<code>double</code>型数据就被不正常得显示为白色图像了。</p><h3 id="解决之道-1"><a href="#解决之道-1" class="headerlink" title="解决之道"></a>解决之道</h3><ul><li>可以利用<code>mat2gray()</code>函数，这个函数是归一化函数，可以把数据归一化到0-1之间，再用<code>imshow()</code>就可以了；</li><li>或者对于一个处理后的黑白图像Img，若为double型可以这样写：<code>imshow(Img/max(Img(:)))</code>；</li><li>还有一种就是：<code>imshow(Img,[])</code>;就是加一个<code>[]</code>，即可以自动调整显示；</li></ul><h2 id="常用命令汇总"><a href="#常用命令汇总" class="headerlink" title="常用命令汇总"></a>常用命令汇总</h2><ul><li><p>（2016.05.19）今天学到一个特别简单的语句，删除元胞数组中空元素：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a(<span class="built_in">cellfun</span>(@<span class="built_in">isempty</span>,a))=[];</span><br></pre></td></tr></table></figure></li><li><p>（2018年6月1日）在命令行敲入如下命令，如果运行出现错误，matlab会自动停在出错的那行，并且保存所有相关变量。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dbstop <span class="keyword">if</span> error</span><br></pre></td></tr></table></figure></li></ul><h2 id="彩蛋"><a href="#彩蛋" class="headerlink" title="彩蛋"></a>彩蛋</h2><ul><li>大神教我们怎么画图，#MATLAB无所不能#，<a href="http://blogs.mathworks.com/graphics/" target="_blank" rel="noopener">戳戳这里</a>~</li><li>Matlab 字体困扰了我很长时间，终于在网上找到了一个比较好的组合，<a href="http://pan.baidu.com/s/1geIRi2R" target="_blank" rel="noopener">猛戳这里</a>！<a href="http://blog.csdn.net/whoispo/article/details/50383362" target="_blank" rel="noopener">原文地址</a></li><li><a href="https://vincentqin.gitee.io/blogresource-1/cv-books/matlab-cmex.pdf" target="_blank" rel="noopener">Matlab与C混合编程</a></li><li><a href="https://vincentqin.gitee.io/blogresource-1/cv-books/matlab-parallel.pdf" target="_blank" rel="noopener">Matlab并行</a></li><li><a href="https://vincentqin.gitee.io/blogresource-1/cv-books/Writing-Fast-Matlab-Code.pdf" target="_blank" rel="noopener">Matlab代码优化：教你写出漂亮的Matlab代码</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Matlab大法好 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Matlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HEXO建站备忘录</title>
      <link href="/posts/build-a-website-using-hexo/"/>
      <url>/posts/build-a-website-using-hexo/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><!-- <meta http-equiv="refresh" content="0; url=https://www.vincentqin.tech/2016/08/09/build-a-website-using-hexo"><link rel="canonical" href="https://www.vincentqin.tech/2016/08/09/build-a-website-using-hexo" /> --><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-a-website-using-hexo/hexo-cover.png"></p><div class="note success">            <p><strong>Hexo</strong>作为建立Blog利器，为我们没有JS基础的小白们提供了建立专属自己博客的机会！经常使用的语法很简单，我们完全可以在10min分钟之内建立自己的Blog，后期的优化才是最耗费时间的。好了，直接进入正文。</p>          </div><a id="more"></a><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hexo clean    # 清除缓存，简写 hexo c</span><br><span class="line">hexo generate # 作用：建立静态页面， 简写 hexo g</span><br><span class="line">hexo deploy   # 部署自己的blog，本人部署在了Git上，简写 hexo d</span><br><span class="line">hexo server   # 以启动本地服务， 可预览，简写 hexo s</span><br><span class="line">hexo new blog_name #　新建以blog_name为名的blog</span><br><span class="line">在.md文档中加入 &lt;!-- more --&gt; 可以显示“阅读全文”</span><br></pre></td></tr></table></figure><h2 id="页面重定向"><a href="#页面重定向" class="headerlink" title="页面重定向"></a>页面重定向</h2><p>由于之前网站post路径名字采用的样式为<code>www.yoursite/yy/mm/dd/post_name/</code>，后来改成了<code>www.yoursite/posts/post_name/</code>，这导致之前的几篇博客连接失败，为此进行重定向（redirect）。<br>原理就是在旧的博客每个页面的<code>&lt;head&gt;</code>部分添加两个标签：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">"refresh"</span> <span class="attr">content</span>=<span class="string">"0; url=http://new.domain.com/same/relative/url/of/old/site/"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"canonical"</span> <span class="attr">href</span>=<span class="string">"http://new.domain.com/same/relative/url/of/old/site/"</span> /&gt;</span></span><br></pre></td></tr></table></figure><p>第一个是给人看的，第二个是给机器看的。</p><p>前者会自动让浏览器跳转到新的域名，后者在搜索引擎的bot下次抓取页面的时候读取，重新索引到新的URL，这里参考了<a href="http://catx.me/2014/03/20/hexo-site-migration-theme/" target="_blank" rel="noopener">猫杀</a>的博文。</p><p>这样使得原始旧的链接：<br><a href="https://www.vincentqin.tech/2017/02/16/LightField%E5%85%89%E5%9C%BA/">https://www.vincentqin.tech/2017/02/16/LightField%E5%85%89%E5%9C%BA/</a>会跳转到新的链接：<a href="https://www.vincentqin.tech/posts/LightField-Toolbox/">https://www.vincentqin.tech/posts/LightField-Toolbox/</a>。</p><h2 id="修复HEXO引用本地图片失败"><a href="#修复HEXO引用本地图片失败" class="headerlink" title="修复HEXO引用本地图片失败"></a>修复HEXO引用本地图片失败</h2><p>修改hexo-asset-image插件，替换成如下内容。</p><figure class="highlight js"><figcaption><span>文件路径：/node_modules/hexo-asset-image/index.js</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">'use strict'</span>;</span><br><span class="line"><span class="keyword">var</span> cheerio = <span class="built_in">require</span>(<span class="string">'cheerio'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-string</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">getPosition</span>(<span class="params">str, m, i</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> str.split(m, i).join(m).length;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> version = <span class="built_in">String</span>(hexo.version).split(<span class="string">'.'</span>);</span><br><span class="line">hexo.extend.filter.register(<span class="string">'after_post_render'</span>, <span class="function"><span class="keyword">function</span>(<span class="params">data</span>)</span>&#123;</span><br><span class="line">  <span class="keyword">var</span> config = hexo.config;</span><br><span class="line">  <span class="keyword">if</span>(config.post_asset_folder)&#123;</span><br><span class="line">    <span class="keyword">var</span> link = data.permalink;</span><br><span class="line"><span class="keyword">if</span>(version.length &gt; <span class="number">0</span> &amp;&amp; <span class="built_in">Number</span>(version[<span class="number">0</span>]) == <span class="number">3</span>)</span><br><span class="line">   <span class="keyword">var</span> beginPos = getPosition(link, <span class="string">'/'</span>, <span class="number">1</span>) + <span class="number">1</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">   <span class="keyword">var</span> beginPos = getPosition(link, <span class="string">'/'</span>, <span class="number">3</span>) + <span class="number">1</span>;</span><br><span class="line"><span class="comment">// In hexo 3.1.1, the permalink of "about" page is like ".../about/index.html".</span></span><br><span class="line"><span class="keyword">var</span> endPos = link.lastIndexOf(<span class="string">'/'</span>) + <span class="number">1</span>;</span><br><span class="line">    link = link.substring(beginPos, endPos);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> toprocess = [<span class="string">'excerpt'</span>, <span class="string">'more'</span>, <span class="string">'content'</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; toprocess.length; i++)&#123;</span><br><span class="line">      <span class="keyword">var</span> key = toprocess[i];</span><br><span class="line"> </span><br><span class="line">      <span class="keyword">var</span> $ = cheerio.load(data[key], &#123;</span><br><span class="line">        ignoreWhitespace: <span class="literal">false</span>,</span><br><span class="line">        xmlMode: <span class="literal">false</span>,</span><br><span class="line">        lowerCaseTags: <span class="literal">false</span>,</span><br><span class="line">        decodeEntities: <span class="literal">false</span></span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">      $(<span class="string">'img'</span>).each(<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line"><span class="keyword">if</span> ($(<span class="keyword">this</span>).attr(<span class="string">'src'</span>))&#123;</span><br><span class="line"><span class="comment">// For windows style path, we replace '\' to '/'.</span></span><br><span class="line"><span class="keyword">var</span> src = $(<span class="keyword">this</span>).attr(<span class="string">'src'</span>).replace(<span class="string">'\\'</span>, <span class="string">'/'</span>);</span><br><span class="line"><span class="keyword">if</span>(!<span class="regexp">/http[s]*.*|\/\/.*/</span>.test(src) &amp;&amp;</span><br><span class="line">   !<span class="regexp">/^\s*\//</span>.test(src)) &#123;</span><br><span class="line">  <span class="comment">// For "about" page, the first part of "src" can't be removed.</span></span><br><span class="line">  <span class="comment">// In addition, to support multi-level local directory.</span></span><br><span class="line">  <span class="keyword">var</span> linkArray = link.split(<span class="string">'/'</span>).filter(<span class="function"><span class="keyword">function</span>(<span class="params">elem</span>)</span>&#123;</span><br><span class="line"><span class="keyword">return</span> elem != <span class="string">''</span>;</span><br><span class="line">  &#125;);</span><br><span class="line">  <span class="keyword">var</span> srcArray = src.split(<span class="string">'/'</span>).filter(<span class="function"><span class="keyword">function</span>(<span class="params">elem</span>)</span>&#123;</span><br><span class="line"><span class="keyword">return</span> elem != <span class="string">''</span> &amp;&amp; elem != <span class="string">'.'</span>;</span><br><span class="line">  &#125;);</span><br><span class="line">  <span class="keyword">if</span>(srcArray.length &gt; <span class="number">1</span>)</span><br><span class="line">srcArray.shift();</span><br><span class="line">  src = srcArray.join(<span class="string">'/'</span>);</span><br><span class="line">  $(<span class="keyword">this</span>).attr(<span class="string">'src'</span>, config.root + link + src);</span><br><span class="line">  <span class="built_in">console</span>.info&amp;&amp;<span class="built_in">console</span>.info(<span class="string">"update link as:--&gt;"</span>+config.root + link + src);</span><br><span class="line">&#125;</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"><span class="built_in">console</span>.info&amp;&amp;<span class="built_in">console</span>.info(<span class="string">"no src attr, skipped..."</span>);</span><br><span class="line"><span class="built_in">console</span>.info&amp;&amp;<span class="built_in">console</span>.info($(<span class="keyword">this</span>));</span><br><span class="line">&#125;</span><br><span class="line">      &#125;);</span><br><span class="line">      data[key] = $.html();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>同时打开站点配置文件<code>_config.yml</code>:<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">post_asset_folder:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><h2 id="插入PDF文档"><a href="#插入PDF文档" class="headerlink" title="插入PDF文档"></a>插入PDF文档</h2><p>官方提供了PDF文档预览的插件，<a href="https://github.com/theme-next/theme-next-pdf" target="_blank" rel="noopener">地址</a>。键入如下命令，即可：</p><h3 id="Step-1-→-Go-to-NexT-dir"><a href="#Step-1-→-Go-to-NexT-dir" class="headerlink" title="Step 1 → Go to NexT dir"></a>Step 1 → Go to NexT dir</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> themes/next</span><br></pre></td></tr></table></figure><h3 id="Step-2-→-Get-module"><a href="#Step-2-→-Get-module" class="headerlink" title="Step 2 → Get module"></a>Step 2 → Get module</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/theme-next-pdf <span class="built_in">source</span>/lib/pdf</span><br></pre></td></tr></table></figure><h3 id="Step-3-→-Set-it-up"><a href="#Step-3-→-Set-it-up" class="headerlink" title="Step 3 → Set it up"></a>Step 3 → Set it up</h3><p>修改主题配置文件<code>_config.yml</code>:</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PDF tag, requires two plugins: pdfObject and pdf.js</span></span><br><span class="line"><span class="comment"># pdfObject will try to load pdf files natively, if failed, pdf.js will be used.</span></span><br><span class="line"><span class="comment"># The following `cdn` setting is only for pdfObject, because cdn for pdf.js might be blocked by CORS policy.</span></span><br><span class="line"><span class="comment"># So, YOU MUST install the dependency of pdf.js if you want to use pdf tag and make it work on all browsers.</span></span><br><span class="line"><span class="comment"># See: https://github.com/theme-next/theme-next-pdf</span></span><br><span class="line"><span class="attr">pdf:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Default height</span></span><br><span class="line"><span class="attr">  height:</span> <span class="number">500</span><span class="string">px</span></span><br><span class="line"><span class="attr">  pdfobject:</span></span><br><span class="line">    <span class="comment"># Use 2.1.1 as default, jsdelivr as default CDN, works everywhere even in China</span></span><br><span class="line"><span class="attr">    cdn:</span> <span class="string">//cdn.jsdelivr.net/npm/pdfobject@2.1.1/pdfobject.min.js</span></span><br><span class="line">    <span class="comment"># CDNJS, provided by cloudflare, maybe the best CDN, but not works in China</span></span><br><span class="line"><span class="comment">#cdn: //cdnjs.cloudflare.com/ajax/libs/pdfobject/2.1.1/pdfobject.min.js</span></span><br></pre></td></tr></table></figure><h3 id="Usage"><a href="#Usage" class="headerlink" title="Usage:"></a>Usage:</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% pdf /path/to/your/file.pdf %&#125;</span><br></pre></td></tr></table></figure><h2 id="根据图像得到Latex公式"><a href="#根据图像得到Latex公式" class="headerlink" title="根据图像得到Latex公式"></a>根据图像得到Latex公式</h2><p>见官网<a href="https://mathpix.com/" target="_blank" rel="noopener">点击这里</a>, <a href="https://mathpix.com/blog/windows-beta" target="_blank" rel="noopener">blog</a>,快捷键<code>Ctrl+Alt+M</code>。</p><p><img alt data-src="https://mathpix.com/images/mathpix-page/snip_mac.png"></p><h2 id="个性化返回顶部"><a href="#个性化返回顶部" class="headerlink" title="个性化返回顶部"></a>个性化返回顶部</h2><p>将 back-to-top 按钮添加图片背景，并添加 CSS3 动效即可。首先，找到自己喜欢的图片素材放到 <code>themes\next\source\images\</code>目录下。<br><figure class="highlight styl"><figcaption><span>文件路径：themes\next\source\css\_custom\custom.styl</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//自定义回到顶部样式</span></span><br><span class="line"><span class="selector-class">.back-to-top</span> &#123;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">49px</span>;  <span class="comment">//图片素材宽度</span></span><br><span class="line">  <span class="attribute">height</span>: <span class="number">49px</span>;  <span class="comment">//图片素材高度</span></span><br><span class="line">  <span class="attribute">bottom</span>: unset;</span><br><span class="line">  transition: all .5s ease-in-out;</span><br><span class="line">  <span class="attribute">background</span>: url(<span class="string">"/images/scroll0.png"</span>);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//隐藏箭头图标</span></span><br><span class="line">  &gt; <span class="selector-tag">i</span> &#123;</span><br><span class="line">    <span class="attribute">display</span>: none;</span><br><span class="line">  &#125;</span><br><span class="line">  &amp;<span class="selector-class">.back-to-top-on</span> &#123;</span><br><span class="line">    <span class="comment">//bottom: unset;</span></span><br><span class="line">    <span class="comment">//top: 100vh &lt; (900px + 200px) ? calc( 100vh - 900px - 200px ) : 0px;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="新增在线聊天tidio"><a href="#新增在线聊天tidio" class="headerlink" title="新增在线聊天tidio"></a>新增在线聊天tidio</h2><p>懒得写了，参考这个博主的<a href="http://yearito.cn/posts/hexo-advanced-settings.html" target="_blank" rel="noopener">教程</a>。</p><h2 id="新增-clustrmaps"><a href="#新增-clustrmaps" class="headerlink" title="新增 clustrmaps"></a>新增 clustrmaps</h2><p>文件路径：<code>themes\next\layout\_partials\footer.swig</code><br>页尾增加了访客地图，去这个<a href="https://clustrmaps.com/" target="_blank" rel="noopener">网站</a>注册，按照步骤添加即可。</p><h2 id="valine-添加邮件提醒"><a href="#valine-添加邮件提醒" class="headerlink" title="valine 添加邮件提醒"></a>valine 添加邮件提醒</h2><p>主要参考这个<a href="https://panjunwen.com/valine-admin-document/" target="_blank" rel="noopener">链接</a>，相当详细，在此不再赘述。</p><h2 id="Gitment-小小改动"><a href="#Gitment-小小改动" class="headerlink" title="Gitment 小小改动"></a>Gitment 小小改动</h2><p>文件：<code>~\themes\next\layout\_third-party\comments\gitment.swig</code>，Gitment与Gitmint的<code>css</code>以及<code>js</code>文件上传到网站相应的目录下，分别是：<code>~\themes\next\source\js\src</code>以及<code>~\themes\next\source\css</code>。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;!<span class="selector-tag">--</span> <span class="selector-tag">LOCAL</span>: <span class="selector-tag">You</span> <span class="selector-tag">can</span> <span class="selector-tag">save</span> <span class="selector-tag">these</span> <span class="selector-tag">files</span> <span class="selector-tag">to</span> <span class="selector-tag">your</span> <span class="selector-tag">site</span> <span class="selector-tag">and</span> <span class="selector-tag">update</span> <span class="selector-tag">links</span> <span class="selector-tag">--</span>&gt;</span><br><span class="line">    &#123;% if theme.gitment.mint %&#125;</span><br><span class="line">        &#123;% set CommentsClass = "Gitmint" %&#125;</span><br><span class="line">        &lt;link rel="stylesheet" href="https://www.vincentqin.tech/css/gitmint-default.css"&gt;</span><br><span class="line">        &lt;script src="https://www.vincentqin.tech/js/src/gitmint.browser.js"&gt;&lt;/script&gt;</span><br><span class="line">    &#123;% else %&#125;</span><br><span class="line">        &#123;% set CommentsClass = "Gitment" %&#125;</span><br><span class="line">        &lt;link rel="stylesheet" href="https://www.vincentqin.tech/css/gitment-default.css"&gt;</span><br><span class="line">        &lt;script src="https://www.vincentqin.tech/js/src/gitment.browser.js"&gt;&lt;/script&gt;</span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">&lt;!<span class="selector-tag">--</span> <span class="selector-tag">END</span> <span class="selector-tag">LOCAL</span> <span class="selector-tag">--</span>&gt;</span><br></pre></td></tr></table></figure><h2 id="Valine-刷新后评论消失"><a href="#Valine-刷新后评论消失" class="headerlink" title="Valine 刷新后评论消失"></a>Valine 刷新后评论消失</h2><p>参考这个<a href="https://github.com/xCss/Valine/issues/92" target="_blank" rel="noopener">Issue</a>，删除文件<code>.\themes\next\layout\_third-party\analytics\lean-analytics.swig</code>中第四行：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"</span>&gt;</span></span><br></pre></td></tr></table></figure><p>但是这样之后无法显示阅读量了。</p><h2 id="删除-Powered-by-Valine"><a href="#删除-Powered-by-Valine" class="headerlink" title="删除 Powered by Valine"></a>删除 Powered by Valine</h2><p>文件路径： <code>~/themes/next/layout/_third-party/comments/valine.swig</code><br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">new Valine(&#123;</span><br><span class="line">...</span><br><span class="line">pageSize:'&#123;&#123; theme.valine.pageSize &#125;&#125;' || 10,</span><br><span class="line">&#125;);</span><br><span class="line">&#123;# 新增以下代码即可，可以移除.info下所有子节点。#&#125;</span><br><span class="line">var infoEle = document.querySelector('#comments .info');</span><br><span class="line">if (infoEle &amp;&amp; infoEle.childNodes &amp;&amp; infoEle.childNodes.length &gt; 0)&#123;</span><br><span class="line">  infoEle.childNodes.forEach(function(item) &#123;</span><br><span class="line">    item.parentNode.removeChild(item);</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="网页压缩"><a href="#网页压缩" class="headerlink" title="网页压缩"></a>网页压缩</h2><p>参考@<a href="http://muyunyun.cn/posts/f55182c5/#快速实现博客压缩" target="_blank" rel="noopener">muyunyun</a>给出的教程，可以进行如下设置。首先安装<a href="https://github.com/chenzhutian/hexo-all-minifier" target="_blank" rel="noopener"><code>hexo-all-minifier</code></a>，这个模块集成了对 html、css、js、image 的优化。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-all-minifier --save</span><br></pre></td></tr></table></figure></p><p>然后在根目录下修改站点配置文件<code>_config.yml</code>，添加如下命令重新部署即可。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">html_minifier:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  ignore_error:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  exclude:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">css_minifier:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  exclude:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">'*.min.css'</span></span><br><span class="line"></span><br><span class="line"><span class="attr">js_minifier:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  mangle:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  output:</span></span><br><span class="line"><span class="attr">  compress:</span></span><br><span class="line"><span class="attr">  exclude:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">'*.min.js'</span></span><br><span class="line"></span><br><span class="line"><span class="attr">image_minifier:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  interlaced:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  multipass:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  optimizationLevel:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  pngquant:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  progressive:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><h2 id="代码区高级设置"><a href="#代码区高级设置" class="headerlink" title="代码区高级设置"></a>代码区高级设置</h2><p>可以参考这里：<a href="https://www.ofind.cn/blog/HEXO/HEXO%E4%B8%8B%E7%9A%84%E8%AF%AD%E6%B3%95%E9%AB%98%E4%BA%AE%E6%8B%93%E5%B1%95%E4%BF%AE%E6%94%B9.html" target="_blank" rel="noopener">HEXO下的语法高亮拓展修改</a>，具体而言，Markdown的代码段的语法是这样的。<br>格式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">```[language] [:title] [lang:language] [line_number:(true|false)] [first_line:number] [mark:#,#-#] [diff:true|false] [url:http...]</span><br><span class="line">       code snippet</span><br><span class="line">```</span><br></pre></td></tr></table></figure><p>支持的语言包括：c, abnf, accesslog, actionscript, ada, apache, applescript, arduino, armasm, asciidoc, aspectj, autohotkey, autoit, avrasm, awk, axapta, bash, basic, bnf, brainfuck, cal, capnproto, ceylon, clean, clojure, clojure-repl, cmake, coffeescript, coq, cos, cpp, crmsh, crystal, cs, csp, css, d, dart, delphi, diff, django, dns, dockerfile, dos, dsconfig, dts, dust, ebnf, elixir, elm, erb, erlang, erlang-repl, excel, fix, flix, fortran, fsharp, gams, gauss, gcode, gherkin, glsl, go, golo, gradle, groovy, haml, handlebars, haskell, haxe, hsp, htmlbars, http, hy, inform7, ini, irpf90, java, javascript, json, julia, kotlin, lasso, ldif, leaf, less, lisp, livecodeserver, livescript, llvm, lsl, lua, makefile, markdown, mathematica, matlab, maxima, mel, mercury, mipsasm, mizar, mojolicious, monkey, moonscript, n1ql, nginx, nimrod, nix, nsis, objectivec, ocaml, openscad, oxygene, parser3, perl, pf, php, pony, powershell, processing, profile, prolog, protobuf, puppet, purebasic, python, q, qml, r, rib, roboconf, rsl, ruby, ruleslanguage, rust, scala, scheme, scilab, scss, smali, smalltalk, sml, sqf, sql, stan, stata, step21, stylus, subunit, swift, taggerscript, tap, tcl, tex, thrift, tp, twig, typescript, vala, vbnet, vbscript, vbscript-html, verilog, vhdl, vim, x86asm, xl, xml, xquery, yaml, zephir。</p><p>以具体的例子进行讲解，以下是一段matlab程序，我们对其位置进行描述同时标记第1,3-4行，修改部分代码。</p><figure class="highlight matlab"><figcaption><span>mark:1,3:4 diff:true first_line</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">r = <span class="number">7</span>;</span><br><span class="line"><span class="built_in">eps</span> = <span class="number">0.0001</span>;</span><br><span class="line">-tic;</span><br><span class="line">+tic</span><br><span class="line">reverseStr = <span class="string">''</span>  ;</span><br><span class="line"><span class="keyword">for</span> d=<span class="number">1</span>:nD</span><br><span class="line">        p = weight_cost(:,:,d);</span><br><span class="line">        q = guidedfilter_color(double(img_view), double(p), r, <span class="built_in">eps</span>);</span><br><span class="line">        weight_cost(:,:,d) = q;</span><br><span class="line">        msg = sprintf(<span class="string">'Processing: %d/%d done!\n'</span>,d, nD)  ;</span><br><span class="line">        fprintf([reverseStr, msg]);</span><br><span class="line">        reverseStr = <span class="built_in">repmat</span>(sprintf(<span class="string">'\b'</span>), <span class="number">1</span>, <span class="built_in">length</span>(msg));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">fprintf(<span class="string">'Final depth estimation completed in %.2f sec\n'</span>, toc);</span><br><span class="line">[~,weightD] = <span class="built_in">max</span>(weight_cost,[],<span class="number">3</span>);</span><br><span class="line">save_img = uint8((<span class="number">256</span>/(nD))*(weightD<span class="number">-1</span>));</span><br><span class="line">imwrite(save_img,strcat(output_path,<span class="string">'SPO_depth.bmp'</span>));</span><br></pre></td></tr></table></figure><p><font color="red">注意，我的网站此处显示有误（先占坑）!</font>另外修改代码块颜色样式,<br><figure class="highlight css"><figcaption><span>文件位置:~/blog/themes/next/source/css/_custom/custom.styl</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// 文章```代码块顶部样式</span><br><span class="line"><span class="selector-class">.highlight</span> <span class="selector-tag">figcaption</span> &#123;</span><br><span class="line">    <span class="attribute">margin</span>: <span class="number">0em</span>;</span><br><span class="line">    <span class="attribute">padding</span>: <span class="number">0.5em</span>;</span><br><span class="line">    <span class="attribute">background</span>: <span class="number">#eee</span>;</span><br><span class="line">    <span class="attribute">border-bottom</span>: <span class="number">1px</span> solid <span class="number">#e9e9e9</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.highlight</span> <span class="selector-tag">figcaption</span> <span class="selector-tag">a</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="built_in">rgb</span>(80, 115, 184);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="修复行内公式显示乱码"><a href="#修复行内公式显示乱码" class="headerlink" title="修复行内公式显示乱码"></a>修复行内公式显示乱码</h2><p>以下解决方案来自<a href="https://www.jianshu.com/p/7ab21c7f0674" target="_blank" rel="noopener">这里</a>。</p><p>更换Hexo的markdown渲染引擎，hexo-renderer-kramed引擎是在默认的渲染引擎hexo-renderer-marked的基础上修改了一些bug，两者比较接近，也比较轻量级。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-pandoc --save</span><br></pre></td></tr></table></figure></p><p>执行上面的命令即可，先卸载原来的渲染引擎，再安装新的。</p><p>然后，跟换引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为hexo-renderer-kramed引擎也有语义冲突的问题。接下来到博客根目录下，找到<code>node_modules\kramed\lib\rules\inline.js</code>，把第11行的escape变量的值做相应的修改：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span></span><br><span class="line">   escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,</span><br></pre></td></tr></table></figure><p>这一步是在原基础上取消了对\,{,}的转义(escape)。同时把第20行的em变量也要做相应的修改。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span></span><br><span class="line">   em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span><br></pre></td></tr></table></figure><p>重新启动hexo（先clean再generate）,问题完美解决。哦，如果不幸还没解决的话，看看是不是还需要在使用的主题中配置mathjax开关。如何使用了主题，要在主题（Theme）中开启mathjax开关，下面以next主题为例，介绍下如何打开mathjax开关。进入到主题目录，找到_config.yml配置问题，把mathjax默认的false修改为true，具体如下：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MathJax Support</span></span><br><span class="line"><span class="attr">mathjax:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  per_page:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>别着急，这样还不够，还需要在文章的Front-matter里打开mathjax开关，如下：</p><pre><code>---title: index.htmldate: 2016-12-28 21:01:30tags:mathjax: true--</code></pre><p>不要嫌麻烦，之所以要在文章头里设置开关，是因为考虑只有在用到公式的页面才加载 Mathjax，这样不需要渲染数学公式的页面的访问速度就不会受到影响了。</p><h2 id="显示文章阅读数量"><a href="#显示文章阅读数量" class="headerlink" title="显示文章阅读数量"></a>显示文章阅读数量</h2><p>另外：<a href="https://notes.wanghao.work/2015-10-21-%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD.html#%E9%85%8D%E7%BD%AELeanCloud" target="_blank" rel="noopener">显示文章阅读量</a>， 服务主要用了<a href="https://leancloud.cn/" target="_blank" rel="noopener">LeanCloud</a>服务提供商</p><h2 id="官方主题设置"><a href="#官方主题设置" class="headerlink" title="官方主题设置"></a>官方主题设置</h2><p>我使用的是<a href="http://theme-next.iissnan.com/" target="_blank" rel="noopener">Next</a>主题， <a href="http://www.arao.me/" target="_blank" rel="noopener">Make the theme more beautiful, recommended</a></p><h2 id="关于评论系统"><a href="#关于评论系统" class="headerlink" title="关于评论系统"></a>关于评论系统</h2><p>多说已死，<del>评论系统转到了Disqus，但是被墙的事实让人感觉不爽</del>。几经周折，从多说转到Disqus，然后在gitment和gitalk之间徘徊，最后还是选择了<a href="https://valine.js.org/#/" target="_blank" rel="noopener">valine</a>，不过它只能在中国区进行评论，于是我还是保留了<a href="http://www.whtis.com/2017/10/19/%E7%BB%99hexo%E5%8D%9A%E5%AE%A2nexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0Gitalk%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/" target="_blank" rel="noopener">gitalk</a>。然而，最后的最后我还是选择了<strong>Hypercomment</strong>。</p><blockquote><p>2018年6月21日 更新</p></blockquote><p>最近评论系统<a href="https://www.hypercomments.com" target="_blank" rel="noopener">HyperComments</a>竟然开始收费了，于是我不得不改用新的评论系统<a href="https://livere.com/" target="_blank" rel="noopener">LiveRe</a> <del><a href="https://gitalk.github.io/" target="_blank" rel="noopener">Gitalk</a>/<a href="https://valine.js.org/" target="_blank" rel="noopener">valine</a></del>。这样一来，原来的评论都看不到了，由此给大家带来的不便，特此道歉！</p><p>Hello everyone, the comment system <a href="https://www.hypercomments.com" target="_blank" rel="noopener">HyperComments</a> is charging recently, so I had to switch to the new comment system <a href="https://livere.com/" target="_blank" rel="noopener">LiveRe</a>. As a result, the original comments are invisible. I deeply apologize for this inconveniences!</p><h2 id="关于旋转头像"><a href="#关于旋转头像" class="headerlink" title="关于旋转头像"></a>关于旋转头像</h2><p>把侧边栏头像变成圆形&amp;鼠标停留在上面出现旋转效果，具体修改文件的位置是<code>next\source\css\_common\components\sidebar\sidebar-author.styl</code>。更为具体的修改过程见<a href="https://ehlxr.me/2016/08/30/%E4%BD%BF%E7%94%A8Hexo%E5%9F%BA%E4%BA%8EGitHub-Pages%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%88%E4%B8%89%EF%BC%89/" target="_blank" rel="noopener">Ehlxr</a>写的这篇博客。<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.site-author-image</span> &#123;</span><br><span class="line">  <span class="attribute">display</span>: block;</span><br><span class="line">  <span class="attribute">margin</span>: <span class="number">0</span> auto;</span><br><span class="line">  <span class="attribute">padding</span>: $site-author-image-padding;</span><br><span class="line">  <span class="attribute">max-width</span>: $site-author-image-width;</span><br><span class="line">  <span class="attribute">height</span>: $site-author-image-height;</span><br><span class="line">  <span class="attribute">border</span>: $site-author-image-border-width solid $site-author-image-border-color;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 头像圆形 */</span></span><br><span class="line">  <span class="attribute">border-radius</span>: <span class="number">80px</span>;</span><br><span class="line">  <span class="attribute">-webkit-border-radius</span>: <span class="number">80px</span>;</span><br><span class="line">  <span class="attribute">-moz-border-radius</span>: <span class="number">80px</span>;</span><br><span class="line">  <span class="attribute">box-shadow</span>: inset <span class="number">0</span> -<span class="number">1px</span> <span class="number">0</span> <span class="number">#333</span>sf;</span><br><span class="line">  <span class="comment">/* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 </span></span><br><span class="line"><span class="comment">    (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/</span></span><br><span class="line">  <span class="attribute">-webkit-animation</span>: play <span class="number">2s</span> ease-out <span class="number">1s</span> <span class="number">1</span>;</span><br><span class="line">  <span class="attribute">-moz-animation</span>: play <span class="number">2s</span> ease-out <span class="number">1s</span> <span class="number">1</span>;</span><br><span class="line">  <span class="attribute">animation</span>: play <span class="number">2s</span> ease-out <span class="number">1s</span> <span class="number">1</span>; </span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 鼠标经过头像旋转360度 */</span></span><br><span class="line">  <span class="attribute">-webkit-transition</span>: -webkit-transform <span class="number">1.5s</span> ease-out;</span><br><span class="line">  <span class="attribute">-moz-transition</span>: -moz-transform <span class="number">1.5s</span> ease-out;</span><br><span class="line">  <span class="attribute">transition</span>: transform <span class="number">1.5s</span> ease-out;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">img</span><span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">  <span class="comment">/* 鼠标经过头像旋转360度 */</span></span><br><span class="line">  <span class="attribute">-webkit-transform</span>: <span class="built_in">rotateZ</span>(360deg);</span><br><span class="line">  <span class="attribute">-moz-transform</span>: <span class="built_in">rotateZ</span>(360deg);</span><br><span class="line">  <span class="attribute">transform</span>: <span class="built_in">rotateZ</span>(360deg);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="背景颜色设置"><a href="#背景颜色设置" class="headerlink" title="背景颜色设置"></a>背景颜色设置</h2><p>其实NEXT主题已经自带了几种动画了，我用的是three_waves；但是呢，存在一个问题就是因为Blog背景是透明的，这样文字和背景动画就有重叠效果了，很不方便阅读，这时把背景色设置为白色即可。添加<strong>background: white</strong>到如下路径<code>next\source\css\_schemes\Muse\_layout.styl</code><br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.header-inner</span>, <span class="selector-class">.container</span> <span class="selector-class">.main-inner</span>, <span class="selector-class">.footer-inner</span> &#123;</span><br><span class="line">  <span class="attribute">background</span>: white;</span><br><span class="line">  +mobile() &#123; width: auto; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="页面宽度设置"><a href="#页面宽度设置" class="headerlink" title="页面宽度设置"></a>页面宽度设置</h2><p>固定主题页面最大宽度，可以参考这个<a href="https://sluggishpj.github.io/blog/2017/10/14/%E4%BF%AE%E6%94%B9NexTPisces%E4%B8%BB%E9%A2%98%E6%A0%B7%E5%BC%8F/" target="_blank" rel="noopener">链接</a>。<br>有时候我们可能会嫌弃博客的页面太小，留白过大。这时候可以对页面宽度进行设置，可以参考Hexo Next主题 <a href="https://github.com/iissnan/hexo-theme-next/issues/759" target="_blank" rel="noopener">Issue #759</a>。对于 Pisces Scheme，需要同时修改 <code>header</code> 的宽度、<code>.main-inner</code> 的宽度以及 <code>.content-wrap</code> 的宽度。例如，使用百分比（Pisces 的布局定义在 <code>source/css/_schemes/Picses/_layout.styl</code> 中）：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.header</span>&#123; <span class="attribute">width</span>: <span class="number">80%</span>; &#125; <span class="comment">/* 80% */</span></span><br><span class="line"><span class="selector-class">.container</span> <span class="selector-class">.main-inner</span> &#123; <span class="attribute">width</span>: <span class="number">80%</span>; &#125; <span class="comment">/* 80% */</span></span><br><span class="line"><span class="selector-class">.content-wrap</span> &#123; <span class="attribute">width</span>: <span class="built_in">calc</span>(100% - 260px); &#125;</span><br></pre></td></tr></table></figure><h2 id="优化友情链接"><a href="#优化友情链接" class="headerlink" title="优化友情链接"></a>优化友情链接</h2><p>新建一个<code>Friends</code>页面：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page Friends</span><br></pre></td></tr></table></figure></p><p>新建样式，进入<code>themes\next\source\css\_custom\custom.styl</code>，在最后新加上几行代码:</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">$shadowColor  = #333;</span><br><span class="line">$themeColor       = #222;</span><br><span class="line">$link-image-size  = 180px;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.link-body</span>&#123;</span><br><span class="line">ul&#123;</span><br><span class="line"><span class="selector-tag">display</span>: <span class="selector-tag">flex</span>;</span><br><span class="line"><span class="selector-tag">justify-content</span>: <span class="selector-tag">space-between</span>;</span><br><span class="line"><span class="selector-tag">align-items</span>: <span class="selector-tag">center</span>;</span><br><span class="line"><span class="selector-tag">flex-wrap</span>: <span class="selector-tag">wrap</span>;</span><br><span class="line"><span class="selector-tag">margin</span>: 0;</span><br><span class="line"><span class="selector-tag">padding</span>: 0;</span><br><span class="line"><span class="selector-class">.link</span>&#123;</span><br><span class="line"><span class="attribute">max-width</span>: $link-image-size;</span><br><span class="line"><span class="attribute">min-width</span>: $link-image-size;</span><br><span class="line"><span class="attribute">max-height</span>: $link-image-size;</span><br><span class="line"><span class="attribute">min-height</span>: $link-image-size;</span><br><span class="line"></span><br><span class="line"><span class="attribute">position</span>: relative;</span><br><span class="line"><span class="attribute">box-shadow</span>: <span class="number">0</span> <span class="number">0</span> <span class="number">1px</span> $shadowColor;</span><br><span class="line"><span class="attribute">magin</span>: <span class="number">6px</span>;</span><br><span class="line"><span class="attribute">width</span>: <span class="number">20%</span>;</span><br><span class="line"><span class="attribute">list-style</span>: none<span class="meta">!important</span>;</span><br><span class="line"><span class="attribute">overflow</span>: hidden;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">6px</span>;</span><br><span class="line">img&#123;</span><br><span class="line"><span class="selector-tag">object-fit</span>: <span class="selector-tag">cover</span>;</span><br><span class="line"><span class="selector-tag">transition</span>: <span class="selector-tag">transform</span> <span class="selector-class">.6s</span> <span class="selector-tag">ease-out</span>;</span><br><span class="line"><span class="selector-tag">vertical-align</span>: <span class="selector-tag">middle</span>;</span><br><span class="line">border-bottom: 4px solid #eee;//#e5642b;</span><br><span class="line"><span class="selector-tag">transition</span>: 0<span class="selector-class">.4s</span> ;</span><br><span class="line"><span class="selector-tag">width</span>: 100%;</span><br><span class="line"><span class="selector-tag">border-radius</span>: 90<span class="selector-tag">px</span> 90<span class="selector-tag">px</span> 90<span class="selector-tag">px</span> 90<span class="selector-tag">px</span>;</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">display</span>: <span class="selector-tag">inline-block</span>;</span><br><span class="line"><span class="selector-tag">float</span>: <span class="selector-tag">none</span>;</span><br><span class="line"><span class="selector-tag">vertical-align</span>: <span class="selector-tag">middle</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.link-name</span>&#123;</span><br><span class="line"><span class="attribute">position</span>: absolute;</span><br><span class="line"><span class="attribute">bottom</span>: <span class="number">0</span>;</span><br><span class="line"><span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line"><span class="attribute">color</span>: <span class="number">#666</span>;</span><br><span class="line"><span class="attribute">text-align</span>: center;</span><br><span class="line"><span class="attribute">text-shadow</span>: <span class="number">0</span> <span class="number">0</span> <span class="number">1px</span> <span class="built_in">rgba</span>(0,0,0,.4);</span><br><span class="line"><span class="attribute">background</span>: <span class="built_in">rgba</span>(255,255,255,.7);</span><br><span class="line">&#125;</span><br><span class="line">&amp;<span class="selector-pseudo">:hover</span>&#123;</span><br><span class="line">img&#123;</span><br><span class="line"><span class="selector-tag">overflow</span>: <span class="selector-tag">hidden</span>;</span><br><span class="line">//transition: 0.4s;</span><br><span class="line"><span class="selector-tag">border-radius</span>: 0 0 0 0;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.link-name</span>&#123;</span><br><span class="line"><span class="attribute">color</span>: $themeColor;</span><br><span class="line"><span class="attribute">text-shadow</span>: <span class="number">0</span> <span class="number">0</span> <span class="number">1px</span> $themeColor;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后编辑站点的<code>source\Friends</code>下的<code>index.md</code>文件，内容如下：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"link-body"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--your friend begin--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"link"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"your_friends_link"</span> <span class="attr">title</span>=<span class="string">"balabala"</span> <span class="attr">target</span>=<span class="string">"_blank"</span> &gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>= <span class="string">"image_path"</span> <span class="attr">alt</span>=<span class="string">"balabala"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"link-name"</span>&gt;</span> balabala<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--your friend end--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--your another friend begin--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"link"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"your_friends_link"</span> <span class="attr">title</span>=<span class="string">"balabala"</span> <span class="attr">target</span>=<span class="string">"_blank"</span> &gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>= <span class="string">"image_path"</span> <span class="attr">alt</span>=<span class="string">"balabala"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"link-name"</span>&gt;</span> balabala<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--your another friend end--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><p>此时，点击友情链接可能不会跳转到相应的页面，参考这个<a href="https://github.com/iissnan/hexo-theme-next/pull/1975/commits/fff58ebf79bd50418cbb00400530852716936675" target="_blank" rel="noopener">issue</a>，作以下修改。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">wrapImageWithFancyBox: <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">$(<span class="string">'.content img'</span>)</span><br><span class="line">  .not(<span class="string">'[hidden]'</span>)</span><br><span class="line">  .not(<span class="string">'.group-picture img, .post-gallery img'</span>)</span><br><span class="line">  .not(<span class="string">'a img'</span>) <span class="comment">// 这里添加</span></span><br><span class="line"><span class="string">``</span><span class="string">` </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">## 增加Gitter</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">参考[sidecar](https://sidecar.gitter.im)的示例，在`</span>themes\next\layout\_layout.swig<span class="string">`的`</span>&lt;<span class="regexp">/body&gt;`前增加如下代码：</span></span><br><span class="line"><span class="regexp">路径：`文件路径：themes\next\layout\_layout.swig`.</span></span><br><span class="line"><span class="regexp">``` html </span></span><br><span class="line"><span class="regexp">  &lt;!-- add gitter on sidebar --&gt;</span></span><br><span class="line"><span class="regexp">  &lt;script&gt;</span></span><br><span class="line"><span class="regexp">    ((window.gitter = &#123;&#125;).chat = &#123;&#125;).options = &#123;</span></span><br><span class="line"><span class="regexp">      room: 'your_chat_room_name'</span></span><br><span class="line"><span class="regexp">    &#125;;</span></span><br><span class="line"><span class="regexp">  &lt;/</span>script&gt;</span><br><span class="line">  &lt;script src=<span class="string">"https://sidecar.gitter.im/dist/sidecar.v1.js"</span> <span class="keyword">async</span> defer&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br></pre></td></tr></table></figure><p>其中的<code>room</code>换成你自己在<a href="https://gitter.im/" target="_blank" rel="noopener">gitter</a>创建的聊天室名字，例如我的是<code>vincentqin-blog-chat/Lobby</code>，所以我的设置<code>room: &#39;vincentqin-blog-chat/Lobby&#39;</code>。之后可以在<code>themes\next\source\css\_custom\custom.styl</code>里增加如下设置：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">// adjust the position of gitter</span><br><span class="line"><span class="selector-class">.gitter-open-chat-button</span> &#123;</span><br><span class="line">      <span class="attribute">right</span>: <span class="number">20px</span>;</span><br><span class="line">  <span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line">  <span class="attribute">background-color</span>: <span class="number">#777</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@<span class="keyword">media</span> (max-width: <span class="number">600px</span>) &#123;</span><br><span class="line">    <span class="selector-class">.gitter-open-chat-button</span>,</span><br><span class="line">    <span class="selector-class">.gitter-chat-embed</span> &#123;</span><br><span class="line">        <span class="attribute">display</span>: none;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="添加页面背景"><a href="#添加页面背景" class="headerlink" title="添加页面背景"></a>添加页面背景</h2><p>将背景图片放在<code>themes\next\source\images</code>下，例如bg.jpg，然后<code>themes\next\source\css\_custom\custom.styl</code>里增加如下设置：</p><figure class="highlight css"><figcaption><span>文件路径：themes\next\source\css\_custom\custom.styl</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@<span class="keyword">media</span> screen and (min-width:<span class="number">720px</span>) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="selector-tag">body</span> &#123;</span><br><span class="line"><span class="attribute">background</span>:<span class="built_in">url</span>(/images/bg.jpg);</span><br><span class="line">    <span class="attribute">background-repeat</span>: no-repeat;</span><br><span class="line">    background-attachment:fixed; // 鼠标滚动背景不动</span><br><span class="line">    <span class="selector-tag">background-position</span><span class="selector-pseudo">:50</span>% 50%;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="selector-id">#footer</span> &#123;</span><br><span class="line">        <span class="attribute">color</span>:<span class="number">#c8c8c8</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><del>~这里是我<a href="https://github.com/Vincentqyw/blog-code/blob/master/themes/next/source/css/_custom/custom.styl" target="_blank" rel="noopener">custom.styl</a>所有配置，仅供参考。</del>~</p><h2 id="添加fork-me-on-github"><a href="#添加fork-me-on-github" class="headerlink" title="添加fork me on github"></a>添加fork me on github</h2><p>文件位置：<code>\themes\next\layout\_layout.swig</code>，在<code>&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;</code>下一行添加如下代码。</p><figure class="highlight js"><figcaption><span>文件路径：\themes\next\layout\_layout.swig</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;a href=<span class="string">"https://www.github.com/Vincentqyw"</span> <span class="class"><span class="keyword">class</span></span>=<span class="string">"github-corner"</span> aria-label=<span class="string">"View source on Github"</span>&gt;<span class="xml"><span class="tag">&lt;<span class="name">svg</span> <span class="attr">width</span>=<span class="string">"80"</span> <span class="attr">height</span>=<span class="string">"80"</span> <span class="attr">viewBox</span>=<span class="string">"0 0 250 250"</span> <span class="attr">style</span>=<span class="string">"fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;"</span> <span class="attr">aria-hidden</span>=<span class="string">"true"</span>&gt;</span><span class="tag">&lt;<span class="name">path</span> <span class="attr">d</span>=<span class="string">"M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"</span>&gt;</span><span class="tag">&lt;/<span class="name">path</span>&gt;</span><span class="tag">&lt;<span class="name">path</span> <span class="attr">d</span>=<span class="string">"M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"</span> <span class="attr">fill</span>=<span class="string">"currentColor"</span> <span class="attr">style</span>=<span class="string">"transform-origin: 130px 106px;"</span> <span class="attr">class</span>=<span class="string">"octo-arm"</span>&gt;</span><span class="tag">&lt;/<span class="name">path</span>&gt;</span><span class="tag">&lt;<span class="name">path</span> <span class="attr">d</span>=<span class="string">"M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"</span> <span class="attr">fill</span>=<span class="string">"currentColor"</span> <span class="attr">class</span>=<span class="string">"octo-body"</span>&gt;</span><span class="tag">&lt;/<span class="name">path</span>&gt;</span><span class="tag">&lt;/<span class="name">svg</span>&gt;</span></span><span class="xml"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><span class="xml"><span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="css"><span class="selector-class">.github-corner</span><span class="selector-pseudo">:hover</span> <span class="selector-class">.octo-arm</span>&#123;<span class="attribute">animation</span>:octocat-wave <span class="number">560ms</span> ease-in-out&#125;@<span class="keyword">keyframes</span> octocat-wave&#123;0%,100%&#123;<span class="attribute">transform</span>:<span class="built_in">rotate</span>(0)&#125;20%,60%&#123;<span class="attribute">transform</span>:<span class="built_in">rotate</span>(-25deg)&#125;40%,80%&#123;<span class="attribute">transform</span>:<span class="built_in">rotate</span>(10deg)&#125;&#125;@<span class="keyword">media</span> (max-width:<span class="number">500px</span>)&#123;<span class="selector-class">.github-corner</span><span class="selector-pseudo">:hover</span> <span class="selector-class">.octo-arm</span>&#123;<span class="attribute">animation</span>:none&#125;<span class="selector-class">.github-corner</span> <span class="selector-class">.octo-arm</span>&#123;<span class="attribute">animation</span>:octocat-wave <span class="number">560ms</span> ease-in-out&#125;&#125;</span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span></span><br></pre></td></tr></table></figure><p>更多样式，参考<a href="http://tholman.com/github-corners/" target="_blank" rel="noopener">这里</a>。</p><h2 id="MarkDown编辑器"><a href="#MarkDown编辑器" class="headerlink" title="MarkDown编辑器"></a>MarkDown编辑器</h2><p>推荐<strong>Haroopad</strong></p><p><img alt data-src="//www.vincentqin.tech/posts/build-a-website-using-hexo/haroopad.png"></p><h2 id="插入PDF文档以及图片"><a href="#插入PDF文档以及图片" class="headerlink" title="插入PDF文档以及图片"></a>插入PDF文档以及图片</h2><ul><li><p>插入PDF文档：<br>将相应的PDF文档放在与博客标题同名的文件夹内，然后再按照如下方式进行插入。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[点我，这里是PDF文档](latex入门教程.pdf)</span><br></pre></td></tr></table></figure><p>  <a href="http://oofx6tpf6.bkt.clouddn.com/latex-tutorial.pdf" target="_blank" rel="noopener">点我，这里是PDF文档</a></p></li><li><p>利用html<code>img</code>标签嵌入图片</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;img src= image_path alt=<span class="string">"Lytro相机"</span> width=<span class="string">"100%"</span>&gt;</span><br><span class="line">&lt;center&gt;Lytro&lt;<span class="regexp">/center&gt;</span></span><br></pre></td></tr></table></figure><p>  注意以上的<code>image_path</code>既可以是图床中的路径，亦可以把图片放在<code>source/images/</code>文件下，然后<code>image_path=/images/logo.png</code>，当然也可以如下插入图片，更加方便。</p>  <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![](<span class="regexp">/images/</span>logo.png)</span><br></pre></td></tr></table></figure></li><li><p>利用插件，以下我在Github上找到的别人已经做好的一个小工具。<br>安装插件<a href="https://github.com/timnew/hexo-tag-asset-res" target="_blank" rel="noopener">hexo-tag-asset-res</a>，打开Git Shell, 在Hexo根目录下, 输入如下代码：</p>  <figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-tag-asset-res --save</span><br></pre></td></tr></table></figure><p>  修改Hexo根目录下_config.yml文件：打开Hexo根目录, 找到站点配置文件<code>_config.yml</code>文件, 用任何一个文本编辑器打开, 找到如下代码：</p>  <figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>  将<code>false</code>改成<code>true</code>即可。测试插入代码：</p>  <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;center&gt;&#123;% asset_img Naruto.jpg Naruto%&#125;&lt;<span class="regexp">/center&gt;</span></span><br></pre></td></tr></table></figure><p>  效果如下：<br>  <img alt data-src="//www.vincentqin.tech/posts/build-a-website-using-hexo/Naruto.jpg"> </p></li></ul><h2 id="配置个性化的字体"><a href="#配置个性化的字体" class="headerlink" title="配置个性化的字体"></a>配置个性化的字体</h2><p>在<code>themes\next\source\css\_variables\custom.styl</code>文件中添加如下内容。</p><figure class="highlight styl"><figcaption><span>文件路径：themes\next\source\css\_variables\custom.styl</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 修改成你期望的字体族</span></span><br><span class="line"><span class="variable">$font</span>-family-base= <span class="string">"Monda"</span>,<span class="string">"Microsoft YaHei"</span>, Verdana, sans-serif</span><br><span class="line"><span class="comment">// 标题，修改成你期望的字体族</span></span><br><span class="line"><span class="variable">$font</span>-family-headings= <span class="string">"Roboto Slab"</span>, Georgia, sans</span><br><span class="line"><span class="comment">// 代码字体</span></span><br><span class="line"><span class="variable">$code</span>-<span class="attribute">font-family</span>= <span class="string">"PT Mono"</span>, <span class="string">"Input Mono"</span>, Consolas, Monaco, Menlo, monospace</span><br><span class="line"><span class="comment">// 博客字体</span></span><br><span class="line"><span class="variable">$font</span>-family-posts= <span class="string">"Monda"</span></span><br><span class="line"><span class="comment">// logo字体</span></span><br><span class="line"><span class="variable">$font</span>-family-logo= <span class="string">"Lobster Two"</span></span><br></pre></td></tr></table></figure><h2 id="在博客中插入网易云音乐"><a href="#在博客中插入网易云音乐" class="headerlink" title="在博客中插入网易云音乐"></a>在博客中插入网易云音乐</h2><p>我们可以利用网易云提供的代码直接在markdown文档里面插入。</p><ul><li>在网页上找到你想要播放的音乐，如下图：</li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-a-website-using-hexo/wangyiMusic.png"> </p><ul><li>点击<strong>生成外链播放器</strong></li></ul><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/build-a-website-using-hexo/wangyiMusicCode.png"></p><p>注意自动播放，以及音乐播放器的大小可调。</p><ul><li>在Markdown文档里插入如下代码</li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;center&gt;<span class="xml"><span class="tag">&lt;<span class="name">iframe</span> <span class="attr">frameborder</span>=<span class="string">"no"</span> <span class="attr">border</span>=<span class="string">"0"</span> <span class="attr">marginwidth</span>=<span class="string">"0"</span> <span class="attr">marginheight</span>=<span class="string">"0"</span> <span class="attr">width</span>=<span class="string">500</span> <span class="attr">height</span>=<span class="string">86</span> <span class="attr">src</span>=<span class="string">"http://music.163.com/outchain/player?type=2&amp;id=29722263&amp;auto=0&amp;height=66"</span>&gt;</span><span class="tag">&lt;/<span class="name">iframe</span>&gt;</span></span><span class="xml"><span class="tag">&lt;/<span class="name">center</span>&gt;</span></span></span><br></pre></td></tr></table></figure><center><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="500" height="86" src="http://music.163.com/outchain/player?type=2&id=29722263&auto=0&height=66"></iframe></center><h2 id="播放视频"><a href="#播放视频" class="headerlink" title="播放视频"></a>播放视频</h2><p>推荐使用<a href="https://github.com/MoePlayer/DPlayer" target="_blank" rel="noopener">Dplayer</a>。首先在站点文件夹根目录安装<code>Dplayer</code>插件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-tag-dplayer --save</span><br></pre></td></tr></table></figure></p><p>然后文章中的写法：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% dplayer url="https://<span class="strong">*****</span><span class="emphasis">*.mp4" "http://*</span><span class="strong">*****</span>.jpg" "api=https://api.prprpr.me/dplayer/" "id=" "loop=false" %&#125;</span><br></pre></td></tr></table></figure><p>要使用弹幕，必须有api和id两项。id 的值自己随便取，唯一要求就是前面这点。可以通过<a href="http://tool.oschina.net/encrypt?type=2" target="_blank" rel="noopener">这里</a>获取id，保证每次都不一样。</p><p>献上<strong>Maddi Jane</strong> 翻唱的<strong>Jessie J</strong>的<strong>Price Tag</strong>。<br><div id="dplayer3" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer3"),"video":{"url":"http://oofx6tpf6.bkt.clouddn.com/Maddi-Jane-Price-Tag.mp4"},"danmaku":{"id":"bbe4286bf164ef6a1497f18a7b42ff944e684b82","api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script></p><h2 id="同时部署"><a href="#同时部署" class="headerlink" title="同时部署"></a>同时部署</h2><p>接下来主要涉及到以hexo框架搭建博客的版本管理。同时部署其实很简单，仅仅修改<strong>站点配置文件</strong>的<code>_config.yml</code>即可。在最后的deploy底下新增一项：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">repo:</span> </span><br><span class="line"><span class="attr">github:</span> <span class="attr">https://github.com/Your_Github_ID/Github_ID.github.io.git</span> </span><br><span class="line"><span class="attr">coding:</span> <span class="attr">https://git.coding.net/Your_Coding_ID/Your_Repo_Name.coding.me.git</span></span><br></pre></td></tr></table></figure><p>以后<code>hexo d</code>时，就会同时部署到github和coding。</p><h2 id="版本管理"><a href="#版本管理" class="headerlink" title="版本管理"></a>版本管理</h2><h3 id="方案-1（推荐）"><a href="#方案-1（推荐）" class="headerlink" title="方案 1（推荐）"></a>方案 1（推荐）</h3><p>下载第三方插件，more information refers to this link <a href="https://github.com/coneycode/hexo-git-backup" target="_blank" rel="noopener">hexo-git-backup</a>. When you are well configured, you can just run the following command.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo backup#或 hexo b</span><br></pre></td></tr></table></figure></p><h3 id="方案-2"><a href="#方案-2" class="headerlink" title="方案 2"></a>方案 2</h3><p>这里涉及到<code>git</code>的部分知识。</p><blockquote><p>目的：实现整个blog源码级别的代码管理，包括<strong>站点配置</strong>&amp;<strong>主题配置</strong>。</p></blockquote><p>首先明确一点，在每次<code>hexo d</code>时，都会自动产生一个名为<code>.deploy_git</code>的文件夹，这个文件夹下包含有<code>hexo g</code>渲染出的各种文件，这些文件就是构成github page或者coding page的重要源码；同时会自动的将这个<code>.deploy_git</code>设置成本地仓库，即产生一个<code>.git</code>的隐藏文件。我们做的事情和以上过程不尽一致，总结起来主要是以下几个命令。<br>首先建立一个名为<code>.gitignore</code>的文件，表示我们并不上传这些文件，原因后续介绍。其内容为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.DS_Store</span><br><span class="line">Thumbs.db</span><br><span class="line">db.json</span><br><span class="line">*.log</span><br><span class="line">node_modules/</span><br><span class="line">public/</span><br><span class="line">.deploy*/</span><br><span class="line">themes/</span><br></pre></td></tr></table></figure><p>接下来就是把blog的源文件夹搞成一个本地仓库，如下命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建仓库</span></span><br><span class="line">git init</span><br><span class="line"><span class="comment"># 为本地仓库添加文件</span></span><br><span class="line">git add -A</span><br><span class="line"><span class="comment"># 提交到本地仓库</span></span><br><span class="line">git commit -m <span class="string">"your message"</span></span><br><span class="line"><span class="comment"># 添加一个名为 origin 的远程，这个名字随便起</span></span><br><span class="line">git remote add origin https://github.com/Your_Github_ID/Your_Repo_Name.git</span><br><span class="line"><span class="comment"># 为其添加 push 到 Github 的地址</span></span><br><span class="line">git remote <span class="built_in">set</span>-url --add --push origin https://github.com/Your_Github_ID/Your_Repo_Name.git</span><br><span class="line"><span class="comment"># 为其添加 push 到 Coding 的地址</span></span><br><span class="line">git remote <span class="built_in">set</span>-url --add --push origin https://git.coding.net/Your_Coding_ID/Your_Repo_Name.git</span><br><span class="line"><span class="comment"># push到远端的master分支</span></span><br><span class="line">git push --<span class="built_in">set</span>-upstream origin master</span><br><span class="line"><span class="comment"># 新建并切换分支</span></span><br><span class="line">git checkout -b <span class="string">"another-branch"</span></span><br><span class="line"><span class="comment"># 各种更改文件......推送到远程</span></span><br><span class="line">git push --<span class="built_in">set</span>-upstream origin another-branch</span><br><span class="line"><span class="comment"># 以后可以直接 git push，不用set了。</span></span><br></pre></td></tr></table></figure><p>通过以上命令，我们就可以同时部署在github仓库<code>https://github.com/Your_Github_ID/Your_Repo_Name.git</code>和coding仓库<code>https://git.coding.net/Your_Coding_ID/Your_Repo_Name.git</code>了。</p><h4 id="设置主题远程仓库"><a href="#设置主题远程仓库" class="headerlink" title="设置主题远程仓库"></a>设置主题远程仓库</h4><p>这时你会发现<code>themes</code>这个文件夹并没有同时被上传到远程仓库，同上操作，将<code>theme/next</code>设置成本地仓库并部署。之所以将这个仓库单独上传，是为了方便切换主题，以及主题升级。</p><h4 id="设置node-modules远程仓库"><a href="#设置node-modules远程仓库" class="headerlink" title="设置node_modules远程仓库"></a>设置node_modules远程仓库</h4><p>之所以将这个模块单独拎出来处理，是因为这个文件夹虽然容量不大，但是其中文件个数特别多。当和blog源文件一同被<code>git add</code>到暂存区之后，git shell的运行速度就会超慢。我的解决思路就是将其创建成一个仓库，这样git shell的速度就会快一些。具体步骤不再赘述，同上。</p><h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>经过建立以上的3个仓库，实现了blog源码级别的版本管理。当然，如果你不想暴露自己的源码，那么你只需要在coding申请一个私有仓库并部署就ok了。虽然看起来有些麻烦，但是一旦配置完毕之后，我们就只需要以下几个步骤就可以实现管理。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hexo clean  <span class="comment"># 不是必要步骤</span></span><br><span class="line">hexo d -g   <span class="comment"># 渲染+部署到github page以及coding page</span></span><br><span class="line">git add .   <span class="comment"># 添加到暂存区</span></span><br><span class="line">git commit -m <span class="string">"your message"</span> <span class="comment"># push到本地仓库</span></span><br><span class="line">git push    <span class="comment"># 上传到远程仓库（站点目录、next主题目录、node_modules目录）</span></span><br></pre></td></tr></table></figure></p><p>Good luck:)</p><h2 id="所有配置集锦"><a href="#所有配置集锦" class="headerlink" title="所有配置集锦"></a>所有配置集锦</h2><p>最后附上我的全部配置。文件位置：<code>themes\next\source\css\_custom\custom.styl</code>。<br><figure class="highlight css"><figcaption><span>文件路径：themes\next\source\css\_custom\custom.styl</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br></pre></td><td class="code"><pre><span class="line">// Custom styles.</span><br><span class="line"></span><br><span class="line">//修改文章内code样式</span><br><span class="line"><span class="selector-tag">code</span> &#123;<span class="attribute">color</span>:<span class="number">#c7254e</span>;<span class="attribute">background</span>:<span class="number">#f9f2f4</span>;<span class="attribute">border</span>:<span class="number">1px</span> solid <span class="number">#d6d6d6</span>;&#125;</span><br><span class="line"></span><br><span class="line">//修改文章中图片样式，改为居中</span><br><span class="line"><span class="selector-class">.posts-expand</span> <span class="selector-class">.post-body</span> <span class="selector-tag">img</span> &#123;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">0</span> auto;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 更改文中链接的颜色</span><br><span class="line"><span class="selector-class">.post-body</span> <span class="selector-tag">p</span> <span class="selector-tag">a</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: $orange;</span><br><span class="line">  <span class="attribute">text-decoration</span>: none;</span><br><span class="line">  <span class="attribute">border-bottom</span>: <span class="number">1</span>;</span><br><span class="line">  &amp;:hover &#123;</span><br><span class="line">    color: $blue;</span><br><span class="line">    //text-decoration: underline;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 增大post之间的margin</span><br><span class="line"><span class="selector-class">.post</span> &#123;</span><br><span class="line">    <span class="attribute">margin-bottom</span>: <span class="number">30px</span>;</span><br><span class="line">    //padding: 45px 36px 36px 36px;</span><br><span class="line">    //box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5);</span><br><span class="line">    <span class="selector-tag">background-color</span>: <span class="selector-tag">rgba</span>(255, 255, 255,0<span class="selector-class">.8</span>);</span><br><span class="line">&#125;</span><br><span class="line">// delete the border of image</span><br><span class="line"><span class="selector-class">.posts-expand</span> <span class="selector-class">.post-body</span> <span class="selector-tag">img</span> &#123;</span><br><span class="line">    <span class="attribute">border</span>: none;</span><br><span class="line">    <span class="attribute">padding</span>: <span class="number">0px</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// [Read More]按钮样式: 黑底绿字</span><br><span class="line"><span class="selector-class">.post-button</span> <span class="selector-class">.btn</span><span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="built_in">rgb</span>(136, 255, 13) <span class="meta">!important</span>;</span><br><span class="line">background-color: rgba(0, 0, 0, 0.75); //black</span><br><span class="line">&#125;</span><br><span class="line">// 页面底部页码</span><br><span class="line"><span class="selector-class">.pagination</span> <span class="selector-class">.page-number</span><span class="selector-class">.current</span> &#123;</span><br><span class="line">    <span class="attribute">border-radius</span>: <span class="number">100%</span>;</span><br><span class="line">    <span class="attribute">background-color</span>: <span class="built_in">rgba</span>(100, 100, 100, 0.75);</span><br><span class="line">&#125;</span><br><span class="line">// 页面底部页码, 去除鼠标置于上方时，数字上方的线</span><br><span class="line"><span class="selector-class">.pagination</span> <span class="selector-class">.prev</span>, <span class="selector-class">.pagination</span> <span class="selector-class">.next</span>, <span class="selector-class">.pagination</span> <span class="selector-class">.page-number</span> &#123;</span><br><span class="line">    <span class="attribute">margin-bottom</span>: <span class="number">10px</span>;</span><br><span class="line">    <span class="attribute">border</span>: none;</span><br><span class="line"><span class="attribute">color</span>: <span class="built_in">rgb</span>(1, 1, 1);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 页面底部页码，鼠标置于上方，黑底绿字</span><br><span class="line"><span class="selector-class">.page-number</span><span class="selector-pseudo">:hover</span>,<span class="selector-class">.page-number</span><span class="selector-pseudo">:active</span>&#123;</span><br><span class="line"><span class="attribute">color</span>: <span class="built_in">rgb</span>(136, 255, 13);</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">100%</span>;</span><br><span class="line">    //background-color: rgba(255, 0, 100, 0.75); //品红</span><br><span class="line">background-color: rgba(0, 0, 0, 0.75); //black</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.pagination</span> <span class="selector-class">.space</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="built_in">rgb</span>(0, 0, 0);</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.pagination</span> &#123;</span><br><span class="line">    <span class="attribute">border</span>: none;</span><br><span class="line">    <span class="attribute">margin</span>: <span class="number">0px</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 已运行时间</span><br><span class="line"><span class="selector-id">#days</span> &#123;</span><br><span class="line">    <span class="attribute">display</span>: block;</span><br><span class="line">    <span class="attribute">color</span>: <span class="built_in">rgba</span>(0, 0, 0,0.75);</span><br><span class="line">    <span class="attribute">font-size</span>: <span class="number">13px</span>;</span><br><span class="line">    <span class="attribute">margin-top</span>: <span class="number">15px</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 自定义页脚跳动的心样式</span><br><span class="line">@<span class="keyword">keyframes</span> heartAnimate &#123;</span><br><span class="line">    0%,100%&#123;<span class="attribute">transform</span>:<span class="built_in">scale</span>(1);&#125;</span><br><span class="line">    10%,30%&#123;<span class="attribute">transform</span>:<span class="built_in">scale</span>(0.9);&#125;</span><br><span class="line">    20%,40%,60%,80%&#123;<span class="attribute">transform</span>:<span class="built_in">scale</span>(1.1);&#125;</span><br><span class="line">    50%,70%&#123;<span class="attribute">transform</span>:<span class="built_in">scale</span>(1.1);&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-id">#heart</span> &#123;</span><br><span class="line">    <span class="attribute">animation</span>: heartAnimate <span class="number">1.0s</span> ease-in-out infinite;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.with-love</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="built_in">rgb</span>(236, 25, 27);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 自定义的文章置顶样式</span><br><span class="line"><span class="selector-class">.post-sticky-flag</span> &#123;</span><br><span class="line">    <span class="attribute">font-size</span>: inherit;</span><br><span class="line">    <span class="attribute">float</span>: right;</span><br><span class="line">    <span class="attribute">color</span>: <span class="built_in">rgb</span>(0, 0, 0);</span><br><span class="line">    <span class="attribute">cursor</span>: help;</span><br><span class="line">    <span class="attribute">transition-duration</span>: <span class="number">0.2s</span>;</span><br><span class="line">    <span class="attribute">transition-timing-function</span>: ease-in-out;</span><br><span class="line">    <span class="attribute">transition-delay</span>: <span class="number">0s</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.post-sticky-flag</span><span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#07b39b</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 右下角返回顶部按钮样式</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.back-to-top</span><span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="built_in">rgb</span>(136, 255, 13);</span><br><span class="line">background-color: rgba(0, 0, 0, 0.75); //black</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 下载样式</span><br><span class="line"><span class="selector-tag">a</span><span class="selector-id">#download</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: inline-block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">0</span> <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">color</span>: <span class="number">#000</span>;</span><br><span class="line"><span class="attribute">background</span>: transparent;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">2px</span> solid <span class="number">#000</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">2px</span>;</span><br><span class="line"><span class="attribute">transition</span>: all .<span class="number">5s</span> ease;</span><br><span class="line"><span class="attribute">font-weight</span>: bold;</span><br><span class="line">&amp;:hover &#123;</span><br><span class="line"><span class="selector-tag">background</span>: <span class="selector-id">#000</span>;</span><br><span class="line"><span class="selector-tag">color</span>: <span class="selector-id">#fff</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 颜色块-黄</span><br><span class="line"><span class="selector-tag">span</span><span class="selector-id">#inline-yellow</span> &#123;</span><br><span class="line"><span class="attribute">display</span>:inline;</span><br><span class="line">//padding:.2em .6em .3em;</span><br><span class="line"><span class="selector-tag">padding</span><span class="selector-pseudo">:.1em</span> <span class="selector-class">.4em</span> <span class="selector-class">.1em</span>;</span><br><span class="line"><span class="selector-tag">font-size</span><span class="selector-pseudo">:90</span>%;</span><br><span class="line"><span class="selector-tag">font-weight</span><span class="selector-pseudo">:bold</span>;</span><br><span class="line"><span class="selector-tag">line-height</span><span class="selector-pseudo">:1</span>;</span><br><span class="line"><span class="selector-tag">color</span>:<span class="selector-id">#fff</span>;</span><br><span class="line"><span class="selector-tag">text-align</span><span class="selector-pseudo">:center</span>;</span><br><span class="line"><span class="selector-tag">white-space</span><span class="selector-pseudo">:nowrap</span>;</span><br><span class="line"><span class="selector-tag">vertical-align</span><span class="selector-pseudo">:baseline</span>;</span><br><span class="line"><span class="selector-tag">border-radius</span><span class="selector-pseudo">:0</span>;</span><br><span class="line"><span class="selector-tag">background-color</span>: <span class="selector-id">#f0ad4e</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 颜色块-绿</span><br><span class="line"><span class="selector-tag">span</span><span class="selector-id">#inline-green</span> &#123;</span><br><span class="line"><span class="attribute">display</span>:inline;</span><br><span class="line">//padding:.2em .6em .3em;</span><br><span class="line"><span class="selector-tag">padding</span><span class="selector-pseudo">:.1em</span> <span class="selector-class">.4em</span> <span class="selector-class">.1em</span>;</span><br><span class="line"><span class="selector-tag">font-size</span><span class="selector-pseudo">:90</span>%;</span><br><span class="line"><span class="selector-tag">font-weight</span><span class="selector-pseudo">:bold</span>;</span><br><span class="line"><span class="selector-tag">line-height</span><span class="selector-pseudo">:1</span>;</span><br><span class="line"><span class="selector-tag">color</span>:<span class="selector-id">#fff</span>;</span><br><span class="line"><span class="selector-tag">text-align</span><span class="selector-pseudo">:center</span>;</span><br><span class="line"><span class="selector-tag">white-space</span><span class="selector-pseudo">:nowrap</span>;</span><br><span class="line"><span class="selector-tag">vertical-align</span><span class="selector-pseudo">:baseline</span>;</span><br><span class="line"><span class="selector-tag">border-radius</span><span class="selector-pseudo">:0</span>;</span><br><span class="line"><span class="selector-tag">background-color</span>: <span class="selector-id">#5cb85c</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 颜色块-蓝</span><br><span class="line"><span class="selector-tag">span</span><span class="selector-id">#inline-blue</span> &#123;</span><br><span class="line"><span class="attribute">display</span>:inline;</span><br><span class="line">//padding:.2em .6em .3em;</span><br><span class="line"><span class="selector-tag">padding</span><span class="selector-pseudo">:.1em</span> <span class="selector-class">.4em</span> <span class="selector-class">.1em</span>;</span><br><span class="line"><span class="selector-tag">font-size</span><span class="selector-pseudo">:90</span>%;</span><br><span class="line"><span class="selector-tag">font-weight</span><span class="selector-pseudo">:bold</span>;</span><br><span class="line"><span class="selector-tag">line-height</span><span class="selector-pseudo">:1</span>;</span><br><span class="line"><span class="selector-tag">color</span>:<span class="selector-id">#fff</span>;</span><br><span class="line"><span class="selector-tag">text-align</span><span class="selector-pseudo">:center</span>;</span><br><span class="line"><span class="selector-tag">white-space</span><span class="selector-pseudo">:nowrap</span>;</span><br><span class="line"><span class="selector-tag">vertical-align</span><span class="selector-pseudo">:baseline</span>;</span><br><span class="line"><span class="selector-tag">border-radius</span><span class="selector-pseudo">:0</span>;</span><br><span class="line"><span class="selector-tag">background-color</span>: <span class="selector-id">#2780e3</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 颜色块-紫</span><br><span class="line"><span class="selector-tag">span</span><span class="selector-id">#inline-purple</span> &#123;</span><br><span class="line"><span class="attribute">display</span>:inline;</span><br><span class="line">//padding:.1em .2em .1em;</span><br><span class="line"><span class="selector-tag">padding</span><span class="selector-pseudo">:.1em</span> <span class="selector-class">.4em</span> <span class="selector-class">.1em</span>;</span><br><span class="line"><span class="selector-tag">font-size</span><span class="selector-pseudo">:90</span>%;</span><br><span class="line"><span class="selector-tag">font-weight</span><span class="selector-pseudo">:bold</span>;</span><br><span class="line"><span class="selector-tag">line-height</span><span class="selector-pseudo">:1</span>;</span><br><span class="line"><span class="selector-tag">color</span>:<span class="selector-id">#fff</span>;</span><br><span class="line"><span class="selector-tag">text-align</span><span class="selector-pseudo">:center</span>;</span><br><span class="line"><span class="selector-tag">white-space</span><span class="selector-pseudo">:nowrap</span>;</span><br><span class="line"><span class="selector-tag">vertical-align</span><span class="selector-pseudo">:baseline</span>;</span><br><span class="line"><span class="selector-tag">border-radius</span><span class="selector-pseudo">:0</span>;</span><br><span class="line"><span class="selector-tag">background-color</span>: <span class="selector-id">#9954bb</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 颜色块-红</span><br><span class="line"><span class="selector-tag">span</span><span class="selector-id">#inline-red</span> &#123;</span><br><span class="line"><span class="attribute">display</span>:inline;</span><br><span class="line">//padding:.2em .6em .3em;</span><br><span class="line"><span class="selector-tag">padding</span><span class="selector-pseudo">:.1em</span> <span class="selector-class">.4em</span> <span class="selector-class">.1em</span>;</span><br><span class="line"><span class="selector-tag">font-size</span><span class="selector-pseudo">:90</span>%;</span><br><span class="line"><span class="selector-tag">font-weight</span><span class="selector-pseudo">:bold</span>;</span><br><span class="line"><span class="selector-tag">line-height</span><span class="selector-pseudo">:1</span>;</span><br><span class="line"><span class="selector-tag">color</span>:<span class="selector-id">#fff</span>;</span><br><span class="line"><span class="selector-tag">text-align</span><span class="selector-pseudo">:center</span>;</span><br><span class="line"><span class="selector-tag">white-space</span><span class="selector-pseudo">:nowrap</span>;</span><br><span class="line"><span class="selector-tag">vertical-align</span><span class="selector-pseudo">:baseline</span>;</span><br><span class="line"><span class="selector-tag">border-radius</span><span class="selector-pseudo">:0</span>;</span><br><span class="line"><span class="selector-tag">background-color</span>: <span class="selector-id">#df3e3e</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 左侧边框红色块级</span><br><span class="line"><span class="selector-tag">p</span><span class="selector-id">#div-border-left-red</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span><br><span class="line"><span class="attribute">border-left-width</span>: <span class="number">5px</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line"><span class="attribute">border-left-color</span>: <span class="number">#df3e3e</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 左侧边框黄色块级</span><br><span class="line"><span class="selector-tag">p</span><span class="selector-id">#div-border-left-yellow</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span><br><span class="line"><span class="attribute">border-left-width</span>: <span class="number">5px</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line"><span class="attribute">border-left-color</span>: <span class="number">#f0ad4e</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 左侧边框绿色块级</span><br><span class="line"><span class="selector-tag">p</span><span class="selector-id">#div-border-left-green</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span><br><span class="line"><span class="attribute">border-left-width</span>: <span class="number">5px</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line"><span class="attribute">border-left-color</span>: <span class="number">#5cb85c</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 左侧边框蓝色块级</span><br><span class="line"><span class="selector-tag">p</span><span class="selector-id">#div-border-left-blue</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span><br><span class="line"><span class="attribute">border-left-width</span>: <span class="number">5px</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line"><span class="attribute">border-left-color</span>: <span class="number">#2780e3</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 左侧边框紫色块级</span><br><span class="line"><span class="selector-tag">p</span><span class="selector-id">#div-border-left-purple</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span><br><span class="line"><span class="attribute">border-left-width</span>: <span class="number">5px</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line"><span class="attribute">border-left-color</span>: <span class="number">#9954bb</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 右侧边框红色块级</span><br><span class="line"><span class="selector-tag">p</span><span class="selector-id">#div-border-right-red</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span><br><span class="line"><span class="attribute">border-right-width</span>: <span class="number">5px</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line"><span class="attribute">border-right-color</span>: <span class="number">#df3e3e</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 右侧边框黄色块级</span><br><span class="line"><span class="selector-tag">p</span><span class="selector-id">#div-border-right-yellow</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span><br><span class="line"><span class="attribute">border-right-width</span>: <span class="number">5px</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line"><span class="attribute">border-right-color</span>: <span class="number">#f0ad4e</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 右侧边框绿色块级</span><br><span class="line"><span class="selector-tag">p</span><span class="selector-id">#div-border-right-green</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span><br><span class="line"><span class="attribute">border-right-width</span>: <span class="number">5px</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line"><span class="attribute">border-right-color</span>: <span class="number">#5cb85c</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 右侧边框蓝色块级</span><br><span class="line"><span class="selector-tag">p</span><span class="selector-id">#div-border-right-blue</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span><br><span class="line"><span class="attribute">border-right-width</span>: <span class="number">5px</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line"><span class="attribute">border-right-color</span>: <span class="number">#2780e3</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 右侧边框紫色块级</span><br><span class="line"><span class="selector-tag">p</span><span class="selector-id">#div-border-right-purple</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span><br><span class="line"><span class="attribute">border-right-width</span>: <span class="number">5px</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line"><span class="attribute">border-right-color</span>: <span class="number">#9954bb</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 上侧边框红色</span><br><span class="line"><span class="selector-tag">p</span><span class="selector-id">#div-border-top-red</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span><br><span class="line"><span class="attribute">border-top-width</span>: <span class="number">5px</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line"><span class="attribute">border-top-color</span>: <span class="number">#df3e3e</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 上侧边框黄色</span><br><span class="line"><span class="selector-tag">p</span><span class="selector-id">#div-border-top-yellow</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span><br><span class="line"><span class="attribute">border-top-width</span>: <span class="number">5px</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line"><span class="attribute">border-top-color</span>: <span class="number">#f0ad4e</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 上侧边框绿色</span><br><span class="line"><span class="selector-tag">p</span><span class="selector-id">#div-border-top-green</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span><br><span class="line"><span class="attribute">border-top-width</span>: <span class="number">5px</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line"><span class="attribute">border-top-color</span>: <span class="number">#5cb85c</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 上侧边框蓝色</span><br><span class="line"><span class="selector-tag">p</span><span class="selector-id">#div-border-top-blue</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span><br><span class="line"><span class="attribute">border-top-width</span>: <span class="number">5px</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line"><span class="attribute">border-top-color</span>: <span class="number">#2780e3</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 上侧边框紫色</span><br><span class="line"><span class="selector-tag">p</span><span class="selector-id">#div-border-top-purple</span> &#123;</span><br><span class="line"><span class="attribute">display</span>: block;</span><br><span class="line"><span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">10px</span> <span class="number">0</span>;</span><br><span class="line"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span><br><span class="line"><span class="attribute">border-top-width</span>: <span class="number">5px</span>;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line"><span class="attribute">border-top-color</span>: <span class="number">#9954bb</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// gitalk config.</span><br><span class="line"><span class="selector-class">.gitalk_title</span> &#123;</span><br><span class="line">    <span class="attribute">display</span>: inline-block;</span><br><span class="line">    <span class="attribute">padding</span>: <span class="number">0</span> <span class="number">15px</span>;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#0a9caf</span>;</span><br><span class="line">    <span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#0a9caf</span>;</span><br><span class="line">    <span class="attribute">border-radius</span>: <span class="number">4px</span>;</span><br><span class="line">    <span class="attribute">cursor</span>: pointer;</span><br><span class="line">    <span class="attribute">font-size</span>: <span class="number">14px</span>;</span><br><span class="line">    // float: left;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.gitalk_title</span><span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#fff</span>;</span><br><span class="line">    <span class="attribute">background</span>: <span class="number">#0a9caf</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.gitalk_container</span> &#123;</span><br><span class="line">    <span class="attribute">margin-bottom</span>: <span class="number">50px</span>;</span><br><span class="line">    <span class="attribute">border-bottom</span>: <span class="number">1px</span> solid <span class="number">#e9e9e9</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">$shadowColor  = #333</span><br><span class="line">$themeColor       = #222</span><br><span class="line">$link-image-size-width   = 180px;</span><br><span class="line">$link-image-size-height  = 230px;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.link-body</span>&#123;</span><br><span class="line">ul&#123;</span><br><span class="line"><span class="selector-tag">display</span>: <span class="selector-tag">flex</span>;</span><br><span class="line"><span class="selector-tag">justify-content</span>: <span class="selector-tag">space-between</span>;</span><br><span class="line"><span class="selector-tag">align-items</span>: <span class="selector-tag">center</span>;</span><br><span class="line"><span class="selector-tag">flex-wrap</span>: <span class="selector-tag">wrap</span>;</span><br><span class="line"><span class="selector-tag">margin</span>: 0;</span><br><span class="line"><span class="selector-tag">padding</span>: 0;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.link</span>&#123;</span><br><span class="line"><span class="attribute">max-width</span>: $link-image-size-width;</span><br><span class="line"><span class="attribute">min-width</span>: $link-image-size-width;</span><br><span class="line"><span class="attribute">max-height</span>: $link-image-size-height;</span><br><span class="line"><span class="attribute">min-height</span>: $link-image-size-height;</span><br><span class="line"></span><br><span class="line"><span class="attribute">position</span>: relative;</span><br><span class="line"><span class="attribute">box-shadow</span>: <span class="number">0</span> <span class="number">0</span> <span class="number">1px</span> $shadowColor;</span><br><span class="line"><span class="attribute">magin</span>: <span class="number">6px</span>;</span><br><span class="line"><span class="attribute">width</span>: <span class="number">20%</span>;</span><br><span class="line"><span class="attribute">list-style</span>: none<span class="meta">!important</span>;</span><br><span class="line"><span class="attribute">overflow</span>: hidden;</span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">6px</span>;</span><br><span class="line">img&#123;</span><br><span class="line"><span class="selector-tag">object-fit</span>: <span class="selector-tag">cover</span>;</span><br><span class="line"><span class="selector-tag">transition</span>: <span class="selector-tag">transform</span> <span class="selector-class">.6s</span> <span class="selector-tag">ease-out</span>;</span><br><span class="line"><span class="selector-tag">vertical-align</span>: <span class="selector-tag">middle</span>;</span><br><span class="line">border-bottom: 4px solid #eee;//#e5642b;</span><br><span class="line"><span class="selector-tag">transition</span>: 0<span class="selector-class">.4s</span> ;</span><br><span class="line"><span class="selector-tag">width</span>: 100%;</span><br><span class="line"><span class="selector-tag">border-radius</span>: 90<span class="selector-tag">px</span> 90<span class="selector-tag">px</span> 90<span class="selector-tag">px</span> 90<span class="selector-tag">px</span>;</span><br><span class="line"><span class="selector-tag">display</span>: <span class="selector-tag">inline-block</span>;</span><br><span class="line"><span class="selector-tag">float</span>: <span class="selector-tag">none</span>;</span><br><span class="line"><span class="selector-tag">vertical-align</span>: <span class="selector-tag">middle</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.link-name</span>&#123;</span><br><span class="line"><span class="attribute">position</span>: absolute;</span><br><span class="line"><span class="attribute">bottom</span>: <span class="number">53px</span>;</span><br><span class="line"><span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line"><span class="attribute">color</span>: <span class="number">#666</span>;</span><br><span class="line"><span class="attribute">text-align</span>: center;</span><br><span class="line">//text-shadow: 0 0 1px rgba(0,0,0,.4);</span><br><span class="line"><span class="selector-tag">background</span>: <span class="selector-tag">rgba</span>(255,255,255,<span class="selector-class">.8</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.link-name-below</span>&#123;</span><br><span class="line"><span class="attribute">position</span>: absolute;</span><br><span class="line"><span class="attribute">bottom</span>: <span class="number">0</span>;</span><br><span class="line"><span class="attribute">font-size</span>: <span class="number">13px</span>;</span><br><span class="line"><span class="attribute">font-weight</span>: <span class="number">300</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">0</span> <span class="number">0</span> <span class="number">15px</span>;</span><br><span class="line"><span class="attribute">line-height</span>: <span class="number">13px</span>;</span><br><span class="line"><span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line"><span class="attribute">color</span>: <span class="number">#666</span>;</span><br><span class="line"><span class="attribute">text-align</span>: center;</span><br><span class="line">//text-shadow: 0 0 1px rgba(0,0,0,.4);</span><br><span class="line">//background: rgba(255,255,255,.7);</span><br><span class="line">&#125;</span><br><span class="line">&amp;<span class="selector-pseudo">:hover</span>&#123;</span><br><span class="line">img&#123;</span><br><span class="line"><span class="selector-tag">overflow</span>: <span class="selector-tag">hidden</span>;</span><br><span class="line">//transition: 0.4s;</span><br><span class="line"><span class="selector-tag">border-radius</span>: 0 0 0 0;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.link-name</span>&#123;</span><br><span class="line"><span class="attribute">color</span>: $themeColor;</span><br><span class="line"><span class="attribute">font-weight</span>: bold;</span><br><span class="line"><span class="attribute">text-align</span>: center;</span><br><span class="line">//text-shadow: 0 0 1px $themeColor;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">// adjust the position of gitter</span><br><span class="line"><span class="selector-class">.gitter-open-chat-button</span> &#123;</span><br><span class="line">      <span class="attribute">right</span>: <span class="number">20px</span>;</span><br><span class="line">  <span class="attribute">padding</span>: <span class="number">10px</span>;</span><br><span class="line">  <span class="attribute">background-color</span>: <span class="built_in">rgba</span>(45,45,45,0.80);</span><br><span class="line">  <span class="attribute">color</span>: <span class="built_in">rgba</span>(255,255,255,0.75)</span><br><span class="line">&#125;</span><br><span class="line">@<span class="keyword">media</span> (max-width: <span class="number">600px</span>) &#123;</span><br><span class="line">    <span class="selector-class">.gitter-open-chat-button</span>,</span><br><span class="line">    <span class="selector-class">.gitter-chat-embed</span> &#123;</span><br><span class="line">        <span class="attribute">display</span>: none;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">@<span class="keyword">media</span> screen and (min-width:<span class="number">1200px</span>) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="selector-tag">body</span> &#123;</span><br><span class="line"><span class="attribute">background</span>:<span class="built_in">url</span>(/images/50.jpg);</span><br><span class="line">    <span class="attribute">background-repeat</span>: no-repeat;</span><br><span class="line">    <span class="attribute">background-attachment</span>:fixed;</span><br><span class="line">    <span class="attribute">background-position</span>:<span class="number">50%</span> <span class="number">50%</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="selector-id">#footer</span> &#123;</span><br><span class="line">        <span class="attribute">color</span>:<span class="number">#c8c8c8</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-id">#coding</span><span class="selector-pseudo">:link</span>,<span class="selector-id">#coding</span><span class="selector-pseudo">:visited</span>&#123;</span><br><span class="line"><span class="attribute">color</span>: <span class="built_in">rgb</span>(153,153,153);</span><br><span class="line">//font-weight: normal;</span><br><span class="line"><span class="selector-tag">text-decoration</span>: <span class="selector-tag">none</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-id">#coding</span><span class="selector-pseudo">:hover</span>,<span class="selector-id">#coding</span><span class="selector-pseudo">:active</span>&#123;</span><br><span class="line"><span class="attribute">color</span>: <span class="built_in">rgb</span>(153,153,153);</span><br><span class="line"><span class="attribute">text-decoration</span>: none;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html" target="_blank" rel="noopener">主题优化</a></li><li><a href="http://yearito.cn/posts/hexo-advanced-settings.html" target="_blank" rel="noopener">Hexo 搭建个人博客系列：进阶设置篇</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 建站 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> github </tag>
            
            <tag> coding </tag>
            
            <tag> hexo </tag>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AR形势与应用</title>
      <link href="/posts/AR-applications/"/>
      <url>/posts/AR-applications/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-2/AR-applications/AR.jpg"></p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p id="div-border-left-red">当前，微软、谷歌、苹果、Facebook 等 IT 巨头都在布局虚拟现实Virtual Reality简称 VR）虚拟现实也许是下一个颠覆人类生活的新技术之一。增强现实Augmented Reality简称 AR）是虚拟现实技术的延伸它可以用来模拟对象让学习者在现实环境背景中看到虚拟生成的模型对象 而且这一模型可以快速生成、操纵和旋转。</p><a id="more"></a><h2 id="1．增强现实公司分类"><a href="#1．增强现实公司分类" class="headerlink" title="1．增强现实公司分类"></a>1．增强现实公司分类</h2><p>Augmented reality增强现实创业的公司大致可以分成如下几种公司类型[1]。（以下是翻译的国外网站的内容）</p><ul><li>AR平台公司（AR platform companies）。<br>这些公司为开发者提供底层的开发工具，以便于开发者能够创造更多更加高级的AR解决方案。这样的公司有：Qualcomm Vuforia, METAIO’s SDK, TotalImmersion。</li><li>AR产品和游戏公司。<br>这些公司主营自己独家AR零售产品例如：书籍、游戏。包括如下公司：Sphero, POPAR, Sony, Microsoft 以及 Nintendo。</li><li>自助DIY AR公司以及通用AR查看器。<br>这些公司专为快速简单的AR体验或活动而设计，提供内容管理工具和基本AR效果菜单。借助自助AR工具，精通技术的个人可以创建简单的体验，例如发布单个视频或简单的动画。AR自助服务公司非常适合发布商，教育工作者，学生和其他想要测试或创建简单的增强现实体验的用户，而无需投资于完全定制的品牌应用体验。一些DIY公司还提供AR查看器，定制服务和白标签选项。这个领域的公司包括Layar，Aurasma，DAQRI和Zappar。</li><li>定制品牌应用开发公司。<br>这些公司直接与品牌营销人员和机构合作，主要为的广告活动、贸易展览和现场活动构建定制的增强现实解决方案。自定义品牌应用程序允许营销人员结合独一无二的定制增强现实体验与个性化服务和项目管理。自定义功能通常包括品牌规格，导航，用户界面，动画，复杂或大规模的AR效果等。服务可以包括3D建模，与其他软件服务或电子商务平台的集成，游戏开发，基于位置的安装，通知，复杂动画，微位置或其他高级AR效果。这个领域的公司包括Appshaker，GravityJack和Marxent。</li><li>行业特定的垂直AR解决方案（Industry-specific vertical AR solutions）。<br>最新出现的AR公司类别是那些提供AR解决方案的专业服务公司，专为服务于专业领域。如奢侈品零售，医疗服务，工业应用，制药公司和化妆品公司。这个领域的公司包括用于广告的Blippar，用于豪华珠宝零售的Holition，用于家具布置的Adornably以及用于消费零售，工业和企业销售工具的Marxent的VisualCommerce®。</li></ul><p>做个不恰当的比喻，我觉得AR是VR的一个延伸，只是把VR的场景换成了现实场景，眼镜换成了透明的。</p><h2 id="2．可行性分析"><a href="#2．可行性分析" class="headerlink" title="2．可行性分析"></a>2．可行性分析</h2><p>一个良好的AR体验，大致可以分成一下几个方面：</p><ul><li>1．    3D眼镜；这应该是VR或者AR最为重要的一部分；</li><li>2．    手柄；可以代替人手去操纵，在一定程度上增加了使用的灵活性。（但我觉得未来手柄一定会被淘汰，因为这只是人手不能被充分利用的代替手段）。</li><li>3．    3D显示屏幕，它可以跟踪用户的头的转动和手的动作，实时调整所看到的3D图像，并允许用户操控一些虚拟物体，就好比他们真正存在。</li><li>4．    待补充</li></ul><h3 id="技术难点"><a href="#技术难点" class="headerlink" title="技术难点"></a>技术难点</h3><p>VR得益于三维游戏的发展，而AR收益于影视领域的跟踪技术（video tracking）的发展。从技术门槛的角度来说，VR、AR和移动端重合的技术有：显示器、运动传感器、处理器、储存、记忆、无线连接等。在硬件上，这些都不是技术难点。<br>VR、AR的难点都在感知和显示，感知是一种映射，VR 映射的是一个lighthouse的空间或者PS camera mapping的一个交叉；在显示上，VR如何精准地匹配用户的头部产生相应的画面，AR则在这基础上算出光照、遮挡等情况并让图像通透不干扰现实中的视线。<br>而VR硬件的难点在于光学的镜片技术和位置追踪技术（SLAM），因为以前的移动端不涉及这些技术。AR的软件难点在于：1、定位相机；2、恢复场景的三维结构。通常情况下，这一技术被称作SLAM（Simultaneous Localization And Mapping）。当然还有一些其他的技术诸如：图像追踪、云端视觉搜索、人脸和表情追踪等。</p><p>目前国内外已经有多家技术公司提供了软件开发方面的AR解决方案和工具，使得全球众多开发者参与到AR应用开发中来。开发者不需要自己搭建系统架构，也不用理解底层SDK复杂的实现方式，只需要将AR模块嵌入到已有的业务逻辑中，就可以通过现成的开源代码或者平台工具，设计并开发属于自己的AR软件产品。就这个层面来讲技术是可以实现的，但就某一个特殊领域的实现方式可能有所差别。</p><h3 id="技术支持"><a href="#技术支持" class="headerlink" title="技术支持"></a>技术支持</h3><p>Metaio 是由德国大众的一个项目衍生出来的一家虚拟现实初创公司，现已被苹果公司收购。 专门从事增强现实和机器视觉解决方案，产品主要包括Metaio SDK 和 Metaio Creator。 Metaio SDK 支持移动设备的AR应用开发，它在内部提供增强现实显示组件ARView，该组件将摄像机层、3D 空间计算以及POI信息的叠加等功能全部封装在一起，用户在使用增强现实功能时，只需要关注用户操作的监听器即可，摄像机层、3D 空间计算、图形识别以及空间信息叠加等逻辑，完全由ARView组件自己处理 。Metaio Creator相对Metaio SDK 来说，使用门槛更低，用户无需掌握移动开发技术，就可以通过 Metaio Creator 用户图形接口中简单的点击、拖拽、拉伸等方式，控制软件中组件的功能，以构建出自己的增强现实结果。（但是被苹果收购了，目前不提供服务）</p><p><img alt="图一Wikitude 官网界面" data-src="https://vincentqin.gitee.io/blogresource-2/AR-applications/160405_WT_HomeSlider_Image_3D-Tracking.png"></p><p>Wikitude 是由美国 Mobilizy 公司于 2008 年秋推出的一款移动增强现实开发平台， 支持 Android、 iOS、Black Berry 以及 Windows Phone 多个手机智能操作系统Wikitude SDK 是一款优秀的增强现实开发工具包， 它能够帮助开发人员减小增强现实应用程序开发的复杂性。 目前，Wikitude SDK 支持载入真实的物理环境向 AR 环境中添加虚拟物体、支持用户与虚拟物体的交互、响应用户的位置变化、AR 环境中信息提示、从本地或网络加载资源等功能。</p><p><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-2/AR-applications/wakingapp.jpg"></p><p>ENTiTi Creator是由以色列一家创业公司 Waking App 开发的一款 AR 作品制作工具，易学易用是它的最大特色。用户可以使用ENTiTi平台上传图片和视频以及相应的动作指令， 并通过简单的逻辑串联，就可以轻松创建出包含3D图像、动画或者游戏的AR/VR 内容。该平台不需要任何编程、完全依靠鼠标拖放就能完成整个创建过程。EN-TiTi是基于云计算的平台，可以在线 3D 视角查看内容，并自动适配各种终端，比如，手机或平台电脑、三星 Gear VR 盒子、Vuzix 智能眼镜等。开发者通过它所发布出来的AR内容，只需要通过一个叫作 EN-TiTi View 软件的入口，就可以轻松访问。 这意味着全球所有开发者所开发出的成千上万的 AR 内容，只需要一个软件即可全部浏览。</p><center><img width="1000px" data-src="https://vincentqin.gitee.io/blogresource-2/AR-applications/realmax.jpg"></center><p>Realmax公司是一个国际化AR生态级企业，在上海、香港、纽约、慕尼黑都设立了分公司，并建立了5个全球实验室，完成了硬件量产、软件算法、应用开发和内容制作的AR技术储备，AR操作系统“Realcast”也有可观的用户量，在工业、幼教、电商、旅游等领域积累了大量客户，是AR领域唯一的一家完成“平台+内容+终端+应用”生态链布局的企业。</p><p>根据以上分析，如果想完成某一个特定的AR或者VR应用，可以使用上述公司提供的SDK，在一定程度上会加快开发速度。</p><hr><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1．    增强现实公司类型：<a href="http://www.marxentlabs.com/augmented-reality-company-primer-5-types-augmented-reality-companies/" target="_blank" rel="noopener">http://www.marxentlabs.com/augmented-reality-company-primer-5-types-augmented-reality-companies/</a><br>2．    Metaio公司主页：<a href="http://www.metaio.eu/" target="_blank" rel="noopener">http://www.metaio.eu/</a><br>3．    AR技术举例以及现有公司介绍：<a href="http://www.marxentlabs.com/what-is-virtual-reality-definition-and-examples/。" target="_blank" rel="noopener">http://www.marxentlabs.com/what-is-virtual-reality-definition-and-examples/。</a><br>4．    Layer公司AR开发SDK：<a href="https://www.layar.com/solutions/#sdk" target="_blank" rel="noopener">https://www.layar.com/solutions/#sdk</a><br>5．    Wikitude官网：<a href="http://www.metaio.eu/index.html" target="_blank" rel="noopener">http://www.metaio.eu/index.html</a><br>6．    ENTiTi Creator 官网：<a href="http://www.wakingapp.com/" target="_blank" rel="noopener">http://www.wakingapp.com/</a><br>7．    Realmax公司官网：<a href="http://www.realmax.com/或者http://www.realmax.com.hk/" target="_blank" rel="noopener">http://www.realmax.com/或者http://www.realmax.com.hk/</a></p><!-- more -->]]></content>
      
      
      <categories>
          
          <category> AR </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown 学习</title>
      <link href="/posts/learning-Markdown/"/>
      <url>/posts/learning-Markdown/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img alt data-src="https://vincentqin.gitee.io/blogresource-1/learning-Markdown/guide-to-markdown-writing.png"></p><!--<a href="https://www.vincentqin.tech"><img border="0" src="i-love-markdown.png" /></a>--><p>本文涉及学习Markdown文本标记语言的一些练习笔记。</p><a id="more"></a><h2 id="Note-Tag-测试"><a href="#Note-Tag-测试" class="headerlink" title="Note Tag 测试"></a>Note Tag 测试</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * note.js | global hexo script.</span><br><span class="line"> *</span><br><span class="line"> * ATTENTION! No need to write this tag in 1 line if u don't want see probally bugs.</span><br><span class="line"> *</span><br><span class="line"> * Usage:</span><br><span class="line"> *</span><br><span class="line"> * &#123;% note [class] %&#125;</span><br><span class="line"> * Any content (support inline tags too).</span><br><span class="line"> * &#123;% endnote %&#125;</span><br><span class="line"> *</span><br><span class="line"> * [class] : default | primary | success | info | warning | danger.</span><br><span class="line"> *           May be not defined.</span><br><span class="line"> */</span><br></pre></td></tr></table></figure><div class="note default">            <h3 id="Test-note-default"><a href="#Test-note-default" class="headerlink" title="Test note default"></a>Test note default</h3><p>昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。</p>          </div><div class="note primary">            <h3 id="Test-note-primary"><a href="#Test-note-primary" class="headerlink" title="Test note primary"></a>Test note primary</h3><p>昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。</p>          </div><div class="note success">            <h3 id="Test-note-success"><a href="#Test-note-success" class="headerlink" title="Test note success"></a>Test note success</h3><p>昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。</p>          </div><div class="note info">            <h3 id="Test-note-info"><a href="#Test-note-info" class="headerlink" title="Test note info"></a>Test note info</h3><p>昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。</p>          </div><div class="note warning">            <h3 id="Test-note-warning"><a href="#Test-note-warning" class="headerlink" title="Test note warning"></a>Test note warning</h3><p>昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。</p>          </div><div class="note danger">            <h3 id="Test-note-danger"><a href="#Test-note-danger" class="headerlink" title="Test note danger"></a>Test note danger</h3><p>昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。</p>          </div><h2 id="Button-标签测试"><a href="#Button-标签测试" class="headerlink" title="Button 标签测试"></a>Button 标签测试</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Usage: &#123;% button /path/to/url/, text, icon [class], title %&#125;</span><br><span class="line">Alias: &#123;% btn /path/to/url/, text, icon [class], title %&#125;</span><br></pre></td></tr></table></figure><h3 id="Button内嵌文字"><a href="#Button内嵌文字" class="headerlink" title="Button内嵌文字"></a>Button内嵌文字</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% button #, Text %&#125;&#123;% button #插入不同颜色的字体, 插入不同颜色的字体,heart %&#125;</span><br></pre></td></tr></table></figure><a class="btn" href="#">Text</a><a class="btn" href="#插入不同颜色的字体"><i class="fa fa-heart"></i>插入不同颜色的字体</a><h3 id="Button内嵌logo"><a href="#Button内嵌logo" class="headerlink" title="Button内嵌logo"></a>Button内嵌logo</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"text-center"</span>&gt;</span><span class="tag">&lt;<span class="name">span</span>&gt;</span>&#123;% btn ##插入不同颜色的字体,, header %&#125;&#123;% btn #,, edge %&#125;&#123;% btn #,, times %&#125;&#123;% btn #,, circle-o %&#125;<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">span</span>&gt;</span>&#123;% btn #,, italic %&#125;&#123;% btn #,, scribd %&#125;<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">span</span>&gt;</span>&#123;% btn #,, google %&#125;&#123;% btn #,, chrome %&#125;&#123;% btn #,, opera %&#125;&#123;% btn #,, diamond fa-rotate-270 %&#125;<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><div class="text-center"><span><a class="btn" href="#插入不同颜色的字体"><i class="fa fa-header"></i>插入不同颜色的字体</a><a class="btn" href="#"><i class="fa fa-edge"></i></a><a class="btn" href="#"><i class="fa fa-times"></i></a><a class="btn" href="#"><i class="fa fa-circle-o"></i></a></span><span><a class="btn" href="#"><i class="fa fa-italic"></i></a><a class="btn" href="#"><i class="fa fa-scribd"></i></a></span><span><a class="btn" href="#"><i class="fa fa-google"></i></a><a class="btn" href="#"><i class="fa fa-chrome"></i></a><a class="btn" href="#"><i class="fa fa-opera"></i></a><a class="btn" href="#"><i class="fa fa-diamond fa-rotate-270"></i></a></span></div><h3 id="Button-Margin"><a href="#Button-Margin" class="headerlink" title="Button Margin"></a>Button Margin</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"text-center"</span>&gt;</span>&#123;% btn #, Almost, adn fa-fw fa-lg %&#125; &#123;% btn #, Over, terminal fa-fw fa-lg %&#125;<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><div class="text-center"><a class="btn" href="#"><i class="fa fa-adn fa-fw fa-lg"></i>Almost</a> <a class="btn" href="#"><i class="fa fa-terminal fa-fw fa-lg"></i>Over</a></div><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"text-right"</span>&gt;</span></span><br><span class="line">&#123;% btn #, Test is finished., check fa-fw fa-lg, Button tag test is finished. %&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><div class="text-right"><a class="btn" href="#" title="Button tag test is finished."><i class="fa fa-check fa-fw fa-lg"></i>Test is finished.</a></div><h2 id="Label-Tag测试文中字体颜色"><a href="#Label-Tag测试文中字体颜色" class="headerlink" title="Label Tag测试文中字体颜色"></a>Label Tag测试文中字体颜色</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">From &#123;% label @fairest creatures %&#125; we desire increase,</span><br><span class="line">That thereby &#123;% label primary@beauty's rose %&#125; might never die,</span><br><span class="line">But as the &#123;% label success@riper %&#125; should by time decease,</span><br><span class="line">His tender heir might &#123;% label info@bear his memory %&#125;:</span><br><span class="line">But thou contracted to thine own bright eyes,</span><br><span class="line">Feed'st thy light's flame with *&#123;% label warning @self-substantial fuel%&#125;*,</span><br><span class="line">Making a famine where ~~&#123;% label default @abundance lies %&#125;~~,</span><br><span class="line">Thy self thy foe, to thy <span class="tag">&lt;<span class="name">mark</span>&gt;</span>sweet self too cruel<span class="tag">&lt;/<span class="name">mark</span>&gt;</span>:</span><br><span class="line">Thou that art now the world's fresh ornament,</span><br><span class="line">And only herald to the gaudy spring,</span><br><span class="line">Within thine own bud buriest thy content,</span><br><span class="line">And &#123;% label danger@tender churl mak'st waste in niggarding %&#125;:</span><br><span class="line">Pity the world, or else this glutton be,</span><br><span class="line">&#123;% label warning@To eat the world's due, by the grave and thee %&#125;.</span><br></pre></td></tr></table></figure><p>From <span class="label default">fairest creatures</span> we desire increase,<br>That thereby <span class="label primary">beauty's rose</span> might never die,<br>But as the <span class="label success">riper</span> should by time decease,<br>His tender heir might <span class="label info">bear his memory</span>:<br>But thou contracted to thine own bright eyes,<br>Feed’st thy light’s flame with <em><span class="label warning">self-substantial fuel</span></em>,<br>Making a famine where <del><span class="label default">abundance lies</span></del>,<br>Thy self thy foe, to thy <mark>sweet self too cruel</mark>:<br>Thou that art now the world’s fresh ornament,<br>And only herald to the gaudy spring,<br>Within thine own bud buriest thy content,<br>And <span class="label danger">tender churl mak'st waste in niggarding</span>:<br>Pity the world, or else this glutton be,<br><span class="label warning">To eat the world's due, by the grave and thee</span>.</p><h2 id="表格Tag测试"><a href="#表格Tag测试" class="headerlink" title="表格Tag测试"></a>表格Tag测试</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tabs First unique name %&#125;</span><br><span class="line"><span class="comment">&lt;!-- tab --&gt;</span></span><br><span class="line">**This is Tab 1.**</span><br><span class="line"><span class="comment">&lt;!-- endtab --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- tab --&gt;</span></span><br><span class="line">**This is Tab 2.**</span><br><span class="line"><span class="comment">&lt;!-- endtab --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- tab --&gt;</span></span><br><span class="line">**This is Tab 3.**</span><br><span class="line"><span class="comment">&lt;!-- endtab --&gt;</span></span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><div class="tabs" id="first-unique-name"><ul class="nav-tabs"><li class="tab active"><a href="#first-unique-name-1">First unique name 1</a></li><li class="tab"><a href="#first-unique-name-2">First unique name 2</a></li><li class="tab"><a href="#first-unique-name-3">First unique name 3</a></li></ul><div class="tab-content"><div class="tab-pane active" id="first-unique-name-1"><p><strong>This is Tab 1.</strong></p></div><div class="tab-pane" id="first-unique-name-2"><p><strong>This is Tab 2.</strong></p></div><div class="tab-pane" id="first-unique-name-3"><p><strong>This is Tab 3.</strong></p></div></div></div><h3 id="插入不同颜色的字体"><a href="#插入不同颜色的字体" class="headerlink" title="插入不同颜色的字体"></a>插入不同颜色的字体</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">table</span>&gt;</span><span class="tag">&lt;<span class="name">tr</span>&gt;</span><span class="tag">&lt;<span class="name">td</span> <span class="attr">bgcolor</span>=<span class="string">LimeGreen</span>&gt;</span><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">white</span> <span class="attr">size</span>=<span class="string">3</span>&gt;</span>我是白色的字体，背景是色的~<span class="tag">&lt;/<span class="name">font</span>&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span><span class="tag">&lt;/<span class="name">tr</span>&gt;</span><span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br></pre></td></tr></table></figure><table><tr><td bgcolor="LimeGreen"><font color="white" size="3">我是白色的字体，背景是色的~</font></td></tr></table><table><tr><td bgcolor="SpringGreen"><font color="white" size="3">我是白色的字体，背景是深灰色的~</font></td></tr></table><table><tr><td bgcolor="LightSeaGreen"><font color="white" size="3">我是白色的字体，背景是浅海绿的~</font></td></tr></table><table><tr><td bgcolor="#0099ff"><font color="white" size="3">我是白色的字体，背景是蓝色的~</font></td></tr></table><table><tr><td bgcolor="Silver"><font color="white" size="3">我是白色的字体，背景是银色的~</font></td></tr></table><table><tr><td bgcolor="DarkGray"><font color="white" size="3">我是白色的字体，背景是淡灰色的~</font></td></tr></table><table><tr><td bgcolor="DimGray"><font color="white" size="3">我是白色的字体，背景是深灰色的~</font></td></tr></table> <h2 id="插入代码"><a href="#插入代码" class="headerlink" title="插入代码"></a>插入代码</h2><p>这里是代码区域</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">% The following is the Matlab Code</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% I want to see the result</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">demo</span><span class="params">()</span></span></span><br><span class="line"></span><br><span class="line">temp=<span class="built_in">zeros</span>(<span class="number">5</span>,<span class="number">6</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="built_in">size</span>(temp,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span>=<span class="number">1</span>:<span class="built_in">size</span>(temp,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        temp(<span class="built_in">i</span>,<span class="built_in">j</span>)=<span class="built_in">rand</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> temp(<span class="built_in">i</span>,<span class="built_in">j</span>)&gt;<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">            temp(<span class="built_in">i</span>,<span class="built_in">j</span>)=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> temp</span><br></pre></td></tr></table></figure><h2 id="插入标题"><a href="#插入标题" class="headerlink" title="插入标题"></a>插入标题</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># 一级标题</span></span><br><span class="line"></span><br><span class="line"><span class="section">## 二级标题</span></span><br><span class="line"></span><br><span class="line"><span class="section">### 三级标题</span></span><br><span class="line"></span><br><span class="line"><span class="section">#### 四级标题</span></span><br><span class="line"></span><br><span class="line"><span class="section">##### 五级标题</span></span><br><span class="line"></span><br><span class="line"><span class="section">###### 六级标题</span></span><br></pre></td></tr></table></figure><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">- </span>文本1</span><br><span class="line"><span class="bullet">- </span>文本2</span><br><span class="line"><span class="bullet">- </span>文本3</span><br><span class="line"><span class="bullet">1. </span>文本一</span><br><span class="line"><span class="bullet">2. </span>文本二</span><br><span class="line"><span class="bullet">3. </span>文本三</span><br></pre></td></tr></table></figure><ul><li>文本1</li><li>文本2</li><li>文本3</li></ul><ol><li>文本一</li><li>文本二</li><li>文本三</li></ol><h2 id="插入图像"><a href="#插入图像" class="headerlink" title="插入图像"></a>插入图像</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![](http://pic1.win4000.com/wallpaper/0/54cae8e69ac8b.jpg)</span><br></pre></td></tr></table></figure><center><img width="100%" data-src="http://pic1.win4000.com/wallpaper/0/54cae8e69ac8b.jpg"></center><p>或者：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">center</span>&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"http://i2.wp.com/posturemag.com/online/wp-content/uploads/2015/07/Kaz7.jpg"</span> <span class="attr">width</span>=<span class="string">"100%"</span> &gt;</span><span class="tag">&lt;/<span class="name">center</span>&gt;</span></span><br></pre></td></tr></table></figure></p><center><img width="100%" data-src="http://i2.wp.com/posturemag.com/online/wp-content/uploads/2015/07/Kaz7.jpg"></center><h2 id="插入链接"><a href="#插入链接" class="headerlink" title="插入链接"></a>插入链接</h2><ul><li>segmentfault上的一个<a href="https://segmentfault.com/markdown" target="_blank" rel="noopener">Markdown学习手册</a></li><li>有道云笔记的Markdown<a href="http://note.youdao.com/iyoudao/?p=2411" target="_blank" rel="noopener">学习指南-基础篇</a></li><li><a href="http://iissnan.com/progit/" target="_blank" rel="noopener">Git学习手册</a></li></ul><h2 id="插入公式"><a href="#插入公式" class="headerlink" title="插入公式"></a>插入公式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$$E=mc^2$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">E=mc^2</script><p>Hexo文档使用Markdown语言对文档进行编辑，Hexo自身对公式可以进行渲染但是效果不佳，我们采用的是mathjax对Markdown中的公式进行渲染。<br>首先<a href="http://2wildkids.com/2016/10/06/%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86Hexo%E5%92%8CMathJax%E7%9A%84%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98/#小结" target="_blank" rel="noopener">修复Hexo与mathjax之间的渲染冲突</a>，然后可以参考mathjax的<a href="http://mlworks.cn/posts/introduction-to-mathjax-and-latex-expression/" target="_blank" rel="noopener">说明文档</a>编辑公式。</p><h2 id="希腊字母对应表"><a href="#希腊字母对应表" class="headerlink" title="希腊字母对应表"></a>希腊字母对应表</h2><div class="table-container"><table><thead><tr><th style="text-align:center">字母名称</th><th style="text-align:center">大写</th><th style="text-align:center">markdown原文</th><th style="text-align:center">小写</th><th style="text-align:center">markdown原文</th></tr></thead><tbody><tr><td style="text-align:center">alpha</td><td style="text-align:center">$A$</td><td style="text-align:center">A</td><td style="text-align:center">$\alpha$</td><td style="text-align:center">\alpha</td></tr><tr><td style="text-align:center">beta</td><td style="text-align:center">$B$</td><td style="text-align:center">B</td><td style="text-align:center">$\beta$</td><td style="text-align:center">\beta</td></tr><tr><td style="text-align:center">gamma</td><td style="text-align:center">$\Gamma$</td><td style="text-align:center">\Gamma</td><td style="text-align:center">$\gamma$</td><td style="text-align:center">\gamma</td></tr><tr><td style="text-align:center">delta</td><td style="text-align:center">$\Delta$</td><td style="text-align:center">\Delta</td><td style="text-align:center">$\delta$</td><td style="text-align:center">\delta</td></tr><tr><td style="text-align:center">epsilon</td><td style="text-align:center">$E$</td><td style="text-align:center">E</td><td style="text-align:center">$\epsilon$</td><td style="text-align:center">\epsilon</td></tr><tr><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">$\varepsilon$</td><td style="text-align:center">\varepsilon</td></tr><tr><td style="text-align:center">zeta</td><td style="text-align:center">$Z$</td><td style="text-align:center">Z</td><td style="text-align:center">$\zeta$</td><td style="text-align:center">\zeta</td></tr><tr><td style="text-align:center">eta</td><td style="text-align:center">$E$</td><td style="text-align:center">E</td><td style="text-align:center">$\eta$</td><td style="text-align:center">\eta</td></tr><tr><td style="text-align:center">theta</td><td style="text-align:center">$\Theta$</td><td style="text-align:center">\Theta</td><td style="text-align:center">$\theta$</td><td style="text-align:center">\theta</td></tr><tr><td style="text-align:center">iota</td><td style="text-align:center">$I$</td><td style="text-align:center">I</td><td style="text-align:center">$\iota$</td><td style="text-align:center">\iota</td></tr><tr><td style="text-align:center">kappa</td><td style="text-align:center">$K$</td><td style="text-align:center">K</td><td style="text-align:center">$\kappa$</td><td style="text-align:center">\kappa</td></tr><tr><td style="text-align:center">lambda</td><td style="text-align:center">$\Lambda$</td><td style="text-align:center">\Lambda</td><td style="text-align:center">$\lambda$</td><td style="text-align:center">\lambda</td></tr><tr><td style="text-align:center">Mu</td><td style="text-align:center">$M$</td><td style="text-align:center">M</td><td style="text-align:center">$\mu$</td><td style="text-align:center">\mu</td></tr><tr><td style="text-align:center">nu</td><td style="text-align:center">$N$</td><td style="text-align:center">N</td><td style="text-align:center">$\nu$</td><td style="text-align:center">\nu</td></tr><tr><td style="text-align:center">xi</td><td style="text-align:center">$\Xi$</td><td style="text-align:center">\Xi</td><td style="text-align:center">$\xi$</td><td style="text-align:center">\xi</td></tr><tr><td style="text-align:center">omicron</td><td style="text-align:center">$O$</td><td style="text-align:center">O</td><td style="text-align:center">$\omicron$</td><td style="text-align:center">\omicron</td></tr><tr><td style="text-align:center">pi</td><td style="text-align:center">$\Pi$</td><td style="text-align:center">\Pi</td><td style="text-align:center">$\pi$</td><td style="text-align:center">\pi</td></tr><tr><td style="text-align:center">rho</td><td style="text-align:center">$P$</td><td style="text-align:center">P</td><td style="text-align:center">$\rho$</td><td style="text-align:center">\rho</td></tr><tr><td style="text-align:center">sigma</td><td style="text-align:center">$\Sigma$</td><td style="text-align:center">\Sigma</td><td style="text-align:center">$\sigma$</td><td style="text-align:center">\sigma</td></tr><tr><td style="text-align:center">tau</td><td style="text-align:center">$T$</td><td style="text-align:center">T</td><td style="text-align:center">$\tau$</td><td style="text-align:center">\tau</td></tr><tr><td style="text-align:center">upsilon</td><td style="text-align:center">$\Upsilon$</td><td style="text-align:center">\Upsilon</td><td style="text-align:center">$\upsilon$</td><td style="text-align:center">\upsilon</td></tr><tr><td style="text-align:center">phi</td><td style="text-align:center">$\Phi$</td><td style="text-align:center">\Phi</td><td style="text-align:center">$\phi$</td><td style="text-align:center">\phi</td></tr><tr><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">$\varphi$</td><td style="text-align:center">\varphi</td></tr><tr><td style="text-align:center">chi</td><td style="text-align:center">$X$</td><td style="text-align:center">X</td><td style="text-align:center">$\chi$</td><td style="text-align:center">\chi</td></tr><tr><td style="text-align:center">psi</td><td style="text-align:center">$\Psi$</td><td style="text-align:center">\Psi</td><td style="text-align:center">$\psi$</td><td style="text-align:center">\psi</td></tr></tbody></table></div><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://almostover.ru/2016-01/hexo-theme-next-test/#" target="_blank" rel="noopener">Hexo Theme Next Test</a></li><li><a href="http://blog.csdn.net/testcs_dn/article/details/45719357/" target="_blank" rel="noopener">Color map</a></li><li><a href="http://www.mohu.org/info/lshort-cn.pdf" target="_blank" rel="noopener">一个关于Latex不短的介绍</a></li><li><a href="http://www.mohu.org/info/symbols/symbols.htm" target="_blank" rel="noopener">Latex常用命令摘录</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 建站 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> 资料 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>复试那些事儿</title>
      <link href="/posts/thu-postgraduate-exam/"/>
      <url>/posts/thu-postgraduate-exam/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文涉及2015年清华自动化系复试相关题目，包括：复试笔试和复试面试。转载需注明<a href="https://github.com/Vincentqyw" target="_blank" rel="noopener">来源</a>，<a href="https://gitee.com/vincentqin/BlogResource-1/raw/master/thu-postgraduate-exam/exam-questions.pdf" target="_blank" rel="noopener">pdf版</a>见此。</p><a id="more"></a><h1 id="Part1-复试笔试"><a href="#Part1-复试笔试" class="headerlink" title="Part1:复试笔试"></a>Part1:复试笔试</h1><p>复试笔试科目根据你选的方向有所不同，如下：</p><div class="table-container"><table><thead><tr><th>专业</th><th>考试科目</th></tr></thead><tbody><tr><td>控制理论与控制工程</td><td>控制理论（经典控制及现代控制）</td></tr><tr><td>控制工程</td><td>控制理论、电子技术、运筹学、信号与系统四门科目中选一门</td></tr><tr><td>检测技术与自动化装置</td><td>电子技术（模电，数电）</td></tr><tr><td>系统工程</td><td>运筹学或控制理论（经典控制及现代控制）</td></tr><tr><td>模式识别与智能系统</td><td>信号与系统</td></tr><tr><td>导航、制导与控制</td><td>控制理论（经典控制及现代控制）</td></tr><tr><td>企业信息化系统与工程</td><td>电子技术（模电，数电）</td></tr><tr><td>生物信息学</td><td>信号与系统</td></tr></tbody></table></div><p>我选的：控制理论。</p><p>3月11号在清华大学3教3300考试。</p><p>详见：清华官网，自动化系2015年硕士生招生复试和录取办法（考生）</p><p>考试时间：2个小时，10：00到12：00</p><p>具体题型：</p><h2 id="1、判断题（2-10’）"><a href="#1、判断题（2-10’）" class="headerlink" title="1、判断题（2*10’）"></a>1、判断题（2*10’）</h2><p>可以说这部分是每年最难的！大部分是现代控制的知识，包括判断叙述的对错，并且解释原因。<br>我记得考的最多是线性系统控制器的问题，这部分和现代控制的知识结合比较紧密，一般会问：在线性定常或者离散系统完全能控/观的条件下，某某控制器是否存在；还有计算典型二阶系统在阻尼比 大于1的情况下单位阶跃响应的超调量；还有关于动态解耦的问题；开环增益 减小一定会使稳态裕度提高等等。题目很基础，注重细节（覆盖面很广），书上都能够找到，只要认真看过书就应该没有问题。我感觉这部分能够拿15分以上就很厉害了。</p><h2 id="2、简单计算题（5-5’）"><a href="#2、简单计算题（5-5’）" class="headerlink" title="2、简单计算题（5*5’）"></a>2、简单计算题（5*5’）</h2><p>基础题目，很简单。包括：根据系统框图写出状态方程、结构图的化简并且写出传递函数、给定状态方程判断其能控性/能观性等。</p><h2 id="3、计算题（15’-15’-15’-10’）"><a href="#3、计算题（15’-15’-15’-10’）" class="headerlink" title="3、计算题（15’+15’+15’+10’）"></a>3、计算题（15’+15’+15’+10’）</h2><p><strong>第一题</strong>：给定系统状态方程：</p><script type="math/tex; mode=display">\left\{\begin{array}{l}{\dot{x}=A x+B u} \\ {y=C x}\end{array}\right.</script><p>, 系统矩阵$A$是6*6的矩阵，$B$是6*1的，$C$是1*6的矩阵，并给定状态量的初值。要求$u$为单位阶跃输入的情况下，系统的输出$y$ 。</p><p>(注: 若用直接法做的话，根本不行，这个题目老师已经凑好，仅需写出状态方程对应的微分方程就能看出这个题目可以简化成2个状态量的题目。)</p><p><strong>第二题</strong>：状态反馈，很简单。但注意的是题目给出的是：$u = f \cdot x$而不是$u = {\rm{ - }}f \cdot x{\rm{ + }}v$</p><p>(注: 三阶或者二阶的都能用对比系数法做，这相对于通法来说简单不少。但注意状态反馈和状态观测器的区别。简单题，不解释了。)</p><p><strong>第三题</strong>：给定系统的开环传递函数<script type="math/tex">{G_O}(s) = \frac{k}{(s +  \ldots )(s +  \ldots )}</script>，给定要求的剪切频率${\omega _c}$和相位裕量${\gamma _c}$，求$k$值。<br>(注: 根据剪切频率和相位裕量确定剪切频率的范围，然后在这个范围取一个值，根据幅值条件求出k值即可。我很怀疑我做错了，我不确定标准答案是怎么做的……)</p><p><strong>第四题</strong>：有限时间状态最优控制器设计。</p><p>关键要写出$Riccati$方程<script type="math/tex">\dot P{\rm{ + }}PA{\rm{ + }}{A^T}P{\rm{ + }}Q{\rm{ - }}PB{R^{ - 1}}{B^T}P{\rm{ = }}0</script>，因为题目给出的一维的，那么上述方程就变为标量的形式。整理出来是一个关于参数$p$的微分方程，解出$p$，写出最优控制<script type="math/tex">u^*</script>，再写出性能指标<script type="math/tex">J^*</script>即可。</p><p>(注: 注意从$J$中直接写出矩阵$Q$和$R$时要注意$J$的系数，否则这两个矩阵就会写错。如果是标量的形式（如上题），那就相对简单多了。但是计算出 就比较麻烦，所以我只计算出了$p$，呵……)</p><h1 id="Part2-复试面试"><a href="#Part2-复试面试" class="headerlink" title="Part2:复试面试"></a>Part2:复试面试</h1><p>时间：早上8：00开始，每人大概持续20分钟。</p><p>流程：进门开始→自我介绍（2分钟左右）→英文文献朗读并翻译（5分钟左右）→抽题（5分钟左右）→自由提问时间（8分钟左右）→结束</p><p>(注: 其实真正面试的时候就不会那么紧张了，最紧张的时刻是在会议室外等待的时间，简直就是煎熬！面试的顺序是按照初试排名，我被分到了E组（A、B、C、D、E组同时 进行）第三个)</p><p>自我介绍：见《自我介绍文档》<br>英文文献：大概150词，有少量陌生单词。朗读的还不错，但是翻译的就真的醉了。记得当时老师问我什么是“multirobot system”，我说是“混合机器系统”，老师说“多机器人系统”，然后我就笑了……</p><p>抽题题目如下（比较简单）：</p><ol><li><p>线性代数： 求<script type="math/tex">\left[ {\begin{array}{*{20}{c}}0&A\\B&0\end{array}} \right]^{ - 1}</script></p></li><li><p>高等数学：</p><script type="math/tex; mode=display">\mathop {\lim }\limits_{a \to 0} \frac{1}{\pi {a^2}}\int\int_Df(x,y)d{\sigma}=   , 其中D{\rm{ = }}\left\{ {(x,y)|{x^2} + {y^2} \le {a^2}} \right\}</script></li><li><p>自动控制原理<br>线性二阶系统的性能指标及其对系统响应的影响；什么叫欠阻尼系统？画出它的单位阶跃输入响应曲线。<br>(注: 我抽的题目很简单很简单吧。抽题题目包括数学、自动控制原理、电路原理、电力电子技术、电力拖动等。选择3道题目，答对即可。注意，每人有一次放弃的机会！一般选择数学和自控，当然选择自己最擅长的学科最好。数学多为线数和概率论，线数多为向量组线性相关还是无关的问题，还有关于矩阵的秩、行列式的值等计算；概率论多为全概率公式和贝叶斯公式的简单应用：共有3枚硬币，其中1枚5元，2枚1元，问题，共取2次，且第一次不放回，两次取得6元的概率。自动控制原理和电路原理也可以选择，但是题目的难易不敢保证。)</p></li><li><p>自由提问<br>就像是聊天，老师会对你的简历或者其他材料里感兴趣的地方提出问题，我的重点就是毕业设计和数模比赛。其中毕业设计占了很大的篇幅，因为我提前看过，所以对此介绍的很详细。<br>记得面试最后，一个老师问我：“有a和b两个不相等的数，不借用第三个变量的情况下，交换两个变量的数值”。然后我就写在了纸上，但是后来我就改错了（囧）……问完这个问题后面试就结束了。</p></li></ol><p>祝大家复试成功，加油！</p>]]></content>
      
      
      <categories>
          
          <category> 资料 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> 李代数 </tag>
            
            <tag> computer vision </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
